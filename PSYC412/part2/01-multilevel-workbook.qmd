---
title: Week 16. Workbook introduction to multilevel data
subtitle: Written by Rob Davies
order: 7
bibliography: references.bib
csl: psychological-bulletin.csl
---

::: callout-warning
This page is currently under construction: come back soon!
:::

# Week 16 Multilevel data workbook overview {#sec-multilevel-workbook-overview}

Welcome to your overview of the work we will do together in **Week 16**.

We are going to learn about a method or approach that is essential in modern data analysis: **multilevel modeling**. 

We are going to invest the next few weeks in working on this approach. 

We are going to do this because multilevel modeling is a very powerful and very flexible technique, and because you will learn best if you take the time to build up and deepen your understanding step-by-step.

## Targets {#sec-multilevel-workbook-targets}

Our learning objectives include the development of key concepts and skills.

1.  **concepts** -- how data can have multilevel structures and what this requires in models
2.  **skills** -- where skills comprise the capacity to:

- use visualization to examine observations within groups
- run linear models over all data and within each class
- use the `lmer()` function to fit models of multilevel data

We are just getting started this week. 
Our plan will be to build depth and breadth in understanding as we progress over the next few weeks.

## Learning resources {#sec-multilevel-workbook-resources}

You will see, next, the lectures we share to explain the concepts you will learn about, and the practical data analysis skills you will develop. Then you will see information about the practical materials you can use to build and practise your skills.

Every week, you will learn best if you *first* watch the lectures *then* do the practical exercises.

::: callout-tip
### Linked resources

To help your learning, you can read about the ideas and the practical coding required for analyses in the chapters I wrote for this course.

[The chapter: Conceptual introduction to multilevel data](../part2/01-multilevel.qmd#sec-intro-multilevel) 
:::

### Lectures {#sec-multilevel-workbook-lectures}

The lecture materials for this week are presented in three short parts.

Click on a link and your browser should open a tab showing the *Panopto* video for the lecture part.

1. Part 1 (17 minutes) **Multilevel data**: What do we mean when we talk about multilevel structured data and why it matters.

```{=html}
<iframe src="https://lancaster.cloud.panopto.eu/Panopto/Pages/Embed.aspx?id=66e89421-4491-4eb9-b1c8-acd4012a183d&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen allow="autoplay" aria-label="Panopto Embedded Video Player" aria-description="01-multilevel-1-of-3" ></iframe>
```

2. Part 2 (13 minutes): Getting started in learning, working with an example from education, children clustered in classes, analyzing data using linear models, seeing the differences between different classes.

```{=html}
<iframe src="https://lancaster.cloud.panopto.eu/Panopto/Pages/Embed.aspx?id=22361165-636b-424c-9e28-acd4012efd6f&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen allow="autoplay" aria-label="Panopto Embedded Video Player" aria-description="01-multilevel-2-of-3" ></iframe>
```

3. Part 3 (13 minutes): Doing multilevel models in practice, coding models using `lmer()`, a first look at analysis results.

```{=html}
<iframe src="https://lancaster.cloud.panopto.eu/Panopto/Pages/Embed.aspx?id=5942b084-457f-4687-aa3d-acd401332b44&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen allow="autoplay" aria-label="Panopto Embedded Video Player" aria-description="01-multilevel-3-of-3" ></iframe>
```

### Lecture slides {#sec-multilevel-workbook-slides}

::: callout-tip
## Download the lecture slides
The slides presented in the videos can be downloaded here:

- [402-week-17-LME-1.pdf](files/402-week-17-LME-1.pdf): high resolution .pdf, exactly as delivered [4 MB];
- [402-week-17-LME-1_1pp.pdf](files/402-week-17-LME-1_1pp.pdf): low resolution .pdf, printable version, one-slide-per-page [359 KB];
- [402-week-17-LME-1_6pp.pdf](files/402-week-17-LME-1_6pp.pdf): low resolution .pdf, printable version, six-slides-per-page [359 KB].

The high resolution version is the version delivered for the lecture recordings. Because the images are produced to be high resolution, the file size is quite big (4 MB) so, to make the slides easier to download, I produced low resolution versions: 1pp and 6pp. These should be easier to download and print out if that is what you want to do.
:::

### Practical materials: data and R-Studio {#sec-multilevel-workbook-practical}

We will be working with data taken from a study on education outcomes in Brazilian children, reported by @golino2014.

You can read more about these data in [Conceptual introduction to multilevel data](../part2/01-multilevel.qmd#sec-intro-multilevel).

::: callout-important
**Get the data**: get the data file and the .R script you can use to do the exercises that will support your learning.

- You can download the files folder for this chapter by clicking on the link [01-multilevel.zip](files/01-multilevel.zip).
:::

The practical materials folder includes:

- `BAFACALO_DATASET.RData`
-  `402-01-multilevel-workbook.R` the workbook you can use to do the practical exercises.

The data file is in the `.RData` format: `.RData` is R's own file format so the code you use to load and access the data for analysis is a bit simpler than you are used to, as we will see.

::: callout-important
You can access the sign-in page for [R-Studio Server here](https://psy-rstudio.lancaster.ac.uk/auth-sign-in?appUri=%2F)
:::

#### Practical materials guide {#sec-multilevel-workbook-practical-guide}

Here, our learning targets are:

- concepts: multilevel data and multilevel modeling
- skills: visualization -- examine overall and within-class trends
- skills: run linear models over all data -- and within each class
- skills: use the lmer() function to fit models of multilevel data

The aims of the practical work are to:

- Get practice running the code so that you can reproduce the figures and results from the lecture and in the book chapter.
- Exercise skills by varying code -- changing variables, changing options -- so that you can see how the code works.
- Use the opportunity to reflect on and evaluate results -- so that we can support growth in development of understanding of main ideas.

#### The practical exercises {#sec-multilevel-workbook-practical-workbook}

Now you will progress through a series of tasks, and challenges, to aid your learning.

::: callout-warning
We will work with the data file:

-   `AFACALO_DATASET.RData`
:::

We again split the steps into into parts, tasks and questions.

We are going to work through the following workflow steps: **each step is labelled as a practical part**.

1. Set-up
2. Load the data
3. Inspect the data
4. Visualize relationships
5. Analyze relationships using linear models
6. Visualize relationships for each class
7. Mixed-effects analysis
8. Optional exercises: extensions

In the following, we will guide you through the tasks and questions step by step.

::: callout-important
An answers version of the workbook will be provided after the practical class.
:::

<!-- #### 10.4.3.1 Practical Part 1: Set-up -->

<!-- To begin, we set up our environment in R. -->

<!-- ##### Practical Task 1 -- Run code to empty the R environment -->

<!-- :::{.callout-tip collapse="true"} -->
<!-- ## Code -->
<!-- ```{r} -->
<!-- #| eval: false -->
<!-- rm(list=ls()) -->
<!-- ``` -->
<!-- ::: -->

<!-- ##### Practical Task 2 -- Run code to load relevant libraries -->

<!-- Notice that in Week 10 we need to work with the libraries `ggeffects`, `patchwork` and `tidyverse`. -->
<!-- Use the `library()` function to make these libraries available to you. -->

<!-- :::{.callout-tip collapse="true"} -->
<!-- ## Code -->
<!-- ```{r} -->
<!-- library("ggeffects") -->
<!-- library("patchwork") -->
<!-- library("psych") -->
<!-- library("tidyverse") -->
<!-- ``` -->
<!-- ::: -->

<!-- #### 10.4.3.2 Practical Part 2: Load the data -->

<!-- ##### Practical Task 3 -- Read in the data file we will be using -->

<!-- The data file is called: -->

<!-- - `2022-12-08_all-studies-subject-scores.csv` -->

<!-- Use the `read_csv()` function to read the data files into R: -->

<!-- :::{.callout-tip collapse="true"} -->
<!-- ## Code -->
<!-- ```{r} -->
<!-- all.studies.subjects <- read_csv("2022-12-08_all-studies-subject-scores.csv") -->
<!-- ``` -->
<!-- ::: -->

<!-- ##### Practical Task 4 -- Get summary statistics for *only* the numeric variables: `AGE` and `HLVA` -->

<!-- Use the `{psych}` library `describe()` function to get a table summary of descriptive statistics for these variables. -->

<!-- :::{.callout-tip collapse="true"} -->
<!-- ## Code -->
<!-- Here, we can do this in two steps: -->

<!-- 1. we select the variables we care about -->
<!-- 2. we get just the descriptive statistics we want for those variables -->

<!-- ```{r} -->
<!-- all.studies.subjects %>% -->
<!--   dplyr::select(AGE, HLVA) %>% -->
<!--   describe(skew = FALSE) -->
<!-- ``` -->
<!-- ::: -->

<!-- >**Pract.Q.1.** What is the mean health literacy (HLVA) score in this sample? -->

<!-- > Pract.A.1. 7.22 -->

<!-- >**Pract.Q.2.** What are the minimum and maximum ages in this sample? -->

<!-- > Pract.A.2. 18-100 -->

<!-- >**Pract.Q.3.** Do you see any reason to be concerned about the data in this sample? -->

<!-- > Hint: It is always a good idea to use your visualization skills to examine the distribution of variable values in a dataset: -->

<!-- ```{r} -->
<!-- all.studies.subjects %>% -->
<!--   ggplot(aes(x = AGE)) + -->
<!--   geom_histogram() -->
<!-- ``` -->

<!-- > Pract.A.3. It looks like the `AGE` of 100 is maybe an error, or an extreme outlier. -->

<!-- #### 10.4.3.3 Practical Part 3: Revision -- using a linear model to answer research questions -- one predictor. -->

<!-- One of our research questions is: -->

<!-- 2. Can people accurately evaluate whether they correctly understand written health information? -->

<!-- We can address this question by examining whether someone's rated evaluation of their own understanding matches their performance on a test of that understanding, and by investigating what variables predict variation in mean self-rated accuracy. -->

<!-- ::: callout-tip -->
<!-- Note that ratings of accuracy are ordinal (categorical ordered) data but that, here, we may choose to examine the average of participants' ratings of their own understanding of health information for the purposes of this example. -->

<!-- - We learn about ordinal models for ordinal data in PSYC412. -->
<!-- ::: -->

<!-- For these data, participants were asked to respond to questions about health information to get `mean.acc` scores and were asked to rate their own understanding of the same information. -->

<!-- - If you *can* evaluate your own understanding then ratings of understanding *should* be associated with performance on tests of understanding. -->

<!-- ##### Practical Task 5 -- Estimate the relation between outcome mean self-rated accuracy (`mean.self`) and tested accuracy of understanding (`mean.acc`) -->

<!-- We can use `lm()` to estimate whether the ratings of accuracy actually predict the outcome tested accuracy levels. -->

<!-- :::{.callout-tip collapse="true"} -->
<!-- ## Code -->
<!-- ```{r} -->
<!-- model <- lm(mean.acc ~ mean.self, data = all.studies.subjects) -->
<!-- summary(model) -->
<!-- ``` -->
<!-- ::: -->

<!-- If you first fit the appropriate model, then read the model summary, you can answer the following questions. -->

<!-- >**Pract.Q.4.** What is the estimate for the coefficient of the effect of the predictor `mean.self` on the outcome `mean.acc` in this model? -->

<!-- > Pract.A.4. 0.058613 -->

<!-- >**Pract.Q.5.** Is the effect significant? -->

<!-- > Pract.A.5. It is significant, p < .05 -->

<!-- >**Pract.Q.6.** What are the values for t and p for the significance test for the coefficient? -->

<!-- > Pract.A.6. t = 12.90, p <2e-16 -->

<!-- >**Pract.Q.7.** What do you conclude is the answer to the research question, given the linear model results? -->

<!-- > Pract.A.7. The model slope estimate suggests that higher levels of rated understanding can predict higher levels of tested understanding so, yes: it does appear that people can evaluate their own understanding. -->

<!-- >**Pract.Q.8.** What is the F-statistic for the regression? Report F, DF and the p-value. -->

<!-- > Pract.A.8. F-statistic: 166.4 on 1 and 559 DF,  p-value: < 2.2e-16 -->

<!-- >**Pract.Q.9.** Is the regression significant? -->

<!-- > Pract.A.9. Yes: the regression is significant. -->

<!-- >**Pract.Q.10.** What is the Adjusted R-squared? -->

<!-- > Pract.A.10. Adjusted R-squared:  0.228 -->

<!-- >**Pract.Q.11.** Explain in words what this R-squared value indicates? -->

<!-- > Pract.A.11. The R-squared suggests that 23% of outcome variance can be explained by the model -->

<!-- #### 10.4.3.4 Practical Part 4: New -- using a linear model to answer research questions -- multiple predictors -->

<!-- One of our research questions is: -->

<!-- 2. Can people accurately evaluate whether they correctly understand written health information? -->

<!-- We have already looked at this question by asking whether ratings of understanding predict performance on tests of understanding. But there is a problem with that analysis -- it leaves open the question:  -->

<!-- - What actually predicts ratings of understanding? -->

<!-- We can look at that follow-up question through the analysis we do next. -->

<!-- ##### Practical Task 6 -- Examine the relation between outcome mean self-rated accuracy (`mean.self`) and  multiple predictors including: health literacy (`HLVA`); vocabulary (`SHIPLEY`); `AGE`; reading strategy (`FACTOR3`); as well as `mean.acc` -->

<!-- We use `lm()`, as before, but now specify each variable listed here by variable name -->

<!-- - outcome: mean self-rated accuracy (`mean.self`) -->
<!-- - predictors: health literacy (`HLVA`); vocabulary (`SHIPLEY`); `AGE`; reading strategy (`FACTOR3`); as well as `mean.acc`. -->

<!-- :::{.callout-tip collapse="true"} -->
<!-- ## Code -->
<!-- ```{r} -->
<!-- model <- lm(mean.self ~ HLVA + SHIPLEY + FACTOR3 + AGE + mean.acc, -->
<!--             data = all.studies.subjects) -->
<!-- summary(model) -->
<!-- ``` -->
<!-- ::: -->

<!-- If you look at the model summary you can answer the following questions   -->

<!-- >**Pract.Q.12.** What predictors are significant in this model? -->

<!-- > Pract.A.12. Health literacy ('HLVA'), reading strategy ('FACTOR3') and performance on tests of accuracy of understanding ('mean.acc') all appear to significantly predict variation in mean ratings of understanding ('mean.self'). -->

<!-- >**Pract.Q.13.** What is the estimate for the coefficient of the effect of the predictor, `mean.acc`, in this model? -->

<!-- > Pract.A.13. 2.502024 -->

<!-- >**Pract.Q.14.** Is the effect significant? -->

<!-- > Pract.A.14. It is significant because p < .05 -->

<!-- >**Pract.Q.15.** What are the values for t and p for the significance test for the coefficient? -->

<!-- > Pract.A.15. t = 6.929, p = 1.18e-11 -->

<!-- >**Pract.Q.16.** What do you conclude is the answer to the follow-up question, what actually predicts ratings of understanding? -->

<!-- > Pract.A.16. Ratings of understanding appear to be predicted by performance on tests of accuracy of understanding, together with variation in health literacy and reading strategy. -->

<!-- #### 10.4.3.5 Practical Part 5: New -- plot predictions from linear models -->

<!-- In this part, the practical work involves plotting the predictions from two linear models but the benefit from this practical work is to better your understanding of what linear models are doing. -->

<!-- So, our research questions is: -->

<!-- 2. Can people accurately evaluate whether they correctly understand written health information? -->

<!-- We can look at this question in two ways: -->

<!-- - we can focus on whether `mean.self` predicts `mean.acc`? -->
<!-- - or, in reverse, whether `mean.acc` predicts `mean.self`? -->

<!-- People could (1.) be good at understanding accurately what they read (measured as `mean.acc`), so that that accurate understanding results in accurate evaluation of their own understanding (measured as `mean.self`). Or people could (2.) be good at judging their own understanding (`mean.self`) so that they can then be accurate in their understanding of what they read. This is not just a tricksy way of talking: it could be that people who are good at evaluating if they understand what they read are able to work more effectively at that understanding, or the opposite could be the case.  -->

<!-- The *direction of cause-and-effect* is just unclear, here.  -->

<!-- ##### Practical Task 7 -- We cannot actually resolve the question of causality but we can learn a lot if we look at two models incorporating the same variables -->

<!-- 1. If we focus on whether (predictor) `mean.self` predicts (outcome) `mean.acc` then the model should be: `mean.acc ~ mean.self` -->
<!-- 2. If we focus on whether (predictor) `mean.acc` predicts (outcome) `mean.self` then the model should be: `mean.self ~ mean.acc` -->

<!-- The two models involve the same variables but arrange the variables differently, in the code, depending on which variable is specified as the outcome versus which variable is specified as the predictor. -->

<!-- ::: callout-tip -->
<!-- Note that the comparison between these models teaches us something about *what* it is that linear models predict. -->

<!-- - You want to focus on the estimate of the coefficient of the predictor variable, for each model, presented in the model summary results. -->
<!-- ::: -->

<!-- >**Pract.Q.17.** Can you write the code for the two models? Can you give each model a distinctive name? -->

<!-- :::{.callout-tip collapse="true"} -->
<!-- ## Code -->
<!-- ```{r} -->
<!-- model.1 <- lm(mean.acc ~ mean.self, data = all.studies.subjects) -->
<!-- summary(model.1) -->

<!-- model.2 <- lm(mean.self ~ mean.acc, -->
<!--               data = all.studies.subjects) -->
<!-- summary(model.2) -->
<!-- ``` -->
<!-- ::: -->

<!-- Take a look at the summary for each model.   -->

<!-- >**Pract.Q.18.** Why do you think it appears that the slope coefficient estimate is different if you compare: -->

<!-- > (1.) the model, mean.acc ~ mean.self -->

<!-- > versus -->

<!-- > (2.) the model, mean.self ~ mean.acc? -->

<!-- > Hint: You may benefit by reflecting on the lectures and practical materials in [the chapter for week 8](../part2/lm-intro.qmd#sec-lm-intro-overview), especially where they concern predictions. -->

<!-- > Pract.A.18. Linear models are prediction models. We use them to predict variation in *outcomes* given some set of predictor variables. Because of this, predictions *must* be scaled in the same way as the *outcome variable*. -->

<!-- > So: -->

<!-- > (1.) the model `mean.acc ~ mean.self` identifies `mean.acc` as the outcome so if we are predicting change in `mean.acc` (scaled 0-1) then we are looking at coefficients that will lie somewhere on the same scale (also 0-1). -->

<!-- > Here: the model suggests that unit change in `mean.self` predicts increase of 0.058613 in `mean.acc`. -->

<!-- > Versus: -->

<!-- > (2.) the model, `mean.self ~ mean.acc` identifies `mean.self` as the outcome so if we are predicting change in `mean.self` (scaled 1-9) then we are looking at coefficients that will lie somewhere on the same scale (also 1-9). -->

<!-- > Here: the model suggests that unit change in `mean.acc` predicts increase of 3.9135 in `mean.self`. -->

<!-- >**Pract.Q.19.** Can you plot the predictions from each model? -->

<!-- :::{.callout-tip collapse="true"} -->
<!-- ## Code -->
<!-- > Pract.A.19. Here is the code to plot the predictions from both models: -->

<!-- First fit the models -- give the model objects distinct names: -->

<!-- ```{r} -->
<!-- model.1 <- lm(mean.acc ~ mean.self, data = all.studies.subjects) -->
<!-- summary(model.1) -->

<!-- model.2 <- lm(mean.self ~ mean.acc, -->
<!--             data = all.studies.subjects) -->
<!-- summary(model.2) -->
<!-- ``` -->

<!-- Then get the predictions: -->

<!-- ```{r} -->
<!-- dat.1 <- ggpredict(model.1, "mean.self") -->
<!-- dat.2 <- ggpredict(model.2, "mean.acc") -->
<!-- ``` -->

<!-- Then make the plots, and arrange them for easy comparison, side-by-side: -->

<!-- ```{r} -->
<!-- plot.1 <- plot(dat.1) -->
<!-- plot.2 <- plot(dat.2) -->

<!-- plot.1 + plot.2 -->
<!-- ``` -->
<!-- ::: -->

<!-- >**Pract.Q.20.** Look at the two plots side-by-side: what do you see? -->

<!-- Hint: Look at changes in height of the prediction line, given changes in x-axis position of the line. -->

<!-- > Pract.A.20. A side-by-side comparison shows that for (1.) `mean.acc ~ mean.self,` increases in `mean.self` from about 2-8 are associated with a change in `mean.acc` from about .4 to about .85, while for (2.) `mean.self ~ mean.acc`, increases in `mean.self` from about .2-1.0 are associated with a change in `mean.acc` from about 4 to about 8. -->

<!-- **What are we learning here?** -->

<!-- Linear models are prediction models. We use them to predict variation in outcomes given some set of predictor variables.  -->

<!-- ::: callout-important -->
<!-- Predictions *must* be scaled in the same way as the outcome variable. -->
<!-- ::: -->

<!-- #### 10.4.3.6 Practical Part 6: New -- estimate the effects of factors as well as numeric variables -->

<!-- ##### Practical Task 8 -- Fit a linear model to examine what variables predict outcome mean self-rated accuracy of `mean.self` -->

<!-- Include in the model both numeric variables and categorical variables as predictors: -->

<!-- - health literacy (`HLVA`); vocabulary (`SHIPLEY`); `AGE`; reading strategy (`FACTOR3`); as well as `mean.acc` and `NATIVE.LANGUAGE`. -->

<!-- :::{.callout-tip collapse="true"} -->
<!-- ## Code -->
<!-- ```{r} -->
<!-- model <- lm(mean.self ~ HLVA + SHIPLEY + FACTOR3 + AGE + mean.acc + -->
<!--                         NATIVE.LANGUAGE, -->
<!--             data = all.studies.subjects) -->
<!-- ``` -->
<!-- ::: -->

<!-- >**Pract.Q.21.** Can you report the estimated effect of `NATIVE.LANGUAGE` (the coding of participant language status: `English` versus `other`)? -->

<!-- Hint: you will need to get a summary of the model you have coded. -->

<!-- :::{.callout-tip collapse="true"} -->
<!-- ## Code -->
<!-- ```{r} -->
<!-- summary(model) -->
<!-- ``` -->
<!-- ::: -->

<!-- > Pract.A.21. The effect of language status (`NATIVE.LANGUAGE`) on mean accuracy of understanding is not significant (estimate = -0.225432, t = -1.750, p = .081) indicating that not being a native speaker of English (`Other`) is not associated with lower self-rated accuracy. -->

<!-- >**Pract.Q.22.** Can you report the overall model and model fit statistics? -->

<!-- > Pract.A.22. We fitted a linear model with mean self-rated accuracy as the outcome and with the predictors: health literacy (HLVA),  reading strategy (FACTOR3), vocabulary (SHIPLEY) and AGE (years), as well as mean accuracy (mean.acc) and language status. The model is significant overall, with F(6, 554) = 45.82, p < .001, and explains 32% of variance (adjusted R2 = 0.32). -->

<!-- >**Pract.Q.23.** Can you plot the predicted effect of `NATIVE.LANGUAGE` given your model? -->

<!-- :::{.callout-tip collapse="true"} -->
<!-- ## Code -->
<!-- We first fit the model, including `NATIVE.LANGUAGE`: -->

<!-- ```{r} -->
<!-- model <- lm(mean.self ~ HLVA + SHIPLEY + FACTOR3 + AGE + mean.acc + -->
<!--               NATIVE.LANGUAGE, -->
<!--             data = all.studies.subjects) -->
<!-- ``` -->

<!-- Then use the `ggpredict()` function to get the predictions -->

<!-- ```{r} -->
<!-- dat <- ggpredict(model, "NATIVE.LANGUAGE") -->
<!-- plot(dat) -->
<!-- ``` -->
<!-- ::: -->

<!-- >**Pract.Q.24.** The plot should give you dot-and-whisker representations of the estimated `mean.self` for `English` versus `Other` participants in the dataset. What is the difference in the estimated `mean.self` between these groups? -->

<!-- Hint: The effect or prediction plot will show you dot-and-whisker representations of predicted outcome `mean.self`. In these plots, the dots represent the estimated `mean.self` while the lines (whiskers) represent confidence intervals. -->

<!-- > Pract.A.24. The difference in the estimated 'mean.self' between these groups is about .2. -->

<!-- >**Pract.Q.25.** Compare the difference in the estimated `mean.self` between these groups, given the plot, with the coefficient estimate from the model summary: what do you see? -->

<!-- > Pract.A.25. The coefficient estimate = -0.225432. This matches the difference shown in the plot. -->

<!-- #### 10.4.3.7 Practical Part 7: New -- examine associations comparing data from different samples -->

<!-- ##### Practical Task 8 -- Use `facet_wrap()` to show how the association between `mean.self` and `mean.acc` can vary between the different studies in the data-set -->

<!-- Change the factor in `facet_wrap()` to show how the association between `mean.self` and `mean.acc` can vary between the different studies -->

<!-- :::{.callout-tip collapse="true"} -->
<!-- ## Code -->
<!-- ```{r} -->
<!-- all.studies.subjects %>% -->
<!--   ggplot(aes(x = mean.self, y = mean.acc)) + -->
<!--   geom_point(size = 2, alpha = .5, colour = "darkgrey") + -->
<!--   geom_smooth(size = 1.5, colour = "red", method = "lm", se = FALSE) + -->
<!--   xlim(0, 10) + -->
<!--   ylim(0, 1.1)+ -->
<!--   theme_bw() + -->
<!--   theme( -->
<!--     axis.text = element_text(size = rel(1.15)), -->
<!--     axis.title = element_text(size = rel(1.5)) -->
<!--   ) + -->
<!--   xlab("Mean self-rated accuracy") + ylab("Mean accuracy") + -->
<!--   facet_wrap(~ study) -->
<!-- ``` -->
<!-- ::: -->

<!-- You may need to edit the x-axis labeling to make it readable -->

<!-- - Can you work out how to do that, given `ggplot()` help information? -->

<!-- Hint: check out the continuous scale information in: -->

<!-- <https://ggplot2.tidyverse.org/reference/scale_continuous.html> -->

<!-- #### 10.4.3.8 Practical Part Optional: New -- save or export plots so that you can insert them in reports -->

<!-- Learning how to export plots from R will help you later with reports. -->

<!-- The task is to save or export a plot that you produce, so that you can insert it in a report or presentation. -->

<!-- There are different ways to do this, we can look at one simple example. -->

<!-- - We can save the last plot we produce using the `{tidyverse}` function `ggsave()`: -->

<!-- ```{r} -->
<!-- ggsave("facet-plots.png") -->
<!-- ``` -->

<!-- Notice, here, that: -->

<!-- - `ggsave("facet-plots...")` -- we need to give the plot a name; -->
<!-- - `ggsave("...png")` -- and we need to tell R what format we require. -->

<!-- The plot is saved as a file with the name you specify, in the working directory (folder) you are using. -->

<!-- R will save the plot in the format you specify: here, I choose .png because .png image files can be imported into Microsoft Word documents easily. -->

<!-- - Notice that `ggsave()` will use pretty good defaults but that you can over-ride the defaults, by adding arguments to specify the plot width, height and resolution (in dpi). -->

<!-- ::: callout-tip -->
<!-- ## Further information you can explore -->
<!-- Check out reference information here: -->

<!-- <https://ggplot2.tidyverse.org/reference/ggsave.html> -->

<!-- and -->

<!-- <http://www.cookbook-r.com/Graphs/Output_to_a_file/> -->
<!-- ::: -->

<!-- #### Practical Task Optional: Now try it for yourself: make a plot, and save it, using what you have learnt so far -->

<!-- You will need to: -->

<!-- 1. Fit a model -->
<!-- 2. Create a set of predictions we can use for plotting -->
<!-- 3. Make a plot -->
<!-- 4. Save it -->

<!-- Then, you can import the saved plot into Word. -->

<!-- :::{.callout-tip collapse="true"} -->
<!-- ## Code -->
<!-- Fit a model: -->

<!-- ```{r} -->
<!-- model <- lm(mean.acc ~ HLVA + SHIPLEY + FACTOR3 + AGE, -->
<!--             data = all.studies.subjects) -->
<!-- ``` -->

<!-- Create a set of predictions we can use for plotting -->

<!-- ```{r} -->
<!-- dat <- ggpredict(model, "FACTOR3") -->
<!-- ``` -->

<!-- Make a plot -->

<!-- ```{r} -->
<!-- p.model <- plot(dat) -->
<!-- p.model + -->
<!--   geom_point(data = all.studies.subjects, -->
<!--              aes(x = FACTOR3, y = mean.acc), size = 1.5, alpha = .75, colour = "darkgrey") + -->
<!--   geom_line(size = 1.5) + -->
<!--   theme_bw() + -->
<!--   theme( -->
<!--     axis.text = element_text(size = rel(1.15)), -->
<!--     axis.title = element_text(size = rel(1.25)), -->
<!--     plot.title = element_text(size = rel(1.4)) -->
<!--   ) + -->
<!--   xlab("Reading strategy (FACTOR3)") + ylab("Mean accuracy") + -->
<!--   ggtitle("Effect of reading strategy on \n mean comprehension accuracy") -->
<!-- ``` -->

<!-- Save it -->

<!-- ```{r} -->
<!-- ggsave("reading-strategy-prediction.png", width = 10, height = 10, units = "cm") -->
<!-- ``` -->
<!-- ::: -->

<!-- Some advice: -->

<!-- - It is often helpful to present plots that are almost square i.e. height about = width. -->
<!-- - This helps to present the best fit line in scatterplots in a way that is easier to perceive. -->
<!-- - We may experiment with width-height (aspect ratio) until we get something that "looks" right.  -->
<!-- - Notice also that different presentation venues will require different levels if image resolution e.g. journals may require .tiff file plots at dpi = 180 or greater. -->

<!-- ### The answers -->

<!-- After the practical class, we will reveal the answers that are currently hidden. -->

<!-- The answers version of the webpage will present my answers for questions, and some extra information where that is helpful. -->

<!-- ### Look ahead: growing in independence -->

<!-- #### General advice -->

<!-- An old saying goes: -->

<!-- > All models are wrong but some are useful -->

<!-- (attributed to George Box). -->

<!-- ::: callout-tip -->
<!-- -   Sometimes, it can be useful to adopt a simpler approach as a way to approximate *get closer to* better methods -->
<!-- -   Box also advises: -->

<!-- > Since all models are wrong the scientist must be alert to what is importantly wrong. It is inappropriate to be concerned about mice when there are tigers abroad. -->

<!-- -   Here, we focus on validity, measurement, generalizability and *critical thinking* -->
<!-- ::: -->

<!-- ### Summary -->

<!-- 1.  Linear models -->

<!-- -   Linear models are a very general, flexible, and powerful analysis method -->
<!-- -   We can use assuming that prediction outcomes (residuals) are normally distributed -->
<!-- -   With potentially multiple predictor variables -->

<!-- 2.  Thinking about linear models -->

<!-- -   Closing the loop: when we plan an analysis we should try to use contextual information -- theory and measurement understanding -- to specify our model -->
<!-- -   Closing the loop: when we critically evaluate our or others' findings, we should consider validity, measurement, and generalizability -->

<!-- 3.  Reporting linear models -->

<!-- -   When we report an analysis, we should report: -->

<!-- 1.  Explain what I did, specifying the method (linear model), the outcome variable (accuracy) and the predictor variables (health literacy, reading strategy, reading skill and vocabulary) -->
<!-- 2.  Report the model fit statistics overall ($F, R^2$) -->
<!-- 3.  Report the significant effects ($\beta, t, p$) and describe the nature of the effects (does the outcome increase or decrease?) -->
