---
title: Week 19. Introduction to Generalized Linear Mixed-effects Models
subtitle: Written by Rob Davies
order: 12
bibliography: references.bib
csl: psychological-bulletin.csl
---

# Week 19 Introduction to Generalized Linear Mixed-effects Models {#sec-glmm-workbook-overview}

Welcome to your overview of the work we will do together in **Week 19**.

We extend our understanding and skills by moving to examine data where the outcome variable is *categorical*: this is a context that requires the use of **Generalized Linear Mixed-effects Models (GLMMs)**.

::: callout-important
Categorical outcomes cannot be analyzed using linear models, in whatever form, without having to make some important compromises.
:::

You need to do something about the categorical nature of the outcome.

## Targets {#sec-glmm-workbook-targets}

Here, we look at **Generalized Linear Mixed-effects Models (GLMMs)**: we can use these models to analyze outcome variables of different kinds, including outcome variables like response accuracy that are coded using discrete categories (e.g. correct vs. incorrect). 

Our aims are to:

1.  Recognize the limitations of alternative methods for analyzing such outcomes, @sec-tradition-limitations.
2.  Understand practically the reasons for using GLMMs when we analyze discrete outcome variables, @sec-glmm-practical-understanding.
3.  Practice running GLMMs with varying random effects structures.
4.  Practice reporting the results of GLMMs, including through the use of model plots.

## Learning resources {#sec-glmm-workbook-resources}

You will see, next, the lectures we share to explain the concepts you will learn about, and the practical data analysis skills you will develop. Then you will see information about the practical materials you can use to build and practise your skills.

Every week, you will learn best if you *first* watch the lectures *then* do the practical exercises.

::: callout-tip
### Linked resources
1. We learned about multilevel structured data in the [conceptual introduction to multilevel data](../part2/01-multilevel.qmd#sec-intro-multilevel) and the [workbook introduction to multilevel data](../part2/01-multilevel-workbook.qmd#sec-multilevel-workbook-overview).
2. We then deepened our understanding by looking at the analysis of data from studies with **repeated-measures designs** in the [conceptual introduction to linear mixed-effects models](../part2/02-mixed.qmd#sec-intro-mixed) and the [workbook introduction to mixed-effects models](../part2/02-mixed-workbook.qmd#sec-mixed-effects-workbook-overview).
3. We further extended our understanding and practice skills in the [chapter on developing linear mixed-effects models](../part2/03-mixed.qmd#sec-dev-mixed) and the [corresponding workbook](../part2/03-mixed-workbook.qmd#sec-dev-mixed-workbook-overview).

This workbook introduction on **Generalized Linear Mixed-effects Models (GLMMs)** is linked to the corresponding [chapter](../part2/04-glmm.qmd#sec-glmm-intro) where the explanation of ideas or of practical analysis steps is set out more extensively or in more depth.
:::

### Lectures {#sec-glmm-workbook-lectures}

The lecture materials for this week are presented in three short parts.

Click on a link and your browser should open a tab showing the *Panopto* video for the lecture part.

1. Part 1 (20 minutes): Understand the reasons for using Generalized Linear Mixed-effects Models (GLMMs); categorical outcomes like response accuracy and the bad compromises involved in (traditional methods) not using GLMMs to analyze them;  understanding what the *generalized* part of this involves.

```{=html}
<iframe src="https://lancaster.cloud.panopto.eu/Panopto/Pages/Embed.aspx?id=bbf38d0e-90e5-4285-82ea-acec012a2f77&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen allow="autoplay" aria-label="Panopto Embedded Video Player" aria-description="04-glmm-1-of-3" ></iframe>
```

2. Part 2 (16 minutes): The questions, design and methods of the working data example study; category coding practicalities; specifying a GLMM appropriate to the design; the GLMM results summary, reading the results, visualizing the results.

```{=html}
<iframe src="https://lancaster.cloud.panopto.eu/Panopto/Pages/Embed.aspx?id=93c898ce-8009-47c6-ad43-acec0130b2ea&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen allow="autoplay" aria-label="Panopto Embedded Video Player" aria-description="04-glmm-2-of-3" ></iframe>
```

3. Part 3 (19 minutes): What random effects should we include; recognizing when a model has got into trouble, and what to do about it; general approaches to model specification; reporting model results.

```{=html}
<iframe src="https://lancaster.cloud.panopto.eu/Panopto/Pages/Embed.aspx?id=078770df-47d0-4a25-9fdd-acec0135f5ea&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all" height="405" width="720" style="border: 1px solid #464646;" allowfullscreen allow="autoplay" aria-label="Panopto Embedded Video Player" aria-description="04-glmm-3-of-3" ></iframe>
```

### Lecture slides {#sec-glmm-workbook-slides}

::: callout-tip
## Download the lecture slides
You can download the lecture slides in two different versions:

- [402-week-20-GLMM.pdf](files/402-week-20-GLMM.pdf): exactly as delivered [700 KB];
- [402-week-20-GLMM_6pp.pdf](files/402-week-20-GLMM_6pp.pdf): printable version, six-slides-per-page [850 KB].

The `GLMM.pdf` version is the version delivered for the lecture recordings. To make the slides easier to download, I produced a six-slide-per-page version, `GLMM_6pp.pdf`. This should be easier to download and print out if that is what you want to do.
:::

### Practical materials: data and R-Studio {#sec-glmm-workbook-practical}

We will be working with data collected for a study investigating word learning in children, reported by @ricketts2021. You will see that the study design has both a repeated measures aspect because each child is asked to respond to multiple stimuli, and a longitudinal aspect because responses are recorded at two time points. Because responses were observed to multiple stimuli for each child, and because responses were recorded at multiple time points, the data have a multilevel structure. These features require the use of mixed-effects models for analysis.

We will see, also, that the study involves the factorial manipulation of learning conditions. This means that, when you see the description of the study design, you will see embedded in it a 2 x 2 factorial design. You will be able to generalize from our work here to many other research contexts where psychologists conduct experiments in which conditions are manipulated according to a factorial design.

However, our focus now is on the fact that the outcome for analysis is the accuracy of the responses made by children to word targets in a spelling task. The categorical nature of accuracy as an outcome is the reason why we now turn to use Generalized Linear Mixed-effects Models.

You can read more about these data in the [chapter on GLMMs](../part2/04-glmm.qmd#sec-glmm-data-study).

We addressed three research questions and tested predictions in relation to each question.

1.  Does the presence of orthography promote greater word learning?

-   We predicted that children would demonstrate greater orthographic learning for words that they had seen (orthography present condition) versus not seen (orthography absent condition).

2.  Will orthographic facilitation be greater when the presence of orthography is emphasized explicitly during teaching?

-   We expected to observe an interaction between instructions and orthography, with the highest levels of learning when the orthography present condition was combined with explicit instructions.

3.  Does word consistency moderate the orthographic facilitation effect?

-   For orthographic learning, we expected that the presence of orthography might be particularly beneficial for words with higher spelling-sound consistency, with learning highest when children saw and heard the word, and these codes provided overlapping information.

::: callout-important
**Get the data**: get the data file and the .R script you can use to do the exercises that will support your learning.

- You can download the files folder for this chapter by clicking on the link [04-glmm.zip](files/04-glmm.zip).
:::

The practical materials folder includes data files and an `.R` script:

In this chapter, we will be working with the data about the orthographic post-test outcome for the Ricketts word learning study:

-   `long.orth_2020-08-11.csv`

The data file is collected together with the .R script:

-   `04-glmm-workbook.R` the workbook you will need to do the practical exercises.

The data come from the @ricketts2021 study, and you can access the analysis code and data for that study, in full, at the OSF repository [here](https://osf.io/e5gzk/?view_only=038118528c7c426c9729983f54138c88)

::: callout-important
You can access the sign-in page for [R-Studio Server here](https://psy-rstudio.lancaster.ac.uk/auth-sign-in?appUri=%2F)
:::

#### Practical materials guide {#sec-glmm-workbook-practical-guide}

The aims of the practical work are to:

- Understand the reasons for using Generalized Linear Mixed-effects models (GLMMs) when we analyze discrete outcome variables.
- Recognize the limitations of alternative methods for analyzing such outcomes.
- Practice running GLMMs with varying random effects structures.
- Practice reporting the results of GLMMs, including through the use of model plots.

My recommendations for learning are that you should aim to:

- run GLMMs of demonstration data;
- run GLMMs of alternate data sets;
- play with the `.R` code used to create examples for the lecture;
- and edit example code to create alternate visualizations.

#### The practical exercises {#sec-glmm-workbook-practical-workbook}

Now you will progress through a series of tasks, and challenges, to aid your learning.

::: callout-warning
We will work with the data file:

-   `long.orth_2020-08-11.csv`
:::

We again split the steps into into parts, tasks and questions.

We are going to work through the following workflow steps: **each step is labelled as a practical part**.

1. Set-up
2. Load the data
3. Tidy data 
4. Analyze the data: random intercepts
5. Analyze the data: model comparisons
6. Exercise analyses
7. Optional: reproduce the plots in the chapter and slides

In the following, we will guide you through the tasks and questions step by step.

::: callout-important
An answers version of the workbook will be provided after the practical class.
:::

#### Practical Part 1: Set-up

To begin, we set up our environment in R.

##### Practical Task 1 -- Run code to load relevant libraries

Use the `library()` function to make the functions we need available to you.

:::{.callout-tip collapse="true"}
## Code
```{r}
library(broom)
library(effects)
library(gridExtra)
library(here)
library(lattice)
library(knitr)
library(lme4)
library(MuMIn)
library(sjPlot)
library(tidyverse)
```
:::

#### Practical Part 2: Load the data

##### Practical Task 2 -- Read in the data file we will be using

Read the data file into R:

:::{.callout-tip collapse="true"}
## Code
```{r}
#| label: readinall
#| message: false
long.orth <- read_csv("long.orth_2020-08-11.csv", 
                      col_types = cols(
                        Participant = col_factor(),
                        Time = col_factor(),
                        Study = col_factor(),
                        Instructions = col_factor(),
                        Version = col_factor(),
                        Word = col_factor(),
                        Orthography = col_factor(),
                        Measure = col_factor(),
                        Spelling.transcription = col_factor()
                      )
                    )
```
:::

You can see, here, that within the `read_csv()` function call, I specify `col_types`, instructing R how to treat a number of different variables. 

- You can read more about this convenient way to control the read-in process [here](https://readr.tidyverse.org/articles/readr.html).

It is always a good to inspect what you have got when you read a data file in to R.

```{r}
#| label: data-summary
#| eval: false
summary(long.orth)
```

Some of the variables included in the `.csv` file are listed, following, with information about value coding or calculation.

-   `Participant` -- Participant identity codes were used to anonymize participation.
-   `Time` -- Test time was coded 1 (time 1) or 2 (time 2). For the Study 1 longitudinal data, it can be seen that each participant identity code is associated with observations taken at test times 1 and 2.
-   `Instructions` -- Variable coding for whether participants undertook training in the explicit} or incidental} conditions.
-   `Word` -- Letter string values showing the words presented as stimuli to the children.
-   `Orthography` -- Variable coding for whether participants had seen a word in training in the orthography absent or present conditions.
-   `Consistency-H` -- Calculated orthography-to-phonology consistency value for each word. -`zConsistency-H` -- Standardized Consistency H scores
-   `Score` -- Outcome variable -- for the orthographic post-test, responses were scored as `1` (correct, if the target spelling was produced in full) or `0` (incorrect, if the target spelling was not produced).

The summary will show you that we have a number of other variables available, including measures of individual differences in reading or reading-related abilities or knowledge, but we do not need to pay attention to them, for our exercises.

#### Practical Part 3: Tidy the data

Tidying the data involves a number of tasks, some essential and some things we do for our convenience.

We are going to first filter the observations, then transform the outcome variable.

- We do this work and use data visualization to examine the impacts of the actions.

::: callout-tip
It is *always* a good idea to first inspect what you have got when you read a data file into R before you do anything more demanding.

-   You **cannot assume** that the data are what you think they are
-   or that the data are structured or coded in the ways that you think (or have been told) they should be structured or coded.
:::

##### Practical Task 3 -- Produce a density plot showing word recognition reaction time, for both correct and incorrect responses

We should examine the distribution of the outcome variable, lexical decision response reaction time (RT in ms). Observations about variable value distributions are a part of *Exploratory Data Analysis* and serve to catch errors in the data-set (e.g. incorrectly recorded scores) but also to inform the researcher's understanding of their own data.

:::{.callout-tip collapse="true"}
## Hint
We shall examine the distribution of the outcome variable, lexical decision response reaction time (RT in ms), using density plots. An alternative method would be to use histograms.

A density plot shows a curve. You can say that the density corresponds to the height of the curve for a given value of the variable being depicted, and that it is related to the probability of observing values of the variable within some range of values [@howell2016fundamental].
:::

:::{.callout-tip collapse="true"}
## Code
```{r}
#| label: fig-rt-all-density
#| fig-cap: "Density plot showing word recognition reaction time, correct and incorrect responses"
#| fig-alt: "Density plot showing word recognition reaction time, correct and incorrect responses. The figure shows a black line, a curve peaking over RT = about 500 but ranging between -2000 and +2000. Under the curve, a series of black tick marks are shown under the x-axis border."
#| warning: false
#| message: false
#| fig-height: 5
#| fig-width: 5
ML.all %>%
  ggplot(aes(x = RT)) +
  geom_density(size=1.5) +
  geom_rug(alpha = .2) +
  ggtitle("Raw RT") +
  theme_bw()  
```

The code to produce @fig-rt-all-density works in a series of steps.

1.  `ML.all %>%` takes the data-set, from the ML study, that we have read in to the R workspace and pipes it to the visualization code, next.
2.  `ggplot(aes(x = RT)) +` creates a plot object in which the x-axis variable is specified as `RT`. The values of this variable will be mapped to geometric objects, i.e. plot features, that you can see, next.
3.  `geom_density(size=1.5) +` first displays the distribution of values in the variable `RT` as a density curve. The argument `size=1.5` tells R to make the line $1.5 \times$ the thickness of the line used by default to show variation in density.

Some further information is added to the plot, next.

4.  `geom_rug(alpha = .2) +` with a command that tells R to add a rug plot below the density curve.
5.  `ggtitle("Raw RT")` makes a plot title.

Notice that beneath the curve of the density plot, you can see a series of vertical lines. Each line represents the x-axis location of an RT observation in the ML study data set. This *rug plot* represents the distribution of RT observations in one dimension.

-   `geom_rug()` draws a vertical line at each location on the x-axis that we observe a value of the variable, RT, named in `aes(x = RT)`.
-   `geom_rug(alpha = .2)` reduces the opacity of each line, using `alpha`, to ensure the reader can see how the RT observations are denser in some places than others.
:::

##### Practical Task 4 -- You should try out alternative visualisation methods to reveal the patterns in the distribution of variables in the ML data-set (or in your own data).

1.  Take a look at the `geoms` documented in the `{ggplot2}` library reference section [here](https://ggplot2.tidyverse.org/reference/#section-layer-geoms).
2.  Experiment with code to answer the following questions:

-   Would a histogram or a frequency polygon provide a more informative view? Take a look [here for advice](https://ggplot2.tidyverse.org/reference/geom_histogram.html).
-   What about a dotplot? Take a look [here for advice](https://ggplot2.tidyverse.org/reference/geom_dotplot.html)

##### Practical Task 5 -- Filter out incorrect and outlier short RT observations

The code example, shown above, delivers a plot (@fig-rt-all-density) showing three peaks in the distribution of RT values. You can see that there is a peak of RT observations around 500-1000ms, another smaller peak around -500ms, and a third smaller peak around -2000ms.

The density plot shows the reaction times recorded for participants' button press 'yes' responses to word stimuli in the lexical decision task. The peaks of negative RTs represent observations that are *impossible*.

- How do we deal with the impossible RT values?

:::{.callout-tip collapse="true"}
## Hint
The density plot shows us that the raw ML lexical decision `RT` variable includes negative RT values corresponding to incorrect response. These have to be removed. 

- We can do this quite efficiently by creating a subset of the original "raw" data, defined according to the RT variable using the `{dpyr}` library `filter()` function.
:::

:::{.callout-tip collapse="true"}
## Code
```{r}
#| label: filter
ML.all.correct <- filter(ML.all, RT >= 200)
```

The filter code is written to subset the data by rows **using a condition** on the values of the `RT` variable.

`ML.all.correct <- filter(ML.all, RT >= 200)` works as follows.

1.  `ML.all.correct <- filter(ML.all ...)` creates a new data-set with a new name `ML.all.correct` from the old data-set `ML.all` using the `filter()` function.
2.  `filter(... RT >= 200)` specifies an argument for the `filter()` function.

In effect, we are asking R to check every value in the `RT` column.

-   R will do a check through the `ML.all` data-set, row by row.
-   *If* a row includes an RT that is greater than or equal to 200 *then* that row will be included in the new data-set `ML.all.correct`. This is what I mean by **using a condition**.
-   *But if* a row includes an RT that is less than 200, then that row will not be included. We express this condition as `RT >= 200`.
:::

##### Practical Task 6 -- Check out the impact of filtering on the number of rows in the data-set

- How can we check that the filter operation worked in the way we should expect it to?

After we have removed negative (error) RTs, we can check that the size of the data-set -- here, the number of rows -- matches our expectations.

- How do we do that?

:::{.callout-tip collapse="true"}
## Hint
The `length()` function will count the elements in whatever object is specified as an argument in the function call.

-   This means that if you put a variable name into the function as in `length(data$variable)` it will count how long that variable is -- how many rows there are in the column.
-   If that variable happens to be, as here, part of a data-set, the same calculation will tell you how many rows there are in the data-set as a whole.
-   If you just enter `length(data)`, naming some data-set, then the function will return a count of the number of columns in the data-set.
:::

:::{.callout-tip collapse="true"}
## Code
```{r}
#| label: length-check
length(ML.all$RT)
length(ML.all.correct$RT)
```

If you run the `length()` function calls then you should see that the *length* or number of observations or rows in the `ML.all.correct` data-set should be smaller than the number of observations in the `ML.all` data-set.
:::

::: callout-tip
It is wise to *check* that the operations you perform to tidy, process or wrangle data *actually do* do what you mean them to do. Checks can be performed, for each processing stage, by:

1.  Forming expectations or predictions about what the operation is supposed to do e.g. filter out some rows by some number;
2.  Check what you get against these predictions e.g. count the number of rows before versus after filtering.
:::

##### Practical Task 7 -- Produce a density plot showing word recognition reaction time, for correct responses only

:::{.callout-tip collapse="true"}
## Code
```{r}
#| label: fig-rt-correct-density
#| fig-cap: "Density plot showing word recognition reaction time, correct responses only"
#| fig-alt: "Density plot showing word recognition reaction time, correct and incorrect responses. The figure shows a black line, a curve peaking over RT = about 500 now ranging between 0 and +2000. Under the curve, a series of black tick marks are shown under the x-axis border. The skew towards high values is now more obvious."
#| warning: false
#| message: false
#| fig-height: 5
#| fig-width: 5
ML.all.correct %>%
  ggplot(aes(x = RT)) +
  geom_density(size=1.5) + 
  geom_rug(alpha = .2) +
  ggtitle("Correct RTs") +
  theme_bw()
```
:::

##### Practical Task 8 -- Vary the filter conditions in different ways:

1.  Change the threshold for including RTs from `RT >= 200` to something else: you can change the number, or you can change the operator from `>=` to a different comparison (try `=, <, <=, >`.
2.  Can you assess what impact the change has?

##### Practical Task 9 -- Transform RT to log base 10 RT

@fig-rt-correct-density shows that we have successfully removed all errors (negative RTs) but now we see just how skewed the RT distribution is. Note the *long tail* of longer RTs.

Generally, we assume that departures from a model's predictions about our observations (the linear model residuals) are normally distributed, and we often assume that the relationship between outcome and predictor variables is linear [@Cohen2003c]. We can ensure that our data are compliant with both assumptions by transforming the RT distribution.

- What transformation should we use?

:::{.callout-tip collapse="true"}
## Hint
Psychology researchers often take the log (often the log base 10) of RT values before performing an analysis. Transforming RTs to the log base 10 of RT values has the effect of correcting the skew -- bringing the larger RTs 'closer' (e.g., $1000 = 3$ in log10) to those near the middle which do not change as much (e.g. $500 = 2.7$ in log10).
:::

:::{.callout-tip collapse="true"}
## Code
```{r}
#| label: log-transform
ML.all.correct$logrt <- log10(ML.all.correct$RT)			
```

The `log10()` function works as follows:-

1.  `ML.all.correct$logrt <- log10(...)` creates a a new variable `logrt`, adding it to the `ML.all.correct` data-set. The variable is created using the transformation function `log10()`.
2.  `log10(ML.all.correct$RT)` creates a the new variable by transforming (to log10) the values of the old variable, `RT`.
:::

##### Practical Task 10 -- Produce a density plot showing log10 transformed reaction time, correct responses only

We can see the effect of the transformation if we plot the log10 transformed RTs (see @fig-rt-correct-log-density). 

- Do we arrive at a distribution that more closely approximates the normal distribution?

:::{.callout-tip collapse="true"}
## Code
```{r}
#| label: fig-rt-correct-log-density
#| fig-cap: "Density plot showing log10 transformed reaction time, correct responses only"
#| fig-alt: "Density plot showing word recognition reaction time, correct and incorrect responses. The figure shows a black line, a curve peaking over logrt = about 2.8 ranging between 2.4 and 3.5. Under the curve, a series of black tick marks are shown under the x-axis border. The skew towards high values is now considerably reduced."
#| warning: false
#| message: false
#| fig-height: 5
#| fig-width: 5
ML.all.correct %>%
  ggplot(aes(x = logrt)) +
  geom_density(size = 1.5) + 
  geom_rug(alpha = .2) +
  ggtitle("Correct log10 RTs") +
  theme_bw()
```
:::

::: callout-tip
There are other log transformation functions and we often see researchers using the natural log instead of the log base 10 as discussed [here](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/log)
:::

##### Practical Task 11 -- Produce a density plot showing log10 transformed reaction time, correct responses, separately for each participant

ML asked all participants in a sample of people to read a selection of words, a sample of words from the language.

For each participant, we will have multiple observations and these observations will not be independent. One participant will tend to be slower or less accurate compared to another. 

- What does this look like?

:::{.callout-tip collapse="true"}
## Code
```{r}
#| label: fig-rt-correct-log-den-by-subj
#| fig-cap: "Density plot showing log10 transformed reaction time, correct responses, separately for each participant"
#| fig-alt: "The figure presents a grid of density plots showing log10 transformed reaction time, correct responses, separately for each participant. A dashed red line is drawn through all plots, indicating the mean RT. The curves vary in both the location of the peak and in the shape of the curve."
#| warning: false
#| message: false
#| fig-height: 10
#| fig-width: 8
ML.all.correct %>%
  group_by(subjectID) %>%
  mutate(mean_logrt = mean(logrt, na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(subjectID = fct_reorder(subjectID, mean_logrt)) %>%
  ggplot(aes(x = logrt)) +
  geom_density(size = 1.25) +
  facet_wrap(~ subjectID) +
  geom_vline(xintercept = 2.778807, colour = "red", linetype = 2) +
  scale_x_continuous(breaks = c(2.5,3)) +
  ggtitle("Plot showing distribution of logRT for each participant; red line shows mean log10 RT") +
  theme_bw()
```
:::

##### Practical Task 12 -- Can you work out how to adapt the plotting code to show a grid of histograms?

##### Practical Task 13 -- Can you work out how to adapt the code to show a grid of plots indicating the distribution of log RT by different items (instead of participants)?

For each stimulus word, there are multiple observations and these observations will not be independent. One stimulus may prove to be more challenging to all participants compared to another, eliciting slower or less accurate responses on average.

- What does this look like?

#### Practical Part 4: Analyze data with lm

##### Practical Task 14 -- Fit a linear model with log10 RT as the outcome and word frequency (`LgSUBTLCD`) as the predictor

We model the effects of interest, using all the data (hence, *complete pooling*) but ignoring the differences between participants. This means we can see something of the 'true' picture of our data through the linear model results though we should be aware that the linear model will miss important information, which the mixed-effects model will include, that would improve its performance.

:::{.callout-tip collapse="true"}
## Hint
The linear model is fit in R using the `lm()` function.
:::

:::{.callout-tip collapse="true"}
## Code
```{r}
ML.all.correct.lm  <- lm(logrt ~
                             
                             LgSUBTLCD,     
                           
                           data = ML.all.correct)

summary(ML.all.correct.lm)
```
:::

Vary the linear model using different outcomes or predictors.

-   The ML study data, like the CP study data, are rich with possibility. It would be useful to experiment with data like this because this will prepare you for similar analysis targets in future.
- Change the predictor from frequency to something else: what do you see when you visualize the relationship between outcome and predictor variables using scatterplots?

::: callout-tip
Specify linear models with different predictors: do the relationships you see in plots match the coefficients you see in the model estimates?

- You should both estimate the effects of variables *and* visualize the relationships between variables using scatterplots. 
- If you combine reflection on the model estimates with evaluation of what the plots show you then you will be able to see how reading model results and reading plots can reveal the correspondences between the two ways of looking at your data.
:::

#### Practical Part 5: Analyze data with lmer

##### Practical Task 15 -- Fit a linear mixed-effects model to estimate the effect of frequency on log RT while taking into account random effects

You should specify, here, a model of the frequency effect that takes into account:

- the random effect of participants on intercepts;
- the random effect of participants on the slopes of the frequency (`LgSUBTLCD`) effect;
- as well as the random effect of items on intercepts.

You can specify the random effects of participants allowing for covariance between the random intercepts and the random slopes.

:::{.callout-tip collapse="true"}
## Code
```{r}
ML.all.correct.lmer  <- lmer(logrt ~

                           LgSUBTLCD +

                           (LgSUBTLCD + 1|subjectID) +

                           (1|item_name),

                         data = ML.all.correct)

summary(ML.all.correct.lmer)
```

As will now be getting familiar, the code works as follows:

1.  `ML.all.correct.lmer  <- lmer(...)` creates a *linear mixed-effects model* object using the `lmer()` function.
2.  `logrt ~ LgSUBTLCD` the fixed effect in the model is expressed as a formula in which the outcome or dependent variable `logrt` is predicted `~` by the independent or predictor variable `LgSUBTLCD` word frequency.

The random effects part of the model is then specified as follows.

3.  We first have the random effects associated with random differences between participants:

-   `(...|subjectID)` adds random effects corresponding to random differences between sample groups (participants subjects) coded by the `subjectID` variable.
-   `(...1 |subjectID)` including random differences between sample groups (`subjectID`) in intercepts coded `1`.
-   `(LgSUBTLCD... |subjectID)` and random differences between sample groups (`subjectID`) in the slopes of the frequency effect coded by using the`LgSUBTLCD` variable name.

4.  Then, we have the random effects associated with random differences between stimuli:

-   `(1|item_name)` adds a random effect to account for random differences between sample groups (`item_name`) in intercepts coded `1`.

5.  `...(..., data = ML.all.correct)` specifies the data-set in which you can find the variables named in the model fitting code.
6.  Lastly, we can then specify `summary(ML.all.correct.lmer)` to get a summary of the fitted model.
:::

- Can you explain the results shown in the model summary?

You can be guided by the in-depth explanation of how. to read the analysis results in the [chapter on developing linear mixed-effects models](../part2/03-mixed.qmd#sec-dev-mixed-lmer-results).

#### Practical Part 6: Compare models with different random effects

##### Practical Task 16 -- Fit a linear mixed-effects model to estimate the effect of frequency on log RT while taking into account different sets of random effects

You can specify models where the fixed effects part is the same (e.g., you assume $RT \sim $ word frequency) but, for various reasons, you may suppose different random effects structures.

In this scenario --- the model fixed effects part is the same but the random effects are different --- you may be required to evaluate which model structure fits the data better, to the extent that that can be assessed, given your sample data.

1. To begin, fit a model with just the fixed effects of intercept and frequency, and the random effects of participants or items on intercepts only. 

- We exclude the `(LgSUBTLCD + ...|subjectID)` specification for the random effect of participants on the slope of the frequency `LgSUBTLCD` effect.

We use REML fitting, if we want to compare (as we will) models with the same fixed effects but different random effects.

:::{.callout-tip collapse="true"}
## Code
```{r}
ML.all.correct.lmer.REML.si  <- lmer(logrt ~ LgSUBTLCD + 
                                    
                                          (1|subjectID) + (1|item_name),

       data = ML.all.correct, REML = TRUE)

summary(ML.all.correct.lmer.REML.si)
```

If you look at the code chunk, you can see that:

-   `REML = TRUE` is the only change to the code: it specifies the change in model fitting method;
-   also, I changed the model name to `ML.all.correct.lmer.REML.si` to be able to distinguish the maximum likelihood from the restricted maximum likelihood model.
:::

Following @baayen2008, we can then run a series of models with just one random effect. 

2. Next, fit a model with just the random effect of items on intercepts.

:::{.callout-tip collapse="true"}
## Code
```{r}
ML.all.correct.lmer.REML.i  <- lmer(logrt ~

       LgSUBTLCD + (1|item_name),

       data = ML.all.correct, REML = TRUE)

summary(ML.all.correct.lmer.REML.i)
```
:::

3. Next, fit a model with just the random effect of participants on intercepts.

:::{.callout-tip collapse="true"}
## Code
```{r}
ML.all.correct.lmer.REML.s  <- lmer(logrt ~

       LgSUBTLCD + (1|subjectID),

       data = ML.all.correct, REML = TRUE)

summary(ML.all.correct.lmer.REML.s)
```
:::

::: callout-tip
Notice how each different model has a distinct name.

- We will use these different names to compare the models, next.
:::

##### Practical Task 17 -- Compare the different models using `anova()`

If we now run Likelihood Ratio Test comparisons of these models, we are effectively examining if one of the random effects can be dispensed with: if its inclusion makes no difference to the likelihood of the model then it is not needed. 

- Is the random effect of subjects on intercepts justified?

:::{.callout-tip collapse="true"}
## Hint
Compare models, first, with `ML.all.correct.lmer.REML.si` versus without `ML.all.correct.lmer.REML.i` the random effect of subjects on intercepts.
:::

:::{.callout-tip collapse="true"}
## Code
```{r}
anova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.i, refit = FALSE)
```
:::

- Is the random effect of items on intercepts justified?

:::{.callout-tip collapse="true"}
## Hint
Compare models with `ML.all.correct.lmer.REML.si` versus without `ML.all.correct.lmer.REML.s` the random effect of items on intercepts.
:::

:::{.callout-tip collapse="true"}
## Code
```{r}
anova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.s, refit = FALSE)
```
:::

We compare models using the `anova()` function.

-   `anova()` does the model comparison, for the models named in the list in brackets.

You can do this for the foregoing series of models but notice that in the code we specify:

- `refit = FALSE`

What happens if we do not add that bit?

What you will see if you run the `anova()` function call, without the`refit = FALSE` argument -- try it -- is that you will then get the warning `refitting model(s) with ML (instead of REML)`. 

Why? 
The immediate reason for this warning is that we have to specify `refit = FALSE` because otherwise R will compare ML fitted models. The refitting occurs by default.

What is the reason for the imposition of this default?

@Pinheiro2000aa advise that if one is fitting models with random effects the estimates are more accurate if the models are fitted using Restricted Maximum Likelihood (REML). That is achieved in the `lmer()` function call by adding the argument `REML=TRUE`. 
@Pinheiro2000aa further recommend (see, e.g., pp.82-) that if you compare models:

- with the same fixed effects 
- but with varying random effects
- then the models *should be fitted using Restricted Maximum Likelihood*.

Read a discussion of the issues, with references, in [chapter on developing linear mixed-effects models](../part2/03-mixed.qmd#sec-dev-mixed-anova-advice).
See that chapter, also, for an in-depth account of what the 'anova()` results tell you.

##### Practical Task 18 -- Now examine whether random slopes are required

Is it justified to add a random effect of participants on the slopes of the frequency effect?

- How should you address this question?

First, you will need to fit a model incorporating the random effect of participants on the slopes of the frequency effect.

:::{.callout-tip collapse="true"}
## Hint
You will want to make sure that this random slopes model is comparable to the next-most complex model: `ML.all.correct.lmer.REML.si`.
:::

:::{.callout-tip collapse="true"}
## Code
```{r}
ML.all.correct.lmer.REML.slopes  <- lmer(logrt ~ LgSUBTLCD + 
                                           
                                          (LgSUBTLCD + 1|subjectID) +
                                          (1|item_name),

       data = ML.all.correct, REML = TRUE)
```

Looking at the code:

-   With `(LgSUBTLCD + 1 |subjectID)` we specify a random effect of subjects on intercepts and on the slope of the frequency effects.
-   We do not specify -- it happens by default -- the estimation of the covariance of random differences among subjects in intercepts and random differences among subjects in the slope of the frequency effect.
:::

Then you will want to compare two models that are the same *except* for the incorporation of the the random effect of participants on the slopes of the frequency effect.

:::{.callout-tip collapse="true"}
## Code
```{r}
anova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.slopes, 
      refit = FALSE)
```
:::

#### Practical Part 7: p-values and significance using `lmerTest`

If you look at the fixed effects summary, you can see that we do not get p-values by default. To calculate p-values, we need to count residual degrees of freedom. The authors of the `{lme4}` library that furnishes the `lmer()` function do not [as e.g. @Baayen2008 discuss] think that it is sensible to estimate the residual degrees of freedom for a model in terms of the number of observations. This is because the number of observations concerns one level of a multilevel data-set that might be structured with respect to some number of subjects, some number of items. This means that one cannot then accurately calculate p-values to go with the t-tests on the coefficients estimates; therefore they do not.

##### Practical Task 19 -- Get p-values using lmerTest

We can run mixed-effects models with p-values from significance tests on the estimates of the fixed effects coefficients using the `library(lmerTest)`.

:::{.callout-tip collapse="true"}
## Code
```{r}
#| message: false
#| warning: false
library(lmerTest)

ML.all.correct.lmer.REML.slopes  <- lmer(logrt ~ LgSUBTLCD + 
                                           
                                            (LgSUBTLCD + 1|subjectID) +
                                            (1|item_name),

                                          data = ML.all.correct, REML = TRUE)

summary(ML.all.correct.lmer.REML.slopes)
```
:::

Basically, the call to access the `lmerTest` library ensures that when we run the `lmer()` function we get a calculation of an approximation to the denominator degrees of freedom that enables the calculation of the p-value for the t-test for the fixed effects coefficient.

::: callout-tip
An alternative approach to doing significance tests of hypothesized effects, is to compare models with versus without the effect of interest.
:::

#### Practical Part 8: Run your own sequence of lmer models

##### Practical Task 20 -- Pick your own predictors and compare the impact of specifying different sets of random effects

It will be useful for you to examine model comparisons with a different set of models for the same data.

You could try to run a series of models in which the fixed effects variable is something different, for example, the effect of word `Length` or the effect of orthographic neighbourhood size `Ortho_N`.

- Run models with these variables or pick your own set of predictor variables to get estimates of fixed effects.

In this series of models, specify a set of models where:

- Each model has the same fixed effect e.g. `logrt ~ Length`;
- But you vary the random effects components so that, at the end, you can assess what random effects are justified by better model fit to data.

In the following, I set out another series of models as an example of what you can do.

1. Random effects of participants on intercepts and slopes, random effect of items on intercepts

:::{.callout-tip collapse="true"}
## Code
```{r}
#| message: false
#| warning: false
ML.all.correct.lmer.REML.slopes  <- lmer(logrt ~ Length + 
                                           
                                           (Length + 1|subjectID) +
                                           (1|item_name),
                                         
                                         data = ML.all.correct, REML = TRUE)

summary(ML.all.correct.lmer.REML.slopes)
```
:::

2. Random effect of participants on intercepts, random effect of items on intercepts

:::{.callout-tip collapse="true"}
## Code
```{r}
#| message: false
#| warning: false
ML.all.correct.lmer.REML.si  <- lmer(logrt ~ Length + 
                                       
                                       (1|subjectID) + 
                                       (1|item_name),
                                     
                                     data = ML.all.correct, REML = TRUE)

summary(ML.all.correct.lmer.REML.si)
```
:::

3. Random effect of items on intercepts

:::{.callout-tip collapse="true"}
## Code
```{r}
#| message: false
#| warning: false
ML.all.correct.lmer.REML.i  <- lmer(logrt ~ Length + 
                                       
                                       (1|item_name),
                                     
                                     data = ML.all.correct, REML = TRUE)

summary(ML.all.correct.lmer.REML.i)
```
:::

4. Random effect of participants on intercepts

:::{.callout-tip collapse="true"}
## Code
```{r}
#| message: false
#| warning: false
ML.all.correct.lmer.REML.s  <- lmer(logrt ~ Length + 
                                      
                                      (1|subjectID),
                                    
                                    data = ML.all.correct, REML = TRUE)

summary(ML.all.correct.lmer.REML.s)
```
:::

5. Model comparisons

I would compare the models in the sequence, shown in the foregoing, one pair of models at a time, to keep it simple.

- When you look at the model comparison, ask: is the difference between the models a piece of complexity (an effect) whose inclusion in the more complex model is justified or warranted by improved model fit to data?

:::{.callout-tip collapse="true"}
## Code
If we run model comparisons, we can see that random effects of subjects or of items on intercepts, as well as random effect of subjects on slopes, are justified by significantly improved model fit to data.

```{r}
#| message: false
#| warning: false
anova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.s, refit=FALSE)
anova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.i, refit=FALSE)
anova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.slopes, refit=FALSE)
```
:::

#### Practical Part 9: Optional -- reproduce the plots in the chapter and slides

Reproduce the plots in the chapter and slides

This part is optional for you to view and work with.

- Run the code, and consider the code steps *only* if you are interested in how the materials for the book chapter were created.

##### Practical Task 21 -- Compare no versus complete versus partial pooling estimates

In the [chapter on developing linear mixed-effects models](../part2/03-mixed.qmd#sec-dev-mixed-no-pooling) I talk about pooling, no pooling and partial pooling, and the impact on estimates.

To produce the plots I discuss there, I adapted Tristan Mahr's code, here:

- <https://www.tjmahr.com/plotting-partial-pooling-in-mixed-effects-models/>

We work through a sequence of fitting models, getting estimates, and producing plots.

1. Get no pooling estimates by running `lm()` for each participant.

:::{.callout-tip collapse="true"}
## Code
```{r}
df_no_pooling <- lmList(logrt ~
                          
                          LgSUBTLCD | subjectID, ML.all.correct) %>% 
  coef() %>% 
  # Subject IDs are stored as row-names. Make them an explicit column
  rownames_to_column("subjectID") %>% 
  rename(Intercept = `(Intercept)`, Slope_frequency = LgSUBTLCD) %>% 
  add_column(Model = "No pooling")

# -- you can uncomment and inspect the tibble produced by these lines of code
# summary(df_no_pooling)
```
:::

2. Make the `subjectID` and `Model` label vectors a factor.

:::{.callout-tip collapse="true"}
## Code
```{r}
df_no_pooling$subjectID <- as.factor(df_no_pooling$subjectID)
df_no_pooling$Model <- as.factor(df_no_pooling$Model)
```
:::

3. In contrast, we might consider a complete pooling model where all the information from the participants is combined together. 

- Fit a single mdoel for all the data pooled together.

:::{.callout-tip collapse="true"}
## Code
```{r}
m_pooled <- lm(logrt ~ LgSUBTLCD, ML.all.correct) 
```
:::

4. Repeat the intercept and slope terms for each participant.

:::{.callout-tip collapse="true"}
## Code
```{r}
df_pooled <- data_frame(
  Model = "Complete pooling",
  subjectID = unique(ML.all.correct$subjectID),
  Intercept = coef(m_pooled)[1], 
  Slope_frequency = coef(m_pooled)[2])

# -- inspect
# summary(df_pooled)
```
:::

5. Make the model label vector a factor.

:::{.callout-tip collapse="true"}
## Code
```{r}
df_pooled$Model <- as.factor(df_pooled$Model)
```
:::

6. We can compare these two approaches: instead of calculating the regression lines with `stat_smooth()`, we can use `geom_abline()` to draw the lines from our dataframe of intercept and slope parameters.

Join the raw data so we can use plot the points and the lines.

:::{.callout-tip collapse="true"}
## Code
```{r}
df_models <- bind_rows(df_pooled, df_no_pooling) %>% 
  left_join(ML.all.correct, by = "subjectID")
```
:::

7. Produce the plot:

:::{.callout-tip collapse="true"}
## Code
```{r}
p_model_comparison <- ggplot(df_models) + 
  aes(x = LgSUBTLCD, y = logrt) + 
  # Set the color mapping in this layer so the points don't get a color
  geom_point(alpha = .05) +
  geom_abline(aes(intercept = Intercept, 
                  slope = Slope_frequency, 
                  color = Model),
              size = .75) +
  facet_wrap(~ subjectID) +
  scale_x_continuous(breaks = 1.5:4 * 1) + 
  theme_bw() + 
  theme(legend.position = "top")

p_model_comparison
```
:::

8. Compare no vs. complete  vs. partial pooling estimates

- Mixed-effects models represent partial pooling of data.

We fit a separate line for each cluster of data, one for each participant.

- the `lmList()` function in `{lme4}` automates this process.

:::{.callout-tip collapse="true"}
## Code
```{r}
df_no_pooling <- lmList(logrt ~
                          
                          LgSUBTLCD | subjectID, ML.all.correct) %>% 
  coef() %>% 
  # Subject IDs are stored as row-names. Make them an explicit column
  rownames_to_column("subjectID") %>% 
  rename(Intercept = `(Intercept)`, Slope_frequency = LgSUBTLCD) %>% 
  add_column(Model = "No pooling")

# -- inspect the tibble produced by these lines of code
# summary(df_no_pooling)

# -- make the subjectID and model label vectors a factor
df_no_pooling$subjectID <- as.factor(df_no_pooling$subjectID)
df_no_pooling$Model <- as.factor(df_no_pooling$Model)
```
:::

In contrast, we might consider a complete pooling model where all the information from the participants is combined together.

- We fit a single model on all the data pooled together.

:::{.callout-tip collapse="true"}
## Code
```{r}
m_pooled <- lm(logrt ~ LgSUBTLCD, ML.all.correct) 

# -- repeat the intercept and slope terms for each participant
df_pooled <- data_frame(
  Model = "Complete pooling",
  subjectID = unique(ML.all.correct$subjectID),
  Intercept = coef(m_pooled)[1], 
  Slope_frequency = coef(m_pooled)[2])

# -- inspect
# summary(df_pooled)

# -- make the model label vector a factor
df_pooled$Model <- as.factor(df_pooled$Model)
```
:::

We can fit a mixed-effects model including the fixed effects of the intercept and the frequency effect

- as well as the random effects due to differences between participants (subject ID) in intercepts or in the slope of the frequency effect, as well as differences between items (item_name) in intercepts.

:::{.callout-tip collapse="true"}
## Code
```{r}
ML.all.correct.lmer  <- lmer(logrt ~
                               
                               LgSUBTLCD + (LgSUBTLCD + 1|subjectID) + (1|item_name),     
                             
                             data = ML.all.correct, REML = FALSE)
# summary(ML.all.correct.lmer)
```
:::

To visualize these estimates, we extract each participants intercept and slope using `coef()`.

:::{.callout-tip collapse="true"}
## Code
```{r}
# -- make a dataframe with the fitted effects
df_partial_pooling <- coef(ML.all.correct.lmer)[["subjectID"]] %>% 
  rownames_to_column("subjectID") %>% 
  as_tibble() %>% 
  rename(Intercept = `(Intercept)`, Slope_frequency = LgSUBTLCD) %>% 
  add_column(Model = "Partial pooling")

# -- inspect the tibble produced by these lines of code
# summary(df_partial_pooling)

# -- make the subjectID and model label vectors both factors
df_partial_pooling$subjectID <- as.factor(df_partial_pooling$subjectID)
df_partial_pooling$Model <- as.factor(df_partial_pooling$Model)
```
:::

Visualize the comparison by producing a plot using a data-set consisting of of all three sets of model estimates.

:::{.callout-tip collapse="true"}
## Code
```{r}
df_models <- rbind(df_pooled, df_no_pooling, df_partial_pooling) %>%
  left_join(ML.all.correct, by = "subjectID")

# -- inspect the differences between the data-sets
# summary(df_pooled)
# summary(df_no_pooling)
# summary(df_partial_pooling)
# summary(ML.all.correct)
# summary(df_models)

# -- we  produce a plot showing the no-pooling, complete-pooling and partial-pooling (mixed effects)
# estimates

p_model_comparison <- ggplot(df_models) +
  aes(x = LgSUBTLCD, y = logrt) +
  # Set the color mapping in this layer so the points don't get a color
  geom_point(alpha = .05) +
  geom_abline(aes(intercept = Intercept,
                  slope = Slope_frequency,
                  color = Model),
              size = .75) +
  facet_wrap(~ subjectID) +
  scale_x_continuous(breaks = 1.5:4 * 1) +
  theme_bw() +
  theme(legend.position = "top")

p_model_comparison
```
:::

9. Zoom in on a few participants:

- use `filter()` to select a small number of participants.

:::{.callout-tip collapse="true"}
## Code
```{r}
df_zoom <- df_models %>% 
  filter(subjectID %in% c("EB5", "JP3", "JL3", "AA1"))

# -- re-generate the plot with this zoomed-in selection data-set

p_model_comparison <- ggplot(df_zoom) + 
  aes(x = LgSUBTLCD, y = logrt) + 
  # Set the color mapping in this layer so the points don't get a color
  geom_point(alpha = .05) +
  geom_abline(aes(intercept = Intercept, 
                  slope = Slope_frequency, 
                  color = Model),
              size = .75) +
  facet_wrap(~ subjectID) +
  scale_x_continuous(breaks = 1.5:4 * 1) + 
  theme_bw() + 
  theme(legend.position = "top")

p_model_comparison
```
:::

##### Practical Task 22 -- Illustrate shrinkage

The partial pooling model pulls (or shrinks) more extreme estimates towards an overall average.
We can visualize this shrinkage effect by plotting a scatterplot of intercept and slope  parameters from each model and connecting estimates for the same participant.

- We use arrows to connect the different estimates for each participant, different estimates from no-pooling (per-participant) compared to partial-pooling (mixed-effects) models.
- The plot shows how more extreme estimates are shrunk towards the global average estimate.

:::{.callout-tip collapse="true"}
## Code
```{r}
df_fixef <- data_frame(
  Model = "Partial pooling (average)",
  Intercept = fixef(ML.all.correct.lmer)[1],
  Slope_frequency = fixef(ML.all.correct.lmer)[2])

# Complete pooling / fixed effects are center of gravity in the plot
df_gravity <- df_pooled %>% 
  distinct(Model, Intercept, Slope_frequency) %>% 
  rbind(df_fixef)
# df_gravity
#> # A tibble: 2 x 3
#>                       Model Intercept Slope_Days
#>                       <chr>     <dbl>      <dbl>
#> 1          Complete pooling  252.3207   10.32766
#> 2 Partial pooling (average)  252.5426   10.45212

df_pulled <- rbind(df_no_pooling, df_partial_pooling)

ggplot(df_pulled) + 
  aes(x = Intercept, y = Slope_frequency, color = Model) + 
  geom_point(size = 2) + 
  geom_point(data = df_gravity, size = 5) + 
  # Draw an arrow connecting the observations between models
  geom_path(aes(group = subjectID, color = NULL), 
            arrow = arrow(length = unit(.02, "npc"))) + 
  # Use ggrepel to jitter the labels away from the points
  ggrepel::geom_text_repel(
    aes(label = subjectID, color = NULL),
    data = df_no_pooling) +
  # Don't forget 373
  # ggrepel::geom_text_repel(
  #   aes(label = subjectID, color = NULL), 
  #   data = filter(df_partial_pooling, Subject == "373")) + 
  theme(legend.position = "right") + 
  # ggtitle("Pooling of regression parameters") + 
  xlab("Intercept estimate") + 
  ylab("Slope estimate") + 
  scale_color_brewer(palette = "Dark2") +
  theme_bw()
```
:::

### The answers

After the practical class, we will reveal the answers that are currently hidden.

The answers version of the webpage will present my answers for questions, and some extra information where that is helpful.

