[
  {
    "objectID": "PSYC411/part1/Week5.html",
    "href": "PSYC411/part1/Week5.html",
    "title": "Week 5. Testing differences between groups",
    "section": "",
    "text": "This week, there are two mini lectures, and the practical workbook working with R-studio.\nBefore the practical on Tuesday, please try to work through the practical workbook in your group.\nBring your questions (and/or answers) to the practical.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 5. Testing differences between groups"
    ]
  },
  {
    "objectID": "PSYC411/part1/Week5.html#overview",
    "href": "PSYC411/part1/Week5.html#overview",
    "title": "Week 5. Testing differences between groups",
    "section": "",
    "text": "This week, there are two mini lectures, and the practical workbook working with R-studio.\nBefore the practical on Tuesday, please try to work through the practical workbook in your group.\nBring your questions (and/or answers) to the practical.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 5. Testing differences between groups"
    ]
  },
  {
    "objectID": "PSYC411/part1/Week5.html#learning-goals",
    "href": "PSYC411/part1/Week5.html#learning-goals",
    "title": "Week 5. Testing differences between groups",
    "section": "5.2 Learning Goals",
    "text": "5.2 Learning Goals\n\nUnderstand when t-tests are appropriately applied to data\nUnderstand the distinction between paired and independent t-tests\nInterpret p-values from t-tests\nDetermine how a paired t-test is calculated\nDetermine how an independent t-test is calculated\nUnderstand effect sizes for t-tests\nBe able to effectively interpret t-test results\nBe able to accurately present t-test results in research reports",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 5. Testing differences between groups"
    ]
  },
  {
    "objectID": "PSYC411/part1/Week5.html#lectures-and-slides",
    "href": "PSYC411/part1/Week5.html#lectures-and-slides",
    "title": "Week 5. Testing differences between groups",
    "section": "5.3 Lectures and slides",
    "text": "5.3 Lectures and slides\n\n5.3.1 Lectures\nWatch Lecture week 5 part 1:\n\nWatch Lecture week 5 part 2:\n\nTake the quiz (not assessed) on the lecture materials.\n\n\n5.3.2 Slides\nDownload the lecture slides for:\n\npart 1\npart 2",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 5. Testing differences between groups"
    ]
  },
  {
    "objectID": "PSYC411/part1/Week5.html#practical-materials",
    "href": "PSYC411/part1/Week5.html#practical-materials",
    "title": "Week 5. Testing differences between groups",
    "section": "5.4 Practical Materials",
    "text": "5.4 Practical Materials\n\n5.4.1 Workbook\n\nPart 1 is some revision from the last 4 weeks.\nPart 2 covers running an independent t-test.\nPart 3 covers running a paired t-test.\nPart 4 provides more practice running paired t-tests.\nPart 5 presents some extras - exploring different datasets and running t-tests on those data.\n\n\n5.4.1.1 Part 1: Revision\n\nTask 1: Checklist: What I can now do\nYou should be able to answer yes to all the following. If you can’t yet, go back to the previous workbooks and repeat your working until you can answer yes, being able to type in and run the commands without referring to your notes.\n\nI can open R-studio\nI can open new libraries using library()\nI can make an R script file\nI can input a file into an object in R-studio using read_csv()\nI can join two files together using inner_join()\nI can select certain variables from an object using select()\nI can select subsets of data using filter() (e.g., I can select participants in two conditions from a data set containing participants in four conditions)\nI can make new variables using mutate()\nI can arrange data according to subsets using group_by()\nI can change format of data from wide to long format using pivot_longer\nI can change format of data from long to wide format using pivot_wider\nI can produce summaries of means and standard deviations for subsets of data after applying group_by() using summarise()\nI can draw histograms of single variables, point plots of two ratio/interval/ordinal variables, bar plots of counts, and box plots of one categorical and one ratio/interval/ordinal variable using ggplot()\nI can run a Chi-squared test and Cramer’s V test using chisq.test() and cramersV()\nI can interpret the results of a Chi-squared test and Cramer’s V test and write up a simple report of the results.\nI can save an R script file.\n\n\n\n\n\n\n\nANSWER\n\n\n\n\n\nHere are some examples of the commands/functions in use:\n\nrm(list=ls())\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# from week4 practical\ndat &lt;- read_csv(\"data/wk4/PSYC411-shipley-scores-anonymous-17_24.csv\"); \n\nRows: 270 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): english_status, Gender\ndbl (6): subject_ID, Age, Shipley_Voc_Score, Gent_1_score, Gent_2_score, aca...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndat$academic_year &lt;- as.factor(dat$academic_year)\n#View(dat)\n\n# from week2 practical\ndat2 &lt;- read_csv(\"data/wk2/ahicesd.csv\")\n\nRows: 992 Columns: 50\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (50): id, occasion, elapsed.days, intervention, ahi01, ahi02, ahi03, ahi...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npinfo &lt;- read_csv(\"data/wk2/participantinfo.csv\")\n\nRows: 295 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (6): id, intervention, sex, age, educ, income\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nall_dat &lt;- inner_join(x = dat2, y = pinfo, by = c(\"id\", \"intervention\"))\nsummarydata &lt;- select(all_dat, id, ahiTotal, cesdTotal, age, occasion)\ndat_17 &lt;- filter(dat, academic_year == \"201718\")\nall_dat2 &lt;- group_by(all_dat, intervention, occasion)\nsummarise(all_dat2, mean(ahiTotal), sd(ahiTotal), n())\n\n`summarise()` has grouped output by 'intervention'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 24 × 5\n# Groups:   intervention [4]\n   intervention occasion `mean(ahiTotal)` `sd(ahiTotal)` `n()`\n          &lt;dbl&gt;    &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt; &lt;int&gt;\n 1            1        0             68.4           14.0    72\n 2            1        1             69.5           13.4    30\n 3            1        2             70.3           15.7    38\n 4            1        3             75.0           12.8    29\n 5            1        4             76.5           16.2    36\n 6            1        5             75.5           14.5    27\n 7            2        0             68.8           13.0    76\n 8            2        1             71.6           12.5    48\n 9            2        2             73.0           14.0    48\n10            2        3             72.5           14.2    43\n# ℹ 14 more rows\n\nggplot(dat, aes(x = Age)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\nWarning: Removed 193 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\nggplot(dat, aes(x = Shipley_Voc_Score, y = Gent_1_score)) + geom_point()\n\nWarning: Removed 10 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nggplot(dat, aes(x = Shipley_Voc_Score, y = Gent_1_score)) + geom_point() + geom_smooth( method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 10 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 10 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nall_dat$occasion &lt;- as.factor(all_dat$occasion)\nggplot(all_dat, aes(x = occasion, y = ahiTotal)) + geom_boxplot()\n\n\n\n\n\n\n\ndat_long &lt;- pivot_longer(dat, names_to = \"test\", values_to = \"score\", cols = c(\"Gent_1_score\", \"Gent_2_score\"))\n# or:\ndat_long &lt;- pivot_longer(dat, names_to = \"test\", values_to = \"score\", cols = starts_with(\"Gent\"))\ndat_wide &lt;- pivot_wider(dat_long, names_from = \"test\", values_from = \"score\")\n\nlibrary(lsr)\nchisq.test(x = all_dat$occasion, y = all_dat$intervention)\n\n\n    Pearson's Chi-squared test\n\ndata:  all_dat$occasion and all_dat$intervention\nX-squared = 9.7806, df = 15, p-value = 0.8333\n\ncramersV(x = all_dat$occasion, y = all_dat$intervention)\n\n[1] 0.05732779\n\n\n\n\n\n\n\n\n5.4.1.2 Part 2: Running an independent t-test\n\nTask 2: Load, prepare, and explore the data\n\nClear out R using rm(list=ls())\nLoad again the data set on the Shipley and Gent vocabulary scores from week 4.\nSet the research question: do people who self-identify as male or female have different scores on the Gent vocabulary test? The research hypothesis is: “People who identify as male or female have different vocabulary scores”. What is the null hypothesis?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThere is no difference between people who self-identify as male or female on vocabulary scores.\n\n\n\n\nTo test the research hypothesis, we will filter people who self-identify as male or female from the data set. To be inclusive, additional research questions would be part of your research project to analyse also people who self-identify as other gender. Run this command to extract a subset of the data (note that the | stands for “or”, and means Gender matches male or gender matches female:\n\n\ndat2 &lt;- filter(dat, Gender == 'Male' | Gender == 'Female')\n\n\nDraw a box plot of Gent vocabulary test 1 scores by gender. For a box plot, note that we need data in “long format”, where each observation is on one line, and we have a column that indicates which condition (in this case Gender) the participant is in. Does it look like there might be a gender effect? What is the direction of the effect?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\ndat &lt;- read_csv(\"data/wk4/PSYC411-shipley-scores-anonymous-17_24.csv\")\n\nRows: 270 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): english_status, Gender\ndbl (6): subject_ID, Age, Shipley_Voc_Score, Gent_1_score, Gent_2_score, aca...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# remember to change academic year to be a nominal/categorical variable, rather than a number:\ndat$academic_year &lt;- as.factor(dat$academic_year)\ndat2 &lt;- filter(dat, Gender == 'Male' | Gender == 'Female')\nggplot(dat2, aes(x = Gender, y = Gent_1_score)) + geom_boxplot()\n\nWarning: Removed 4 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nThe graph indicates that maybe males are slightly higher, but looks like a lot of overlap.\n\n\n\n\nNote that unless we had filtered the data, the box plot would contain ‘NA’ as well, which stands for missing data. In a data set it’s always a good idea to call missing data ‘NA’ rather than just leaving them blank because this could be interpreted as a zero or as an error of filling in data. Missing values make things untidy, so it’s good practice to focus only on the variables we need for the t-test and remove all other missing values. Use select() to get just the Gender and Gent_1_score variables, and put this in a new object called ‘dat3’.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\ndat3 &lt;- select(dat2, Gender, Gent_1_score)\n\n\n\n\n\nNext, in order to run a t-test we have to remove any rows of data which contain a ‘NA’ - either in the Gender or the Gent_1_score variables. We do this using drop_na(dat3), put the result in a new object called ‘dat4’. Run this command:\n\n\ndat4 &lt;- drop_na(dat3)\n\n\nNow, redraw the box plot from Step 21. Check there are just two groups still.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\ndat3 &lt;- select(dat2, Gender, Gent_1_score)\ndat4 &lt;- drop_na(dat3)\nggplot(dat2, aes(x = Gender, y = Gent_1_score)) + geom_boxplot()\n\nWarning: Removed 4 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nYes, two groups.\n\n\n\n\nCompute mean and SDs for people who self-identify as male or female on Gent vocabulary test 1 scores.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse group_by() and summarise().\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\ndat5 &lt;- group_by(dat4, Gender)\nsummarise(dat5, mean(Gent_1_score), sd(Gent_1_score))\n\n# A tibble: 2 × 3\n  Gender `mean(Gent_1_score)` `sd(Gent_1_score)`\n  &lt;chr&gt;                 &lt;dbl&gt;              &lt;dbl&gt;\n1 Female                 58.6               14.0\n2 Male                   62.2               14.5\n\n# Or, if you know how to use the %&gt;% (pipe) ...:\ndat4 %&gt;% group_by(Gender) %&gt;% summarise(mean(Gent_1_score), sd(Gent_1_score))\n\n# A tibble: 2 × 3\n  Gender `mean(Gent_1_score)` `sd(Gent_1_score)`\n  &lt;chr&gt;                 &lt;dbl&gt;              &lt;dbl&gt;\n1 Female                 58.6               14.0\n2 Male                   62.2               14.5\n\n\n\n\n\n\n\nTask 3: Run the independent t-test and measure effect size\n\nConduct an independent t-test using this command:\n\n\nt.test(Gent_1_score ~ Gender, data = dat4 ) \n\n\n\n‘Gent_1_score ~ Gender’ : the ~ can be interpreted as ‘by’, i.e., compute Gent_1_score by Gender\n\n\nThe results should look like this, do yours?\n\n    Welch Two Sample t-test\n\ndata:  Gent_1_score by Gender\nt = -1.6693, df = 83.298, p-value = 0.09881\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n -7.982423  0.697279\nsample estimates:\nmean in group Female   mean in group Male \n            58.57561             62.21818 \n\nThe key part of the results to look at is the one that has t = -1.6693, df = 83.298, p-value = 0.09881. This is the result that you report: t(83.30) = -1.67, p = .099.\n\nThe value is negative because the function includes Female before Male - and Female score is lower than Male score. What matters is how far away from zero the t-test is (either positively or negatively). The df value is slightly odd because the t.test() function figures out degrees of freedom in a technical way which takes into account differences in variance in the data between the two groups. We can just use the value that the t.test() function gives us.\n\nIs this a significant difference?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nNo, it isn’t. The p-value is greater than 0.05.\n\n\n\n\nNow we need to compute the effect size, using Cohen’s d. You need to load the library lsr then use this command:\n\n\ncohensD(Gent_1_score ~ Gender, method = \"unequal\", data = dat4)\n\n\nIt’s pretty much the same as the t-test() command except that we use ‘method = ’unequal’. For a paired t-test you would use ‘method = ’paired’\n\n\nWhat is the effect size? Make a brief report of the results - reporting means and SDs, the t-test, p-value, and Cohen’s d. Discuss your brief report in your group.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nd = 0.27\nAn ideal brief report of the results would state what the research question/hypothesis is, describe the means and SDs of the groups being compared, and then report the t-test statistic, with p-value and Cohen’s d effect size, then provide a brief interpretation of what the results mean.\nBased on Hyde and Linn (1988), we hypothesised that people who self-identify as female may score slightly higher than males in terms of vocabulary scores. Males (mean = 62.2, SD = 14.5) scored higher than females (mean = 58.6, SD = 14.0) on the first time participants attempted the Gent vocabulary test, however this difference was not significant, t(83.30) = -1.67, p = .099, Cohen’s d = 0.27. We did not find evidence to support the hypothesis.\nReference\nHyde, J. S., & Linn, M. C. (1988). Gender differences in verbal ability: A meta-analysis. Psychological Bulletin, 104(1), 53–69. https://doi.org/10.1037/0033-2909.104.1.53\n\n\n\n\nMake sure all commands are in the source window, save them as a new R script file.\n\n\n\nTask 4: Practise running another independent t-test\n\nNext research question: do people who are native English speakers have different vocabulary scores than those who learned English as a second language? What is the research hypothesis and the null hypothesis?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nResearch Hypothesis: People who speak English as a native language have higher vocabulary scores than those with English as a second language.\nNull Hypothesis: There is no difference in vocabulary scores between native English and second language English speakers.\n\n\n\n\nRepeat the Steps 22-30 in Tasks 2 and 3 except using english_status in place of Gender throughout.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\ndat &lt;- read.csv(\"data/wk4/PSYC411-shipley-scores-anonymous-17_24.csv\")\ndat2 &lt;- select(dat, english_status, Gent_1_score)\ndat3 &lt;- drop_na(dat2)\nggplot(dat3, aes(x = english_status, y = Gent_1_score)) + geom_boxplot()\n\n\n\n\n\n\n\ndat4 &lt;- group_by(dat3, english_status)\nsummarise(dat4, mean(Gent_1_score), sd(Gent_1_score), n())\n\n# A tibble: 2 × 4\n  english_status `mean(Gent_1_score)` `sd(Gent_1_score)` `n()`\n  &lt;chr&gt;                         &lt;dbl&gt;              &lt;dbl&gt; &lt;int&gt;\n1 ESL                            49.1              15.3     95\n2 native                         65.2               9.36   163\n\nt.test(Gent_1_score ~ english_status, data = dat3 )\n\n\n    Welch Two Sample t-test\n\ndata:  Gent_1_score by english_status\nt = -9.2983, df = 135.56, p-value = 3.256e-16\nalternative hypothesis: true difference in means between group ESL and group native is not equal to 0\n95 percent confidence interval:\n -19.57080 -12.70599\nsample estimates:\n   mean in group ESL mean in group native \n            49.09474             65.23313 \n\ncohensD(Gent_1_score ~ english_status, method = \"unequal\", data = dat3 )\n\n[1] 1.27044\n\n\n\n\n\n\nWrite a brief report of the results, including means and SDs for native speakers and ESL speakers, t-test, p-value, and Cohen’s d. Discuss your report in your group.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe hypothesised that English as an additional language would result in lower overall vocabulary scores than English as a first language, because of the relation between length of time learning a language and language skills (Davies et al., 2017). Native English speakers (mean = 65.4, SD = 9.8) scored significantly higher on the Gent vocabulary test than speakers of English as a second language (mean = 48.3, SD = 15.7), t(127.79) = -9.08, p &lt; .001, Cohen’s d = 1.30, a large effect size. Thus, the hypothesis was supported, with native English speakers scoring significantly higher on the Gent vocabulary test than those with English as an additional language.\nReference\nDavies, R. A. I., Arnell, R., Birchenough, J., Grimmond, D., & Houlson, S. (2017). Reading Through the Life Span: Individual Differences in Psycholinguistic Effects. Journal of Experimental Psychology: Learning, Memory, and Cognition, 43(8), 1298-1338. https://doi.org/10.1037/xlm0000366\n\n\n\n\nSave your R script file.\n\n\n\n\n5.4.1.3 Part 3: Conducting a paired t-test\n\nTask 5: Conducting a paired t-test\n\nClear out R-studio before we get started again using rm(list=ls())\nWe are going to investigate again the data from this paper: Woodworth, R.J., O’Brien-Malone, A., Diamond, M.R. and Schuez, B., 2017. Data from, “Web-based Positive Psychology Interventions: A Reexamination of Effectiveness”. Journal of Open Psychology Data, 6(1).\n\nOur research question is whether happiness scores are affected by the interventions. We will look at the pre-test (occasion 0) and the first test after the intervention (occasion 1).\n\nWhat is the research hypothesis and what is the null hypothesis?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nRH: happiness scores change (increase) from the first to the second occasion of testing.\nNH: happiness scores do not change across occasions.\n\n\n\n\nFor a paired t-test we can only include data from people who have produced scores at both occasions of testing. So, we need a slightly different version of the data, which you can download here for the ahicesd.csv file and here for the participantinfo2.csv file.\n\nRemind yourself what these data mean.\n\nOnce again, join the ahicesd.csv and participantinfo2.csv data in R-studio by aligning the names for the participant numbers in these two data sets (see week 2 workbook for reminders about this).\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nlibrary(tidyverse)\ndat &lt;- read_csv(\"data/wk5/ahicesd.csv\")\n\nRows: 992 Columns: 50\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (50): id, occasion, elapsed.days, intervention, ahi01, ahi02, ahi03, ahi...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npinfo &lt;- read_csv(\"data/wk5/participantinfo2.csv\")\n\nNew names:\nRows: 147 Columns: 7\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" dbl\n(7): ...1, id, intervention, sex, age, educ, income\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\nall_dat &lt;- inner_join(x = dat, y = pinfo, by = c(\"id\", \"intervention\"))\n\n\n\n\n\nLet’s select only the relevant variables. Use select() to select only id, ahiTotal, and occasion variables, and save this as a new object called ‘summarydata’\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nsummarydata &lt;- select(all_dat, c(\"id\", \"ahiTotal\", \"occasion\"))\n\n\n\n\n\nUse filter to pull out only occasion == 0 or occasion == 1 scores\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nuse occasion == 0 | occasion == 1'), save this as a new object called summarydata2\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nsummarydata2 &lt;- filter(summarydata, occasion == 0 | occasion == 1)\n\n\n\n\n\nHere is where we would usually remove all the NA values, but there aren’t any in this file (so we don’t need drop_na()).\nNow, we need to make sure occasion is treated as a categorical variable, rather than a continuous variable, so we need to convert it to a factor:\n\n\nsummarydata2$occasion &lt;-as.factor(summarydata2$occasion)\n\n\nNow, draw a box plot of ahiTotal scores by occasion (why do we use a box plot?)\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nggplot(summarydata2, aes(x = occasion, y = ahiTotal)) + geom_boxplot()\n\n\n\n\n\n\n\n\nWe use a boxplot because we have one nominal/categorical variable and one interval/ratio/ordinal measure.\n\n\n\n\nCompute mean and SD for each occasion\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nggplot(summarydata2, aes(x = occasion, y = ahiTotal)) + geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n\nIn order to run the paired t-test, we first need to make sure that the paired values (in this case the measures from the same person) are on the same row of the data. So, let’s use pivot_wider to put the ahiTotal scores for occasion 0 in one column, and the ahiTotal scores for occasion 1 in another column:\n\n\nsummarydata2_wide &lt;- pivot_wider(summarydata2, names_from = occasion, values_from = ahiTotal, names_prefix = \"occasion_\")\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nNote that we use the names_prefix = “occasion_” as an extra argument to make the column names “occasion_0” and “occasion_1”.\nWhat would happen if we didn’t use names_prefix? It would still work, but it’s a bit more awkward to work with the output.\n\n\n\n\nThen, we can run the paired t-test in the following way:\n\n\nt.test(summarydata2_wide$occasion_0, summarydata2_wide$occasion_1, paired = TRUE)\n\nIs the result significant?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nyes, because p &lt; .05.\n\n\n\n\nNow run Cohen’s d: it’s similar to the way we do the paired t-test:\n\n\ncohensD( summarydata2_wide$occasion_0, summarydata2_wide$occasion_1, method =  \"paired\")\n\nWhat is the value for Cohen’s d?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nd = 0.4059904\n\n\n\n\nWrite up a brief report of the result and discuss in your group.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe tested whether participants’ happiness scores at first testing after the interventions were different than their scores prior to the interventions. We found that, prior to the intervention, scores were significantly lower (M = 69.3, SD = 12.3) than they were immediately after the interventions (M = 72.4, SD = 12.6), t(146) = = 4.92, p &lt; .001, Cohen’s d = 0.41, a medium effect. The results indicate that the intervention had a positive effect on happiness scores.\n\n\n\n\nSave your R script file.\n\n\n\n\n5.4.1.4 Part 4: More practise running a paired t-test\nWe are going to figure out whether people have different scores the first and second time they take the Gent vocabulary test.\n\nGo back to the vocabulary scores data. Load the data into dat, and make another object dat2 that contains only the subject_ID, Gent_1_score and Gent_2_score.\nSome people did not do all the tests - look at participant 46 for instance. To do a t-test we need data where the person does both tests. We can filter out the scores where there are no NAs by repeating the drop_na we did at step 23, above. Call the new data object dat3.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\ndat &lt;- read.csv(\"data/wk4/PSYC411-shipley-scores-anonymous-17_24.csv\"); \ndat$academic_year &lt;- as.factor(dat$academic_year)\ndat2 &lt;- select(dat, subject_ID, Gent_1_score, Gent_2_score)\ndat3 &lt;- drop_na(dat2)\n\n\n\n\n\nRun the paired t-test.\n\nEach row contains a score for each person for each Gent test, and so we are ready to run the paired t-test:\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nt.test(dat3$Gent_1_score, dat3$Gent_2_score, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  dat3$Gent_1_score and dat3$Gent_2_score\nt = -2.2126, df = 246, p-value = 0.02784\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -2.4488231 -0.1422701\nsample estimates:\nmean difference \n      -1.295547 \n\ncohensD( dat3$Gent_1_score, dat3$Gent_2_score, , method =  \"paired\")\n\n[1] 0.1407865\n\n\n\n\n\n\nGet mean and SD of the scores for the tests.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nsummarise(dat3, mean(Gent_1_score), sd(Gent_1_score), mean(Gent_2_score), sd(Gent_2_score))\n\n  mean(Gent_1_score) sd(Gent_1_score) mean(Gent_2_score) sd(Gent_2_score)\n1           59.16194         13.99441           60.45749         14.89002\n\n\n\n\n\n\nIn order to draw a box plot of the Gent vocabulary scores taken at the first and second occasion, we will need the data in “long format”, so that there is a column saying which test it is and a column reporting the scores on the Gent tests. Apply the pivot_longer command and then draw the box plot.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\ndat4 &lt;- pivot_longer(dat3, names_to = \"test\", values_to = \"score\", cols = c(\"Gent_1_score\", \"Gent_2_score\"))\nggplot(dat4, aes(x = test, y = score)) + geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.4.1.5 Part 5: Extras\n\nIn the vocabulary scores data, is there a significant difference between males and females for your academic year group?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\ndat2 &lt;- filter(dat, academic_year == \"202324\")\nggplot(dat2, aes(x = Gender, y = Gent_1_score)) + geom_boxplot()\n\n\n\n\n\n\n\ndat3 &lt;- group_by(dat2, Gender)\nsummarise(dat3, mean(Gent_1_score), sd(Gent_1_score), mean(Gent_2_score), sd(Gent_2_score), n())\n\n# A tibble: 2 × 6\n  Gender `mean(Gent_1_score)` `sd(Gent_1_score)` `mean(Gent_2_score)`\n  &lt;chr&gt;                 &lt;dbl&gt;              &lt;dbl&gt;                &lt;dbl&gt;\n1 Female                 62.2               7.85                 61.6\n2 Male                   61.3              13.4                  57.1\n# ℹ 2 more variables: `sd(Gent_2_score)` &lt;dbl&gt;, `n()` &lt;int&gt;\n\n# or:\ndat2 %&gt;% group_by(Gender) %&gt;% summarise(mean(Gent_1_score), sd(Gent_1_score), mean(Gent_2_score), sd(Gent_2_score), n())\n\n# A tibble: 2 × 6\n  Gender `mean(Gent_1_score)` `sd(Gent_1_score)` `mean(Gent_2_score)`\n  &lt;chr&gt;                 &lt;dbl&gt;              &lt;dbl&gt;                &lt;dbl&gt;\n1 Female                 62.2               7.85                 61.6\n2 Male                   61.3              13.4                  57.1\n# ℹ 2 more variables: `sd(Gent_2_score)` &lt;dbl&gt;, `n()` &lt;int&gt;\n\nt.test(Gent_1_score ~ Gender, data = dat2 )\n\n\n    Welch Two Sample t-test\n\ndata:  Gent_1_score by Gender\nt = 0.16297, df = 7.6522, p-value = 0.8748\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n -11.68411  13.44601\nsample estimates:\nmean in group Female   mean in group Male \n            62.16667             61.28571 \n\ncohensD(Gent_1_score ~ Gender, method = \"unequal\", data = dat2 )\n\n[1] 0.08004559\n\nt.test(Gent_2_score ~ Gender, data = dat2 )\n\n\n    Welch Two Sample t-test\n\ndata:  Gent_2_score by Gender\nt = 0.58606, df = 6.695, p-value = 0.5771\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n -13.55749  22.38289\nsample estimates:\nmean in group Female   mean in group Male \n            61.55556             57.14286 \n\ncohensD(Gent_2_score ~ Gender, method = \"unequal\", data = dat2 )\n\n[1] 0.3007885\n\n\nnothing significant…\n\n\n\n\nAre there significant differences for the other vocabulary test measures between males and females, or between those with English as first or second language?\nThe data from this paper are called ClassDraw.csv available here. The data are on osf as well, but they’re not properly formatted, so I adjusted them and put them here.\n\nJalava, S. T., Wammes, J. D., & Cheng, K. (2023). Drawing your way to an A: Long-lasting improvements in classroom quiz performance following drawing. Psychonomic Bulletin & Review, 30, 1939–1945. https://doi.org/10.3758/s13423-023-02294-2\nThere are some useful tips in the results of this study about the benefit of doodling…\nMy challenge to you:\n\nCan you make a ggplot that looks a bit like Figure 2 from this study?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\njalava &lt;- read_csv(\"data/wk5/ClassDraw.csv\")\n\nRows: 168 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Gender\ndbl (8): ID, Age, write_1, write_2, draw_1, draw_2, exam_draw, exam_write\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\njalava1 &lt;- pivot_longer(jalava, names_to = c(\"test\",\".value\"), names_sep=\"_\" , values_to = \"score\", cols = c(\"write_1\",\"write_2\",\"draw_1\",\"draw_2\"))\njalava2 &lt;- pivot_longer(jalava1, names_to = \"test_time\", values_to = \"score\", cols = c(`1`,`2`))\n\njalava2$test_time &lt;- as.factor(jalava2$test_time)\nggplot(jalava2, aes(x = test_time, y = score, fill = test)) + geom_boxplot()\n\nWarning: Removed 48 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n# median looks very similar (boxplots very overlapping), but means are a bit different:\njalava2 %&gt;% group_by(test, test_time) %&gt;% summarise(mean(score, na.rm=TRUE), sd(score, na.rm=TRUE))\n\n`summarise()` has grouped output by 'test'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 4\n# Groups:   test [2]\n  test  test_time `mean(score, na.rm = TRUE)` `sd(score, na.rm = TRUE)`\n  &lt;chr&gt; &lt;fct&gt;                           &lt;dbl&gt;                     &lt;dbl&gt;\n1 draw  1                               0.486                     0.215\n2 draw  2                               0.322                     0.216\n3 write 1                               0.467                     0.212\n4 write 2                               0.3                       0.212\n\n\n\n\n\n\nIf you’ve made some progress on the de Zubicaray et al. (2024) data analysis, to the point where you have replicated (more or less) the multiple regression analysis from the first study in that paper, then here is the next step.\n\nNote, this is a major challenge, but it will be a step into new research!\nThe task is to take the de Zubicaray data, and add a new variable which determines whether the word is a palindrome or not. A palindrome is a word that is spelled the same backwards as forwards (e.g., abba, rotavator).\nI would like to know if being a palindrome or not affects processing of the words. My prediction is that being a palindrome helps access the word.\nWhat you will need to do is apply a function called “palindrome” to the words in the de Zubicaray dataset, to make a new variable, called or some other name then run a multiple regression with all the other variables plus this one.\nHere is a script with the function called palindrome included, with some comments to show how it is put into practice. You will need to add the function into the top of an analysis script (e.g., rmd or r script) file and make sure that is loaded along with the other commands you are using.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 5. Testing differences between groups"
    ]
  },
  {
    "objectID": "PSYC411/part1/Week5.html#data",
    "href": "PSYC411/part1/Week5.html#data",
    "title": "Week 5. Testing differences between groups",
    "section": "5.4.2 Data",
    "text": "5.4.2 Data\nData referred to in this workbook:\n\nPSYC411-shipley-scores-anonymous-17_24.csv\nahicesd.csv adapted for t-test\nparticipantinfo2.csv adapted for t-test.\nClassDraw.csv",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 5. Testing differences between groups"
    ]
  },
  {
    "objectID": "PSYC411/part1/Week5.html#answers",
    "href": "PSYC411/part1/Week5.html#answers",
    "title": "Week 5. Testing differences between groups",
    "section": "5.4.3 Answers",
    "text": "5.4.3 Answers\nThe answers to the workbook will appear below each question in the workbook, above, after the practical has finished, so you can check your work.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 5. Testing differences between groups"
    ]
  },
  {
    "objectID": "PSYC411/part1/Week5.html#extras",
    "href": "PSYC411/part1/Week5.html#extras",
    "title": "Week 5. Testing differences between groups",
    "section": "5.5 Extras",
    "text": "5.5 Extras\nOptionally, if you can give us your (anonymised) feedback on how the course is going from your perspective, that would be very welcome.\nAlso optionally, read the articles on the importance of statistical understanding and insights from good data visualisation:\n\nHow scientists can be better at statistics. Note that this article is hosted on the Spectator website, and (content warning) refers to Harold Shipman, a serial killer.\nFlorence Nightingale and data visualisation. Note that this is hosted on Scientific American.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 5. Testing differences between groups"
    ]
  },
  {
    "objectID": "PSYC411/part1/Week3.html",
    "href": "PSYC411/part1/Week3.html",
    "title": "Week 3. Drawing graphs from data",
    "section": "",
    "text": "Material for week 3 will be released after week 2 practical.\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 3. Drawing graphs from data"
    ]
  },
  {
    "objectID": "PSYC411/part1/Week3.html#overview",
    "href": "PSYC411/part1/Week3.html#overview",
    "title": "Week 3. Drawing graphs from data",
    "section": "",
    "text": "This week, there are three mini lectures, the practical workbook working with R-studio, and the first assignment (due by Friday of week 3).\nBefore the practical on Tuesday, please try to work through the practical workbook in your group.\nBring your questions (and/or answers) to the practical.\nThe assignment is on moodle here.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 3. Drawing graphs from data"
    ]
  },
  {
    "objectID": "PSYC411/part1/Week3.html#learning-goals",
    "href": "PSYC411/part1/Week3.html#learning-goals",
    "title": "Week 3. Drawing graphs from data",
    "section": "3.2 Learning Goals",
    "text": "3.2 Learning Goals\n\nSee how data gathering for vocabulary knowledge can be conducted\nUnderstand different types of reproducibility\nUnderstand the relations between observed score, true score, and error score\nUnderstand multiple types of graphs and how to construct them in Rstudio\nManipulate data, including filtering, grouping and summarizing data in Rstudio",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 3. Drawing graphs from data"
    ]
  },
  {
    "objectID": "PSYC411/part1/Week3.html#lectures-and-slides",
    "href": "PSYC411/part1/Week3.html#lectures-and-slides",
    "title": "Week 3. Drawing graphs from data",
    "section": "3.3 Lectures and slides",
    "text": "3.3 Lectures and slides\n\n3.3.1 Lectures\n\nWatch Lecture 3 part 1\n\n\n\nHave a go at the Gent Vocabulary test, and record your score.\nHave another go at the Gent Vocabulary test, and record your score again.\nHave a go at the Shipley Vocabulary test, and record your score.\nFill in your vocabulary scores into our course database: What is your vocabulary?\n\nWatch Lecture week3 part2\n\nWatch Lecture week3 part3\n\nDo the quick quiz (not assessed).\n\n\n3.3.2 Slides\nDownload the lecture slides for:\n\npart 2 here\npart 3 here\n\nI haven’t put slides for part 1 here because they just said to do the different vocabulary tests.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 3. Drawing graphs from data"
    ]
  },
  {
    "objectID": "PSYC411/part1/Week3.html#practical-materials",
    "href": "PSYC411/part1/Week3.html#practical-materials",
    "title": "Week 3. Drawing graphs from data",
    "section": "3.4 Practical Materials",
    "text": "3.4 Practical Materials\n\n3.4.1 Workbook\nPart 1 is revision from last week.\nPart 2 is to reproduce the Lecture week 3 part 3 analyses.\nPart 3 is to practise manipulating data.\nPart 4 is starting making visualisations of data.\nPart 5 is some more advanced manipulation of data.\nParts 6 and 7 are extra: extending your knowledge.\nIf you’ve followed a course in R studio before then Parts 1, 6 and 7 will extend your knowledge.\n\n3.4.1.1 Part 1: Revision from last week\nIf you had a look at the data from Stamkou et al.’s study from last week, then that’s great, in which case follow Task 1, otherwise just go straight to Task 2.\n\nTask 1: Describe Stamkou et al.’s study from last week\n\nTalk to your group about what you did.\nFrom the Stamkou et al. data, can you look at whether there was a change in happiness ratings from the baseline (PracticeHappy) to after watching the video (VideoHappy)?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nRemember first to load in the libraries Hmisc and tidyverse, and then use get.spss to load in the data: There are two ways to load in data that comes from spss - when the datafile ends with .sav The way we looked at doing this in week 1 workbook used the library Hmisc and then the function get.spss The other way is to use the library haven, and the function read_sav\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nlibrary(Hmisc)\ndat &lt;- spss.get(\"stamkou_Study 1_edited data.sav\")\n\nor:\n\nlibrary(haven)\ndat &lt;- read_sav(\"stamkou_Study 1_edited data.sav\")\n\n\n\n\n\nIf you used get.spss, though, the data, as you may have seen, is rather messy, and needs some tidying up. If you used read_sav then these issues don’t apply. But for get.spss, in particular, the ratings of happiness are a bit all over the place. (this is because get.spss keeps information about responses but also what those responses mean and sometimes mixes them up together). We want these to just be numbers, 1,2,3,4, but instead they have the coding for what the numbers mean as well. If you’ve loaded in your data set into an object called “dat”: then the following lines will work to tidy things up:\n\n\ndat$PracticeHappy &lt;- gsub(\"\\n1=not happy at all\\n\", \"1\", dat$PracticeHappy)\ndat$PracticeHappy &lt;- gsub(\"\\n2=a bit happy\\n\", \"2\", dat$PracticeHappy)\ndat$PracticeHappy &lt;- gsub(\"\\n3=quite happy\\n\", \"3\", dat$PracticeHappy)\ndat$PracticeHappy &lt;- gsub(\"\\n4=very happy\\n\", \"4\", dat$PracticeHappy)\ndat$PracticeHappy &lt;- as.numeric(dat$PracticeHappy)\ndat$VideoHappy &lt;- gsub(\"\\n1=not happy at all\\n\", \"1\", dat$VideoHappy)\ndat$VideoHappy &lt;- gsub(\"\\n2=a bit happy\\n\", \"2\", dat$VideoHappy)\ndat$VideoHappy &lt;- gsub(\"\\n3=quite happy\\n\", \"3\", dat$VideoHappy)\ndat$VideoHappy &lt;- gsub(\"\\n4=very happy\\n\", \"4\", dat$VideoHappy)\ndat$VideoHappy &lt;- as.numeric(dat$VideoHappy)\n\n\nNow, you can look at plotting the happiness ratings at baseline against happiness ratings after the children watch the video. You can also look at the means and standard deviations of the ratings to see if there is a numerical difference. What are the means and SDs?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nRemember: You will need to load in the tidyverse library at this stage.\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nlibrary(tidyverse)\nplot(dat$PracticeHappy, dat$VideoHappy)\nsummarise(dat, mean(PracticeHappy), sd(PracticeHappy), mean(VideoHappy, na.rm=T), sd(VideoHappy, na.rm=T))\n\nmean(PracticeHappy) sd(PracticeHappy) mean(VideoHappy, na.rm = T) sd(VideoHappy, na.rm = T)\n1 2.638889 0.7822321 1.983051 0.828982\nor you could use:\n\nmean(dat$PracticeHappy); sd(dat$PracticeHappy); mean(dat$VideoHappy); sd(dat$VideoHappy)\n\n\n\n\n\nLater in the workbook, we use the functions group_by and summarise, so you could use these to look at the means separately for the emotion conditions (EmotionCondition) - how would you do that? What are the means and SDs? What effect does there seem to be of the emotion conditions on happiness mood change? If you don’t yet know how to do this, then come back to it after you’ve completed the rest of the workbook.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\ndat2 &lt;- group_by(dat, EmotionCondition)\nsummarise(dat2, mean(PracticeHappy), sd(PracticeHappy), mean(VideoHappy, na.rm=T), sd(VideoHappy, na.rm=T))\n\nEmotionCondition mean(PracticeHappy) sd(PracticeHappy) mean(VideoHappy, na.rm = T) sd(VideoHappy, na.rm = T)\n1 Awe 2.68 0.800 2.08 0.747\n2 Joy 2.5 0.701 2.22 0.956\n3 Control 2.74 0.835 1.62 0.648\nhappiness decreases in all conditions, but most for Control, mid for Awe, least for Joy conditions\n\n\n\n\n\n\nPart 2: Reproduce the Lecture week3 part3 analyses\n\nTask 2: Load in the data, draw a histogram, find means and standard deviations\n\nCreate a new r script, called psyc411_week3.r, and clear out R studio ready for a new script using rm(list=ls()).\nDownload the data files from last week once again on the vocabulary tests here: PSYC411-shipley-scores-anonymous-17_18.csv.\nLoad the data into an object called “dat” using read_csv(), what command line do you use? (remember to set the working directory)\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\ndat &lt;- read_csv(\"PSYC411-shipley-scores-anonymous-17_18.csv\")\n\n\n\n\n\nView the data. What command do you use?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nView(dat)\n\n\n\n\n\nWe can make a histogram of the first time people took the Gent vocabulary test:\n\n\nhist(dat$Gent_1_score)\n\n\nAnd a histogram of the second time people took the Gent test, what command line do you use?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nhist(dat$Gent_2_score)\n\n\n\n\n\nWe will use a function called summarise. This is more powerful than just using mean(dat$Gent_1_score) and sd(dat$Gent_1_score), etc. because it can give us summary statistics on subgroups of data, and can give us much more than just means and SDs. Have a look at the documentation for the function:\n\n\n?summarise\n\nWe use it like this: first put the object that the variables are in, then tell summarise what to do with each variable - eg compute the mean or sd or number of times it’s measured (that’s the n() part), or whatever:\nsummarise(dat, mean(Gent_1_score), sd(Gent_1_score), mean(Gent_2_score), sd(Gent_2_score), n())\nHowever, this won’t have worked - it will just say NA for means and SDs. This is because summarise doesn’t work if there are missing values (called NA). So, we have to tell summarise to ignore the NAs.\nsummarise(dat, mean(Gent_1_score, na.rm=T), sd(Gent_1_score, na.rm=T), mean(Gent_2_score, na.rm=T), sd(Gent_2_score, na.rm=T), n())\nWhat is the mean and standard deviation of Gent_1_score and Gent_2_score?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nsummarise(dat, mean(Gent_1_score, na.rm=T), sd(Gent_1_score, na.rm=T), mean(Gent_2_score, na.rm=T), sd(Gent_2_score, na.rm=T), n())\n\nmean(Gent_1_score, na.rm = T) sd(Gent_1_score, na.rm = T) mean(Gent_2_score, na.rm = T) sd(Gent_2_score, na-rm= T) n()\n57.4 16.0 57.1 18.7 81\n\n\n\n\n\nTask 3: Use ggplot to draw some histograms:\n\nNow we are going to use another way of making graphs. This is more flexible than the hist function. Here is how to make a histogram of the Gent vocabulary scores:\n\n\nggplot(dat, aes(x = Gent_1_score) ) + geom_histogram(fill=\"blue\") + labs(title=\"Gent Vocabulary Test 1\", x = \"Vocabulary Score\", y = \"Frequency\")\n\n\nAnd for the second Gent test:\n\n\nggplot(dat, aes(x = Gent_2_score) ) + geom_histogram(fill=\"red\") + labs(title=\"Gent Vocabulary Test 2\", x = \"Vocabulary Score\", y = \"Frequency\")\n\n\n\n\n\n\n\nNote\n\n\n\nBreaking it down:\nggplot(dat, aes(x = Gent_1_score)): this calls the plotting function ggplot\ndat: we specify the data set we will use\nand we set the data for the plot, in this case we say that the x value (so that’s what will be along the x-axis in the graph) is the Gent_1_score. We put this inside aes(), which stands for “aesthetic”.\n+ geom_histogram(fill=\"blue\"): this adds a graph of type histogram and colours it blue\n+ labs(title=\"Gent Vocabulary Test\", x = \"Vocabulary Score\", y = \"Frequency\"): this adds labels to the graph: title, the x-axis label and the y-axis label.\n\n\n\n\n\n4.1.3.3 Part 3: Practise manipulating data\n\nTask 4: Practise manipulating data\n\nLet’s keep only some of the variables from the dataset dat - let’s remove Gender_code, and Dyslexia_diagnosis. Keep the other variables using select() and load this into summarydata\n\n\nsummarydata &lt;- select(dat, subject_ID, Age, english_status, Gender, Shipley_Voc_Score, Gent_1_score, Gent_2_score, academic_year)\n\n\nNext we will have a bit more of a wander around the data to get a feel for it. We will first use the function arrange(), which changes the order of observations (rows):\n\n\narrange(summarydata, Shipley_Voc_Score)\n\nWhat is the lowest score of a participant on the Shipley Vocabulary questionnaire? (You may like to make a new object, which is the result of the arrange function, then look at it in View).\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n10\n\n\n\n\nIf you want to order from highest to lowest, you have to use the desc() function:\n\n\narrange(summarydata, desc(Shipley_Voc_Score))\n\nWhat is the highest value on the Shipley Vocabulary Test? How many participants have this highest score?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n38, 2 participants\n\n\n\n\nNext we will use the filter() function. This includes or excludes certain observations (rows). Let’s just include the participants with English as a first language and put this into a new object, called summarydata_enl.\n\n\nsummarydata_enl &lt;- filter(summarydata, english_status == 'native')\n\nWhat are the mean and SD values of the Shipley Vocabulary test for the native speakers? (use the summarise function)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nsummarise(summarydata_enl, mean(Shipley_Voc_Score), sd(Shipley_Voc_Score))\n\nmean = 32.1, sd = 3.25\n\n\n\n\nMake another variable with the z-scores of the Shipley Vocabulary test (see week 1 workbook, 1.4.1.1 Part 1, Task 4). What are the maximum and minimum z-scores? (you can use min() and max() in summarise to compute these)?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nsummarydata$zshipley &lt;- scale(summarydata$Shipley_Voc_Score)\nsummarise(summarydata, min(zshipley), max(zshipley))\n\nMin: -2.46, max = 1.43\n\n\n\n\nMaking subgroups of data using group_by function. Another handy function in the tidyverse library is group_by. This makes subgroups of data which can then be looked at separately in other functions, such as summarise. Above, we filtered just the “native” English speakers, but we can specify subgroups of speakers instead. We make a new object of the grouped data (it looks just the same, but the groups are behind the scenes):\n\nsummarydata2 &lt;- group_by(summarydata, english_status)\n\nThen we summarise dat2. What is the mean, SD and number of participants for the Shipley_Voc_Score for the ESL and native English speakers groups?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nsummarise(summarydata2, mean(Shipley_Voc_Score), sd(Shipley_Voc_Score), n())\n\nenglish_status mean(Shipley_Voc_Score) sd(Shipley_Voc_Score) n()\n1 ESL 21.6 6.76 33\n2 native 32.1 3.25 42\n3 NA 30.3 4.93 6\n\n\n\n\nRemember to save your script file as you go along.\n\n\n\n\n3.1.4.4 Part 4: Graphing data\n\nTask 5: graphing data using histograms\n\nPreviously we used plot to draw a scatter plot, and hist to draw a histogram. Now, we’re going to use ggplot which can draw all kinds of graphs, with a great deal more flexibility. We are going to represent the data to reflect the following relations:\n\n\nEnglish status and gender\nAge and vocabulary score\nGender and vocabulary score\nAcademic year and vocabulary score\nAcademic year and age\nEnglish status and vocabulary score\nEnglish status and age\n\nBut first, let’s repeat reproducing the histogram from the overhead slides to look at the distribution of variables:\n\nggplot(summarydata, aes(x = Gent_1_score)) +\n  geom_histogram(fill=\"blue\") + \n  labs(title=\"Gent Vocabulary Test 1\", x = \"Vocabulary Score\", y = \"Frequency\")\n\n\nNow draw a histogram with Shipley_Voc_Score as the variable and colour it orange. Remember to change the title to something appropriate.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nggplot(summarydata, aes(x = summarydata$Shipley_Voc_Score)) + geom_histogram(fill=\"orange\") + labs(title=\"Shipley Vocabulary Test\", x = \"Vocabulary Score\", y = \"Frequency\")\n\nor how about this for a version that separates out the gender in the same figure:\n\nggplot(summarydata, aes(x = Shipley_Voc_Score, fill=Gender )) + geom_histogram(bins = 10) + labs(title=\"Shipley Vocabulary Test\", x = \"Vocabulary Score\", y = \"Frequency\")\n\n\n\n\n\n\nTask 6: graphing data using bar graphs\n\nNext let’s look at English status and gender. What types of variable are these? Nominal? Ordinal? Interval/ratio?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nboth nominal. So we can only count them.\n\n\n\n\nWe will draw a bar graph of the counts. We use geom_bar() for this:\n\n\nFirst try this:\n\n\nggplot(summarydata, aes(x = Gender)) + \n  geom_bar()\n\n\nThis just draws counts of Gender\nNow let’s draw Gender and English Status together:\n\n\nggplot(summarydata, aes(x = Gender, fill = english_status)) + \n  geom_bar(position = \"dodge\")\n\nNote 1: We use position dodge so that it puts the bars next to each other (what happens if you leave out position = dodge?)\nNote 2: We use fill = english_status so that it fills the different bars with different colours according to different english statuses.\nWhat is the general pattern of counts? Are there proportional differences by English status according to gender?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nproportionally more males who are native English. But we don’t (yet) know if that’s significant…\n\n\n\n\n\nTask 7: graphing data using scatterplot\n\nNext we’ll look at Age and Shipley Vocabulary Score. What types of data are these?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\ninterval/ratio, so we can draw a point plot (scatterplot).\n\n\n\n\nWe will draw a point plot of these values:\n\n\nggplot(summarydata, aes(x= Age, y = Shipley_Voc_Score)) + geom_point()\n\nWe can add + labs(title = \"Age by Shipley Vocabulary Score\", x = \"Age\", y = \"Shipley Vocabulary Score\") to tidy up presentation a bit.\n\nggplot(summarydata, aes(x= Age, y = Shipley_Voc_Score)) + \n  geom_point() + \n  labs(title = \"Age by Shipley Vocabulary Score\", x = \"Age\", y = \"Shipley Vocabulary Score\")\n\n\nWhat is the relation between age and Shipley Vocabulary score?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nit looks positive.\n\n\n\n\n\nTask 8: Draw and interpret a box plot\n\nNext on the list of relations to check is gender and vocabulary score. Let’s look at Gent_1_score against Gender. What type of variables are these?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\none ratio, one nominal, so need a box plot.\n\n\n\n\nWe will draw a box plot (you could draw a bar graph, but box plots tend to be preferred for these combinations of variables - use a bar graph for counts):\n\n\nggplot(summarydata, aes(x= Gender, y = Gent_1_score)) + \n  geom_boxplot() \n\n\nAgain we can tidy this up by adding labels:\n\n\nggplot(summarydata, aes(x= Gender, y = Gent_1_score)) + geom_boxplot() + labs(title = \"Vocabulary Score by Gender\", x = \"Gender\", y = \"Gent Vocabulary Score Test 1\")\n\n\nInterpreting box plots: The horizontal line indicates the median. The box indicates where 50% of the data lie. The lines indicate an estimate of the range of the data (minimum and maximum values). The dots indicate outliers. A large box indicates larger standard deviation. If the boxes don’t overlap much then this indicates there may be a difference between the groups.\n\nAre there differences in Vocabulary according to gender?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nmaybe females slightly lower than males.\n\n\n\n\nNow for the other relations:\n\n\nAcademic year and vocabulary score\nAcademic year and age\nEnglish status and vocabulary score\nEnglish status and age\n\nAt the moment, R is interpreting Academic year as a number. We need to turn it into a nominal variable (called a “factor” in R studio):\n\nsummarydata$academic_year &lt;- as.factor(summarydata$academic_year)\n\nDraw a graph for each of these relations.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThese are categorical against ratio variables, so all box plots:\n\nggplot(summarydata, aes(x = academic_year, y = Gent_1_score) ) + geom_boxplot()\nggplot(summarydata, aes(x = academic_year, y = Age) ) + geom_boxplot()\nggplot(summarydata, aes(x = english_status, y = Gent_1_score) ) + geom_boxplot()\nggplot(summarydata, aes(x = english_status, y = Age) ) + geom_boxplot()\nggplot(summarydata, aes(x = Age, fill = english_status) ) + geom_histogram()\n\n\n\n\n\nSave your R script.\n\n\n\n\n3.4.1.5 Part 5: More practise manipulating data\n\nWhat are the minimum and maximum values for the ESL and English native speakers?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nsummarise(summarydata2, min(Shipley_Voc_Score), max(Shipley_Voc_Score))\n\nenglish_status min(Shipley_Voc_Score) max(Shipley_Voc_Score) 1 ESL 10 38 2 native 24 38 3 NA 24 35\n\n\n\n\n\n3.4.1.6 Part 6: Extending your knowledge\n\nAn important part of using R-studio is that people share their code, their tutorials, and help one another out in troubleshooting. Searching for commands and how to use them on search engines is absolutely fine, and supplementing your learning with others’ tutorials is also what the community does. Glasgow University psychology department has made some of their course materials available and so for more practise in using the different data manipulation functions, you could have a look at https://psyteachr.github.io/msc-conv/data-wrangling-1.html\n\nIgnore the part about making a new R markdown document, use instead an R script.\nJust go straight to clear out the environment rm(list=ls()) then load the libraries tidyverse() and babynames() where the babynames() library contains the database on babies’ names that you can explore in the example (you’ll need to install.packages(“babynames”) first).\nAnd then begin at Section “4.4: Activity 2: Look at the data”.\n\nFor the data that you downloaded from the papers for your extra exercises in weeks 1 and 2, construct some graphs to show relations between variables.\n\n\n\n\n\n\n\nNote\n\n\n\nUse geom_bar() for bar graphs of counts Use geom_box() for box plots showing means Use geom_point() to show relations between two or more variables Use geom_histogram() to show distributions of one or more variables\n\n\n\n\n3.4.1.7 Part 7: Extending everyone’s knowledge (Optional extra)\nIf you want a real challenge, where we can get to the boundary of doing some original research, then here is another optional extra:\nThis is a recently published paper that draws on lots of different sources of secondary data. It is a really impressive example of what you can do with already existing data.\nThis is from the study de Zubicaray, G. I., Kearney, E., Guenther, F., McMahon, K. L., & Arciuli, J. (2024). Statistical relationships between surface form and sensory meanings of English words influence lexical processing. Journal of Experimental Psychology: Human Perception and Performance, in press.\nMy challenge to you is to get all the datasets together that this paper refers to, and see if you can repeat the first analysis in their paper. They actually have the r scripts (in this case they are r-markdown (.rmd) files) as well as the data available.\nIf you can do that, then I will be amazed and impressed, and then I’ve got another task for you, which you’ll have to ask me for in the practical.\n\n\n\n3.4.2 Data\nHere is a link to the data referred to in this practical:\n\nPSYC411-shipley-scores-anonymous-17_18.csv\n\n\n\n3.4.3 Answers\nThe answers to the workbook now appear below each question in the workbook, above, after the practical has finished, so you can check your work.\nIt’s really important for your learning that you have a go first of all at the workbook before looking at the answers.\n&lt;!—- ## 3.5 Extras",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 3. Drawing graphs from data"
    ]
  },
  {
    "objectID": "PSYC411/part1/Week1.html",
    "href": "PSYC411/part1/Week1.html",
    "title": "Week 1. Introducing Data",
    "section": "",
    "text": "This week, there are three mini lectures, and then a practical workbook to get you going with R-studio.\nBefore the practical on Tuesday, please try to work through the practical workbook (in the first practical we will form groups of people to work together on the workbooks, for now you can work on the practical workbook individually or with anyone else on the course you are in touch with!).\nBring your questions (and/or answers) to the practical.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 1. Introducing Data"
    ]
  },
  {
    "objectID": "PSYC411/part1/Week1.html#overview",
    "href": "PSYC411/part1/Week1.html#overview",
    "title": "Week 1. Introducing Data",
    "section": "",
    "text": "This week, there are three mini lectures, and then a practical workbook to get you going with R-studio.\nBefore the practical on Tuesday, please try to work through the practical workbook (in the first practical we will form groups of people to work together on the workbooks, for now you can work on the practical workbook individually or with anyone else on the course you are in touch with!).\nBring your questions (and/or answers) to the practical.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 1. Introducing Data"
    ]
  },
  {
    "objectID": "PSYC411/part1/Week1.html#learning-goals",
    "href": "PSYC411/part1/Week1.html#learning-goals",
    "title": "Week 1. Introducing Data",
    "section": "1.2 Learning Goals",
    "text": "1.2 Learning Goals\nBy the end of Week 1, you should be able to:\n\nUnderstand the importance of data analysis and statistics\nIdentify types of data in psychology (nominal, ordinal, interval, ratio)\nUnderstand means and standard deviations\nUnderstand standardized scores (Z-scores)\nUse R-studio to begin to manipulate data, investigate means and standard deviations, and convert scores into Z-scores",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 1. Introducing Data"
    ]
  },
  {
    "objectID": "PSYC411/part1/Week1.html#lectures-and-slides",
    "href": "PSYC411/part1/Week1.html#lectures-and-slides",
    "title": "Week 1. Introducing Data",
    "section": "1.3 Lectures and slides",
    "text": "1.3 Lectures and slides\n\n1.3.1 Lectures\nWatch Lecture week1 part1:\n\n\nWatch Lecture week1 part2, stop halfway through and do the Lecture week1 part2 quiz (not assessed), by clicking here\nThen come back and watch the end of Lecture week 1 part2:\n\nWatch Lecture week1 part3:\n\nTake the quiz on the lecture material (not assessed), by clicking here\n\n\n1.3.2 Lecture slides\nDownload the lecture slides for\n\npart 1 here\npart 2 here",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 1. Introducing Data"
    ]
  },
  {
    "objectID": "PSYC411/part1/Week1.html#practical-materials",
    "href": "PSYC411/part1/Week1.html#practical-materials",
    "title": "Week 1. Introducing Data",
    "section": "1.4 Practical Materials",
    "text": "1.4 Practical Materials\n\n1.4.1 Workbook\nIn your group (or on your own until you’ve formed a group), work through this workbook, note any problems and questions you have, and come prepared to the practical class to go through the tasks and ask your questions.\nIf you’ve done statistics using R-studio before then Parts 1 and 2 will be just revision for you. In which case, Part 3 is where you can focus your work.\nPart 1 of this workbook reproduces what you saw in the week 1 part 3 lecture.\nPart 2 gives you some more exercises in using R studio for finding means, standard deviations, z scores, and drawing histograms.\nPart 3 provides some more extended tasks you can do to practise exploring what R -studio can do and develop your skills further. If you are new to R-studio then parts 1 and 2 cover what you need to know, and Part 3 contains some more extending, optional exercises.\nThere are answers provided after each question. It is really important that you try to answer the questions yourself first, and then check your answers against ours. If you go straight to the answer you’ll be very quick in getting through the workbook but you won’t unfortunately learn very much: Smith, G. (1998). Learning statistics by doing statistics, Journal of Statistics Education, 6:3.\n\n1.4.1.1 Part One: repeat the steps from lecture 1 part 3\n\nTask One: Open Rstudio\n\nStartup Rstudio\n\nTo log in to the R server, first make sure that you have the VPN switched on, or you will need to be connected to the university network (Eduroam). To set up the VPN, follow ISS instructions here or connecting to Eduroam here.\nWhen you are connected, navigate to https://psy-rstudio.lancaster.ac.uk, where you will be shown a login screen that looks like the below. Click the option that says “Sign in with SAML”.\n\nThis will take you through to the University login screen, where you should enter your username (e.g. ivorym) and then your university password. This will then redirect you to the R server where you can start using RStudio!\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you have already logged in through the university login, then you may not see the username/password screen. When you click login, you will be redirected straight to RStudio. This is because the server shares the login information securely across the university.\n\n\n\nWhat does RStudio look like?\nWhen RStudio starts, it will look something like this: \nRStudio has three panels or windows: there are tabs for Console (taking up the left hand side), Environment (and History top right), Current file (bottom right). You will also see a 4th window for a script or set of commands you develop, also (on the left hand side).\n\n\n\nTask Two: using the console\n\n\n\n\n\n\nTip\n\n\n\nText that is highlighted with a grey background denotes code, rather than typical prose. Code is different to other forms of writing, such as essays, because the syntax, order and words need to be quite specific. For some longer chunks of code, as you will see below, they are formatted slightly differently.\n\n\n\nIn the “console” part of the R window, next to the &gt;, type 10 + 30. Press return.\n\n\n10 + 30                        \n\n\n\n\n\n\n\nTip\n\n\n\nIf you hover your mouse over the box that includes the code snippet in this webpage, a ‘copy to clipboard’ icon will appear in the top right corner of the box. Click that to copy the code. Now you can easily paste it into your script.\n\n\nIt should give you the answer 40.\n\nIn the console, type a &lt;- 40 and press Return.\n\n\na &lt;- 40                      \n\n\nNow type a and press return. It should give you the answer 40. a is called an object, think of it like a bucket that you can keep a number, or some numbers, or actually all kinds of stuff in.\nNow let’s look at a function, sqrt. sqrt is a function that takes the square root of whatever is inside the brackets. In the console, type sqrt(13). Press Return.\nNow find the square root of the object a by typing sqrt(a). Press return.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\na&lt;- 40\nsqrt(a)\n\n[1] 6.324555\n\n\n\n\n\n\n\nTask Three: finding distributions\n\nMake a new object b, and put the following list of children’s attachment scores into it\n\n\nb &lt;- c( 4, 1, 5, 3, 8, 2, 2, 6, 8, 5, 4, 1, 6, 5, 4, 5, 7, 9, 10, 1, 1, 3, 5, 4, 6, 4, 8, 6, 5, 5, 7, 8, 9, 8, 8, 2, 1, 4, 3, 2, 5, 1, 5, 6, 8, 6, 7, 2, 7)\n\n\nCheck it works by typing b, press return.\nFind the mean of these numbers by typing mean(b).\nFind the median of these numbers by typing median(b).\nFind the standard deviation of these numbers by typing sd(b).\nDraw a histogram of these numbers by typing hist(b).\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nb &lt;- c( 4, 1, 5, 3, 8, 2, 2, 6, 8, 5, 4, 1, 6, 5, 4, 5, 7, 9, 10, 1, 1, 3, 5, 4, 6, 4, 8, 6, 5, 5, 7, 8, 9, 8, 8, 2, 1, 4, 3, 2, 5, 1, 5, 6, 8, 6, 7, 2, 7)\nmean(b)\n\n[1] 4.938776\n\nmedian(b)\n\n[1] 5\n\nsd(b)\n\n[1] 2.503399\n\nhist(b)\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask Four: z scores\n\nMake a new object b_z and assign to it the z scores of the values from b:\n\n\nb_z &lt;- scale(b)\n\n\nCheck that it worked by typing b_z.\nDraw a histogram of b_z by typing hist(b_z).\n\n\n\n\n1.4.1.2 Part Two: extra practice\n\nTask Five: investigating distributions\n\nLet’s make three new objects, with the marks from three people’s university masters courses. They are called annie, saj, and carrie and they took 10 courses each. We use the special notation c() to indicate a list, each number in the list is separated by a comma. Type the following into the console:\n\n\nannie &lt;- c(55, 95, 85, 65, 65, 85, 65, 95, 65, 75)\nsaj &lt;- c(65, 85, 95, 75, 65, 55, 55, 75, 95, 85)\ncarrie &lt;- c(75, 65, 95, 95, 55, 85, 75, 55, 95, 55)\n\n\nWho has the highest average (mean) score for their course?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nuse the mean() function\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThey all have the same mean = 75.\n\nannie &lt;- c(55, 95, 85, 65, 65, 85, 65, 95, 65, 75)\nsaj &lt;- c(65, 85, 95, 75, 65, 55, 55, 75, 95, 85)\ncarrie &lt;- c(75, 65, 95, 95, 55, 85, 75, 55, 95, 55)\nmean(annie)\n\n[1] 75\n\nmean(saj)\n\n[1] 75\n\nmean(carrie)\n\n[1] 75\n\n\n\n\n\n\nWho has the most variable scores for their course?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nuse the sd() function\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nAnnie has the least variable scores sd = 14.14214, then saj sd = 14.90712, then carrie sd = 16.99673. Carrie is most variable.\n\nsd(annie); sd(saj); sd(carrie)\n\n[1] 14.14214\n\n\n[1] 14.90712\n\n\n[1] 16.99673\n\n\n\n\n\n\nWhat is the median score for each student?. What does this mean about the distribution of each students’ scores? Use the function hist() to draw the distributions to help you see.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nyou can use the summary() function, or the median() function\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nmedian(annie) = 70; median(saj) and median(carrie) are both median = 75. So, annie’s scores are positive skewed – more lower values and a few (very) high values. saj looks like a uniform (flat) distribution, and carrie is different than normal distribution – maybe bimodal? In each case it’s very hard to tell from so few data points, and more data would make it clearer.\n\nmedian(annie); median(saj); median(carrie)\n\n[1] 70\n\n\n[1] 75\n\n\n[1] 75\n\nhist(annie); hist(saj); hist(carrie)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask Six: standardised scores: Z scores\n\nMake a new object called annie_z and use the function scale to convert annie’s scores to z-scores: in the console type:\n\n\nannie_z &lt;- scale(annie)\n\n\nYou can have a look at the standardised scores of annie, by just typing annie_z. What z score corresponds to annie’s highest initial score of 95?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nmean = 0, sd = 1. So that’s good, the z-score conversion worked.\n\nannie_z &lt;- scale(annie)\nmean(annie_z)\n\n[1] 0\n\nsd(annie_z)\n\n[1] 1\n\n\n\n\n\n\nWhat is the mean and standard deviation of annie_z’s standardised scores?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n1.4142136 You could find this by just eyeballing (searching through) the data, or by using the function max:\n\nannie_z &lt;- scale(annie)\nmax(annie); max(annie_z)\n\n[1] 95\n\n\n[1] 1.414214\n\n\n\n\n\n\nDraw a histogram of annie’s standardised scores, in the console type hist(annie_z). Between which z values are the highest number of scores?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBetween -1 and -0.5.\n\nhist(annie_z)\n\n\n\n\n\n\n\n\n\n\n\n\nBonus extra: If you want to find out the proportion of scores lower than a particular score you can do it like this in R-studio: pnorm(x) where x is the z-score you’re interested in. What is the proportion of scores lower than annie’s highest grade score?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\npnorm(1.4142136) = 0.9213504, 92% of scores (in a population) are lower than her highest score.\n\npnorm(1.4142136)\n\n[1] 0.9213504\n\n\n\n\n\n\n\n\n1.4.1.3 Part Three: Extras\nIf you’ve whizzed through the previous tasks, then you can move on to the following activities to explore further the functionality of R studio.\n\nTask Seven: Exploring operators.\nSo far, we’ve just looked at + as an operator. Go to this page: https://www.statmethods.net/management/operators.html\n\nIn the console, assign the object d to be 100 multiplied by 246.\nIn the console, assign the object e to be 84 divided by 32.1.\nAssign the variable f to 8 to the power of 4 (in R this is called exponentiation).\nWhat is the result of d added to e all divided by f\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n6.006498\n\nd &lt;- 100 * 246; e &lt;- 84/32.1; f &lt;- 8^4;\nf &lt;- 8**4 # this does the same thing as 8^4\n(d+e)/f \n\n[1] 6.006498\n\n\n\n\n\n\n\nTask Eight: Exploring functions\nSo far, we’ve just looked at the square root function sqrt(). Go to this page: https://www.statmethods.net/management/functions.html\n\nWhat is the result of abs(-5.3)? What does the abs function do?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n5.3. It takes the sign (negative) away from the number.\n\nabs(-5.3)\n\n[1] 5.3\n\n\n\n\n\n\nUsing the seq() function, generate a sequence of numbers from 0 to 30 in intervals of 3.\nAssign the sequence generated in step 28 to a new object. Now compute the mean of the sequence of numbers. (remember that objects can be a single number, or a sequence of numbers (called an array or a vector) or anything you want to put into it – remember, think of objects as buckets).\n\n\n\n\n\n\n\nStuck? Here’s the solution\n\n\n\n\n\nTry out the following code, pay special attention to how the sentences above “convert” into R code.\n\nsequence &lt;- seq(0,30, 3)\nmean(sequence)\n\n\n\n\n\nAssign the sequence generated in 31 to a new object. Now compute the mean of the sequence of numbers. (remember that objects can be a single number, or a sequence of numbers (called an array or a vector) or anything you want to put into it – remember, think of objects as buckets).\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nmean = 15.\n\ng &lt;- seq(0,30,3); mean(g)\n\n[1] 15\n\n\n\n\n\n\n\nTask Nine: Exploring others’ data\nHave a look at this article: Scullin, M. K., Gao, C., & Fillmore, P. (2021). Bedtime music, involuntary musical imagery, and sleep. Psychological Science, 32(7), 985-997. https://journals.sagepub.com/doi/10.1177/0956797621989724\nAbstract Many people listen to music for hours every day, often near bedtime. We investigated whether music listening affects sleep, focusing on a rarely explored mechanism: involuntary musical imagery (earworms). In Study 1 (N = 199, mean age = 35.9 years), individuals who frequently listen to music reported persistent nighttime earworms, which were associated with worse sleep quality. In Study 2 (N = 50, mean age = 21.2 years), we randomly assigned each participant to listen to lyrical or instrumental-only versions of popular songs before bed in a laboratory, discovering that instrumental music increased the incidence of nighttime earworms and worsened polysomnography-measured sleep quality. In both studies, earworms were experienced during awakenings, suggesting that the sleeping brain continues to process musical melodies. Study 3 substantiated this possibility by showing a significant increase in frontal slow oscillation activity, a marker of sleep-dependent memory consolidation. Thus, some types of music can disrupt nighttime sleep by inducing long-lasting earworms that are perpetuated by spontaneous memory-reactivation processes.\nThe data from the study is available on this osf website. The data we will look at is from the first study, the data set called “Earworm_MTurk_OSF.sav” on the osf site. These data are saved in spss format, which is not great for R-studio. We can still read it in, though, using a function called spss.get()\n\nBrowse the paper to see what it is about. Focus on Study 1.\nFrom the osf website download the data file: Earworm_MTurk_OSF.sav, and also download the codebook file: Earworm_MTurk_Codebook.xlsx. The codebook tells you what each of the measures are in the data file.\nLoad the data into R-studio: in the bottom right panel of R-studio, click on Upload, and browse to the Earworm_MTurk_OSF.sav file. It should then appear in the list of files in that bottom right panel.\nThat step means we can access the data, but it isn’t yet loaded into R-studio.\n\nSo, next load the data file into R-studio so we can work on it. You might have noticed that the data file is in SPSS format (that’s what the .sav ending to the file means). But, we can still load that in to R-studio.\nTo do that first, load the library Hmisc: library(Hmisc)\n\nlibrary(Hmisc)\n\n\nThen, use the function spss.get():\n\n\ndat &lt;- spss.get(\"Earworm_MTurk_OSF.sav\")\n\n\nThat should have made you a new object in R-studio called “dat” which contains the data from the study.\nNext, we can have a look at the data. Here are a few questions to get you going:\n\n\nHow many male, how many female participants?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n112 male, 87 female\n\nlibrary(Hmisc)\ndat &lt;- spss.get(\"data/PSYC411_wk1/EArworm_MTurk_OSF.sav\")\n# note that my use of the spss.get function here looks in the folder data and the subfolder PSYC411_wk1 because that's where I've put my EArworm_MTurk_OSF.sav data file.\nsummary(dat$Gender)\n\n\n\n\n\nHow many people never have earworms in the middle of the night?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n97\n\nsummary(dat$Earworms.MiddleOfTheNight)\n\n\n\n\n\nFor the Stanford Sleepiness Scale, how many participants felt “Somewhat foggy, let down”?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n17\n\nsummary(dat$StanfordSleepinessScale)\n\n\n\n\n\nWhat was the mean, SD and range of age of the participants? Does your calculation of mean age correspond to that given in the paper?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nmean = 35.9, sd = 10.9, range 20-73.\n\nlibrary(tidyverse)\nsummarise(dat, mean(Age), sd(Age), min(Age), max(Age))\n# summarise does lots of things at the same time, you could also just do them one by one:\nmean(dat$Age); sd(dat$Age); min(dat$Age); max(dat$Age)\n\n\n\n\n\nCan you work out the mean age of the male and female participants separately?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\ndat %&gt;% group_by(Gender) %&gt;% summarise(mean(Age))\n\n\n\n\n\nExplore the data, see if you can remember tasks for separating different subgroups, graphing relations, comparing groups.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n# this is just one example, other commands and ways are also legitimate:\ndat %&gt;% ggplot( aes(x = PSQI.TST, y = PSQI.SOL)) + geom_point()\n\n\n\n\n\n\n\n\n1.4.2 Data\n(There are no data that you need for today’s practical, other than the link to the data for the earworm study, but when there are data sets you need, you will find them in this data section.)\n\n\n1.4.3 Answers\nThe answers to the workbook have now appeared below each question in the workbook, above, after the practical has finished, so you can check your answers.\nIt’s really important for your learning that you have a go first of all at the workbook before looking at the answers.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 1. Introducing Data"
    ]
  },
  {
    "objectID": "PSYC411/part1/Week1.html#extras",
    "href": "PSYC411/part1/Week1.html#extras",
    "title": "Week 1. Introducing Data",
    "section": "1.5 Extras",
    "text": "1.5 Extras\nOptionally, watch the lecture by Tim Harford on the importance of understanding statistics. Note this video is hosted on facebook.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 1. Introducing Data"
    ]
  },
  {
    "objectID": "PSYC411/part2/report-preface.html",
    "href": "PSYC411/part2/report-preface.html",
    "title": "Week 6. The structured research report – Quick start",
    "section": "",
    "text": "The structured research report assignment: We want you to analyse previously collected data and write a report about your findings.\nHere, we present a guide in five parts:\n\n\n\n\n\n\nTip\n\n\n\n\nWhat you have to write.\nHow you can do the analysis work.\nWhy we are asking you to do this.\nWhere you can get in-depth information.\nHow you can expect us to grade your work.\n\n\n\nWe provide a summary guide to What you have to write next, and then a quick guide to How you can do the analysis work you need to do, so that you can do the writing.\nWe share a lecture on Why we are asking you to do this work: what you will learn and how you will benefit.\nAt the end of this page, in How we will mark your work, we detail what work of different grades looks like, so that you can both understand what we want you to do, and how you can do it.\nIf you want more in-depth guidance or support, we provide detailed step-by-step notes on the work you can plan to do in a chapter we have written to support you: How you can do the analysis work. If you want an explanation of why we are asking you to do this work, so you can think about best practice in science, we have got that for you in our chapter: Why we are asking you to do this.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 6. The structured research report -- Quick start"
    ]
  },
  {
    "objectID": "PSYC411/part2/report-preface.html#sec-report-intro",
    "href": "PSYC411/part2/report-preface.html#sec-report-intro",
    "title": "Week 6. The structured research report – Quick start",
    "section": "",
    "text": "The structured research report assignment: We want you to analyse previously collected data and write a report about your findings.\nHere, we present a guide in five parts:\n\n\n\n\n\n\nTip\n\n\n\n\nWhat you have to write.\nHow you can do the analysis work.\nWhy we are asking you to do this.\nWhere you can get in-depth information.\nHow you can expect us to grade your work.\n\n\n\nWe provide a summary guide to What you have to write next, and then a quick guide to How you can do the analysis work you need to do, so that you can do the writing.\nWe share a lecture on Why we are asking you to do this work: what you will learn and how you will benefit.\nAt the end of this page, in How we will mark your work, we detail what work of different grades looks like, so that you can both understand what we want you to do, and how you can do it.\nIf you want more in-depth guidance or support, we provide detailed step-by-step notes on the work you can plan to do in a chapter we have written to support you: How you can do the analysis work. If you want an explanation of why we are asking you to do this work, so you can think about best practice in science, we have got that for you in our chapter: Why we are asking you to do this.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 6. The structured research report -- Quick start"
    ]
  },
  {
    "objectID": "PSYC411/part2/report-preface.html#sec-report-quick-what",
    "href": "PSYC411/part2/report-preface.html#sec-report-quick-what",
    "title": "Week 6. The structured research report – Quick start",
    "section": "6.2 What you have to write",
    "text": "6.2 What you have to write\nLet’s begin with what you need to write to complete this coursework.\n\nSubmission deadline: On Friday 17th January 2025\nYou will submit a structured report.\nPresenting an analysis: what you did; what you found; the context; and the implications.\n\n\n\n\n\n\n\nTip\n\n\n\nYou will be able to find the submission point on Moodle here when it is revealed.\n\n\nWhat is a structured report?\nYou will submit short answers to a series of questions:\n\nDescribe the questions or the predictions you examine in your analysis.\nExplain the research background: you are analyzing previously collected data, so briefly explain why the people who originally collected the data did that work.\nExplain if you are attempting to repeat the analysis that the original researchers did or if you are doing something different.\nExplain the motivation for the questions or the predictions your analysis examines.\nSummarize the methods that were used to collect the data you analyzed.\nIdentify what variables are included in the analysis: what variable is the outcome (or dependent) variable, and what variables are the predictor (or independent) variables.\nDescribe the analysis method you use to address the question or test the predictions.\nExplain your analysis method choices: why are you using this method?\nIdentify the model you use in your analysis.\nPresent a summary of the analysis results: use text and plots to show what you found.\nExplain the implications of the results, in theoretical or in policy terms.\nCritically evaluate your findings: reflect on the strength or the limitations of the evidence you present, as answers to the questions or as tests of the predictions you outlined.\nPredict: how can future research build on the work you have done?\n\n\n\n\n\n\n\nTip\n\n\n\n\nYou will write each answer in sentences, organized in one or more paragraphs.\nYou will submit your analysis code in an appendix.\nWhere appropriate, you will include plots.\nSubmit your report as a series of answers to the questions listed in a single document.\nYou can use each question as a heading in the document.\nWord count limit: no more than 1500 words are allowed for all materials except references, appendices, and the content of tables.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 6. The structured research report -- Quick start"
    ]
  },
  {
    "objectID": "PSYC411/part2/report-preface.html#sec-report-quick-how",
    "href": "PSYC411/part2/report-preface.html#sec-report-quick-how",
    "title": "Week 6. The structured research report – Quick start",
    "section": "6.3 How you can do the analysis work",
    "text": "6.3 How you can do the analysis work\nReports will concern, usually, findings from analyses of data collected in previous studies or data accessed from online sources. These data will usually be associated with a published report in a journal like Psychological Science or a pre-print archive like PsyArXiv.\nWe expect students to use one of the analysis methods taught in the module.\n\n\n\n\n\n\nTip\n\n\n\nIf you want to know more about how to do this work, you can read in-depth step-by-step guidance in the how-to chapter.\n\n\nYou can see information on APA formatting of statistics and numbers in the OWL Purdue guide. Though the APA guidelines are the authoritative guide.\n\n6.3.1 Report checklist\n\n\n\n\n\n\nChecklist\n\n\n\nTo help you organize your work in preparing for the research report assignment, we share a downloadable checklist: PSYC411-research-report-checklist.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 6. The structured research report -- Quick start"
    ]
  },
  {
    "objectID": "PSYC411/part2/report-preface.html#sec-report-quick-why",
    "href": "PSYC411/part2/report-preface.html#sec-report-quick-why",
    "title": "Week 6. The structured research report – Quick start",
    "section": "6.4 Why we are asking you to do this",
    "text": "6.4 Why we are asking you to do this\nWe are asking you to do the structured research report because doing the work — the analysis and the writing — will build your awareness and understanding of how psychological science really works, and will strengthen your sense of what best practice looks like in modern science.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 6. The structured research report -- Quick start"
    ]
  },
  {
    "objectID": "PSYC411/part2/report-preface.html#sec-report-intro-lecture",
    "href": "PSYC411/part2/report-preface.html#sec-report-intro-lecture",
    "title": "Week 6. The structured research report – Quick start",
    "section": "6.5 Lectures and slides",
    "text": "6.5 Lectures and slides\n\n6.5.1 Lecture recordings – videos\nClick on a link and your browser should open a tab showing the Panopto video for the lecture part. You should be able to access the videos anywhere; you should not need to be on campus or logged on to the university VPN to view the videos.\n\nOverview (20 minutes): the key ideas, the scientific context\n\n\n\nOverview (20 minutes): the research workflow, multiverse analyses\n\n\n\nOverview (20 minutes): kinds of reproducibility, open data, and doing better science.\n\n\n\n\n6.5.2 Lecture recordings – slides\nYou can download the lecture slides as a single downloadable .html file that you can open in any browser: 411-research-report.html. This can be opened in a browser and presents the slides as they are delivered.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 6. The structured research report -- Quick start"
    ]
  },
  {
    "objectID": "PSYC411/part2/report-preface.html#sec-report-quick-how-mark",
    "href": "PSYC411/part2/report-preface.html#sec-report-quick-how-mark",
    "title": "Week 6. The structured research report – Quick start",
    "section": "6.6 How we will mark your work",
    "text": "6.6 How we will mark your work\nA distinction requires the following.\nA. Background and methods\n\nThere should be a coherent, logical, argument for how the background or context leads to the questions or predictions.\nThere should be a concise explanation of the background or context for the research: Why did the original study authors do the research? Why are you doing the analysis you report?\nThere should be a specific concrete statement of the research question that your analysis addresses, or the hypothesis that your analysis tests.\nIn the method information, there should be a concise summary of the properties of the variables — how data were collected, how variable values were coded or scored — so that the reader can understand the structure of the data.\n\nB. Analysis and results\n\nThere should be a clear account of which variables are included in the analyses, so that the reader can understand what outcome (dependent) variable is being analyzed using what predictor (independent) variables.\nThere should be a scholarly explanation of the analysis you did, including an account of why you chose to use the method you use, given possible alternatives and, if relevant, referring to justifications for your choice that are identified in the statistical literature.\nThere should be a clear identification of what analysis you did, using what method, outlining the elements and structure of the model whose results you present. -There should be a clear presentation of the results, presenting a correct interpretation of statistical information concerning the size, the direction (or sign), and the significance of any effect you estimate or difference you test. The presentation should explain how the outcome is expected to vary, on average, given the differences or the effects estimated using your analyses.\nThere should be full and correct use of APA conventions in the presentation of statistical results.\nWhere appropriate, results are presented using both text and relevant, informative, visualizations.\n\nC. Implications and critical reflection\n\nThere should be an accurate account of whether or how the analysis results address the questions, or how the analysis results support or disconfirm the predictions stated.\nThere should be a scholarly explanation of the implications of the results: what do the results suggest we now know that we did (or did not) know before?\nThere should be a critical analysis of the strengths or limitations of the evidence: how confident can we be, how uncertain should we be, that the results that are presented do or do not suggest what you think they suggest.\nThere should be a constructive analysis of how future research could build on your findings to further extend understanding or evaluate methodological concerns.\n\nA merit will be awarded, by comparison, if:\nA. Background and methods\n\nThere is a concise explanation of the background or context for the research. There is limited or partial information on why the original study authors did the research, or why you are you doing the analysis you report. We do not see an argument explaining the reasons motivating your research.\nThere is a clear statement of the research question, or the hypothesis or prediction that your analysis tests. But the question or the prediction may be vague, or stated in general not specific terms.\nThe summary of the properties of the variables may not be sufficiently clear or informative about the structure of the data that the reader can understand what data are involved in analyses.\n\nB. Analysis and results\n\nThere is a clear account of which variables are included in the analyses, so that the reader can understand what outcome (dependent) variable is being analyzed using what predictor (independent) variables.\nThere is an explanation of the analysis you did, but there is limited explanation of why you chose to use the method you use. There is no or there is limited awareness of possible alternatives. No informed justification is presented for analysis choices.\nThere is a clear identification of what analysis you did, using what method, outlining the elements and structure of the model whose results you present.\nThere is evidence that the analysis method you chose was used correctly, that the method was appropriate to the question and the data, and that the code you used could do the analysis you say it did.\nThere is a clear presentation of the results, presenting a correct interpretation of statistical information concerning the size, the direction (or sign), and the significance of any effect you estimate or difference you test. The presentation should explain how the outcome is expected to vary, on average, given the differences or the effects estimated using your analyses.\nThere is full and correct use of APA conventions in the presentation of statistical results.\nWhere appropriate, results are presented using both text and relevant, informative, visualizations.\n\nC. Implications and critical reflection\n\nThere is an accurate account of whether or how the analysis results address the questions, or how the analysis results support or disconfirm the predictions stated.\nThere is some explanation of the implications of the results but this account is not linked to previous research (in the literature).\nThere is limited reflection on the strengths or limitations of the evidence. We may see generic but not well informed discussion of the evidence specific to your analysis or your data.\nThere is limited discussion of future research.\n\nA pass will be awarded, by comparison, if:\nA. Background and methods\n\nThere is a concise explanation of the background or context for the research. There is limited or no information on why the original study authors did the research or why you are you doing the analysis you report.\nThere is a statement of the research question, or the hypothesis or prediction that your analysis tests. But the question or the prediction may be vague, or stated in general terms only.\nThe summary of the properties of the variables provides information on the structure of the data but it may be unclear how one or more variables were coded or scored. There is limited engagement with questions of measurement reliability or validity.\n\nB. Analysis and results\n\nThere is a clear account of which variables are included in the analyses, so that the reader can understand what outcome (dependent) variable is being analyzed using what predictor (independent) variables.\nThere is an explanation of the analysis you did, but there is no explanation of why you chose to use the method you use. Limited or no informed justification is presented for analysis choices.\nThere is a clear identification of what analysis you did, using what method, outlining the elements and structure of the model whose results you present.\nThere is evidence that the analysis method you chose was used correctly, that the method was appropriate to the question and the data, and that the code you used could do the analysis you say it did.\nThere is a clear presentation of the results, presenting a correct interpretation of statistical information concerning the direction (or sign) and the significance of any effect you estimate or difference you test. The presentation should explain how the outcome is expected to vary, on average, given the differences or the effects estimated using your analyses.\nThere may be partial use of APA conventions in the presentation of statistical results.\nAppropriate visualizations may be included but without comment or discussion.\n\nC. Implications and critical reflection\n\nThere is a summary account of the results but it may not clearly explain whether or how the analysis results address the questions, or how the analysis results support or do not support the predictions stated.\nThere is limited or no explanation of the implications of some of the results.\nThere is limited or no reflection on the strengths or limitations of the evidence.\nThere is no discussion of future research.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 6. The structured research report -- Quick start"
    ]
  },
  {
    "objectID": "PSYC411/part2/how.html",
    "href": "PSYC411/part2/how.html",
    "title": "Week 6. How you can do the analysis work",
    "section": "",
    "text": "The PSYC411 Research Report assignment requires students to locate, access, analyse and report previously collected data.\n\n\n\n\n\n\nTip\n\n\n\nHere, we answer the question:\n\nHow can the assignment be done?\n\n\n\nWe outline the workflow you can follow, proceeding through a series of steps, to complete the essential tasks. Look at this outline, make a plan, and then follow the advice, taking it one step at a time.\nIf you want to understand why we think you will benefit from doing this, you can read the explanation in the chapter Why we are asking you to do this.\nIf you want to remind yourself about what you are expected to do, you can read about that in the section What you have to write.\nWe provide information on assessment criteria and our approach to marking in the section How we will mark your work.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 6. How you can do the analysis work"
    ]
  },
  {
    "objectID": "PSYC411/part2/how.html#sec-how",
    "href": "PSYC411/part2/how.html#sec-how",
    "title": "Week 6. How you can do the analysis work",
    "section": "",
    "text": "The PSYC411 Research Report assignment requires students to locate, access, analyse and report previously collected data.\n\n\n\n\n\n\nTip\n\n\n\nHere, we answer the question:\n\nHow can the assignment be done?\n\n\n\nWe outline the workflow you can follow, proceeding through a series of steps, to complete the essential tasks. Look at this outline, make a plan, and then follow the advice, taking it one step at a time.\nIf you want to understand why we think you will benefit from doing this, you can read the explanation in the chapter Why we are asking you to do this.\nIf you want to remind yourself about what you are expected to do, you can read about that in the section What you have to write.\nWe provide information on assessment criteria and our approach to marking in the section How we will mark your work.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 6. How you can do the analysis work"
    ]
  },
  {
    "objectID": "PSYC411/part2/how.html#sec-how-variety",
    "href": "PSYC411/part2/how.html#sec-how-variety",
    "title": "Week 6. How you can do the analysis work",
    "section": "The variety of things students do",
    "text": "The variety of things students do\nStudents have taken a variety of approaches to the assignment.\n\nAnalyzing public data that have been previously analysed — Some students choose to complete an analysis of a publicly available data-set that has been analysed previously, where the analysis report has been published in a journal article.\nUsing demonstration data — Some students choose to complete an analysis of one of the data-sets used for practical exercises in class: the example or demonstration data we collect together as curated data-sets.\nAnalyzing public data that have not been previously analysed — Some students choose to complete an analysis of a publicly available data-set where an analysis report has not been published in a journal article.\n\nAsk in class or on the Moodle discussion forum for advice about any one of these approaches.\nI consider, next, working with data-sets where an analysis of the data has been presented in the article. I then discuss Working with data that are not associated with a published analysis.\nOur advice on working with data-sets presented without an analysis will overlap in key respects with our advice on working with curated demonstration data.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 6. How you can do the analysis work"
    ]
  },
  {
    "objectID": "PSYC411/part2/how.html#sec-publishedanalysis",
    "href": "PSYC411/part2/how.html#sec-publishedanalysis",
    "title": "Week 6. How you can do the analysis work",
    "section": "Working with data associated with a published analysis",
    "text": "Working with data associated with a published analysis\nIn the following, I split our guidance into two parts.\n\nI look next at the task of locating, accessing and checking the data.\nThen I look at the task of how you can Plan the analysis you want to do.\n\nObviously, you cannot consider an analysis if you cannot be sure that you can work with the data (Minocher et al., n.d.).\n\nLocate, access and check the data\nAt the start of your work on the assignment, you will need to (1.) locate then (2.) access data for analysis, and then you will need to (3.) check that the data are usable. I set out advice on doing each step, following. Work through the steps: one step at a time.\n\nLocate\nIt is usually helpful to find a data-set where the data have been collected in a study within a topic area you care about, or could be interested in. It is helpful because you will need to work with the data and it will be motivating if you are interested in what the data concern. And it is helpful because, often, you will need to do a bit of reading on related research to learn about the context for the data collection, and you will usually want to read research sources that interest you.\n\n\n\n\n\n\nTip\n\n\n\nThe task here is:\n\nDo a search: look for an article with usable data in a topic area that interests you.\n\n\n\nThere are at least two ways you can do this. Both should be reasonably quick methods to get to a usable data-set.\n\nDo a search on Google scholar).\nDo a search on the webpages of a journal.\n\nMost psychological research is published in journals like Psychological Science. If you want, you can look at a list of psychology journals here.\nIn a journal like Psychological Science you can look through lists of previously published articles (in issues, volumes, by year) on the journal webpage. Here is the list of issues for Psychological Science..\n\nKey words\nIn both methods, you are looking for an article associated with data (and maybe analysis code) you can access and that you are sure you can use. In both methods, you need to first think about some key words to use in your search. Ask yourself:\n\nWhat are you interested in? What population, intervention or effect, comparison, or outcome?\n\nThen:\n\nWhat words do people use, in articles you have seen, when they talk about this thing?\n\nYou can use these words, and maybe consider alternate terms. For example, I am interested in reading comprehension or development reading comprehension but researchers working on reading development might also refer to children reading comprehension.\nYou want to be as efficient as possible so combine your search for articles in an interesting topic area with your search for accessible data. We can learn, here, from the evidence on researcher practice, discussed in the Why chapter section Data and code sharing: we can look for specific markers that data associated with an article should be accessible.\nIf you are doing a search (1.) on Google scholar), I would use the key words related to your topic plus words like: open data badge; open science badge. So, for example, I would do a search for the words: reading comprehension open data badge. I have done this: you can try it. The search results will list articles related to the topic of reading comprehension, where the authors claim to have earned the open data badge because they have made data available.\nIf you are doing a search (2.) in a journal list of articles, then what you are looking for are articles that interest you and which are listed with open data badges. In the listing for Psychological Science (here)) a quick read of the journal issue articles index shows that article titles are listed together with symbols representing the open science badges that authors have claimed.\nIn other journals (e.g., PLOS ONE, PeerJ, Collabra), you may be looking for interesting articles with the words Data Availability Statement, Data Accessibility Statement, Supplementary data or Supplementary materials in the article webpage somewhere. Journals like PeerJ or Collabra, in particular, make it easy to locate data associated with published articles on their web pages.\nIn Collabra, you can find published articles through the journal webpage (here). If you click on the title of any article, and look at the article webpage, then on the left of the article text, you can see an index of article contents and that index lists the Data Availability Statement. Click on that and you are often taken to a link to a data repository.\n\n\n\nAccess\nIf you have located an interesting article with evidence (an open data badge or a data accessibility statement) that the authors have shared their data, you need to check that you can access the data. Most of the time, now, you are looking for a link you can use to go directly to the shared data. The link is often presented as a hyperlink on a webpage, associated with Digital Object Identifiers (DOIs) or Universal resource locators (URLs). Or, increasingly, you are looking for a link to a data repository on a site like the Open Science Framework (OSF).\n\n\n\n\n\n\nTip\n\n\n\nThe task here is:\n\nAccess the data associated with the article you have found.\n\n\n\nHere are some recent examples from my work that you can check, to give you a sense of where or how to find the accessible link to the shared data.\n\nRicketts, J., Dawson, N., & Davies, R. (2021). The hidden depths of new word knowledge: Using graded measures of orthographic and semantic learning to measure vocabulary acquisition. Learning and Instruction, 74, 101468. https://doi.org/10.1016/j.learninstruc.2021.101468\n\n\nRodríguez-Ferreiro, J., Aguilera, M., & Davies, R. (2020). Semantic priming and schizotypal personality: Reassessing the link between thought disorder and enhanced spreading of semantic activation. PeerJ, 8, e9511. https://doi.org/10.7717/peerj.9511\n\nThese are both open access articles.\nIf you look at the webpage for Rodríguez-Ferreiro et al. (2020), (here)), you can do a search in the article text for the keyword OSF (on the article webpage, use keys CMD-F plus OSF). You are checking to see if you can click on the link and and if clicking on the link takes you to a repository listing the data for the article. The Rodríguez-Ferreiro et al. (2020) article is associated with a data plus analysis code repository (OSF))\nNotice that on the repository webpage, you can see a description of the project plus .pdf files and a folder data-set and Code. If you can click through to the folders, and download the datafiles, you have accessed the data successfully.\nI have guided you, here, through to the Rodríguez-Ferreiro et al. (2020) data repository, can you find the data for the Ricketts et al. (2021) repository?\n\n\nCheck\nIf you have located an interesting article with data that you can access, and if you have read the Why chapter discussion on Enabling others to check or query analyses, then you will be ready, next, to check that you can use the data.\n\n\n\n\n\n\nTip\n\n\n\nThe task here is:\n\nCheck the data and the data documentation to make sure you can understand what you have got and whether you can use it.\n\n\n\nWhat make data usable are:\n\nInformation in the article, or in the data repository documentation, on the study design and data collection methods: you need to be able to understand where the data came from, how they were collected, and why.\nClear data documentation: you need to find information on the variables, the observations, the scoring, the coding, and whether and how the data were processed to get them from raw data state to the data ready for analysis.\n\nData documentation is often presented as a note or a wiki page or a miniature paper and may be called a codebook, data dictionary, guide to materials or something similar. You will need to check that you can find information on (examples shown are from the Rodríguez-Ferreiro et al. (2020) OSF guide to materials):\n\nwhat the data files are called, e.g., PrimDir-111019.csv;\nhow the named data files correspond to the studies presented in the report;\nwhat the data file columns are called and what variables the column data represent, e.g., relation, coding for prime-target relatedness condition ...;\nhow scores or responses in columns were collected or calculated, e.g., age, giving the age in years ...;\nhow coding was done, if coding was used, e.g., biling, giving the bilingualism status;\nwhether data were processed, how missing values were coded, whether participants or observations were excluded before analysis, e.g., Missing values in the rt column ... coded as NA\n\n\n\n\n\n\n\nWarning\n\n\n\nIf these information are not presented, or are not clear: walk away.\n\n\n\n\n\nPlan the analysis you want to do\nAfter you have found an interesting article, and have confirmed that you can use the associated data, you will need to plan what analysis you want to do.\n\n\n\n\n\n\nTip\n\n\n\nThe task here is:\n\nIdentify and understand the analysis in the article.\nWork out what analysis you want to do.\n\n\n\nStudents have taken a variety of approaches to the assignment.\n\nSome students choose to complete a reanalysis of the data, in an attempt to reproduce the results presented in the article: you can learn about this in the Why chapter discussion on Methods reproducibility, and you can use the advice in the next section on Completing a reanalysis of the data.\nSome students choose to complete an alternate analysis of the data, varying elements of the analysis: you can learn about this in the Why chapter discussion on Multiverse analyses, and you can use the advice in the following section on Completing an alternate analysis of the data.\n\nEither way, you will want to first make sure you can identify exactly what the authors of the original study did, how they did it, and why they did it.\nYou can process the key article information efficiently using the QALMRI method we discussed in the PSYC413 class on graduate writing skills (Brosowsky et al., n.d.; Kosslyn & Rosenberg, 2005), summarised in the slides from that class. You are first aiming to locate information on the broad and the specific question the study addresses, the methods the study authors used to collect data, the results they report, and the conclusions they present given the results.\nCan you find these bits of information?\n\nCompleting a reanalysis of the data\nAre you interested in attempting a methods reproducibility test?\nFollowing Hardwicke and colleagues (Hardwicke et al., n.d.; Hardwicke et al., 2018), it would be sensible to focus on identifying the primary or substantive result for a study in an article.\n\nThe result is substantive if the researchers state that it is (e.g., “Our critical analysis is…”), if it is emphasized in the abstract, or if it is presented in a table or figure.\n\nAs we discussed in the class on graduate writing skills, the article authors should signal what they consider to be the primary result for a study by telling you that a result is critical or key or that a result is the or an answer to their research question.\n\n\n\n\n\n\nTip\n\n\n\n\nAn article may present multiple studies: focus on one.\nThe results section of an article, for a study, may list multiple results: identify the primary or substantive result.\n\n\n\nYou will want to identify a result that is both substantive and straightforward (Hardwicke et al., n.d.; Hardwicke et al., 2018).\n\nThe result is Straightforward if the outcome could be calculated using the kind of test you have been learning about or will learn about (e.g., t-test, correlation, the linear model)\n\nPsychological science researchers use a variety of data analysis methods and not all the analyses that you read about will be analyses done using methods that you know about. The use of the methods we teach — t-test, correlation, and the linear model — are very very common; that is why we teach them. But you may also see reports of analyses done using methods like ANOVA, and multilevel or (increasingly) linear mixed-effects models (Meteyard & Davies, 2020).\nIn research on the reproducibility of results in the literature (see the Why chapter section on Enabling others to check or query analyses), the researchers attempting to reproduce results often focused on answering the research question the original authors stated using the data the original authors shared. This does not mean that they always tried to exactly reproduce an analysis or an analysis result. Sometimes, that was not possible.\nSometimes, you will encounter an article and a data-set you are interested in but the analysis presented in the article looks a bit complicated, or more complex than the methods you have learned would allow you to do. In this situation, don’t give up.\nWhat you can do – maybe with our advice – is identify a part of the primary result that you can try to reproduce. For example, what if the original study authors report a linear mixed-effects analysis of the effects of both prime relatedness and schizotypy score on response reaction time (Rodríguez-Ferreiro et al., 2020)? Maybe you have not learned about mixed-effects models, or you have not learned about analysing the effects of two variables but you have (you will) learn about analysing the effect of one variable using the linear model method: OK then, do an analysis of the shared data using the method you know.\nYou may be helped, here, by knowing about two good-enough (mostly true) insights from statistical analysis:\n\nMany of the common analysis methods you see used in psychological science can be coded as a linear model.\nMore advanced common analysis methods — (Generalized) Linear Mixed-effects Models (GLMMs) — can be understood as more sophisticated versions of the linear model. (Conversely, the linear model can be understood as an approximation of a GLMM.)\n\nThere is a nice discussion of the idea that common statistical tests are linear models here.\n\n\n\n\n\n\nTip\n\n\n\n\nIdentify the analysis method used to get the result you are interested in.\nIf it is complex or unfamiliar, discuss whether a simpler method can be used.\nIf the result is complex, discuss whether you can attempt to reproduce a part or a simpler result.\n\n\n\n\n\nCompleting an alternate analysis of the data\nAre you interested in attempting a different analysis than the analysis you see in the journal article?\nIt can be interesting and important work to complete a simpler analysis of shared data. Sometimes, we learn that a simpler analysis provides a good account of the behaviour we observe, perhaps as good an account as that produced using other, more complex, analyses. This can happen if, for example, our theory predicts that two effects should work together but an analysis shows that we can explain behaviour in an account in which the two effects are independent. For example, Ricketts et al. (2021) predicted that children should learn words more effectively if they were shown the spellings of the words and if they were told they would be helped by seeing the spelling but, in our data, we found that just seeing the spellings was enough to explain the learning we observed.\nIn completing analyses that vary from original analyses, we are engaging in the kind of work people do when they do multiverse analyses, also known as sensitivity checks or robustness checks (see the Why chapter section on the Analysis multiverse).\n\n\n\n\n\n\nTip\n\n\n\nIn planning an alternate or multiverse analysis, do not suppose that you need to do multiple analyses: you do not.\n\n\nIn planning an alternate or multiverse analysis, you will want to begin by critically evaluating the analysis you see described in the published article. I talk about how to do this, next.\nBefore we go on, note that I previously discussed an example of how to critically evaluate the results of published research in the context of Rodríguez-Ferreiro et al. (2020). Take a look at the Introduction of that article. There, we summarised the analyses researchers did previously and used the information about the analyses to explain inconsistencies in the research literature. We found limitations in the analyses that people did that had (negative) consequences for the strength of the conclusions we can take from the data.\n\nCritically evaluate the analysis description\nIf you revisit our discussion of multiverse analyses, you will see that we discussed two things:\n\nanalyses of the impact on results of varying how you construct data-sets before analysis — see the Why chapter section on The data multiverse;\nanalyses of the impact on results of varying what analysis method you use, or how you use the method — see the Why chapter section on Analysis multiverses.\n\nLearning about (1.) the ways in which data-set construction can vary, and (2.) the ways in which analysis choices can vary are both good preparation for approaching the task of critically evaluating any description of the analysis you see in any published article.\nAs we noted in the Why chapter section on multiverse data, you almost always have to process the data you collect (in an experiment or a survey) before you can analyse the data. Often, this means you need to code for responses to survey questions, e.g., asking people to self-report their gender, or you need to identify and code for people making errors when they try to do the experimental task you set them, or you need to process the data to exclude participants who took too long to do the task (if taking too long is a problem). Not all of these processing steps will have an impact on the results but some might. This is why you can sometimes do useful and sometimes original research work in reanalysing previously published data.\nYou can begin your analysis planning work by first identifying exactly what data processing the original study authors did then identifying what different data processing they could have done. Remember the research we discussed in relation to reproducibility studies: you need to be prepared for the possibility that it is sometimes challenging to identify what researchers did to process their data for analysis (see more information in. To identify the information you need, look for keywords like code, exclude, process, tidy, transform in the text of the article, or look for words like this in the documentation you find in the data repository.\nWhen you have identified this information, you can then consider three questions:\n\nWhat data processing steps were completed before analysis?\nWhat were the reasons given explaining why these processing steps were completed?\nWhat could happen to the results if different choices were made?\n\nWorking through these questions can then get you to a good plan for an analysis of the data. For example, a simple but useful analysis you can do is to check what happens to the results if you do an analysis with data from all the participants tested, if participants were originally excluded (for some reason) in the data processing step. Obviously, if the original study authors only share processed data (after exclusions), you cannot do this kind of work. Another simple but useful analysis you can do is to check what happens to the results if you change the coding of variables. Sometimes different coding of categorical variables (e.g., ethnicity) are reasonable. For example, you can ask: what happens if you analyse the impact of the variable given a different coding? (In case you are reading these notes and thinking about recoding a factor, there are some useful functions you can use; read about them here.)\n\n\n\n\n\n\nTip\n\n\n\n\nDo you want to check the impact of varying data processing choices: check, do you need and have access to the raw data? can you see how to recode variables?\n\n\n\nAs we noted in the Why chapter section on multiverse analyses, when we consider how to answer a research question with a data-set, it is often possible to imagine multiple different analysis methods: potentially, all reasonable alternatives. Most often, this is most clearly apparent when we are looking at an observational data-set or data collected given a cross-sectional study design.\nIn cross-sectional or observational studies, we typically are not manipulating experimental conditions, and we are often analysed data using some kind of linear model. We often collect data or have access to data on a number of different variables relevant to our interests. For example, in studies I have done on how people read (R. Davies et al., 2013; R. A. I. Davies et al., 2017), we wanted to know what factors would predict or influence how people do basic reading tasks like reading aloud. We collected information on many different kinds of word properties and on the attributes of the participants we tested. (Note: the papers are associated with data repositories in Supplementary Materials.) It is often an open question which variables should be included in a prediction model of the observed outcome (reading response reaction times). Therefore, if you are interested in a study like this, and can access usable data from the study, it will typically be true that you are able to sensibly motivate a different analysis of the study data using a different choice of variables.\nAs discussed in a number of interesting analyses over the years (e.g., Patel et al., 2015), researchers may be interested in the specific impact of one particular predictor variable (e.g., we may be interested in whether it is easier to read words we learned early in life), but will need to include in their analysis that variable plus other variables known to affect the outcome. In that situation, the effect of the variable of interest may appear to be different depending on what other variables are also analysed. This makes it interesting and useful to check the impact of different analysis choices.\nWe will look at data like these, for analyses involving the linear model, in our classes on this method.\n\n\n\n\n\n\nTip\n\n\n\n\nDo you want to check the impact of different analysis choices: check, do you need and have access to a choice of variables?\nCan you think of some reasons to justify using a different choice of variables in your analysis.\n\n\n\n\n\n\n\nSummary: working with data associated with a published analysis\nHere’s a quick summary of the advice we have discussed so far.\n\nAt the start of your work, you will need to (1.) locate then (2.) access data for analysis, and then you will need to (3.) check that the data are usable.\nOnce you have confirmed you have found interesting data you can use, you should plan your analysis.\nStudents do a variety of kinds of analysis. Whatever your interest, you first will want to first make sure you can identify exactly what the authors of the original study did, how they did it, and why they did it.\nIf you are interested in completing a reanalysis, attempting a methods reproducibility test (can you repeat a result, given shared data?) you will perhaps benefit from focusing on a result that is both substantive and straightforward.\nIf you are interested in doing an alternate or multiverse analysis, you can critically evaluate the data processing and the data analysis choices that the original study authors made. You can consider whether other choices would be appropriate, and might sensibly motivate a (limited) investigation of the impact of different analysis choices on the results.\n\nWhat if you access interesting data that were shared but that are not associated with a published analysis? We talk about that situation, next.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 6. How you can do the analysis work"
    ]
  },
  {
    "objectID": "PSYC411/part2/how.html#sec-noanalysis",
    "href": "PSYC411/part2/how.html#sec-noanalysis",
    "title": "Week 6. How you can do the analysis work",
    "section": "Working with data that are not associated with a published analysis",
    "text": "Working with data that are not associated with a published analysis\nA number of data-sets have been published online with information about the data but with no analysis. You can look for data that may be interest you in a number of different places, now, but I would focus on one. I talk about that next. Then I offer some guidance on Thinking about analyses of open data.\n\nLooking for open data\nWicherts and colleagues set up the Journal of Open Psychology Data (JOPD) to make it easier for Psychologists to share experimental data. A link to the journal webpage is here) Usually, a data paper reports a study and provides a link to a downloadable data-set.\nSome data-sets that I have looked at in JOPD and other places include those described following.\n\n\n\n\n\n\nTip\n\n\n\nI identify these examples because they present interesting, rich, and readily accessible data-sets that could be used in a variety of different kinds of analyses.\n\n\n\nWicherts intelligence and personality data\nWicherts did what he recommended and put a large data-set online here\nYou can analyse these data in a number of different interesting ways. You can explore relationships between gender, intelligence and personality differences.\nThe data file and an explanatory document are located at the end of the article. Read the article, it’s worth your time.\nWicherts reports:\n\nThe file includes data from our freshman-testing program called “Testweek” (Busato et al., 2000, Smits et al., 2011 and Wicherts and Vorst, 2010) in which 537 students (age: M = 21.0, SD = 4.3) took the Advanced Progressive Matrices ( Raven, Court, & Raven, 1996), a test of Arithmetic, a Number Series test, a Hidden Figures Test, a test of Vocabulary, a test of Verbal Analogies, and a Logical Reasoning test (Elshout, 1976).\nAlso included are data from a Dutch big five personality inventory (Elshout & Akkerman, 1975), the NEO-PI-R (Hoekstra, Ormel, & Fruyt, 1996), scales of social desirability and impression management (based on work by Paulhus, 1984 and Wicherts, 2002), sex of the participants, and grade point averages of the freshmen’s first trimester that may act as outcome variable.\n\n\n\nSmits personality data\nSmits and colleagues (including Wicherts) put an even larger data-set online at the Journal of Open Psychology Data here)\nYou will need to register to be able to download the data but the process is simple.\nThe Smits data-set includes Big-5 personality scores for several thousand individuals recorded over a series of years. You can analyse these data in interesting ways including examining changes in personality scores among students over different years.\n\n\nEmbodied terror management\nTjew A Sin and colleagues shared a data-set at the Journal of Open Psychology Data on an interesting study they did to test the idea that interpersonal touch or simulated interpersonal touch can relieve existential concerns (fear of death) among individuals with low self-esteem. The data can be found here)\nThe Tjew A Sin can be downloaded from a link to a repository location, given at the end of the article. You will likely need to register to download the data. Note that the spreadsheets holding the study data include 999 values to code for missing data. Note also that the data spreadsheets include (in different columns) scores per participant for various measures e.g. mortality anxiety or self-esteem. The measures are explained in the paper. To use the data, you will need to work out the simple process of how to sum the scores across items to get e.g. a measure of self-esteem for each person.\n\n\nDemographic influences on disgust\nBerger and Anaki shared data on the disgust sensitivity of a large sample of individuals. The data are from the administration of the Disgust Scale to a set of Hebrew speakers. They can be found here)\nThe experimenters collected data on participants’ characteristics so that analyses of the way in which sensitivity varies in relation to demographic attributes is possible. You will see that the disgust scale is explained in the paper. The different disgust scores, for each item in the disgust scale, can be found in different columns. The disgust scores, for person, are calculated overall as values: Mean_general_ds, Mean_core, Mean_Animal_reminder, Mean_Contamination\nWhen you download the data-set, you may need to change the file name — adding a suffix: .txt (for the tab delimited file), to be opened in Excel, or .sav (for the SPSS data file), to be opened in SPSS — to the file name to allow you to open it in the appropriate application.\n\n\n\nThinking about analyses of open data\nThe availability of rich, curated, clearly usable data-sets with many variables can make it challenging to decide what to do.\nI would advise beginning with an exploratory analysis of the data you have accessed. You will want to begin by using the data visualization skills we have taught you to examine:\n\nThe distributions of the variables that interest you using histograms, density plots or bar charts.\nThe potential relationship between variables using scatterplots.\n\nIn such Exploratory Data analyses, you are interested in what the data visualization tells you about the nature of the data-set you have accessed. The papers associated with the data-sets can sometimes offer only outline information: how the data were collected, coded, and processed. You may need to satisfy yourself that there is nothing odd or surprising about the distributions of scores. This stage can help you to identify problems like survey responses with implausible scores.\nThe work you do in exploring, and summarizing, the data variables that interest you will often constitute a substantial element of the work you can do and present for your report. You may discuss, for advice, what parts of this work will be interesting or useful to present.\nThen, our advice is simple.\n\n\n\n\n\n\nTip\n\n\n\n\nWhen working with open data-sets, consider keeping the analysis simple.\n\n\n\nNote that simple is relative. Do what interests you. Work with the methods you have learned or will learn (the linear model).\nIn practice, you will find that part of the challenge is located not in using the data or in running an analysis like a linear model, it is in (1.) justifying or motivating the analysis and (2.) explaining the implications of your findings.\nWorking on the thinking you must develop to motivate an analysis or to explain implications requires you to do some (limited) reading of relevant research. (Relevant sources will be cited in data papers, as part of their outline of the background for their data collection.) If you consider the advice we discussed in the graduate class on developing writing skills, you will see that there I talked about how you might extract data from a set of relevant sources (papers) to get an understanding of the questions people ask, the assumptions they make. That is the kind of process you can follow to develop your thinking around the analysis you will do. What you are looking for is information you can use so that you can say something brief about, for example, why it might be interesting to analyse, say, whether personality (measured using the Big-5) varies given differences in gender or differences between population cohorts. The reading and the conceptual development should be fairly limited, not extensive, but should be sufficient that you can write something sensible when you introduce and then when you discuss your analysis results.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 6. How you can do the analysis work"
    ]
  },
  {
    "objectID": "PSYC411/part2/how.html#sec-how-summary",
    "href": "PSYC411/part2/how.html#sec-how-summary",
    "title": "Week 6. How you can do the analysis work",
    "section": "Summary: how",
    "text": "Summary: how\nIn this chapter, I have outlined some advice on how you might approach the task of locating, accessing, and analyzing previously collected data.\n\n\n\n\n\n\nTip\n\n\n\nThe main advice is to think about your workflow in stages, then progress through the work one step at a time.\n\n\nYou will need to begin by assuring yourself that you can find a data-set that interests you, and that you can access and use the data. The usability of data will require clear, understandable, descriptions in the published article (if any) about the research question and hypothesis, the study design, the data collection methods, the data processing steps, and the data analysis (if any). Sometimes, useful information about data processing and data analysis can be found in detail in repository documentation (e.g., in guides to materials) but only referenced in the text of the article.\nIf you know you can locate, access and have checked data as usable, you will want to think about what analysis you want to do the data. The approach you take depending on what aims you would like to pursue.\nIf you are interested in attempting a methods reproducibility test (i.e., checking if you can repeat presented results, given shared data), then you will first need to identify a substantive and straightforward result to try to reproduce. If you identify a primary result to examine, you will want to check that you can work with the data that have been shared, and then that you can use the analysis methods you have learned to reproduce some or all of the result that interests you.\nIf you are interested in doing an alternate or a different analysis (from what may be presented), you may need to consider the information you can locate on data processing and on data analysis choices. Did the original study authors process the data before sharing it, how? are the raw data available? What analyses did the authors do and why? When you consider this information, you may critically evaluate the choices made. In the context of this critical evaluation, you may find good reasons to justify doing a different analysis, whether to examine the impact of making different data processing choices, or to examine the impact of using a different analysis method, or of applying the same method differently (e.g., by including different variables).\nIn considering an analysis of data shared without a published set of results, you may want to keep your approach simple. Focus on what analysis you can do using the methods you have learned. And think about the understanding you will need to develop, to justify the analysis you do, and to make sense, in the discussion of your report of the analysis results you will present.\nIt is always a good idea to explore your data using visualization techniques throughout your workflow.\n\n\n\n\n\n\nTip\n\n\n\n\nYou can always get advice, do not hesitate to ask.\nWe are happy to discuss your thinking, especially in class.\n\n\n\n\nReferences\n\n\nBrosowsky, N., Parshina, O., Locicero, A., & Crump, M. (n.d.). Teaching undergraduate students to read empirical articles: An evaluation and revision of the QALMRI method. https://doi.org/10.31234/osf.io/p39sc\n\n\nDavies, R. A. I., Birchenough, J. M. H., Arnell, R., Grimmond, D., & Houlson, S. (2017). Reading through the life span: Individual differences in psycholinguistic effects. Journal of Experimental Psychology: Learning Memory and Cognition, 43(8). https://doi.org/10.1037/xlm0000366\n\n\nDavies, R., Barbón, A., & Cuetos, F. (2013). Lexical and semantic age-of-acquisition effects on word naming in spanish. Memory and Cognition, 41(2), 297–311. https://doi.org/10.3758/s13421-012-0263-8\n\n\nHardwicke, T. E., Bohn, M., MacDonald, K., Hembacher, E., Nuijten, M. B., Peloquin, B. N., deMayo, B. E., Long, B., Yoon, E. J., & Frank, M. C. (n.d.). Analytic reproducibility in articles receiving open data badges at the journal psychological science: An observational study. Royal Society Open Science, 8(1), 201494. https://doi.org/10.1098/rsos.201494\n\n\nHardwicke, T. E., Mathur, M. B., MacDonald, K., Nilsonne, G., Banks, G. C., Kidwell, M. C., Hofelich Mohr, A., Clayton, E., Yoon, E. J., Henry Tessler, M., Lenne, R. L., Altman, S., Long, B., & Frank, M. C. (2018). Data availability, reusability, and analytic reproducibility: evaluating the impact of a mandatory open data policy at the journal Cognition. Royal Society Open Science, 5(8), 180448. https://doi.org/10.1098/rsos.180448\n\n\nKosslyn, S. M., & Rosenberg, R. S. (2005). Fundamentals of psychology: The brain, the person, the world, 2nd ed. Pearson Education New Zealand.\n\n\nMeteyard, L., & Davies, R. A. I. (2020). Best practice guidance for linear mixed-effects models in psychological science. Journal of Memory and Language, 112. https://doi.org/10.1016/j.jml.2020.104092\n\n\nMinocher, R., Atmaca, S., Bavero, C., McElreath, R., & Beheim, B. (n.d.). Estimating the reproducibility of social learning research published between 1955 and 2018. Royal Society Open Science, 8(9), 210450. https://doi.org/10.1098/rsos.210450\n\n\nPatel, C. J., Burford, B., & Ioannidis, J. P. A. (2015). Assessment of vibration of effects due to model specification can demonstrate the instability of observational associations. Journal of Clinical Epidemiology, 68(9), 1046–1058. https://doi.org/10.1016/j.jclinepi.2015.05.029\n\n\nRicketts, J., Dawson, N., & Davies, R. (2021). The hidden depths of new word knowledge: Using graded measures of orthographic and semantic learning to measure vocabulary acquisition. Learning and Instruction, 74, 101468. https://doi.org/10.1016/j.learninstruc.2021.101468\n\n\nRodríguez-Ferreiro, J., Aguilera, M., & Davies, R. (2020). Semantic priming and schizotypal personality: reassessing the link between thought disorder and enhanced spreading of semantic activation. PeerJ, 8, e9511. https://doi.org/10.7717/peerj.9511",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 6. How you can do the analysis work"
    ]
  },
  {
    "objectID": "PSYC411/part2/hypotheses-associations.html",
    "href": "PSYC411/part2/hypotheses-associations.html",
    "title": "Week 7. Hypotheses and associations",
    "section": "",
    "text": "Welcome to your overview of the work we will do together in Week 7.\nThis week, we focus on methods that allow us to make sense of the evidence for associations.\nLooking ahead to your professional lives, our aim is to help you to build the understanding and to learn the skills that will ensure that you can practise, inspire and manage the most effective ways to make sense of associations.\nWe will work in the context of a live research project with potential real world impacts: the Clearly understood project. Working in a concrete context will help you to make sense of what you are doing, even if you are interested in other topics.\n\n\n\n\n\n\nTip\n\n\n\nThe Clearly understood project aims to fix the problem that we are not sure how health information should be communicated so that everyone can understand it.\nWe ask the research questions:\n\nWhat person attributes predict success in understanding?\nCan people accurately evaluate whether they correctly understand written health information?\n\n\n\nAs we work together, we will be revisiting some of the ideas and techniques you have seen in previous classes, so that you can consolidate your learning. Then, we will extend your development with some new ideas to strengthen your skills.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 7. Hypotheses and associations"
    ]
  },
  {
    "objectID": "PSYC411/part2/hypotheses-associations.html#sec-associations-overview",
    "href": "PSYC411/part2/hypotheses-associations.html#sec-associations-overview",
    "title": "Week 7. Hypotheses and associations",
    "section": "",
    "text": "Welcome to your overview of the work we will do together in Week 7.\nThis week, we focus on methods that allow us to make sense of the evidence for associations.\nLooking ahead to your professional lives, our aim is to help you to build the understanding and to learn the skills that will ensure that you can practise, inspire and manage the most effective ways to make sense of associations.\nWe will work in the context of a live research project with potential real world impacts: the Clearly understood project. Working in a concrete context will help you to make sense of what you are doing, even if you are interested in other topics.\n\n\n\n\n\n\nTip\n\n\n\nThe Clearly understood project aims to fix the problem that we are not sure how health information should be communicated so that everyone can understand it.\nWe ask the research questions:\n\nWhat person attributes predict success in understanding?\nCan people accurately evaluate whether they correctly understand written health information?\n\n\n\nAs we work together, we will be revisiting some of the ideas and techniques you have seen in previous classes, so that you can consolidate your learning. Then, we will extend your development with some new ideas to strengthen your skills.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 7. Hypotheses and associations"
    ]
  },
  {
    "objectID": "PSYC411/part2/hypotheses-associations.html#sec-associations-goals",
    "href": "PSYC411/part2/hypotheses-associations.html#sec-associations-goals",
    "title": "Week 7. Hypotheses and associations",
    "section": "7.2 Our learning goals",
    "text": "7.2 Our learning goals\nThis week, we develop your critical thinking and we strengthen your practical skills.\n1. Critical thinking\n\nConcepts: how we go from ideas and questions to hypotheses\n\n2. Practical skills\n\nConcepts – associations: correlations, estimates and hypothesis tests\nSkills – visualizing covariation\nSkills – writing the code\nSkills – estimating correlations\nSkills – hypothesis tests for correlations\nSkills – interpreting and reporting correlations",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 7. Hypotheses and associations"
    ]
  },
  {
    "objectID": "PSYC411/part2/hypotheses-associations.html#sec-associations-resources",
    "href": "PSYC411/part2/hypotheses-associations.html#sec-associations-resources",
    "title": "Week 7. Hypotheses and associations",
    "section": "7.3 Learning resources",
    "text": "7.3 Learning resources\nYou will see, next, the lectures we share to explain the concepts behind the critical thinking and analysis skills you will develop, then you will see information about the practical materials you can use to practise your skills.\n\n\n\n\n\n\nTip\n\n\n\nWe think you will learn best if you first watch the lectures then do the practical exercises.\n\n\n\n7.3.1 Lectures\nThe lecture materials for this week are presented in five short parts.\nClick on a link and your browser should open a tab showing the Panopto video for the lecture part.\n\nPart 1 (12 minutes) Overview: The concepts and skills we will learn about in weeks 6-10, and why it helps to embed the classes in the context of a research project.\n\n\n\nPart 2 (18 minutes): Hypotheses, measurements and associations – how you can think critically.\n\n\n\nPart 3 (14 minutes): The live research project we will use to put our practical skills exercises and critical thinking challenges into context.\n\n\n\nPart 4 (20 minutes): Seeing, thinking and talking about associations, correlations.\n\n\n\nPart 5 (12 minutes): How we use R to estimate and test correlations. How we write about correlation results\n\n\n\n\n7.3.2 Lecture slides\n\n\n\n\n\n\nDownload the lecture slides\n\n\n\nThe slides presented in the videos can be downloaded here:\n\nThe slides exactly as presented (22 MB).\n\nYou can download the web page .html file and click on it to open it in any browser (e.g., Chrome, Edge or Safari). The slide images are high quality so the file is quite big and may take a few seconds to download.\n\n\nWe are going to work through some practical exercises, next, to stimulate your thinking and learn the practical skills you need to see, test and talk about correlation analyses of the associations between variables.\n\n\n7.4 Practical materials: data and R-Studio\nWe will work with two data files which you can download by clicking on their names (below):\n\nstudy-one-general-participants.csv.\nstudy-two-general-participants.csv.\n\nOnce you have downloaded the files, you will need to upload them to the R-Studio server to access and use the R files.\n\n\n\n\n\n\nImportant\n\n\n\nHere is a link to the sign-in page for R-Studio Server\n\n\n\n\n7.4.1 Practical materials guide\nYou will find that the practical exercises are simpler to do if you follow these steps in order.\n\nThe data — We will take a quick look at what is inside the data files so you know what everything means.\nThe how-to guide — We will go through the practical analysis and visualization coding steps, showing all the code required for each step.\nThe practical exercises — We will set out the tasks, questions and challenges that you should complete to learn the practical skills we target this week.\n\n\n\n\n\n\n\nTip\n\n\n\nWe show you how to do everything you need to do in the practical exercises, first, in the how-to guide.\n\nStart by looking at the how-to guide to understand what steps you need to follow in the lab activity.\n\nIf you want to make it more challenging for yourself, go straight to Step 3.\n\n\nWe will take things step-by-step:\n\ndifferent parts for different phases of the analysis workflow;\ndifferent tasks for different things you need to do;\ndifferent questions to examine different ideas or coding challenges.\n\n\n7.4.1.1 The data files\nEach of the data files we will work with has a similar structure.\nHere are what the first few rows in the data file study.one.gen looks like:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparticipant_ID\nmean.acc\nmean.self\nstudy\nAGE\nSHIPLEY\nHLVA\nFACTOR3\nQRITOTAL\nGENDER\nEDUCATION\nETHNICITY\n\n\n\n\nstudyone.1\n0.49\n7.96\nstudyone\n34\n33\n7\n53\n11\nNon-binary\nHigher\nWhite\n\n\nstudyone.10\n0.85\n7.28\nstudyone\n25\n33\n7\n60\n11\nFemale\nHigher\nWhite\n\n\nstudyone.100\n0.82\n7.36\nstudyone\n43\n40\n8\n46\n12\nMale\nFurther\nWhite\n\n\nstudyone.101\n0.94\n7.88\nstudyone\n46\n33\n11\n51\n15\nMale\nHigher\nWhite\n\n\nstudyone.102\n0.58\n6.96\nstudyone\n18\n32\n3\n51\n12\nMale\nSecondary\nMixed\n\n\nstudyone.103\n0.84\n7.88\nstudyone\n19\n37\n13\n45\n19\nFemale\nFurther\nAsian\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe webpage has a slider under the data table window, so you can scroll across the columns: move your cursor over the window to show the slider.\n\n\nWhen you look at the data table, you can see the columns:\n\nparticipant_ID participant code\nmean.acc average accuracy of response to questions testing understanding of health guidance\nmean.self average self-rated accuracy of understanding of health guidance\nstudy variable coding for what study the data were collected in\nAGE age in years\nHLVA health literacy test score\nSHIPLEY vocabulary knowledge test score\nFACTOR3 reading strategy survey score\nGENDER gender code\nEDUCATION education level code\nETHNICITY ethnicity (Office National Statistics categories) code\n\n\n\n\n7.4.2 The how-to guide\nWe will take things step-by-step.\nWe split .Rmd scripts by steps, tasks and questions:\n\ndifferent parts for different phases of the analysis workflow;\ndifferent tasks for different things you need to do;\ndifferent questions to examine different ideas or coding challenges.\n\n\n\n\n\n\n\nTip\n\n\n\n\nMake sure you start and work your way, in order, through each part, task and question.\nComplete each task before you move on to the next task.\n\n\n\nIn the how-to guide, we hide example code and answer information in boxes so that you can test yourself first. Click on the box to then reveal the code or the answer.\n\n7.4.2.1 How-to Part 1: Set-up\nTo begin, we set up our environment in R.\n\nHow-to Task 1 – Run code to empty the R environment\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nrm(list=ls())                            \n\n\n\n\n\n\nHow-to Task 2 – Run code to load libraries\nLoad libraries using library().\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nlibrary(\"tidyverse\")\n\n\n\n\n\n\n\n7.4.2.2 How-to Part 2: Load and examine the data\n\nHow-to Task 3 – Read in the data file we will be using\nThe code in the how-to guide was written to work with the data file:\n\nstudy-one-general-participants.csv.\n\nRead in the data file – using read_csv().\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nstudy.one.gen &lt;- read_csv(\"study-one-general-participants.csv\")\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can choose your own file name, but be sure to give the data-set a distinct name, e.g., study.one.gen so that R can distinguish between the different data you will work with.\n\n\n\n\nHow-to Task 4 – Inspect the data file\nUse the summary() function to take a look.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nsummary(study.one.gen)\n\n participant_ID        mean.acc        mean.self        study          \n Length:169         Min.   :0.3600   Min.   :3.440   Length:169        \n Class :character   1st Qu.:0.7600   1st Qu.:6.080   Class :character  \n Mode  :character   Median :0.8400   Median :7.080   Mode  :character  \n                    Mean   :0.8163   Mean   :6.906                     \n                    3rd Qu.:0.9000   3rd Qu.:7.920                     \n                    Max.   :0.9900   Max.   :9.000                     \n      AGE           SHIPLEY           HLVA           FACTOR3     \n Min.   :18.00   Min.   :23.00   Min.   : 3.000   Min.   :34.00  \n 1st Qu.:24.00   1st Qu.:33.00   1st Qu.: 7.000   1st Qu.:46.00  \n Median :32.00   Median :35.00   Median : 9.000   Median :51.00  \n Mean   :34.87   Mean   :34.96   Mean   : 8.905   Mean   :50.33  \n 3rd Qu.:42.00   3rd Qu.:38.00   3rd Qu.:10.000   3rd Qu.:55.00  \n Max.   :76.00   Max.   :40.00   Max.   :14.000   Max.   :63.00  \n    QRITOTAL        GENDER           EDUCATION          ETHNICITY        \n Min.   : 6.00   Length:169         Length:169         Length:169        \n 1st Qu.:12.00   Class :character   Class :character   Class :character  \n Median :13.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :13.36                                                           \n 3rd Qu.:15.00                                                           \n Max.   :19.00                                                           \n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nsummary() will give you either descriptive statistics for variable columns classified as numeric or will tell you that columns in the dataset are not numeric.\nsummary(...) is a function and, again, you put the name of the dataset inside the brackets to view it.\n\n\n\n\nQ.1. What is the mean of mean.acc?\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nA.1. 0.8163\n\n\n\n\n\nQ.2. What class is the variable study?\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nA.2. character\n\n\n\n\n\nQ.3. Does the summary indicate if any variable has missing values (NA)?\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nA.3. No\n\n\n\n\n\n\nHow-to Task 5 – Change the class or type of a variable\nYou can use the as.factor() function you used in Week 5.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nstudy.one.gen$study &lt;- as.factor(study.one.gen$study)\n\n\n\n\n\nQ.4. After you have done this, what information does summary() give you about the variable study?\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nA.4. We can see the number 169 beside the word studyone: this tells us that there are 169 observations, in the column, each one is a value: the word or character string studyone.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nRemember from Week 3 that we can only count how many times a category value (or factor level) occurs: here, we are counting how many times the word studyone occurs in the factor column study.\n\n\n\n\n\n7.4.2.3 How-to Part 3: Visualize associations\n\nHow-to Task 6 – Draw scatterplots to examine associations between variables\nYou have seen these code moves before, in previous classes (weeks 3 and 4): we are consolidating skills by practising your coding in different contexts, using different data.\nWe extend your skills by adding some new moves.\nCreate scatterplots to examine the association between some variables.\n\n\n\n\n\n\nTip\n\n\n\nWe are working with geom_point() and you need x and y aesthetic mappings.\n\n\nFor example, we can draw a scatterplot to examine the association between mean.self and mean.acc.\n\nggplot(data = study.one.gen, aes(x = mean.self, y = mean.acc)) + \n    geom_point()\n\n\n\n\n\n\n\n\nExamine this plot — it shows: the possible association between x-axis variable mean.self and y-axis variable mean.acc.\nYou have seen this kind of code before but it will help your learning if, now, we take a look at the code step-by-step.\nThe plot code moves through the following steps:\n\nggplot(...) makes a plot.\nggplot(data = study.one.gen, ...) uses the study.one.gen data-set.\nggplot(...aes(x = mean.self, y = mean.acc)) uses two aesthetic mappings.\ngeom_point() show the mappings as points.\n\n\n\n\n\n\n\nTip\n\n\n\nWhat are aesthetic mappings?\nAesthetic mappings translate data information – numbers or values in column variables – into things you can see in plots.\nFor a scatterplot, we need to translate values for two variables into the position of each point in the plot.\nHere:\n\nx = mean.self maps mean.self values to x-axis (horizontal, left to right) positions.\ny = mean.acc maps mean.acc values to y-axis (vertical, bottom to top) positions.\n\n\n\nNow do scatterplots with any pair of numeric variables you like.\nRemember that we saw with summary() that not every variable consists of numbers.\n\n\n\n\n\n\nTip\n\n\n\n\n\nCheck out the example code.\n\nggplot(data = study.one.gen, aes(y = mean.self, x = mean.acc)) +\n    geom_point()  \n\nggplot(data = study.one.gen, aes(x = AGE, y = mean.self)) +\n  geom_point()  \n  \nggplot(data = study.one.gen, aes(x = SHIPLEY, y = mean.self)) +\n  geom_point()  \n\nggplot(data = study.one.gen, aes(x = HLVA, y = mean.self)) +\n  geom_point()  \n\n\n\n\n\n\nHow-to Task 7 – Edit the scatterplots to change how they look\nEdit the appearance of a plot step-by-step.\nWe are going to edit:\n\nthe appearance of the points using alpha and size;\nthe colour of the background using theme_bw();\nthe appearance of the labels using labs().\n\nWe make the changes, one change at a time.\nYou have seen one of these moves before and you can guess at how to do the others. Click on the drop-down view to see the code but, if you want a challenge, try first to write the code on your own.\n\nthe appearance of the points using alpha and size\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.one.gen, aes(x = HLVA, y = mean.self)) +\n  geom_point(alpha = 0.5, size = 2) \n\n\n\n\n\n\n\n\n\n\n\n\nthe colour of the background using theme_bw()\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.one.gen, aes(x = HLVA, y = mean.self)) +\n  geom_point(alpha = 0.5, size = 2)  +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\nthe appearance of the labels using labs()\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.one.gen, aes(x = HLVA, y = mean.self)) +\n  geom_point(alpha = 0.5, size = 2)  +\n  theme_bw() +\n  labs(x = \"HLVA\", y = \"mean self rated accuracy\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe arguments alpha and size can change the appearance of most geometric objects (geoms) in ggplot:\n\nin the code example, here, we vary the alpha number to change how opaque or transparent the points are;\nand we vary the size number to vary the size of the points.\n\n\n\n\n\nHow-to Task 8 – Now experiment\nThere are no right answers here: edit the appearance of your plots by changing alpha, size and colour of points.\n\n\n\n\n\n\nTip\n\n\n\nPlay is an important part of learning.\n\nExperimenting with how plots look is a key element in becoming a master at data visualization. You won’t know what looks good to you unless you try different things.\n\n\n\nIf you really want to extend your skills, it is really important that you learn how to find useful online help.\n\nQ.5. Can you find the ggplot reference page on scatterplots?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nDo a search with the keywords: ggplot reference geom_point\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA.5. You can find reference information here: https://ggplot2.tidyverse.org/reference/geom_point.html\n\n\n\n\n\nQ.6. Can you change the colour of the points to a colour you like?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nDo a search with the keywords: ggplot colours.\nUseful information on colour can be found here:\nhttps://r-graphics.org/recipe-colors-setting\nor\nhttp://www.cookbook-r.com/Graphs/Colors_(ggplot2)/\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA.6. Here is how you do it: R will recognize many English colour names.\n\n\nggplot(data = study.one.gen, aes(x = HLVA, y = mean.self)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"hotpink\")  +\n  theme_bw() +\n  labs(x = \"HLVA\", y = \"mean self rated accuracy\")\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNow you: experiment!\n\n\n\n\n\n7.4.2.4 How-to Part 4: Analyse associations\n\nHow-to Task 9 – Use correlation to to answer a research question\nExamine associations between variables using correlation.\nOne of our research questions is:\n\nCan people accurately evaluate whether they correctly understand written health information?\n\nWe can answer this question by examining whether mean self-rated accuracy of understanding correlates with mean accuracy of understanding. The logic is that if we can accurately rate our own understanding (from bad to good) then that rating should be associated – should be correlated with – how accurately we can actually respond to questions that test that understanding.\n\nQ.7. How do we examine the correlation between mean self-rated accuracy (mean.self) and mean accuracy (mean.acc)?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nRemember from the Week 6 lecture, that we use cor.test():\n\n\n\nCan you figure out how to code a correlation test? It helps with your learning if you first try to anticipate what the code will look like. Then reveal the code, below, to see what you guessed right. (Getting some things right, and some things wrong, is part of the learning process.)\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nA.7. Here is how you do it: take a look at the code and the results of the correlation test.\n\n\ncor.test(study.one.gen$mean.acc, study.one.gen$mean.self, method = \"pearson\",  alternative = \"two.sided\")\n\n\n    Pearson's product-moment correlation\n\ndata:  study.one.gen$mean.acc and study.one.gen$mean.self\nt = 7.1936, df = 167, p-value = 2.026e-11\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3619961 0.5937425\nsample estimates:\n      cor \n0.4863771 \n\n\n\n\n\n\nQ.8. What is r, the correlation coefficient?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA.8. r = 0.4863771\n\n\n\n\n\nQ.9. Is the correlation significant?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA.9. r is significant\n\n\n\n\n\nQ.10. What are the values for t and p for the significance test for the correlation?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA.10. t = 7.1936, p = 2.026e-11\n\n\n\n\n\nQ.11. What do you conclude, given the correlation results?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nReview the scatterplot you drew earlier (or draw one now) to examine the shape of the association between these variables.\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA.11. mean.acc and mean.self are positively correlated suggesting that as # mean.acc scores increase so also do mean.self scores\n\n\n\n\n\n\n\n7.4.3 The practical exercises\nNow you will progress through a series of tasks, and challenges, to test what you have learnt.\n\n\n\n\n\n\nWarning\n\n\n\nNow we will work with the data file\n\nstudy-two-general-participants.csv\n\n\n\nWe again split the steps into into parts, tasks and questions.\nWe are going to work through the following workflow steps:\n\nEmpty the R environment\nLoad relevant libraries\nRead in the data file\nInspect the data\nChange the type classification of variables if necessary\nDraw scatterplots to visualize the association between pairs of variables\nEstimate and test the correlations between pairs of variables\n\n\n\n\n\n\n\nTip\n\n\n\n\nThe how-to guide showed you what functions you needed and how you should write the function code.\nUse the code structure you have seen and change it to use the data required for these practical exercises: you will need to change the data-set name, the variable names, to get the code to work for the following exercises.\nIn learning how to code, the process of adapting example code is a key skill: we are learning what can change, and what has to stay the same.\n\n\n\nIn the following, we will guide you through the tasks and questions step by step.\n\n\n\n\n\n\nImportant\n\n\n\nAn answers version of the workbook will be provided after the practical class.\n\n\n\n\n7.4.3.1 Practical Part 1: Set-up\nTo begin, we set up our environment in R.\n\nPractical Task 1 – Run code to empty the R environment\n\nrm(list=ls())\n\n\n\nPractical Task 2 – Run code to load relevant libraries\n\nlibrary(\"tidyverse\")\n\n\n\n\n7.4.3.2 Practical Part 2: Load the data\n\nPractical Task 3 – Read in the data file we will be using\nThe data file for the practical exercises is:\n\nstudy-two-general-participants.csv\n\nUse the read_csv() function to read the data file into R.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nstudy.two.gen &lt;- read_csv(\"study-two-general-participants.csv\")\n\nRows: 172 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): participant_ID, study, GENDER, EDUCATION, ETHNICITY\ndbl (7): mean.acc, mean.self, AGE, SHIPLEY, HLVA, FACTOR3, QRITOTAL\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nWhen you code this, you can choose your own file name, but be sure to give the data object you create a distinct name e.g. study.two.gen.\n\n\nPractical Task 4 – Inspect the data file\nUse the summary() or head() functions to take a look\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nsummary(study.two.gen)\n\n participant_ID        mean.acc        mean.self        study          \n Length:172         Min.   :0.4107   Min.   :3.786   Length:172        \n Class :character   1st Qu.:0.6786   1st Qu.:6.411   Class :character  \n Mode  :character   Median :0.7679   Median :7.321   Mode  :character  \n                    Mean   :0.7596   Mean   :7.101                     \n                    3rd Qu.:0.8393   3rd Qu.:7.946                     \n                    Max.   :0.9821   Max.   :9.000                     \n      AGE           SHIPLEY           HLVA           FACTOR3     \n Min.   :18.00   Min.   :23.00   Min.   : 3.000   Min.   :29.00  \n 1st Qu.:25.00   1st Qu.:32.75   1st Qu.: 7.750   1st Qu.:47.00  \n Median :32.50   Median :36.00   Median : 9.000   Median :51.00  \n Mean   :35.37   Mean   :35.13   Mean   : 9.064   Mean   :51.24  \n 3rd Qu.:44.00   3rd Qu.:39.00   3rd Qu.:11.000   3rd Qu.:56.25  \n Max.   :76.00   Max.   :40.00   Max.   :14.000   Max.   :63.00  \n    QRITOTAL        GENDER           EDUCATION          ETHNICITY        \n Min.   : 6.00   Length:172         Length:172         Length:172        \n 1st Qu.:12.00   Class :character   Class :character   Class :character  \n Median :14.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :13.88                                                           \n 3rd Qu.:16.00                                                           \n Max.   :20.00                                                           \n\n\n\n\n\n\nPract.Q.1. What is the median of AGE?\n\n\nPract.A.1. 32.50\n\n\nPract.Q.2. What class is the variable ETHNICITY?\n\n\nPract.A.2. character\n\n\nPract.Q.3. Does the summary indicate if any variable has missing values (NAs)?\n\n\nPract.A.3. No\n\n\n\nPractical Task 5 – Change the class or type of the variable ETHNICITY to factor\nYou can use the as.factor() function you have used before: how do you write the code for these data?\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nstudy.two.gen$ETHNICITY &lt;- as.factor(study.two.gen$ETHNICITY)\n\n\n\n\n\nPract.Q.4. After you have done this, what information does summary() give you about the variable ETHNICITY?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nsummary(study.two.gen)\n\n participant_ID        mean.acc        mean.self        study          \n Length:172         Min.   :0.4107   Min.   :3.786   Length:172        \n Class :character   1st Qu.:0.6786   1st Qu.:6.411   Class :character  \n Mode  :character   Median :0.7679   Median :7.321   Mode  :character  \n                    Mean   :0.7596   Mean   :7.101                     \n                    3rd Qu.:0.8393   3rd Qu.:7.946                     \n                    Max.   :0.9821   Max.   :9.000                     \n      AGE           SHIPLEY           HLVA           FACTOR3     \n Min.   :18.00   Min.   :23.00   Min.   : 3.000   Min.   :29.00  \n 1st Qu.:25.00   1st Qu.:32.75   1st Qu.: 7.750   1st Qu.:47.00  \n Median :32.50   Median :36.00   Median : 9.000   Median :51.00  \n Mean   :35.37   Mean   :35.13   Mean   : 9.064   Mean   :51.24  \n 3rd Qu.:44.00   3rd Qu.:39.00   3rd Qu.:11.000   3rd Qu.:56.25  \n Max.   :76.00   Max.   :40.00   Max.   :14.000   Max.   :63.00  \n    QRITOTAL        GENDER           EDUCATION         ETHNICITY  \n Min.   : 6.00   Length:172         Length:172         Asian: 15  \n 1st Qu.:12.00   Class :character   Class :character   Black:  5  \n Median :14.00   Mode  :character   Mode  :character   Mixed:  7  \n Mean   :13.88                                         White:145  \n 3rd Qu.:16.00                                                    \n Max.   :20.00                                                    \n\n\n\n\n\n\nPract.A.4. We can see that ETHNICITY lists observations following UK Office National Statistics ethnicity grouping:\n\n\nAsian: 15\nBlack: 5\nMixed: 7\nWhite: 145\n\n\n\n\n7.4.3.3 Practical Part 3: Visualise the associations between variables\n\nPractical Task 6 – Create a scatterplot to examine the association between some variables\nFor this practical exercise, you always want to use the outcome mean.acc as the y-axis variable so:\n\ny = mean.acc\n\nThen you can use each numeric predictor variable as the x-axis variable so:\n\nx = mean.self\n\nProduce scatterplots with every numeric predictor variable in the study.two.gen dataset\n\n\n\n\n\n\nTip\n\n\n\nRemember what we saw with summary(): not every variable consists of numbers\nIf the summary() does not show you a mean for a variable, then R does not think that variable is numeric\n\n\nNow, let’s build some intuition.\nScientists often use scatterplots to get an intuitive understanding of the relationships between variables.\n\n\n\n\n\n\nTip\n\n\n\nYou can read a scatterplot to learn about the size and the direction of an association between two variables (see the week 6 lecture):\n\nIs the cloud of points more diffuse (the association is weaker) or more tightly clustered (the association is stronger)?\nDoes the cloud of points slope upwards (the association is more positive) or slope downwards (the association is negative).\n\n\n\nThe following questions ask you to look at plots, and make some judgments about what the plots tell you.\nIt can be hard to decide what an association looks like so make a decision based on what you see.\nRight now, we are working to build your intuitions so: reflect, trust your intuition, and make a decision.\n\n\n\n\n\n\nTip\n\n\n\nFirst draw the plot, then answer the question.\n\n\n\nPract.Q.5. What is the shape (direction) of the association between mean.self and mean.acc?\n\n\nPract.A.5. Increase in mean.self is associated with increase in mean.acc\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = mean.self, y = mean.acc)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\nPract.Q.6. What is the shape (direction) of the association between AGE and mean.acc?\n\n\nPract.A.6. There is no clear association between AGE and mean.acc\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = AGE, y = mean.acc)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\nPract.Q.7. What is the shape (direction) of the association between SHIPLEY and mean.acc?\n\n\nPract.A.7. Increase in SHIPLEY is associated with increase in mean.acc\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\nPract.Q.8. What is the shape (direction) of the association between HLVA and mean.acc?\n\n\nPract.A.8. Increase in HLVA is associated with increase in mean.acc\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = HLVA, y = mean.acc)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\nPract.Q.9. What is the shape (direction) of the association between FACTOR3 and mean.acc?\n\n\nPract.A.9. Increase in FACTOR3 is associated with increase in mean.acc\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = FACTOR3, y = mean.acc)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\nPract.Q.10. What is the shape (direction) of the association between QRITOTAL and mean.acc?\n\n\nPract.A.10. Increase in QRITOTAL is associated with increase in mean.acc\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = QRITOTAL, y = mean.acc)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.4.3.4 Practical Part 4: Learn how to edit plotting code\n\nPractical Task 7 – Edit the appearance of one plot step-by-step\nEdit your plotting code to make plots with a professional appearance.\nEdit a scatterplot to adjust\n\nthe appearance of the points using alpha, size and colour;\nthe colour of the background using theme_bw();\nthe appearance of the labels using labs().\n\nIn the following, we ask you to edit one plot element at a time. You can work out what to do.\n\nPract.Q.11. Can you edit the appearance of the points in a scatterplot using alpha, size and colour?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = HLVA, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n\nPract.Q.12. Can you edit the appearance of the plot background?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = HLVA, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"red\")   +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\nPract.Q.13. Can you edit the appearance of the labels using labs()?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = HLVA, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"red\")   +\n  theme_bw() +\n  labs(x = \"HLVA\", y = \"mean accuracy\")\n\n\n\n\n\n\n\n\n\n\n\n\nPract.Q.14. Can you find online information about colour blind palettes?\n\nTry doing a search with the keywords: ggplot colour blind\n\nPract.A.14. – Here is one:\n\nhttp://www.cookbook-r.com/Graphs/Colors_(ggplot2)/\n\nNow it’s your turn: experiment!\n\nWhat about different colour words? Replace \"red\" with \"...\"?\nWhat about a different size? Replace size = 2 with size =  ... using a different number.\nWhat about a different level of transparency (alpha)? Replace alpha = 0.5 with alpha =  ... using a different fraction.\n\n\n\n\n\n7.4.3.5 Practical Part 5: Use correlation to to answer the research questions\n\nPractical Task 8 – Examine the correlation between mean accuracy (mean.acc) and some numeric predictor variables\n\nPract.Q.15. What is r (given as cor in the output) for the correlation between HLVA and mean.acc?\n\nCan you figure out the code to do the calculation?\n\n\n\n\n\n\nCode\n\n\n\n\n\n\ncor.test(study.two.gen$HLVA, study.two.gen$mean.acc, method = \"pearson\",  alternative = \"two.sided\")\n\n\n    Pearson's product-moment correlation\n\ndata:  study.two.gen$HLVA and study.two.gen$mean.acc\nt = 7.5288, df = 170, p-value = 2.866e-12\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3787626 0.6044611\nsample estimates:\n      cor \n0.5000559 \n\n\n\n\n\n\nPract.A.15. r = 0.5000559\n\n\nPract.Q.16. Is the correlation significant?\n\n\nPract.A.16. r is significant\n\n\nPract.Q.17. What are the values for t and p for the significance test for the correlation?\n\n\nPract.A.17. t = 7.5288, p = 2.866e-12\n\n\nPract.Q.18. What do you conclude, given the correlation results?\n\nMaybe draw a scatterplot to examine the shape of the association.\n\nPract.A.18. HLVA and mean.acc are positively correlated suggesting that as HLVA scores increase so also do mean.acc scores\n\n\nPract.Q.19. What is r (given as cor in the output) for the correlation between mean.self and mean.acc?\n\n\nPract.A.19. r = 0.5460792\n\nCan you figure out the code to do the calculation?\n\n\n\n\n\n\nCode\n\n\n\n\n\n\ncor.test(study.two.gen$mean.self, study.two.gen$mean.acc, method = \"pearson\",  alternative = \"two.sided\")\n\n\n    Pearson's product-moment correlation\n\ndata:  study.two.gen$mean.self and study.two.gen$mean.acc\nt = 8.4991, df = 170, p-value = 9.356e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.4317217 0.6431596\nsample estimates:\n      cor \n0.5460792 \n\n\n\n\n\n\nPract.Q.20. Is the correlation between AGE and mean.acc significant?\n\n\nPract.A.20. r is not significant\n\nCan you figure out the code to do the calculation?\n\n\n\n\n\n\nCode\n\n\n\n\n\n\ncor.test(study.two.gen$AGE, study.two.gen$mean.acc, method = \"pearson\",  alternative = \"two.sided\")\n\n\n    Pearson's product-moment correlation\n\ndata:  study.two.gen$AGE and study.two.gen$mean.acc\nt = 0.30121, df = 170, p-value = 0.7636\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.1269774  0.1721354\nsample estimates:\n       cor \n0.02309589 \n\n\n\n\n\n\nPract.Q.21. What are the values for t and p for the significance test for the correlation between QRITOTAL and mean.acc?\n\n\nPract.A.21. t = 6.4711, p = 9.993e-10\n\nCan you figure out the code to do the calculation?\n\n\n\n\n\n\nCode\n\n\n\n\n\n\ncor.test(study.two.gen$QRITOTAL, study.two.gen$mean.acc, method = \"pearson\",  alternative = \"two.sided\")\n\n\n    Pearson's product-moment correlation\n\ndata:  study.two.gen$QRITOTAL and study.two.gen$mean.acc\nt = 6.4711, df = 170, p-value = 9.993e-10\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3159538 0.5571417\nsample estimates:\n    cor \n0.44457 \n\n\n\n\n\n\nPract.Q.22. What do you conclude, given the correlation results, about the association between SHIPLEY and mean.acc?\n\n\nPract.A.22. SHIPLEY and mean.acc are positively correlated suggesting that as HLVA scores increase so also do mean.acc scores\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\ncor.test(study.two.gen$SHIPLEY, study.two.gen$mean.acc, method = \"pearson\",  alternative = \"two.sided\")\n\n\n    Pearson's product-moment correlation\n\ndata:  study.two.gen$SHIPLEY and study.two.gen$mean.acc\nt = 6.8493, df = 170, p-value = 1.299e-10\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3390103 0.5746961\nsample estimates:\n      cor \n0.4650537 \n\n\n\n\n\n\n\n\n\n7.4.4 The answers\nAfter the practical class, we will reveal the answers that are currently hidden.\nThe answers version of the webpage will present my answers for questions, and some extra information where that is helpful.\n\n\n7.4.5 Look ahead: growing in independence\n\n\n\n\n\n\nImportant\n\n\n\n\nEvery problem you ever have: someone has had it before, solved it, and written a blog (or tweet or toot) about it.\nR is free open statistical software: everything you use is contributed, discussed and taught by a community of R users online, in open forums.\nLearning to navigate this knowledge is an introduction to the future of knowledge sharing.\n\n\n\n\n\n7.4.6 Optional exercises: to stretch you\nOne of the convenient and powerful things about R plotting code is that you can create a series of plots and put them together in a grid of plots for east comparison: we do that here.\nWe will use the patchwork library: check it out\nhttps://patchwork.data-imaginist.com/articles/patchwork.html\nWe get the library like this:\n\nlibrary(patchwork)\n\nHere’s an example:\nFirst create two plots: give them both names\n\np.AGE &lt;- ggplot(data = study.two.gen, aes(x = AGE, y = mean.acc)) +\n  geom_point() \n#\np.SHIPLEY &lt;- ggplot(data = study.two.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point()  \n\nSecond put the two plots together by calling their names.\n\np.AGE  + p.SHIPLEY\n\n\n\n\n\n\n\n\nNow you try it:\n\nCreate two plots, using QRITOTAL and SHIPLEY as predictors, and mean.acc as the outcome\nThen make a grid to present them side by side.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\np.QRITOTAL &lt;- ggplot(data = study.two.gen, aes(x = QRITOTAL, y = mean.acc)) +\n  geom_point()\n#\np.SHIPLEY &lt;- ggplot(data = study.two.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point()\n\n# -- second put them together\np.QRITOTAL  + p.SHIPLEY",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 7. Hypotheses and associations"
    ]
  },
  {
    "objectID": "PSYC411/part2/lm-intro.html",
    "href": "PSYC411/part2/lm-intro.html",
    "title": "Week 8. Introduction to the linear model",
    "section": "",
    "text": "Welcome to your overview of the work we will do together in Week 8.\nThis week, we focus on learning how to predict people: predicting observations about us (e.g., our attributes) or about the things we make or do. To do this, we will learn to think about and work with linear models.\nLooking ahead to your professional lives, learning about linear models will equip you to do behavioural analysis in a wide range of contexts, from clinical research to business intelligence.\nAs students, now, learning about linear model analyses in the context of the Clearly understood project will help you to understand how to critically evaluate research when you read about it (or write about it) in research reports.\nYou will see that the linear model is very flexible and powerful so learning to use it effectively involves not only learning to code analyses but also learning to think critically about what it is you want to do with it.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 8. Introduction to the linear model"
    ]
  },
  {
    "objectID": "PSYC411/part2/lm-intro.html#sec-lm-intro-overview",
    "href": "PSYC411/part2/lm-intro.html#sec-lm-intro-overview",
    "title": "Week 8. Introduction to the linear model",
    "section": "",
    "text": "Welcome to your overview of the work we will do together in Week 8.\nThis week, we focus on learning how to predict people: predicting observations about us (e.g., our attributes) or about the things we make or do. To do this, we will learn to think about and work with linear models.\nLooking ahead to your professional lives, learning about linear models will equip you to do behavioural analysis in a wide range of contexts, from clinical research to business intelligence.\nAs students, now, learning about linear model analyses in the context of the Clearly understood project will help you to understand how to critically evaluate research when you read about it (or write about it) in research reports.\nYou will see that the linear model is very flexible and powerful so learning to use it effectively involves not only learning to code analyses but also learning to think critically about what it is you want to do with it.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 8. Introduction to the linear model"
    ]
  },
  {
    "objectID": "PSYC411/part2/lm-intro.html#sec-lm-intro-goals",
    "href": "PSYC411/part2/lm-intro.html#sec-lm-intro-goals",
    "title": "Week 8. Introduction to the linear model",
    "section": "8.2 Our learning goals",
    "text": "8.2 Our learning goals\nWe continue to develop your critical thinking while we strengthen your practical skills.\n1. Critical thinking\n\nConcepts: how we go from ideas and questions to hypotheses.\n\nAs psychologists, we often want to ask questions like these:\n\nDoes variation in one measure (X) predict variation in another variable (Y)?\nWhat are the factors that influence outcome Y?\nIs a theoretical model consistent with observed behaviour?\n\n2. Practical skills\nTo enable you to answer questions like these, you will learn how to:\n\nSkills – code linear models;\nSkills – identify and interpret model statistics;\nConcepts and skills – critically evaluate the results;\nConcepts and skills – communicate the results.\n\nAs we progress, we will continue to strengthen your skills in building professional visualizations.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 8. Introduction to the linear model"
    ]
  },
  {
    "objectID": "PSYC411/part2/lm-intro.html#sec-lm-intro-resources",
    "href": "PSYC411/part2/lm-intro.html#sec-lm-intro-resources",
    "title": "Week 8. Introduction to the linear model",
    "section": "8.3 Learning resources",
    "text": "8.3 Learning resources\nYou will see, next, the lectures we share to explain the concepts behind the critical thinking and analysis skills you will develop, then you will see information about the practical materials you can use to practise your skills.\nEvery week, you will learn best if you first watch the lectures then do the practical exercises.\n\n\n\n\n\n\nLinked resources\n\n\n\n\nIn the chapter for week 7, we share materials to support your development of critical thinking about associations, and your development of practical skills in working with correlation-based analyses.\n\n\n\n\n8.3.1 Lectures\nThe lecture materials for this week are presented in four short parts.\nClick on a link and your browser should open a tab showing the Panopto video for the lecture part.\n\nPart 1 (12 minutes) Focus on the linear model: The concepts and skills we will learn about in week 8: our aims, the kind of questions we can answer with linear models, and how we get from those questions to sensible linear model analyses.\n\n\n\nPart 2 (15 minutes): the first key concept — how we use linear models to predict average outcomes.\n\n\n\nPart 3 (12 minutes): the second key concept — how we ensure that linear models make the best predictions given the data we have.\n\n\n\nPart 4 (14 minutes): the key skills — coding linear models, reading model results, and writing reports of those results.\n\n\n\n\n8.3.2 Lecture slides\n\n\n\n\n\n\nDownload the lecture slides\n\n\n\nThe slides presented in the videos can be downloaded here:\n\nThe slides exactly as presented (12 MB).\n\nYou can download the web page .html file and click on it to open it in any browser (e.g., Chrome, Edge or Safari). The slide images are high quality so the file is quite big and may take a few seconds to download.\n\n\nWe are going to work through some practical exercises, next, to develop your critical thinking and practical skills for working with linear models.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 8. Introduction to the linear model"
    ]
  },
  {
    "objectID": "PSYC411/part2/lm-intro.html#sec-lm-intro-practical",
    "href": "PSYC411/part2/lm-intro.html#sec-lm-intro-practical",
    "title": "Week 8. Introduction to the linear model",
    "section": "8.4 Practical materials: data and R-Studio",
    "text": "8.4 Practical materials: data and R-Studio\nWe will work with two data files which you can download by clicking on their names (below):\n\nstudy-one-general-participants.csv.\nstudy-two-general-participants.csv.\n\nOnce you have downloaded the files, you will need to upload them to the R-Studio server to access and use the R files.\n\n\n\n\n\n\nImportant\n\n\n\nHere is a link to the sign-in page for R-Studio Server\n\n\n\n8.4.1 Practical materials guide\nYou will find that the practical exercises are simpler to do if you follow these steps in order.\n\nThe data — We will take a quick look at what is inside the data files so you know what everything means.\nThe how-to guide — We will go through the practical analysis and visualization coding steps, showing all the code required for each step.\nThe practical exercises — We will set out the tasks, questions and challenges that you should complete to learn the practical skills we target this week.\n\nThis week — Week 8 — we consolidate what you have been learning, so your critical thinking and practical skills have a firm foundation then we extend your skills by learning how to do new things.\n\n\n\n\n\n\nWeek 8 parts\n\n\n\n\nSet-up\nLoad the data\nRevision: developing histograms to examine the distributions of variables.\nRevision: developing scatterplots to examine associations between variables.\nRevision: using correlations to to answer research questions, making sure you are comfortable.\nNew: using a linear model to answer research questions.\nNew: using a linear model to make predictions.\n\n\n\nWe learn these skills so that we can answer research questions like:\n\nWhat person attributes predict success in understanding?\nCan people accurately evaluate whether they correctly understand written health information?\n\nThese kinds of research questions can often be answered through analyses using linear models. We typically use linear models to estimate the association between predictors and outcomes.\nWhen we do these analyses, we need to think about how we report the results:\n\nWe usually need to report information about the kind of model we specify;\nWe will need to report the nature of the association estimated in our model;\nWe usually need to decide if (i.) the association is significant? (ii.) does the association reflect a positive or negative relationship between outcome and predictor? (iii.) is the association we see in our sample data relatively strong or weak?\n\nWe will consolidate and extend learning on data visualization so that we can produce eye-catching, appealing, plots for professional audiences.\n\n8.4.1.1 The data files\nEach of the data files we will work with has a similar structure.\nHere are what the first few rows in the data file study.two.gen looks like:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparticipant_ID\nmean.acc\nmean.self\nstudy\nAGE\nSHIPLEY\nHLVA\nFACTOR3\nQRITOTAL\nGENDER\nEDUCATION\nETHNICITY\n\n\n\n\nstudytwo.1\n0.4107143\n6.071429\nstudytwo\n26\n27\n6\n50\n9\nFemale\nHigher\nAsian\n\n\nstudytwo.10\n0.6071429\n8.500000\nstudytwo\n38\n24\n9\n58\n15\nFemale\nSecondary\nWhite\n\n\nstudytwo.100\n0.8750000\n8.928571\nstudytwo\n66\n40\n13\n60\n20\nFemale\nHigher\nWhite\n\n\nstudytwo.101\n0.9642857\n8.500000\nstudytwo\n21\n31\n11\n59\n14\nFemale\nHigher\nWhite\n\n\nstudytwo.102\n0.7142857\n7.071429\nstudytwo\n74\n35\n7\n52\n18\nMale\nHigher\nWhite\n\n\nstudytwo.103\n0.7678571\n5.071429\nstudytwo\n18\n40\n11\n54\n15\nFemale\nFurther\nWhite\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe webpage has a slider under the data table window, so you can scroll across the columns: move your cursor over the window to show the slider.\n\n\nWhen you look at the data table, you can see the columns:\n\nparticipant_ID participant code\nmean.acc average accuracy of response to questions testing understanding of health guidance\nmean.self average self-rated accuracy of understanding of health guidance\nstudy variable coding for what study the data were collected in\nAGE age in years\nHLVA health literacy test score\nSHIPLEY vocabulary knowledge test score\nFACTOR3 reading strategy survey score\nGENDER gender code\nEDUCATION education level code\nETHNICITY ethnicity (Office National Statistics categories) code\n\n\n\n\n8.4.2 The how-to guide\nWe will take things step-by-step.\nMake sure you complete each part, task and question, in order, before you move on to the next one.\n\n8.4.2.1 How-to Part 1: Set-up\nTo begin, we set up our environment in R.\n\nHow-to Task 1 – Run code to empty the R environment\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nrm(list=ls())                            \n\n\n\n\n\n\nHow-to Task 2 – Run code to load libraries\nLoad libraries using library().\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nlibrary(\"patchwork\")\nlibrary(\"tidyverse\")\n\n\n\n\n\n\n\n8.4.2.2 How-to Part 2: Load the data\n\nHow-to Task 3 – Read in the data file we will be using\nThe code in the how-to guide was written to work with the data file:\n\nstudy-one-general-participants.csv.\n\nRead in the data file – using read_csv().\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nstudy.one.gen &lt;- read_csv(\"study-one-general-participants.csv\")\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nLast week, we started working with the online help information on R functions.\nLet’s build on that.\nYou can read the technical information about the read_csv() function here:\nhttps://readr.tidyverse.org/reference/read_delim.html\n\nTake a look around the {tidyverse} webpages: expert professionals use information like this whenever they try to figure out how to do something.\n\n\n\n\n\nHow-to Task 4 – Inspect the data file\nUse the summary() function to take a look.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nsummary(study.one.gen)\n\n participant_ID        mean.acc        mean.self        study          \n Length:169         Min.   :0.3600   Min.   :3.440   Length:169        \n Class :character   1st Qu.:0.7600   1st Qu.:6.080   Class :character  \n Mode  :character   Median :0.8400   Median :7.080   Mode  :character  \n                    Mean   :0.8163   Mean   :6.906                     \n                    3rd Qu.:0.9000   3rd Qu.:7.920                     \n                    Max.   :0.9900   Max.   :9.000                     \n      AGE           SHIPLEY           HLVA           FACTOR3     \n Min.   :18.00   Min.   :23.00   Min.   : 3.000   Min.   :34.00  \n 1st Qu.:24.00   1st Qu.:33.00   1st Qu.: 7.000   1st Qu.:46.00  \n Median :32.00   Median :35.00   Median : 9.000   Median :51.00  \n Mean   :34.87   Mean   :34.96   Mean   : 8.905   Mean   :50.33  \n 3rd Qu.:42.00   3rd Qu.:38.00   3rd Qu.:10.000   3rd Qu.:55.00  \n Max.   :76.00   Max.   :40.00   Max.   :14.000   Max.   :63.00  \n    QRITOTAL        GENDER           EDUCATION          ETHNICITY        \n Min.   : 6.00   Length:169         Length:169         Length:169        \n 1st Qu.:12.00   Class :character   Class :character   Class :character  \n Median :13.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :13.36                                                           \n 3rd Qu.:15.00                                                           \n Max.   :19.00                                                           \n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nsummary() will give you either descriptive statistics for variable columns classified as numeric or will tell you that columns in the dataset are not numeric.\n\n\n\n\n\n\n8.4.2.3 How-to Part 3: Developing histograms to examine the distributions of variables\n\nHow-to Task 5 – Edit histogram plotting code to visualize distributions like a pro\nYou have seen how to produce histograms before, in a previous class (week 3): here we are consolidating skills by practising them in different contexts, using different data.\nWe extend your skills by adding some new moves.\nWhen we learn about creating a plot, it helps us to identify what each code element is doing. Here’s an example: run the line of code and see the result in the Plots window in R-Studio:\n\nggplot(data = study.one.gen, aes(x = mean.acc)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\nThese are the plotting code elements and what they are doing:\n\nggplot(...) you tell R you want to make a plot using the ggplot() function;\nggplot(data = study.one.gen ...) you tell R you want to make a plot with the study.one.gen data;\nggplot(..., aes(x = mean.acc)) you tell R that you want to make a plot with the variable mean.acc;\nhere, you specify the aesthetic mapping, x = mean.acc;\nggplot(...) + geom_histogram() you tell R you want to show the distribution of values of mean.acc as a geometric object: a histogram.\n\nYou have seen the plotting code arranged in two different ways, in one line, as above, or in a series of steps, like this:\n\nggplot(data = study.one.gen, aes(x = mean.acc)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNotice that it has no impact whether you write the plotting code in one line, or in a series of lines.\n\nR cares very much about how you spell function or variable names.\nR does not usually care about line breaks.\n\nLearning what does or does not matter is a key lesson, when you learn a language, whatever the language.\n\n\nIf we break the plotting code into steps, it makes the code easier for you to read, and it will make it easier to add edits, one at a time. Let’s see how that works, next.\nTo help you learn, we are going to work on editing the code to do things, and we are going to explain why we want to be able to do these things.\nThese are the things we want to do:\n\nnew – the appearance of the bars using binwidth;\nrevision – the colour of the background using theme_bw();\nrevision – the appearance of the labels using labs();\nnew – setting the x-axis limits to reflect the full range of possible scores on the x-axis variable using xlim();\nnew – adding annotation – here, a vertical line using geom_vline() – to focus the attention of the audience for a plot on specific information about the variable distribution (here, the sample average for the variable).\n\n\n\n\n\n\n\nTip\n\n\n\n\nClick on each Code tab to see how the code changes, and to see how the plot changes as a result.\nClick on each Why tab for an explanation of why we want to do this.\n\nYou may need to hover your cursor over the space to the right of the Code tab to see the Why tab.\n\n\n\nnew – the appearance of the bars using binwidth\n\n\nCodeWhy\n\n\n\nggplot(data = study.one.gen, aes(x = SHIPLEY)) + \n  geom_histogram(binwidth = 2)\n\n\n\n\n\n\n\n\n\n\nHistogram binwidth has to be a number. The best way to learn what the number tells R to do is to experiment with different values.\n\nNotice that in the code we set the binwidth to 2.\nIf you are producing a histogram, binwidth has to be a number larger than the potential minimum and smaller than the potential maximum for the variable you are plotting: so for the SHIPLEY vocabulary test, because the test scores can only range between 0-40, the binwidth number you use in your plotting code has to be bigger than 0 but smaller than 40.\n\nIn general, the larger (wider) the binwidth, the less detail you see about the distribution of values in the variable.\n\nIn a histogram, each bar you draw represents a collection of values. The binwidth tells R how many values to collect together, for each bar. So, the bigger the width, the bigger (wider and taller) the bar because it represents more observations.\n\nWhether or not you want to present detail is then a question concerning your communication aims: do you want to give an impression, or identify a specific detail?\n\n\n\n\nrevision – the colour of the background using theme_bw()\n\n\nCodeWhy\n\n\n\nggplot(data = study.one.gen, aes(x = SHIPLEY)) + \n  geom_histogram(binwidth = 2) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\nIn general, a grey background is argued to be a good way to present information visually but we often use a white background because it helps the plot elements to be visible, especially, for example, if we are giving a presentation.\n\n\n\n\nrevision – the appearance of the labels using labs()\n\n\nCodeWhy\n\n\n\nggplot(data = study.one.gen, aes(x = SHIPLEY)) + \n  geom_histogram(binwidth = 2) +\n  theme_bw() +\n  labs(x = \"Vocabulary (SHIPLEY)\", y = \"frequency count\")\n\n\n\n\n\n\n\n\n\n\nYou cannot assume that the audience for your plots will understand what you are referring to, if you just use data-set variable names e.g. SHIPLEY. They may not be familiar with your data-set. This means you need to use axis labels that will make sense to most people in your audience.\n\n\n\n\nnew – setting the x-axis limits using xlim()\n\n\nCodeWhy\n\n\n\nggplot(data = study.one.gen, aes(x = SHIPLEY)) + \n  geom_histogram(binwidth = 2) +\n  theme_bw() +\n  labs(x = \"Vocabulary (SHIPLEY)\", y = \"frequency count\") +\n  xlim(0,40)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\n\n\nIt is often useful to set the axis limits like this: because we want to show the audience where the sample values are distributed compared to where they could be distributed, given the measure.\nThis makes sense, e.g., if you want to see the relative age distribution of a sample compared to the population, or because we want to give the audience a more accurate picture of the data.\n\n\n\n\n\nnew – adding annotation – here, a vertical line\n\n\nCodeWhy\n\n\n\nggplot(data = study.one.gen, aes(x = SHIPLEY)) + \n  geom_histogram(binwidth = 2) +\n  theme_bw() +\n  labs(x = \"Vocabulary (SHIPLEY)\", y = \"frequency count\") +\n  xlim(0,40) +\n  geom_vline(xintercept = mean(study.one.gen$SHIPLEY), colour = \"red\", linewidth = 1.5)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\nNotice that we used the geom_vline() element to plot a vertical line at the location on the x-axis we define.\nWe do this in the steps:\n\ngeom_vline(...) – draw a vertical line;\n...xintercept... – draw the line so it hits the x-axis (intercepts the x-axis);\n...xintercept = mean(study.one.gen$SHIPLEY... – at a location defined by the mean of the variable mean(study.one.gen$SHIPLEY);\n...colour = \"red\", size = 1.5.. – make the line red and one and a half times the default thickness.\n\n\n\nWe can use annotations, like this vertical line, when we want to draw the attention of the audience for our plot to a specific feature of the data.\n\n\n\nWhat are we learning here?\nYou can see that while the data stays the same, the appearance of the plot changes, as we add each edit to the plotting code.\nThe lesson that we are learning, here, is not just that you can arrange plotting code in steps but that in general in R you can build things (like plots) one action at a time.\n\n\n\n\n\n\nFurther information you can explore\n\n\n\nWe can define the limits on the x-axis and on the y-axis, see the {ggplot2} library reference information on setting limits is here:\nhttps://ggplot2.tidyverse.org/reference/lims.html\nThe {ggplot2} reference information for drawing lines is here:\nhttps://ggplot2.tidyverse.org/reference/geom_abline.html\n\n\n\n\n\n8.4.2.4 How-to Part 4: Developing scatterplots to examine associations between variables\n\nHow-to Task 6 – Edit scatterplot code to visualize associations like a pro\nYou have seen these code moves before, in previous classes (weeks 3 and 4): we are consolidating skills by practising your coding in different contexts, using different data.\nWe extend your skills by adding some new moves.\nWe create scatterplots to examine the association between pairs of variables. For example, we can draw a scatterplot to examine the association between variation in the values of SHIPLEY and of mean.acc.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nggplot(data = study.one.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\nThe plot code moves through the following steps:\n\nggplot(...) makes a plot.\nggplot(data = study.one.gen, ...) uses the study.one.gen data-set.\nggplot(...aes(x = SHIPLEY, y = mean.acc)) uses two aesthetic mappings.\ngeom_point() show the mappings as points.\n\nWe are now going to edit:\n\nrevision and new – the appearance of the points using alpha, size, shape, and colour;\nrevision – the colour of the background using theme_bw();\nrevision – the appearance of the labels using labs();\nnew – the x-axis and y-axis limits using lim().\n\nWe make the changes, one change at a time.\nClick on the drop-down view to see the code but, if you want a challenge, first try to write the code on your own, using what you have learned so far.\n\nrevision and new – the appearance of the points using alpha, size, shape, and colour\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.one.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = 'square') \n\n\n\n\n\n\n\n\n\n\n\n\nrevision – the colour of the background using theme_bw()\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.one.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = 'square')   +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\nrevision – the appearance of the labels using labs()\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.one.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = 'square')   +\n  theme_bw() +\n  labs(x = \"Vocabulary (SHIPLEY)\", y = \"mean accuracy\")\n\n\n\n\n\n\n\n\n\n\n\n\nnew – the x-axis and y-axis limits using lim()\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.one.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = 'square')   +\n  theme_bw() +\n  labs(x = \"SHIPLEY\", y = \"mean accuracy\") +\n  xlim(0, 40) + ylim(0, 1)\n\n\n\n\n\n\n\n\nNotice that:\n\nI set the x-axis limits to the minimum (0) and maximum (40) possible values for the SHIPLEY variable.\nI set the y-axis limits to the minimum (0) and maximum (1) possible values for the mean accuracy variable mean.acc.\n\n\n\n\nWhat are we learning here?\nIt is generally a good idea to show the minimum value (the origin) for each variable.\n\nNot doing this, i.e., showing a more narrow slice of the sample range is an easy way to exaggerate the strength of associations or to imply incorrectly the breadth in variation.\n\nYou can change the transparency (alpha), size, colour and shape of important parts of a plot. Here, we are changing the appearance of the points. But you can also change the transparency (alpha), size, colour and shape of reference lines added to a plot.\n\n\n\n\n\n\nFurther information you can explore\n\n\n\nThe {ggplot2} geom_point() reference information is here:\nhttps://ggplot2.tidyverse.org/reference/geom_point.html\n\nwhere you can see some examples of the edits we have done.\n\nSome useful information about shape options is here:\nhttp://www.cookbook-r.com/Graphs/Shapes_and_line_types/\nSome useful information about colour options is here:\nhttp://www.cookbook-r.com/Graphs/Colors_(ggplot2)/\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNow you: experiment!\n\n\n\n\n\n8.4.2.5 How-to Part 5: Using correlations to to answer research questions\n\nHow-to Task 7 – Examine the correlation between a pair of variables\nIn the Clearly understood project, one of our research questions is:\n\nWhat person attributes predict success in understanding?\n\nWe can answer this question by doing a correlation analysis. This is because we can expect that if vocabulary knowledge predicts success in understanding then variation in vocabulary knowledge should be associated with variation in success of understanding. We measured how successful study participants were in understanding health information: recording outcome mean.acc as the accuracy of response to questions designed to examine understanding. We measured a key person attribute, vocabulary knowledge using the SHIPLEY vocabulary test. We can then examine the association between this pair of variables using correlation.\nHere, we examine the correlation between the mean accuracy (mean.acc) of understanding of health information, and vocabulary (SHIPLEY) knowledge, for each person in our participant sample.\nCan you figure out how to code the correlation analysis?\nIt helps with your learning if you first try to predict what the code will look like. Then reveal the code, below, to see what you guessed right.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\ncor.test(study.one.gen$SHIPLEY, study.one.gen$mean.acc, method = \"pearson\",  alternative = \"two.sided\")\n\n\n    Pearson's product-moment correlation\n\ndata:  study.one.gen$SHIPLEY and study.one.gen$mean.acc\nt = 4.5855, df = 167, p-value = 8.846e-06\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1931990 0.4620413\nsample estimates:\n     cor \n0.334406 \n\n\n\n\n\n\nQ.1. What is r, the correlation coefficient?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA.1. r = 0.334406\n\n\n\n\n\nQ.2. Is the correlation significant?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA.2. r is significant, p &lt; .05\n\n\n\n\n\nQ.3. What are the values for t and p for the significance test for the correlation?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA.3. t = 4.5855, p = 8.846e-06\n\n\n\n\n\nQ.4. What do you conclude is the answer to the research question, given the correlation results?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nReview the scatterplot you drew earlier (or draw one now) to examine the shape of the association between these variables.\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA.4. Vocabulary and mean.acc are positively correlated suggesting that as SHIPLEY scores increase so also do mean.acc scores.\n\n\n\n\n\n\n\n8.4.2.6 How-to Part 6: New: using a linear model to answer research questions\n\nHow-to Task 8 – Code and read the results from a linear model analysis\nAs we have seen, one of our research questions is:\n\nWhat person attributes predict success in understanding?\n\nWe can examine the relation between outcome mean accuracy (mean.acc) of understanding and vocabulary (SHIPLEY) knowledge by testing if person vocabulary score predicts understanding.\nWe do this through a linear model analysis.\nWe complete the analysis using the lm() function.\n\nmodel &lt;- lm(mean.acc ~ SHIPLEY, data = study.one.gen)\nsummary(model)\n\n\nCall:\nlm(formula = mean.acc ~ SHIPLEY, data = study.one.gen)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.42871 -0.04921  0.02079  0.07480  0.18430 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.44914    0.08053   5.577 9.67e-08 ***\nSHIPLEY      0.01050    0.00229   4.585 8.85e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1115 on 167 degrees of freedom\nMultiple R-squared:  0.1118,    Adjusted R-squared:  0.1065 \nF-statistic: 21.03 on 1 and 167 DF,  p-value: 8.846e-06\n\n\nLet’s work through the elements of the linear model code so we can see what everything does:\n\nmodel &lt;- lm(...) – fit the model using lm(...), giving the model a name here, we just call it “model”.\n...lm(mean.acc ~ SHIPLEY...) – tell R you want a model of the outcome mean.acc predicted ~ by the predictor SHIPLEY.\n...data = study.one) – tell R that the variables you name in the formula are in the study.one dataset.\nsummary(model) – ask R for a summary of the model you called “model”: this is how you get the results.\n\n\n\n\n\n\n\nTip\n\n\n\nNotice that R has a general formula syntax: outcome ~ predictor or y ~ x\n\nand uses the same format across a number of different functions;\neach time, on the left of the tilde symbol ~ you identify the output or outcome variable;\nbut on the right of the tilde ~ you identify an input or predictor variable or set of predictor variables.\n\n\n\nIf you look at the model summary you can answer the following questions.\n\nQ.5. What is the estimate for the coefficient of the effect of the predictor, SHIPLEY?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA.5. 0.01050\n\n\n\n\n\nQ.6. Is the effect significant?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA.6. It is significant, p &lt; .05\n\n\n\n\n\nQ.7. What are the values for t and p for the significance test for the coefficient?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA.7. t = 4.585, p = 8.85e-06\n\n\n\n\n\nQ.8. What do you conclude is the answer to the research question, given the linear model results?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA.8. The model slope estimate suggests that as SHIPLEY scores increase so also do mean.acc scores.\n\n\n\n\n\n\n\n8.4.2.7 How-to Part 7: New: using a linear model to generate predictions\n\nHow-to Task 9 – Fit a linear model and plot the model predictions\nWe can use the model we have just fitted to plot the model predictions.\n\nThe estimates of the coefficients (given in summary results) are the information we need to generate predictions.\nWhat we are predicting is how values of the outcome variable change, on average, given different values in one or more predictor variables.\nWe can produce predictions by working with linear model estimates of (1.) the intercept and (2.) the coefficient of the effect of each predictor variable.\nThe effect of a predictor tells you how the outcome changes (how much the outcome increases or decreases), given different values of the predictor.\n\nWe are going to do this prediction in two steps:\n\nrevision – fit a linear model to estimate the relationship between the outcome and the predictor variables.\nnew – draw a scatterplot and add a line to show the model predictions, given the model estimates.\n\nWe work through these steps in turn.\nStep 1. First fit a linear model and get a summary of the estimates\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nmodel &lt;- lm(mean.acc ~ SHIPLEY, data = study.one.gen)\nsummary(model)\n\n\nCall:\nlm(formula = mean.acc ~ SHIPLEY, data = study.one.gen)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.42871 -0.04921  0.02079  0.07480  0.18430 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.44914    0.08053   5.577 9.67e-08 ***\nSHIPLEY      0.01050    0.00229   4.585 8.85e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1115 on 167 degrees of freedom\nMultiple R-squared:  0.1118,    Adjusted R-squared:  0.1065 \nF-statistic: 21.03 on 1 and 167 DF,  p-value: 8.846e-06\n\n\n\n\n\n\nQ.9. What is the coefficient estimate for the intercept?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA.9. 0.44914\n\n\n\n\n\nQ.10. What is the coefficient estimate for the slope of SHIPLEY?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA.10. 0.01050\n\n\n\n\nStep 2. Second, draw a scatterplot and add a line to show the model predictions\nWe use the geom_abline() function to draw the line.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.one.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = 'square')   +\n  geom_abline(intercept = 0.44914, slope = 0.01050, colour = \"red\", linewidth = 1.5) +\n  theme_bw() +\n  labs(x = \"SHIPLEY\", y = \"mean accuracy\") +\n  xlim(0, 40) + ylim(0, 1)\n\n\n\n\n\n\n\n\n\n\n\nYou can see that the only new thing we do here is to:\n\nadd the geom_abline(...) function;\nand, inside the brackets, add information about the intercept and the slope estimates.\n\nNote that we can get the prediction line drawn for us automatically.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nggplot(data = study.one.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  # geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = 'square')   +\n  geom_smooth(method = 'lm', colour = \"purple\", alpha = .2, linewidth = 2.5, se = FALSE) +\n  geom_abline(intercept = 0.44914, slope = 0.01050, colour = \"red\", linewidth = 1) +\n  theme_bw() +\n  labs(x = \"SHIPLEY\", y = \"mean accuracy\") +\n  xlim(0, 40) + ylim(0, 1)  \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nHere, I turned the points off by commenting them out, adding # to the start of the line.\n\nI added geom_smooth(method = 'lm', ...) to draw a prediction line.\n\nYou can compare the red prediction line I drew using the model estimates with the purple line I used geom_smooth() to draw automatically, to see that they are identical.\nYou have seen geom_smooth() before: this shows you how it works.\n\n\n\nWhat are we learning here?\nWe want to generate model predictions because doing this unlocks a key concept in understanding why we use linear models and what we do.\n\nLinear models are an analysis method based on the prediction of the average change in outcome variable values, given different values of one or more predictor variables.\n\n\n\n\n\n\n\nFurther information you can explore\n\n\n\nYou can get {ggplot2} reference information to see what geom_smooth() does:\nhttps://ggplot2.tidyverse.org/reference/geom_smooth.html\nYou can see reference information on drawing lines here:\nhttps://ggplot2.tidyverse.org/reference/geom_abline.html\n\n\n\n\n\n8.4.3 The practical exercises\nNow you will progress through a series of tasks, and challenges, to test what you have learnt.\n\n\n\n\n\n\nWarning\n\n\n\nNow we will work with the data file\n\nstudy-two-general-participants.csv\n\n\n\nWe again split the steps into into parts, tasks and questions.\nWe are going to work through the following workflow steps: each step is labelled as a practical part.\n\nSet-up\nLoad the data\nRevision: developing histograms to examine the distributions of variables.\nRevision: developing scatterplots to examine associations between variables.\nRevision: using correlations to to answer research questions — making sure you are comfortable with the calculation and interpretation of correlation analyses.\nNew: using a linear model to answer research questions.\nNew: using a linear model to make predictions.\n\n\n\n\n\n\n\nTip\n\n\n\n\nThe how-to guide showed you what functions you needed and how you should write the function code.\nUse the code structure you have seen and change it to use the data required for these practical exercises: you will need to change the data-set name, the variable names, to get the code to work for the following exercises.\nIdentify what code elements must change, and what code elements have to stay the same.\n\n\n\nIn the following, we will guide you through the tasks and questions step by step.\n\n\n\n\n\n\nImportant\n\n\n\nAn answers version of the workbook will be provided after the practical class.\n\n\n\n\n8.4.3.1 Practical Part 1: Set-up\nTo begin, we set up our environment in R.\n\nPractical Task 1 – Run code to empty the R environment\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nrm(list=ls())\n\n\n\n\n\n\nPractical Task 2 – Run code to load relevant libraries\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlibrary(\"tidyverse\")\n\n\n\n\n\n\n\n8.4.3.2 Practical Part 2: Load the data\n\nPractical Task 3 – Read in the data file we will be using\nThe data file for the practical exercises is:\n\nstudy-two-general-participants.csv\n\nUse the read_csv() function to read the data file into R.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nstudy.two.gen &lt;- read_csv(\"study-two-general-participants.csv\")\n\nRows: 172 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): participant_ID, study, GENDER, EDUCATION, ETHNICITY\ndbl (7): mean.acc, mean.self, AGE, SHIPLEY, HLVA, FACTOR3, QRITOTAL\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nWhen you code this, you can choose your own file name, but be sure to give the data object you create a distinct name e.g. study.two.gen.\n\n\nPractical Task 4 – Inspect the data file\nUse the summary() function to take a look.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nsummary(study.two.gen)\n\n participant_ID        mean.acc        mean.self        study          \n Length:172         Min.   :0.4107   Min.   :3.786   Length:172        \n Class :character   1st Qu.:0.6786   1st Qu.:6.411   Class :character  \n Mode  :character   Median :0.7679   Median :7.321   Mode  :character  \n                    Mean   :0.7596   Mean   :7.101                     \n                    3rd Qu.:0.8393   3rd Qu.:7.946                     \n                    Max.   :0.9821   Max.   :9.000                     \n      AGE           SHIPLEY           HLVA           FACTOR3     \n Min.   :18.00   Min.   :23.00   Min.   : 3.000   Min.   :29.00  \n 1st Qu.:25.00   1st Qu.:32.75   1st Qu.: 7.750   1st Qu.:47.00  \n Median :32.50   Median :36.00   Median : 9.000   Median :51.00  \n Mean   :35.37   Mean   :35.13   Mean   : 9.064   Mean   :51.24  \n 3rd Qu.:44.00   3rd Qu.:39.00   3rd Qu.:11.000   3rd Qu.:56.25  \n Max.   :76.00   Max.   :40.00   Max.   :14.000   Max.   :63.00  \n    QRITOTAL        GENDER           EDUCATION          ETHNICITY        \n Min.   : 6.00   Length:172         Length:172         Length:172        \n 1st Qu.:12.00   Class :character   Class :character   Class :character  \n Median :14.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :13.88                                                           \n 3rd Qu.:16.00                                                           \n Max.   :20.00                                                           \n\n\n\n\n\nThis time:\n\nPay attention to what you see, for the numeric variables, in the information about minimum Min. and maximum Max. values.\n\n\n\n\n8.4.3.3 Practical Part 3: Revision – developing histograms to examine the distributions of variables\n\nPractical Task 5 – Practise editing the appearance of a histogram plot step-by-step\nStart by constructing a basic histogram.\n\nDraw a histogram plot to visualize the distribution of any numeric variable from the study.two.gen data-set.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nUse the line-by-line format to break the plot code into steps.\nIt will make it easier to read, and it will make it easier to add edit.\n\n\n\n\n\nPick a numeric variable in the dataset.\nRun the code to produce a histogram to show the distribution of values for that variable in the data.\n\nCan you work out how to do it without looking at the code example?\nClick on the button to see the code example: compare it to the code you wrote.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = SHIPLEY)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\nNext, we are going to edit:\n\nnew – the appearance of the bars using binwidth;\nrevision – the appearance of the background;\nrevision – the appearance of the labels using labs();\nnew – setting the x-axis limits to reflect the full range of possible scores on the x-axis variable using xlim();\nnew – adding annotation – here, a vertical line using geom_vline().\n\n\nPract.Q.1. Can you edit the appearance of the bars by specifying a binwidth value?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nRemember:\n\nThe binwidth number has to be a value between the smallest possible value and the largest possible value for the variable. For example, if the variable is AGE then the smallest possible value will be 0. So binwidth has to be some number bigger than 0.\nYou can experiment with different numbers to find a balance between a histogram that shows detail (more, narrow, bars) and a histogram that shows an impression (fewer, wider, bars).\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = SHIPLEY)) +\n  geom_histogram(binwidth = 2)\n\n\n\n\n\n\n\n\n\n\n\n\nPract.Q.2. Can you edit the appearance of the background?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThere are a range of themes that you can use. Why not pick one of the options shown here:\nhttps://ggplot2.tidyverse.org/reference/ggtheme.html\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = SHIPLEY)) +\n  geom_histogram(binwidth = 2) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\nPract.Q.3. Can you edit the appearance of the labels?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = SHIPLEY)) +\n  geom_histogram(binwidth = 2) +\n  theme_bw() +\n  labs(x = \"SHIPLEY\", y = \"frequency count\")\n\n\n\n\n\n\n\n\n\n\n\n\nPract.Q.4. Can you edit the x-axis limits?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nWhat you want to do, here, is to show the smallest possible value (often, not always 0) for values in the variable.\n\nThe idea is that you want to see the distribution of values in the sample (the histogram), for the variable, in the context of the full possible range of values for that variable.\nYou can use the summary() information to make your choice of limits.\n\nDrawing a histogram like this allows you to show the reader how the sample compares to the wider population.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = SHIPLEY)) +\n  geom_histogram(binwidth = 2) +\n  theme_bw() +\n  labs(x = \"Vocabulary (SHIPLEY)\", y = \"frequency count\") +\n  xlim(0,40)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nPract.Q.5. Can you add annotation: a vertical line to show the mean value of the variable you are plotting?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = SHIPLEY)) +\n  geom_histogram(binwidth = 2) +\n  theme_bw() +\n  labs(x = \"Vocabulary (SHIPLEY)\", y = \"frequency count\") +\n  xlim(0,40) +\n  geom_vline(xintercept = mean(study.two.gen$SHIPLEY), colour = \"red\", size = 1.5)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nPract.Q.6. Where can you find information on how to define the limits on the x-axis and on the y-axis?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou can see the information in this week’s how-to guide but try a search online for the keywords: ggplot reference xlim.\n\n\n\n\nPract.A.6. See ggplot reference information on setting limits here:\n\nhttps://ggplot2.tidyverse.org/reference/lims.html\n\nPract.Q.7. Where can you find information on how to draw a reference line?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou can see the information in this week’s how-to but try a search online for the keywords: ggplot reference vline.\n\n\n\n\nPract.A.7. See ggplot reference information on adding lines here:\n\nhttps://ggplot2.tidyverse.org/reference/geom_abline.html\n\n\n\n8.4.3.4 Practical Part 4: Developing scatterplots to examine associations between variables\n\nPractical Task 6 – Create a scatterplot to examine the association between some variables\nCreate three scatterplots to visualize the relationship between (1.) the outcome mean.acc and (2.) each of three numeric potential predictor variables SHIPLEY, HLVA and AGE.\nCheck first if you can write the code you need to produce each scatterplot. Click on the button to see the code example: compare it to the code you wrote.\n\n\n\n\n\n\nCode\n\n\n\n\n\nCheck out the example code for each of the scatterplots we are asking you to do.\n\nNotice what changes and what stays the same.\n\n\nggplot(data = study.two.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point()\n\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = HLVA, y = mean.acc)) +\n  geom_point()\n\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = AGE, y = mean.acc)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractical Task 7 – Edit the appearance of each plot step-by-step\n\nYou may want to use the same plot edit choices for all plots.\n\n\nProducing plots with a consistent appearance will make it easier for your audience to read your plots.\n\n\nYou can find links to relevant information on options in the how-to guide.\n\n\nUse the information to make the plots pleasing in appearance to you.\n\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nDo not be afraid to select, copy then paste code to re-use it and save yourself the effort of typing out the code over and over again.\nBut be careful to make sure that you change variable names, and that things like axis values are sensible for each variable.\n\n\n\n\n\nPract.Q.8. Can you edit the appearance of the points using alpha, size, shape, and colour?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nCheck out the example code for each of the scatterplots.\n\nNotice what changes and what stays the same.\n\n\nggplot(data = study.two.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = 'square')\n\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = HLVA, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = 'square')\n\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = AGE, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = 'square')\n\n\n\n\n\n\n\n\n\n\n\n\nPract.Q.9. Can you edit the appearance of the background?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nCheck out the example code for each of the scatterplots.\n\nNotice what changes and what stays the same.\n\n\nggplot(data = study.two.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = 'square')   +\n  theme_bw()\n\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = HLVA, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = 'square')   +\n  theme_bw()\n\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = AGE, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = 'square')   +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\nPract.Q.10. Can you edit the appearance of the labels?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nCheck out the example code for each of the scatterplots.\n\nNotice what changes and what stays the same.\n\n\nggplot(data = study.two.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = 'square')   +\n  theme_bw() +\n  labs(x = \"SHIPLEY\", y = \"mean accuracy\")\n\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = HLVA, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = 'square')   +\n  theme_bw() +\n  labs(x = \"HLVA\", y = \"mean accuracy\")\n\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = AGE, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = 'square')   +\n  theme_bw() +\n  labs(x = \"Age (Years)\", y = \"mean accuracy\")\n\n\n\n\n\n\n\n\n\n\n\n\nPract.Q.11. Can you set the x-axis and y-axis limits to 0 (as the minimum) versus the sample data maximum values for the variables you are plotting?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nFor these plots the y-axis limits will be the same because the outcome stays the same across plots.\nThe x-axis limits will be different for each different predictor variable.\nCheck out the information in the summary() of the dataset.\nThe minimum values for the variables will often be 0, e.g., if you are looking at data from ability tests and people who do the tests can get 0. But if you are looking at, e.g., ratings data then the minimum value could be 1, e.g., because people are asked to rate something on a scale from 1-9.\n\n\n\n\nCheck first if you can write the code you need to produce each scatterplot. Click on the button to see the code example: compare it to the code you wrote.\n\n\n\n\n\n\nCode\n\n\n\n\n\nCheck out the example code for each of the scatterplots.\n\nNotice what changes and what stays the same.\n\n\nggplot(data = study.two.gen, aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = 'square')   +\n  theme_bw() +\n  labs(x = \"SHIPLEY\", y = \"mean accuracy\") +\n  xlim(0, 40) + ylim(0, 1)\n\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = HLVA, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = 'square')   +\n  theme_bw() +\n  labs(x = \"HLVA\", y = \"mean accuracy\") +\n  xlim(0, 16) + ylim(0, 1)\n\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = AGE, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = 'square')   +\n  theme_bw() +\n  labs(x = \"Age (Years)\", y = \"mean accuracy\") +\n  xlim(0, 80) + ylim(0, 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.4.3.5 Practical Part 5: Using correlations to to answer research questions — making sure you are comfortable\nOne of our research questions is:\n\nWhat person attributes predict success in understanding?\n\n\nPractical Task 8 – Examine the correlations between the outcome variable and predictor variables\nRun three correlations to look at this question:\n\nbetween mean accuracy and SHIPLEY;\nbetween mean accuracy and HLVA;\nbetween mean accuracy and AGE.\n\nCheck first if you can write the code you need to complete each correlation analysis. Click on the button to see the code example: compare it to the code you wrote.\n\n\n\n\n\n\nCode\n\n\n\n\n\nCheck out the example code for doing each of the correlation analyses.\n\nNotice what changes and what stays the same.\n\n\ncor.test(study.two.gen$SHIPLEY, study.two.gen$mean.acc, method = \"pearson\",  alternative = \"two.sided\")\n\n\n    Pearson's product-moment correlation\n\ndata:  study.two.gen$SHIPLEY and study.two.gen$mean.acc\nt = 6.8493, df = 170, p-value = 1.299e-10\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3390103 0.5746961\nsample estimates:\n      cor \n0.4650537 \n\ncor.test(study.two.gen$HLVA, study.two.gen$mean.acc, method = \"pearson\",  alternative = \"two.sided\")\n\n\n    Pearson's product-moment correlation\n\ndata:  study.two.gen$HLVA and study.two.gen$mean.acc\nt = 7.5288, df = 170, p-value = 2.866e-12\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3787626 0.6044611\nsample estimates:\n      cor \n0.5000559 \n\ncor.test(study.two.gen$AGE, study.two.gen$mean.acc, method = \"pearson\",  alternative = \"two.sided\")\n\n\n    Pearson's product-moment correlation\n\ndata:  study.two.gen$AGE and study.two.gen$mean.acc\nt = 0.30121, df = 170, p-value = 0.7636\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.1269774  0.1721354\nsample estimates:\n       cor \n0.02309589 \n\n\n\n\n\nNow use the results from the correlations to answer the following questions.\n\nPract.Q.12. What is r, the coefficient for the correlation between mean.acc and SHIPLEY?\n\n\nPract.A.12. r = 0.4650537\n\n\nPract.Q.13. Is the correlation between mean.acc and HLVA significant?\n\n\nPract.A.13. – r is significant, p &lt; .05\n\n\nPract.Q.14. What are the values for t and p for the significance test for the correlation between mean.acc and AGE?\n\n\nPract.A.14. t = 0.30121, p = 0.7636\n\n\nPract.Q.15. For which pair of outcome-predictor variables is the correlation the largest?\n\n\nPract.A.15. – The correlation is the largest between mean.acc and HLVA.\n\n\nPract.Q.16. What is the sign or direction of each of the correlations?\n\n\nPract.A.16. – All the correlations are positive.\n\n\n\n\n8.4.3.6 Practical Part 6: Using a linear model to to answer research questions\nNow we can use linear models to try to answer the question:\n\nWhat person attributes predict success in understanding?\n\n\nPractical Task 9 – Examine the relation between outcome mean accuracy (mean.acc) and each of the predictors: SHIPLEY, HLVA and AGE\nYou need to run three separate lm() analyses:\n\nwith mean accuracy mean.acc as the outcome and SHIPLEY as the predictor;\nwith mean accuracy mean.acc as the outcome and HLVA as the predictor;\nwith mean accuracy mean.acc as the outcome and AGE as the predictor.\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou need to use lm() to do the analyses.\n\nBe careful to identify the outcome and predictor variables correctly.\nRemember that analysis code is arranged like this:\n\n\nlm(outcome.variable ~ predictor.variable, data = data.set)\n\nWith:\n\nlm() asking R to do the linear model analysis;\noutcome.variable ~ ... specified on the left of the ~;\nthe predictor.variable ~ ... specified on the right of the ~;\nand data.set identifying to R what dataset you are working with.\n\n\n\n\nCheck first if you can write the code you need to complete each linear model analysis. Click on the button to see the code example: compare it to the code you wrote.\n\n\n\n\n\n\nCode\n\n\n\n\n\nCheck out the example code for each of the models.\n\nNotice what changes and what stays the same.\n\n\nmodel.1 &lt;- lm(mean.acc ~ SHIPLEY, data = study.two.gen)\nsummary(model.1)\n\n\nCall:\nlm(formula = mean.acc ~ SHIPLEY, data = study.two.gen)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.285771 -0.076662  0.002099  0.079416  0.257793 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.308027   0.066426   4.637 7.01e-06 ***\nSHIPLEY     0.012854   0.001877   6.849 1.30e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.107 on 170 degrees of freedom\nMultiple R-squared:  0.2163,    Adjusted R-squared:  0.2117 \nF-statistic: 46.91 on 1 and 170 DF,  p-value: 1.299e-10\n\nmodel.2 &lt;- lm(mean.acc ~ HLVA, data = study.two.gen)\nsummary(model.2)\n\n\nCall:\nlm(formula = mean.acc ~ HLVA, data = study.two.gen)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.27457 -0.06777  0.01474  0.08025  0.23146 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.522016   0.032544  16.040  &lt; 2e-16 ***\nHLVA        0.026207   0.003481   7.529 2.87e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1047 on 170 degrees of freedom\nMultiple R-squared:  0.2501,    Adjusted R-squared:  0.2456 \nF-statistic: 56.68 on 1 and 170 DF,  p-value: 2.866e-12\n\nmodel.3 &lt;- lm(mean.acc ~ AGE, data = study.two.gen)\nsummary(model.3)\n\n\nCall:\nlm(formula = mean.acc ~ AGE, data = study.two.gen)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.34684 -0.08464  0.01084  0.08323  0.21968 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.7519965  0.0267206  28.143   &lt;2e-16 ***\nAGE         0.0002136  0.0007092   0.301    0.764    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1208 on 170 degrees of freedom\nMultiple R-squared:  0.0005334, Adjusted R-squared:  -0.005346 \nF-statistic: 0.09073 on 1 and 170 DF,  p-value: 0.7636\n\n\n\n\n\nIf you look at the model summary you can answer the following questions.\n\nPract.Q.17. What is the estimate for the coefficient of the effect of the predictor HLVA on mean.acc?\n\n\nPract.A.17. 0.026207\n\n\nPract.Q.18. Is the effect significant?\n\n\nPract.A.18. It is significant, p &lt; .05\n\n\nPract.Q.19. What are the values for t and p for the significance test for the coefficient?\n\n\nPract.A.19. t = 7.529, p = 2.87e-12\n\n\nPract.Q.20. How would you describe in words the shape or direction of the association between HLVA and mean.acc?\n\n\nPract.A.20. The slope coefficient – and a scatterplot (draw it) – suggest that as HLVA scores increase so also do mean accuracy scores.\n\n\nPract.Q.21. How how would you describe the relations apparent between the predictor and outcome in all three models?\n\n\nPract.A.21. It is possible to see, given coefficient estimates, that the association between predictor and outcome is positive for each model: mean accuracy appears to increase for increasing values of SHIPLEY vocabulary, HLVA health literacy, and age.\n\n\n\n\n8.4.3.7 Practical Part 7: Using a linear model to make predictions\n\nPractical Task 10 – Fit a linear model and use the results to plot the model predictions\nWe are going to draw a scatterplot and add a line.\n\nThe line will show the model predictions, given the model intercept and effect coefficient estimates.\n\nStep 1. First fit a linear model and get a summary of the estimates\nFit a model with mean accuracy mean.acc as the outcome and HLVA as the predictor.\nCheck if you can write the code you need to complete the linear model analysis. Click on the button to see the code example: compare it to the code you wrote.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nmodel &lt;- lm(mean.acc ~ HLVA, data = study.two.gen)\nsummary(model)\n\n\nCall:\nlm(formula = mean.acc ~ HLVA, data = study.two.gen)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.27457 -0.06777  0.01474  0.08025  0.23146 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.522016   0.032544  16.040  &lt; 2e-16 ***\nHLVA        0.026207   0.003481   7.529 2.87e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1047 on 170 degrees of freedom\nMultiple R-squared:  0.2501,    Adjusted R-squared:  0.2456 \nF-statistic: 56.68 on 1 and 170 DF,  p-value: 2.866e-12\n\n\n\n\n\nYou will need to record some information from the model summary so you can use it next.\n\nPract.Q.22. What is the coefficient estimate for the intercept?\n\n\nPract.A.22. 0.522016\n\n\nPract.Q.23. What is the coefficient estimate for the slope of HLVA?\n\n\nPract.A.23. 0.026207\n\nStep 2. Second, draw a scatterplot and add a line to show the model predictions\nCheck first if you can write the code you need to produce the prediction plot. Click on the button to see the code example: compare it to the code you wrote.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = HLVA, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2, colour = \"blue\", shape = 'square')   +\n  geom_abline(intercept = 0.522016, slope = 0.026207,\n              colour = \"red\", size = 1.5) +\n  theme_bw() +\n  labs(x = \"HLVA\", y = \"mean accuracy\") +\n  xlim(0, 15) + ylim(0, 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou have now completed the Week 8 practical exercises and questions\n\n\n\n\n\n\nImportant\n\n\n\nPredicting human behaviour is at the heart of:\n\nPsychological science, and our collective attempt to understand ourselves.\nBehavioural analytics, and the ways businesses work with what we know about people.\n\nThis is an important step in your developmental journey: Well done!\n\nWe will continue to deepen and extend your skills and understanding but everything builds on the key lessons we have been learning here.\n\n\n\n\n\n\nThe answers\nAfter the practical class, we will reveal the answers that are currently hidden.\nThe answers version of the webpage will present my answers for questions, and some extra information where that is helpful.\n\n\nLook ahead: growing in independence\n\n\n\n\n\n\nImportant\n\n\n\nEvery professional using R spends a lot of time on one or two R information websites. The most useful include:\n\nthe {tidyverse} webpages;\nStackOverflow;\nthe R bloggers aggregator webpages.\n\nCan you find them?\nIs there a question that has been puzzling you about what you are doing? Can you find the answer on one of these websites?",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 8. Introduction to the linear model"
    ]
  },
  {
    "objectID": "PSYC412/part1/Week15.html",
    "href": "PSYC412/part1/Week15.html",
    "title": "Week 15. Poisson regression",
    "section": "",
    "text": "Previously, we looked at logistic regression in the context of a binomial outcome variable, that is, a two-level variable such as correct vs. incorrect, or looking to the left vs. the right. Poisson regression is another type of generalized linear model that is particularly useful for count data.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 15. Poisson regression"
    ]
  },
  {
    "objectID": "PSYC412/part1/Week15.html#lectures",
    "href": "PSYC412/part1/Week15.html#lectures",
    "title": "Week 15. Poisson regression",
    "section": "Lectures",
    "text": "Lectures\nThe lecture material for this week follows the recommended chapters in Winter (2020) – see under ‘Reading’ below – and is presented below:\nPoisson regression (~28 min)\n\nSlides Transcript",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 15. Poisson regression"
    ]
  },
  {
    "objectID": "PSYC412/part1/Week15.html#reading",
    "href": "PSYC412/part1/Week15.html#reading",
    "title": "Week 15. Poisson regression",
    "section": "Reading",
    "text": "Reading\n\nWinter (2020)\nLink\nChapter 13 provides a clear introduction to Poisson regression and its implementation in R.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 15. Poisson regression"
    ]
  },
  {
    "objectID": "PSYC412/part1/Week15.html#pre-lab-activities",
    "href": "PSYC412/part1/Week15.html#pre-lab-activities",
    "title": "Week 15. Poisson regression",
    "section": "Pre-lab activities",
    "text": "Pre-lab activities\nAfter having watched the lectures and read the textbook chapters you’ll be in a good position to try these activities. Completing them before you attend your lab session will help you to consolidate your learning and help move through the lab activities more smoothly.\n\nPre-lab activity 1: Getting a feel for Poisson data\nTo get a feel for Poisson data, we’ll use the rpois() function to generate random data that is Poisson-distributed. rpois() needs two bits of information: lambda, and how many numbers you want to generate.\nAs usual, before we get stuck in we need to set up a few things.\n\nTASK: Add code to clear the environment. HINT: rm(list=ls())\n\nNext we need to tell R which libraries to use. For this pre-lab activity, we just need the tidyverse library.\n\nTASK: Add code to load relevant libraries. HINT: library()\n\nOk, now let’s play around with different lambdas to get a feel for the Poisson distribution.\n\nTASK: Copy the code below to your script and run it. Then change the value of lambda in the rpois() function and see how the distribution changes.\n\n\nlambda2 &lt;- rpois(n = 1000, lambda = 2)\n\nlambda2 &lt;- as.data.frame(lambda2)\n\nggplot(data = lambda2, mapping = aes(x = lambda2)) +\n  geom_bar()\n\nQUESTION: What do you notice about the Poisson distribution if you choose a high value for lambda?\n\n\nPre-lab activity 2: Getting ready\n\nGet your files ready\nDownload the 412_week15_forStudents.zip file and upload it into a new folder in RStudio Server.\n\n\nRemind yourself of how to access and work with the RStudio Server.\n\nSign in to the RStudio Server. Note that when you are not on campus you need to log into the VPN first (look on the portal if you need more information about that).\nCreate a new folder for this week’s work.\nUpload the zip-file to the folder you have created on the RStudio server. Note you can either upload a single file or a zip-file.\n\n\n\n\n\n\n\nIf you have difficulty uploading files to the server\n\n\n\nIf you get error messages when attempting to upload a file or a folder with files to the server, you can try the following steps:\n\nClose the R Studio server, close your browser and start afresh.\nOpen the R Studio server in a different browser.\nFollow a work around where you use code to directly download the file to the server. The code to do that will be available at the start of the lab activity where you need that particular file. The code to download the file you need to complete the quiz is below.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 15. Poisson regression"
    ]
  },
  {
    "objectID": "PSYC412/part1/Week15.html#lab-activities",
    "href": "PSYC412/part1/Week15.html#lab-activities",
    "title": "Week 15. Poisson regression",
    "section": "Lab activities",
    "text": "Lab activities\nIn this lab, you’ll gain understanding of and practice with:\n\nwhen and why to apply Poisson regression to answer questions in psychological science\nconducting Poisson regression in R\ninterpreting the R output of Poisson regression\nreporting results for Poisson regression following APA guidelines\n\n\nLab activity 1: Visual dominance\nWinter et al. (2018) showed that, on average, English words that were rated as strongly associated with the visual modality are more frequent than words more strongly associated with other sensory modalities. In this week’s lab activity we will retrace that analysis focusing on the subset of adjectives (the paper also included verbs and nouns). We’ll use sensory modality ratings as reported by Lynott and Connell (2009; see here for more info; data file: lynott_connell_2009_modality.csv) and word frequencies as reported by the English Lexicon Project (data file: ELP_full_length_frequency.csv). The research question is: Do English speakers use ‘visual’ adjectives more frequently than adjectives more strongly associated with other sensory modalities?\n\nStep 1: Set up\n\n\n\n\n\n\nSet your working directory\n\n\n\nThe folder you were asked to download under ‘Pre-lab activity 2: Getting ready for the lab class’ contains the data files we’ll need. Make sure you have set your working directory to this folder by right-clicking on it and selecting ‘Set as working directory’.\n\n\n\n\n\n\n\n\nEmpty the R environment\n\n\n\nBefore you do anything else, when starting a new analysis, it is a good idea to empty the R environment. This prevents objects and variables from previous analyses interfering with the current one. To do this, you can click on the little broom icon in the top right of the Environment pane, or you can use rm(list=ls()).\n\n\nBefore we can get started we need to tell R which libraries to use. For this analysis we’ll need broom, tidyverse, MASS and pscl.\n\nTASK: Load the relevant libraries. If you are unsure how to do that, you can look at the ‘Hint’ below for a clue by expanding it. After that, if you are still unsure, you can view the code by expanding the ‘Code’ section below.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the library() function.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nThe code to do this is below.\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(MASS)\nlibrary(pscl)\n\n\n\n\n\n\n\n\n\nIf you couldn’t upload files to the server, do this:\n\n\n\nIf you experienced difficulties with uploading a folder or a file to the server, you can use the code below to directly download the file you need in this lab activity to the server (instead of first downloading it to you computer and then uploading it to the server). Remember that you can copy the code to your clipboard by clicking on the ‘clipboard’ in the top right corner.\n\n\n\ndownload.file(\"https://github.com/mg78/2324_PSYC402/blob/main/data/week15/402_week15_forStudents/ELP_full_length_frequency.csv?raw=true\", destfile = \"ELP_full_length_frequency.csv\")\n\n\ndownload.file(\"https://github.com/mg78/2324_PSYC402/blob/main/data/week15/402_week15_forStudents/lynott_connell_2009_modality.csv?raw=true\", destfile = \"lynott_connell_2009_modality.csv\")\n\n\nTASK: Finally, read in the two data files (lynott_connell_2009_modality.csv and ELP_full_length_frequency.csv) and have a look at them.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the read_csv() function and the head() function.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nThe code to do this is below.\nlyn &lt;- read_csv('lynott_connell_2009_modality.csv')\nELP &lt;- read_csv('ELP_full_length_frequency.csv')\nhead(lyn)\nhead(ELP)\n\n\n\nQUESTION 1: Which variables do you need to address the research question?\n\n\nStep 2: A bit of data wrangling\nWe need to combine the information in the data files to be able to do any analyses. We can use a ‘join’ to do this. Have a look at the online book by Hadley Wickam and Gareth Grolemund (here) to remind yourself what a ‘join’ is. In particular, have a look at the inner_join() and the left_join().\nQUESTION 2: Which ‘join’ is most appropriate, the inner_join() or the left_join()? Also, does it matter which data file you specify as x and which one as y? If so, why does it matter?\n\nTASK: Add code to join the two data files and store the resulting table in an object called both. Try out the different joins and use head() to inspect the result.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou should end up with a table that has 423 observations of at least 8 variables.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nboth &lt;- left_join(x = lyn, y = ELP, by = 'Word')\n\n\n\nNext, we want to select only the variables we need. We want to use the select() function from dplyr. Because the MASS library is also loaded and that library also contains a select() function, we need to tell R specifically to use the one from dplyr. You can do this by using dplyr::, like this:\n\nboth &lt;- both %&gt;%\n  dplyr::select(Word, DominantModality:Smell, Log10Freq)\n\n\nTASK: Add the code above to your script and run it.\n\nFinally, to apply Poisson regression, we need the frequency variable as positive integers.\n\nTASK: Use the code below to transform the frequency variable to raw values. Don’t forget to add it to your script and run it.\n\n\nboth &lt;- mutate(both, Freq = 10 ^ Log10Freq)\n\nQUESTION 3: What does this line of code do. Write a comment to summarise its function.\n\n\nStep 3: Visualise the data\nTo get a better feel for the data, let’s make some scatterplots.\n\nTASK: Add code to make scatterplots with Freq on the y axis and each of the sensory modality ratings on the respective x axis. To be able to see more easily what is going on, limit the y-axis to values between 0 and 20000.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nMake 5 different scatterplots using ggplot() withgeom_point() and geom_smooth(). You can use ylim() to limit the values on the y-axis.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nggplot(both, aes(x = Sight, y = Freq)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  theme_bw() +\n  ylim(c(0, 20000))\n\nggplot(both, aes(x = Touch, y = Freq)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  theme_bw()  +\n  ylim(c(0, 20000))  \n\nggplot(both, aes(x = Sound, y = Freq)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  theme_bw() +\n  ylim(c(0, 20000))\n\nggplot(both, aes(x = Taste, y = Freq)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  theme_bw() +\n  ylim(c(0, 20000))\n\nggplot(both, aes(x = Smell, y = Freq)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  theme_bw() +\n  ylim(c(0, 20000))\n\n\n\nQUESTION 4: What do you conclude from the scatterplots?\n\n\nStep 4: The regression model\nWe are going to fit a Poisson regression model with Taste, Smell, Touch, Sight and Sound as predictors (all of these are continuous rating scales).\n\nTASK: Fit a Poisson regression model for ‘Freq’ as a function of ‘Taste’, ‘Smell’, ‘Touch’, ‘Sight’ and ‘Sound’.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the glm() function with family = poisson.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nfreqMod &lt;- glm(Freq ~ Sight + Taste + Smell + Sound + Touch,\n               data = both,\n               family = poisson)\n\nsummary(freqMod)\n\n\n\nQUESTION 5: How do you interpret the output of the Poisson regression?\n\n\nStep 5: Overdispersion\nIn the lecture we saw that it is possible that the variance is larger than theoretically expected for a given lambda. If this happens, we are dealing with what’s called ‘overdispersion’. You can compensate for this by using a variant of Poisson regression that is called ‘negative binomial regression’. In negative binomial regression the variance is uncouples from the mean.\n\nTASK: Fit a negative binomial regression model for ‘Freq’ as a function of ‘Taste’, ‘Smell’, ‘Touch’, ‘Sight’ and ‘Sound’.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the glm.nb() function.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nfreqMod_nb &lt;- glm.nb(Freq ~ Sight + Taste + Smell + Sound + Touch,\n    data = both)\nsummary(freqMod_nb)\n\n\n\nNext, check whether there is significant overdispersion by performing a likelihood ratio test, comparing the likelihood of the negative binomial model against the likelihood of the corresponding Poisson model.\n\nTASK: Use the odTest() function to perform an ‘overdispersion’ test.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the odTest() function and pass object that identifies your model as the argument.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nodTest(freqMod_nb)\n\n\n\nQUESTION 6: What do you conclude from the results of the overdispersion test?\nQUESTION 7: How do you interpret the negative binomial regression output? Do English speakers use visual adjectives more frequently? What about smell adjectives in comparison?",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 15. Poisson regression"
    ]
  },
  {
    "objectID": "PSYC412/part1/Week15.html#answers",
    "href": "PSYC412/part1/Week15.html#answers",
    "title": "Week 15. Poisson regression",
    "section": "Answers",
    "text": "Answers\nWhen you have completed all of the lab content, you may want to check your answers with our completed version of the script for this week. Remember, looking at this script (studying/revising it) does not replace the process of working through the lab activities, trying them out for yourself, getting stuck, asking questions, finding solutions, adding your own comments, etc. Actively engaging with the material is the way to learn these analysis skills, not by looking at someone else’s completed code…\n\nYou can download the R-script that includes the relevant code here: 412_wk15_labAct1_withAnswers.R.\n\nWhich variables do you need to address the research question? Our dependent variable is word frequency, so we need a variable that tells us how frequent words are. This information is contained in the English Lexicon Project data file, variable Log10Freq. The sensory modality ratings as reported in the in the data file supplied by Lynott and Connell (2009) are our predictors. That means we need the variables ‘Sight’, ‘Touch’, ‘Sound’, ‘Taste’, and ‘Smell’. You also need a variable that is common across the two files to help with merging them together (i.e., by  = ), this would be ‘Word’.\nWhich ‘join’ is most appropriate, the inner_join() or the left_join()? Also, does it matter which datafile you specify as x and which one as y? If so, why does it matter? A left_join() keeps all observations in table x, adding the information from table y for those observations. That means that it does matter which table you specify as x and which one as y. ELP contains word frequencies for many more words than we have sensory modality ratings for (in lyn). When specifying ELP as x, you keep 33075 rows, adding a lot of NAs for words we do not have sensory modality ratings for. Therefore it makes more sense to specify lyn as x, and add the word frequency information for the words we have sensory modality ratings for. An inner_join() matches pairs of observations whenever their keys are equal, and unmatched rows are not included in the result. This means that generally inner joins are not appropriate because it is too easy to lose observations.\nWhat does this line of code do. Write a comment to summarise its function.\n\n\nboth &lt;- mutate(both, Freq = 10 ^ Log10Freq)\n\nUse the mutate() function to add a variable Freq to the table both. Freq is derived from the variable Log10Freq by taking it to the power of 10. We do this because that reverses the log transformation and will therefore give us the raw values, all positive integers that we need for Poisson regression.\n\nWhat do you conclude from the scatterplots? Sight: moderate positive association - the visual rating is higher for more frequent words. Smell: weak negative association - when word frequency goes up, the smell rating goes down. There are a* lot of low smell ratings for adjectives (lots of dots clode to 0). Sound: weak positive association - when word frequency goes up, so does the sound rating. Touch: weak positive association - when word frequency goes up, so does the touch rating. Taste: no assocation - line is pretty much horizontal; similar to smell, there are a lot of low taste ratings (cluster of rating scores around 0).\nHow do you interpret the output of the Poisson regression? Estimate for Sight is indeed positive and largest (compared to the other predictors), while the estimate for Smell is negative and the estimate for Taste is very close to 0. All predictors are significant.\nWhat do you conclude from the results of the overdispersion test? The result of the overdispersion test is significant (p &lt; 2.2e-16), indicating that there is significant overdispersion. The negative binomial regression model is therefore more appropriate.\nHow do you interpret the negative binomial regression output? Do English speakers use visual adjectives more frequently? What about smell adjectives in comparison? The estimates for all predictors have now changed and not all of them are now significant. Also note that the standard errors of the estimates have increased substantially. Sight still shows a significant positive relationship with word frequency and has by far the largest estimate suggesting it contributes most. So, yes, English speakers use visual adjectives more frequently. For smell adjectives the estimate is negative and significant, suggesting that for more frequent words, the Smell ratings are smaller.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 15. Poisson regression"
    ]
  },
  {
    "objectID": "PSYC412/part1/Week11.html",
    "href": "PSYC412/part1/Week11.html",
    "title": "Week 11. Recap of the linear model and practising data-wrangling in R",
    "section": "",
    "text": "Before we start covering new material, we want to spent some time on recapping the basic concepts of the linear model (correlation, simple regression, multiple regression). You all come from different educational backgrounds and therefore have vastly different knowledge of, and experience with statistics. Therefore, please follow your own judgement as to whether you feel you want to/need to revisit material outlining the theoretical background to and the practical implementation in R for these topics. Below we provide some guidance as to materials that are relevant. Just to be clear: We don’t expect you to watch and/or read and/or do everything, please have a look at what you feel you need and spend some time with those materials.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 11. Recap of the linear model and practising data-wrangling in R"
    ]
  },
  {
    "objectID": "PSYC412/part1/Week11.html#sec-wk11-lectures",
    "href": "PSYC412/part1/Week11.html#sec-wk11-lectures",
    "title": "Week 11. Recap of the linear model and practising data-wrangling in R",
    "section": "Lectures",
    "text": "Lectures\nThe linear model was discussed in weeks 7 to 10 of PSYC411, so that is a good place to start.\nAlternatively, if you don’t feel confident about the material, these recorded lectures might help.\n\nThe linear model: theory (~30 min) An introduction to the linear model and linear regression. I follow material as discussed in Chapter 4 of Bodo Winter’s book Statistics for Linguists: An Introduction using R (see below under ‘Reading’).\n\n\nSlides Transcript\n\nHow to build a linear model in R (~14 min) In this video I demonstrate how to build a linear model in R by talking you through a simple linear regression script (you can download it by clicking on the link below the video). If you are unclear on what different parts of the lm() function do, or how to read the output, this video might help clarify that.\n\n\nSlides Transcript Example R-script\n\nMultiple regression: theory (~35 min) An introduction to multiple regression. I follow material as discussed in Chapter 5 of Bodo Winter’s book Statistics for Linguists: An Introduction using R (see below under ‘Reading’).\n\n\nSlides Transcript\n\nCentering and standardising (~5 min) Brief explanation of what centering and standardising are.\n\n\nSlides Transcript",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 11. Recap of the linear model and practising data-wrangling in R"
    ]
  },
  {
    "objectID": "PSYC412/part1/Week11.html#sec-wk11-reading",
    "href": "PSYC412/part1/Week11.html#sec-wk11-reading",
    "title": "Week 11. Recap of the linear model and practising data-wrangling in R",
    "section": "Reading",
    "text": "Reading\n\nMiller & Haden (2013)\nLink\nChapter 10 gives you a brief overview of what correlation and regression are. Chapter 11 introduces correlation in more detail. Chapters 12 and 14 provide accessible overviews of simple and multiple regression, respectively. All these chapters are really short but provide a good basis to understanding. We consider this the minimum level of understanding you should acquire.\n\n\nWinter (2020)\nLink\nChapter 4 provides and excellent conceptual introduction to the linear model and also explains how this is implemented in R (highly recommended).\nChapter 5 takes a slightly different approach to the one taken in Miller & Haden (2013) to introducing correlation. If you already understand the basic theory behind correlation, this will be an interesting read. Chapter 5 also clearly explains what centering and standardizing are and why you need to bother with these linear transformations.\nChapter 6 provides an excellent overview of multiple regression and also explains how this is implemented in R.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 11. Recap of the linear model and practising data-wrangling in R"
    ]
  },
  {
    "objectID": "PSYC412/part1/Week11.html#sec-wk11-pre-labactvities",
    "href": "PSYC412/part1/Week11.html#sec-wk11-pre-labactvities",
    "title": "Week 11. Recap of the linear model and practising data-wrangling in R",
    "section": "Pre-lab activities",
    "text": "Pre-lab activities\nAfter having watched the lectures and read the textbook chapters you’ll be in a good position to try these activities. Completing them before you attend your lab session will help you to consolidate your learning and help move through the lab activities more smoothly.\n\nPre-lab activity 1: Visualising the regression line\nHave a look at this visualisation of the regression line by Ryan Safner.\nIn this shiny app, you see a randomly-generated set of data points (within specific parameters, to keep the graph scaled properly). You can choose a slope and intercept for the regression line by using the sliders. The graph also displays the residuals as dashed red lines. Moving the slope or the intercept too much causes the generated line to create much larger residuals. The shiny app also calculates the sum of squared errors (SSE) and the standard error of the regression (SER), which calculates the average size of the error (the red numbers). These numbers reflect how well the regression line fits the data, but you don’t need to worry about those for now.\nIn the app he uses the equation Y = aX + b in which b is the intercept and a is the slope.\nThis is slightly different from the equation you saw during the lecture. There we talked about Y = b0 + b1*X + e. Same equation, just different letters. So b0 in the lecture is equivalent to b in the app and b1 in the lecture is equivalent to a in the app.\nPre-lab activity questions:\n\nChange the slider for the intercept. How does it change the regression line?\nChange the slider for the slope. How does it change the regression line?\nWhat happens to the residuals (the red dashed lines) when you change the slope and the intercept of the regression line?\n\n\n\nPre-lab activity 2: Data-wrangling in R\nData comes in lots of different formats. One of the most common formats is that of a two-dimensional table (the two dimensions being rows and columns). Usually, each row stands for a separate observation (e.g. a participant), and each column stands for a different variable (e.g. a response, category, or group). A key benefit of tabular data is that it allows you to store different types of data-numerical measurements, alphanumeric labels, categorical descriptors-all in one place.\nIt may surprise you to learn that scientists actually spend far more of time cleaning and preparing their data than they spend actually analysing it. This means completing tasks such as cleaning up bad values, changing the structure of tables, merging information stored in separate tables, reducing the data down to a subset of observations, and producing data summaries. Some have estimated that up to 80% of time spent on data analysis involves such data preparation tasks (Dasu & Johnson, 2003)!\nMany people seem to operate under the assumption that the only option for data cleaning is the painstaking and time-consuming cutting and pasting of data within a spreadsheet program like Excel. We have witnessed students and colleagues waste days, weeks, and even months manually transforming their data in Excel, cutting, copying, and pasting data. Fixing up your data by hand is not only a terrible use of your time, but it is error-prone and not reproducible. Additionally, in this age where we can easily collect massive datasets online, you will not be able to organise, clean, and prepare these by hand.\nIn short, you will not thrive as a psychologist if you do not learn some key data wrangling skills. Although every dataset presents unique challenges, there are some systematic principles you should follow that will make your analyses easier, less error-prone, more efficient, and more reproducible.\nSome of the functions you’ll need in this week’s lab activity are mentioned below. You’ve used these functions before, but the following recipes summarise what each one does and how to use it.\n\n\n\n\n\n\nRecipes - How to use them\n\n\n\nEach ‘recipe’ has the same structure.\n\nFirst, it summarises what it is that you want to achieve when using that specific function. In the case of select() it says “You want to extract specific columns from a data frame and return them as a new, smaller data frame.”\nThen, it outlines a number of steps that you need to carry out when using this function. For select() it outlines 2 steps: 1. Pass the dataframe to the function. 2. List the column(s) to return.\nFinally, there is an example talks you through using the function with some data. For select() it uses an example with data on the weather.\nAdditional information appears in extra boxes with a light-bulb icon. If you find those confusing, don’t worry about them at this stage.\n\n\n\n\nTASK Have a look at each ‘recipe’ and read through it. Try to understand each step.\n\n\nUnder R Basics, have a look at the recipe to Read a CSV file or remind yourself how to make sure to Obey R’s naming rules. Of course, you are very welcome to review other recipes in this category as well.\nReview the recipes in the Transform Tables category to remind yourself how to extract values from a table, calculate summary statistics, derive new variables and more by using dplyr verbs such as select(), filter(), mutate(), rename(), group_by() and summarise().\nUnder Visualize Data, you’ll find recipes to create a histogram or a scatterplot (and much more).\n\nPlease note that there are often different ways to do the same or similar things in R. This means you might encounter slightly different functions or styles of coding in different materials. This is not something to worry about. Just make sure you’re clear on what a bit of code achieves and choose the function/style that you feel most comfortable with.\n\n\n\n\n\n\nTip\n\n\n\nIn the Posit recipes, you’ll see the function call for for instance read_csv() written as readr::read_csv(). This specifies the package or library the function is from (readr) as well as the function (read_csv()). As long as you have loaded the tidyverse library, you don’t need to load readr separately, because it is part of the tidyverse library. If you want to know more about the tidyverse library, see here.\n\n\n\n\nPre-lab activity 3: Getting ready for the lab class\n\nRemind yourself of how to access and work with the RStudio Server.\n\nRevisit PSYC411 to remind yourself of how to access the RStudio Server.\n\n\n\nGet your files ready\nDownload the 412_week11_lab.zip folder and upload the files into a new folder in RStudio Server.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 11. Recap of the linear model and practising data-wrangling in R"
    ]
  },
  {
    "objectID": "PSYC412/part1/Week11.html#sec-wk11-labactivities",
    "href": "PSYC412/part1/Week11.html#sec-wk11-labactivities",
    "title": "Week 11. Recap of the linear model and practising data-wrangling in R",
    "section": "Lab activities",
    "text": "Lab activities\n\n\n\nIn this lab, you’ll gain understanding of and practice with:\n\nwhen and why to apply simple and multiple regression to answer questions in psychological science\nconducting multiple regression in R\ninterpreting the R output of simple and multiple linear regression\nreporting results for simple and multiple linear regression following APA guidelines\n\n\nLab activity 1: Interpreting and reporting results\nHave a look at the R output below.\nR Output 1\n\n\nWhat is the outcome or dependent variable?\nWhat is the predictor or independent variable?\nIs the overall model significant?\nHow much variance does the model account for?\n\nThinking about assumptions, what do you conlcude from the plots and output below?\n\nDoes the relationship appear linear?\nDo the residuals show normality and homoscedasticity?\n\nScatterplot\n\nQQ-plot\n\nR Output 2\n\n\n\nLab activity 2: Conducting simple and multiple regression\n\nBackground\nToday, to help get a practical understanding of regression, you will be working with real data and using regression to explore the question of whether there is a relationship between voice acoustics and ratings of perceived trustworthiness.\n\nThe Voice\nThe prominent theory of voice production is the source-filter theory (Fant, 1960) which suggests that vocalisation is a two-step process: air is pushed through the larynx (vocal chords) creating a vibration, i.e. the source, and this is then shaped and moulded into words and utterances as it passes through the neck, mouth and nose, and depending on the shape of those structures at any given time you produce different sounds, i.e. the filter. One common measure of the source is pitch (otherwise called Fundamental Frequency or F0 (F-zero)) (Titze, 1994), which is a measure of the vibration of the vocal chords, in Hertz (Hz); males have on average a lower pitch than females for example. Likewise, one measure of the filter is called formant dispersion (measured again in Hz), and is effectively a measure of the length of someone’s vocal tract (or neck). Height and neck length are suggested to be negatively correlated with formant dispersion, so tall people tend to have smaller formant dispersion. So all in, the sound of your voice is thought to give some indication of what you look like.\nMore recently, work has focussed on what the sound of your voice suggests about your personality. McAleer, Todorov and Belin (2014) suggested that vocal acoustics give a perception of your trustworthiness and dominance to others, regardless of whether or not it is accurate. One extension of this is that trust may be driven by malleable aspects of your voice (e.g. your pitch) but not so much by static aspects of your voice (e.g. your formant dispersion). Pitch is considered malleable because you can control the air being pushed through your vocal chords (though you have no conscious control of your vocal chords), whereas dispersion may be controlled by the structure of your throat which is much more rigid due to muscle, bone, and other things that keep your head attached. This idea of certain traits being driven by malleable features and others by static features was previously suggested by Oosterhof and Todorov (2008) and has been tested with some validation by Rezlescu, Penton, Walsh, Tsujimura, Scott and Banissy (2015).\nSo, the research question today is: Can vocal acoustics, namely pitch and formant dispersion, predict perceived trustworthiness from a person’s voice? We will only look at male voices today, but you have the data for female voices as well should you wish to practice (note that in the field, tendency is to analyse male and female voices separately as they are effectively sexually dimorphic). As such, we hypothesise that a linear combination of pitch and dispersion will predict perceived vocal trustworthiness in male voices. This is what we will analyse.\nTo complete this lab activity, you can use the R-script (412_wk11_labAct2.R) that you downloaded as part of the ‘pre-lab activities’ as a template. Work through the activity below, adding relevant bits of code to your script as you go along.\n\n\n\nStep 1: Background and set up\n\n\n\n\n\n\nSet your working directory\n\n\n\nThe folder you were asked to download under ‘Pre-lab activity 3: Getting ready for the lab class’ contains the data files we’ll need. Make sure you have set your working directory to this folder by right-clicking on it and selecting ‘Set as working directory’.\n\n\n\n\n\n\n\n\nEmpty the R environment\n\n\n\nBefore you do anything else, when starting a new analysis, it is a good idea to empty the R environment. This prevents objects and variables from previous analyses interfering with the current one. Use the code snippet below to clear the environment.\n\n\n\nrm(list=ls())\n\n\n\n\n\n\n\nTip\n\n\n\nIf you hover your mouse over the box that includes the code snippet, a ‘copy to clipboard’ icon will appear in the top right corner of the box. Click that to copy the code. Now you can easily paste it into your script.\n\n\nBefore we can get started we need to tell R which libraries to use. For this analysis we’ll need broom, car, and tidyverse.\n\nTASK: Load the relevant libraries. If you are unsure how to do that, you can look at the ‘Hint’ below for a clue by expanding it. After that, if you are still unsure, you can view the code by expanding the ‘Code’ section below.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the library() function.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nThe code to do this is below.\nlibrary(broom)\nlibrary(car)\nlibrary(tidyverse)\n\n\n\nIn this lab, we are setting out to test whether a linear combination of pitch and dispersion will predict perceived vocal trustworthiness in male voices. We’ll be working with two data files:\n\nvoice_acoustics.csv - shows the VoiceID, the sex of the voice, and the pitch and dispersion values\nvoice_ratings.csv - shows the VoiceID and the ratings of each voice by 28 participants on a scale of 1 to 9 where 9 was extremely trustworthy and 1 was extremely untrustworthy.\n\n\nTASK: Read in both files, have a look at the layout of the data and familiarise yourself with it. The ratings data is rather messy and in a different layout to the acoustics but can you tell what is what?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the read_csv() function.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nThe code to do this is below.\nacoustics &lt;- read_csv(\"voice_acoustics.csv\")\nratings &lt;- read_csv(\"voice_ratings.csv\")\n\n\n\n\nQUESTION 1 How are the acoustics data and the ratings data organised (wide or long)? Are both data files ‘tidy’? If you need more info on what that means, have a look here.\n\n\n\nStep 2: Restructuring the ratings data\nWe are going to need to do some data-wrangling before we do any analysis! Specifically, we need the change the ratings data to the long format.\nHere we’ll use the pivot_longer() function (see here or type ?pivot_longer in the Console for more info) to restructure the ratings data from wide to long and store the resulting table as ‘ratings_tidy’.\n\nTASK: Use the code snippet below to restructure the data. Have a look at each line of code (and the comments in green) and check that you understand how it works.\n\n\nratings_tidy &lt;- pivot_longer(\n  data = ratings,    # the data you want to restructure\n  cols = P1:P28,     # columns you want to restructure\n  names_to = \"participant\", # variable name that captures whatever is across the columns\n  # (in this case P1 to P28 for the 28 different participants)\n  values_to = \"rating\") # variable name that captures whatever is in the cells\n  # (in this case numbers for ratings)\n\n\n\nStep 3: Calculate mean trustworthiness rating for each voice\nNow that we have the ratings data into a tidy format, the next step is to calculate the mean rating for each voice. Remember that each voice is identified by the ‘VoiceID’ variable.\n\nTASK: Calculate the mean rating for each voice and store the resulting table in a variable named ‘ratings_mean’.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse group_by() and summarise(). Are you using the tidy data? Also, remember that if there are any missing values (NAs) then na.rm = TRUE would help.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nratings_mean &lt;- ratings_tidy %&gt;%\n  group_by(VoiceID) %&gt;%\n  summarise(mean_rating = mean(rating))\n\n\n\n\n\nStep 4: Join the data together\nOk, before we get ahead of ourselves, in order to perform the regression analysis we need to combine the data from ‘ratings_mean’ (the mean ratings) with ‘acoustics’ (the pitch and dispersion ratings). Also, as we said, we only want to analyse male voices today.\n\nTASK: Join the two tables and keep only the data for the male voices, call the resulting table ‘joined’.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the inner_join() function (making use of the variable that is common across both tables) to join. See here or type ?inner_join in the Console for more info. Use the filter() function to only keep male voices. Remember that the Boolean operator for exactly equal is ==.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\njoined &lt;- ratings_mean %&gt;%\n  inner_join(acoustics, \"VoiceID\") %&gt;%\n  filter(sex == \"M\")\n\n\n\n\n\nStep 5: Spreading the data\nWe are starting to get an understanding of our data and we want to start thinking about the regression. However, the regression would be easier to work with if Pitch and Dispersion were in separate columns. This can be achieved using the pivot_wider() function (see here or type ?pivot_wider in the Console for more info). This is basically the inverse of pivot_longer(). It increases the number of columns and decreases the number of rows.\n\nTASK: Use the code snippet below to spread the data. Have a look at each line of code (and the comments in green) and check that you understand how it works.\n\n\njoined_wide &lt;- joined %&gt;%\n  pivot_wider(\n    names_from = measures, # name of the categorical column to spread\n    values_from = value) # name of the data to spread\n\n\nQUESTION 2 Why do we not need to specify within the pivot_wider() function which data to use?\n\n\n\nStep 6: Visualising the data\nAs always, it is a good idea to visualise your data.\n\nTASK: Now that we have all the variables in one place, make two scatterplots, one of mean trustworthiness rating with dispersion and one for mean trustworthiness rating and pitch.\n\n\n\n\n\n\n\nHint 1\n\n\n\n\n\nFor this you’ll need the ggplot() function together with geom_point() and geom_smooth(). Make sure to give your axes some sensible labels.\n\n\n\n\n\n\n\n\n\nHint 2\n\n\n\n\n\nBelow is a template for the code. Make sure to specify the data frame and the relevant variables.\nggplot(DATA, aes(x = variable X, y = variable Y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  theme_bw() +\n  labs (y = \"Label for variable Y\")\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nggplot(joined_wide, aes(x = Dispersion, y = mean_rating)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  theme_bw() +\n  labs (y = \"Mean Trustworthiness Rating\")\n\nggplot(joined_wide, aes(x = Pitch, y = mean_rating)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  theme_bw() +\n  labs (y = \"Mean Trustworthiness Rating\")\n\n\n\n\nQUESTION 3 Looking at the scatterplots, how would you describe the relationships between trustworthiness and dispersion and trustworthiness and pitch in terms of direction and strength? Which one of the two seems stronger?\n\n\n\nStep 7: Conducting and interpreting simple regression\nWith all the variables in place and having gained a better understanding of our data by inspecting the scatterplots, we’re ready now to start building two simple linear regression models:\n\nPredicting trustworthiness mean ratings from Pitch\nPredicting trustworthiness mean ratings from Dispersion\n\n\nTASK: Use the lm() function to run the following two regression models and use the summary() function to look at the output of each model. Store the first model in a table called ‘mod_pitch’ and store the second model in ‘mod_disp’.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nBelow is a template for the code. Make sure to specify the data frame and the relevant variables.\nlm(dv ~ iv, data = my_data)\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nmod_disp &lt;- lm(mean_rating ~ Dispersion, joined_wide)\nmod_disp_sum &lt;- summary(mod_disp)\n\nmod_pitch &lt;- lm(mean_rating ~ Pitch, joined_wide)\nmod_pitch_sum &lt;- summary(mod_pitch)\n\n\n\n\nQUESTION 4 What do you conclude from the output of these models? Which model is significant? Which predictors are significant? How much variance does each model describe?\n\n\n\nStep 8: Conducting and interpreting multiple regression\nNow let’s look at both predictors in the same model. Before we do this, it is sensible to center and standardise the predictors.\nLook at the code below. Can you follow how the predictors are first centered (_c) and then standardised (_z)?\nHere I do this by hand because I think it makes it clearer, even though there are functions that do this in one step (scale()).\n\njoined_wide &lt;- mutate(joined_wide,\n                      Dispersion_c = Dispersion - mean(Dispersion),\n                      Dispersion_z = Dispersion_c / sd(Dispersion_c),\n                      Pitch_c = Pitch - mean(Pitch),\n                      Pitch_z = Pitch_c / sd(Pitch_c))\n\n\nTASK: Now use the centered and standardised data for the multiple regression. Use the lm() function to run a model for predicting trustworthiness mean ratings from Pitch and Dispersion, and store the model in ‘mod_pitchdisp_z’. Use the ‘summary()’ function to look at the output.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nBelow is a template for the code. Make sure to specify the data frame and the relevant variables.\nlm(dv ~ iv1 + iv2, data = my_data)\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nmod_pitchdisp_z &lt;- lm(mean_rating ~ Pitch_z + Dispersion_z, joined_wide)\nmod_pitchdisp_z_sum &lt;- summary(mod_pitchdisp_z)\n\n\n\n\nQUESTION 5 What do you conclude from the output of this model? Is the overall model significant? Which predictors are significant? How much variance does the model describe? Which model would you say is best for predicting ratings of trustworthiness, the Pitch only, the Dispersion only or the Pitch+Dispersion model?\n\n\n\nStep 9: Checking assumptions\nNow that we’ve established which model best fits the data, let’s check whether it meets the assumptions of linearity, normality and homoscedasticity.\n\nTASK: Check the assumptions of linearity, normality and homoscedasticity.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou can use the crPlots() and the qqPlot() functions to check linearity. The shapiro.test() can be used to check normality of the residuals and the residualPlot() and nvcTest() functions to check homoscedasticity of the residuals. These plots are from base R rather than using the ggplot() function. See here for a brief explanation of base R plots and why you might want to use them.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\ncrPlots(mod_pitch)\n\nqqPlot(mod_pitch$residuals)\nshapiro.test(mod_pitch$residuals)\n\nresidualPlot(mod_pitch)\nncvTest(mod_pitch)\n\n\n\n\nQUESTION 6 What do you conclude from the graphs and output? Should we also check for collinearity?\n\n\n\nStep 10: Writing up the results\n\nTASK: Write up the results following APA guidelines.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe Purdue writing lab website is helpful for guidance on punctuating statistics. The APA Style 7th Edition Numbers and Statistics Guide is also useful.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 11. Recap of the linear model and practising data-wrangling in R"
    ]
  },
  {
    "objectID": "PSYC412/part1/Week11.html#sec-wk11-answers",
    "href": "PSYC412/part1/Week11.html#sec-wk11-answers",
    "title": "Week 11. Recap of the linear model and practising data-wrangling in R",
    "section": "Answers",
    "text": "Answers\nWhen you have completed all of the lab content, you may want to check your answers with our completed version of the script for this week. Remember, looking at this script (studying/revising it) does not replace the process of working through the lab activities, trying them out for yourself, getting stuck, asking questions, finding solutions, adding your own comments, etc. Actively engaging with the material is the way to learn these analysis skills, not by looking at someone else’s completed code…\n\n\nLab activity 1: Interpreting and reporting results\n\nWhat is the outcome or dependent variable? Word reading\nWhat is the predictor or independent variable? Non-word reading\nIs the overall model significant? Yes, F(1,50) = 69.03, p &lt; .001\nHow much variance does the model account for? 58%\nDoes the relationship appear linear? Yes. The dots and the pink line assemble quite closely on the dashed line.\nDo the residuals show normality and homoscedasticity? The qq-plot suggests that the residuals are normally distributed as the dots fall close to the solid blue line and within the range of the dashed blue lines. The Shapiro-Wilk test of normality confirms this (it is not significant). Similarly, the output of the non-constant variance score tests is not significant suggesting that the residuals are homoscedastic.\n\n\n\nLab activity 2: Conducting simple and multiple regression\nYou can download the R-script that includes the relevant code and answers to the questions here: 412_wk11_labAct2_withAnswers.R.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 11. Recap of the linear model and practising data-wrangling in R"
    ]
  },
  {
    "objectID": "PSYC412/part1/Week12.html",
    "href": "PSYC412/part1/Week12.html",
    "title": "Week 12. Categorical predictors",
    "section": "",
    "text": "So far, all the predictors in the models we’ve looked at were continuous variables. What if you wanted to know whether a response differed between two or more discrete groups? Hang on, you might say, that sounds like doing an ANOVA. True, you might have used ANOVA to assess whether group means differed in previous stats courses. ANOVAs—to some degree—are just a special type of regression where you have categorical predictors. This week we’ll look at how to model responses as a function of categorical predictors and we’ll combine categorical predictors to model how a predictor might affect the outcome variable differently across two different groups. For example, we might be interested in whether the amount of time adolescents use digital devices (screen-time) predicts their well-being. Additionally, we might want to know whether well-being is different for adolescent boys and girls and whether the relationship between screen-time and well-being differs for these two groups. By fitting a regression model in which we combine a continuous (screen-time) and a categorical (sex) predictor, we can do exactly that. We’ll be working on that in the lab.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 12. Categorical predictors"
    ]
  },
  {
    "objectID": "PSYC412/part1/Week12.html#sec-wk12-lectures",
    "href": "PSYC412/part1/Week12.html#sec-wk12-lectures",
    "title": "Week 12. Categorical predictors",
    "section": "Lectures",
    "text": "Lectures\nThe lecture material for this week follows the recommended chapters in Winter (2020) – see under ‘Reading’ below – and is presented in two parts. The videos have captions, in case you find that helpful.\n\n\n\n\n\n\nTip\n\n\n\nYou’ll find links to the slides and transcripts underneath each video. You can also download them all at once here\n\n\n\nCategorical predictors (~17 min)\n\n\nSlides Transcript\n\nInteractions (~18 min)\n\n\nSlides Transcript\nIf you are (relatively) new to using R and RStudio and not yet confident in using various functions from the dplyr package, the video below will be useful:\n\nData wrangling with dplyr (~10 minutes) Watch this part before you complete the pre-lab activities and before you attend the lab session.\n\n\nSlides Transcript",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 12. Categorical predictors"
    ]
  },
  {
    "objectID": "PSYC412/part1/Week12.html#sec-wk12-reading",
    "href": "PSYC412/part1/Week12.html#sec-wk12-reading",
    "title": "Week 12. Categorical predictors",
    "section": "Reading",
    "text": "Reading\n\nBlogpost by Professor Dorothy Bishop\nIn this very short blogpost Professor Dorothy Bishop explains the links between ANOVA and Regression.\n\n\nWinter (2020)\nLink\nChapter 7 provides an excellent overview of using categorical predictors in regression models and explains how this is implemented in R.\nChapter 8 explains what interactions are and how to model and interpret them.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 12. Categorical predictors"
    ]
  },
  {
    "objectID": "PSYC412/part1/Week12.html#sec-wk12-pre-labactivities",
    "href": "PSYC412/part1/Week12.html#sec-wk12-pre-labactivities",
    "title": "Week 12. Categorical predictors",
    "section": "Pre-lab activities",
    "text": "Pre-lab activities\nAfter having watched the lectures and read the textbook chapters you’ll be in a good position to try these activities. Completing them before you attend your lab session will help you to consolidate your learning and help move through the lab activities more smoothly.\n\nPre-lab activity 1: More data-wrangling in R\nWe’ll continue to practice data-wrangling in R. This week, we’ll work with some questionnaire data. You might have collected questionnaire data using Qualtrics, Pavlovia or Gorilla. The data we’ll be working with in this pre-lab activity were collected using the Gorilla platform. The questionnaire collects some demographic information. A data collection platform typically provides you with an output file that is quite complex. For instance, in addition to responses to the questions of interest, it will collect information on task name, task version, various time stamps and other information on the data collection session. To be able to do any statistics on the data, you’ll have to extract the relevant information from this larger data file. We can do this by using various functions from the dplyr package. This cheatsheet provides a useful overview.\n\nStep 1: Background and set up\n\n\n\n\n\n\nSet your working directory\n\n\n\nMake sure you have set your working directory to the correct folder by right-clicking on it and selecting ‘Set as working directory’.\n\n\n\n\n\n\n\n\nEmpty the R environment\n\n\n\nBefore you do anything else, when starting a new analysis, it is a good idea to empty the R environment. This prevents objects and variables from previous analyses interfering with the current one. To do this, you can click on the little broom icon in the top right of the Environment pane, or you can use rm(list=ls()).\n\n\nBefore we can get started we need to tell R which libraries to use. For this analysis we’ll need tidyverse.\n\nTASK: Load the relevant libraries. If you are unsure how to do that, you can look at the ‘Hint’ below for a clue by expanding it. After that, if you are still unsure, you can view the code by expanding the ‘Code’ section below.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the library() function.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nThe code to do this is below.\nlibrary(tidyverse)\n\n\n\n\n\n\n\n\n\nIf you couldn’t upload files to the server, do this:\n\n\n\nIf you experienced difficulties with uploading a folder or a file to the server, you can use the code below to directly download the file you need in this lab activity to the server (instead of first downloading it to you computer and then uploading it to the server). Remember that you can copy the code to your clipboard by clicking on the ‘clipboard’ in the top right corner.\n\n\n\ndownload.file(\"https://github.com/lu-psy-r/MSc/blob/main/PSYC412/part1/Week12-files/data_exp_72623-v17_questionnaire-i5jp.csv?raw=true\", destfile = \"data_exp_72623-v17_questionnaire-i5jp.csv\")\n\n\nTASK: Read in the data file.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the read_csv() function.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nThe code to do this is below.\ndemo &lt;- read_csv(\"data_exp_72623-v17_questionnaire-i5jp.csv\")                                   \n\n\n\n\n\nStep 2: Have a look what is in this file\nThe first thing to do when developing an R-script to clean up data prior to data analysis is to have a look what is in the file and identify the rows and columns that you need.\n\nTASK: View the data file. See whether you can identify which columns we need to extract the questionnaire responses. If you are unsure how to do that, you can look at the ‘Hint’ below for a clue by expanding it. After that, if you are still unsure, you can view the code by expanding the ‘Code’ section below.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou can use View() to open the file in a separate tab. You can also inspect the file by using the head() function to look at the first few rows or the glimpse() function to get an overview of all rows and columns. Type ?head or ?dplyr::glimpse in the Console to get more information about those functions.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nThe code to do this is below:\nView(demo)                                  # Look at the data frames\nhead(demo)\nglimpse(demo)\n\n\n\nYou’ll find that the data file has 31 columns and 157 rows. The questionnaire responses are in the last column (Response). Additionally, we need information in the column Question Key to make sure we extract the labels associated with the responses. Finally, we need participant numbers (stored in Participant Private ID) to be able to link responses to a particular participant.\n\n\nStep 3: Extract relevant columns and rows\n\nTASK: Extract the columns Participant Private ID, Question Key and Response, and assign the result to a new data frame.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the select() function.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nThe code to do this is below:\nselect(`Participant Private ID`, `Question Key`, `Response`)\n\n\n\nYou’ll see that the new data frame still has 157 observations or rows, but now we only have a much more manageable 3 variables or columns. If you scroll through the data file, you’ll see that the responses from different participants are in separate rows (meaning that the data are in ‘long’ format). You’ll also see that it separates different participants by rows with the text “BEGIN QUESTIONNAIRE” and “END QUESTIONNAIRE”. We need to get rid of those before we transform the data frame to a ‘wide’ format.\n\nTASK: Extract all rows, except ones that have “BEGIN QUESTIONNAIRE” or “END QUESTIONNAIRE” in the column Question Key.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the filter() function, in combination with the logical operator for ‘not’. See here for more info on logical operators.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nThe code to do this is below:\nfilter(!`Question Key` == \"BEGIN QUESTIONNAIRE\")\nfilter(!`Question Key` == \"END QUESTIONNAIRE\")\n\n\n\n\n\nStep 4: Change the format from ‘long’ to ‘wide’\nNow that we’ve cleaned up the column and rows, we want to change the format to wide to have a separate column for each question, and have one row per participant.\n\nTASK: Change the format to wide.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the pivot_wider() function.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nThe code to do this is below:\npivot_wider(names_from = `Question Key`, values_from = Response)\n\n\n\n\n\nStep 5: Rename variables\nHave another look at the data frame. We now have separate columns for different questions such as participant number, age, gender etc. However, some of the columns have awkward names, consisting of multiple words (such as ‘Participant Private ID’). Let’s rename those to make it easier to work with. Also note that we have data for 10 participants. For the purpose of this exercise, we’ll focus on the variables age, gender, language background and handedness. So don’t worry about renaming the other ones.\n\nTASK: Rename the variables that have multi-word names. Focus on age, gender, language background and handedness.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the rename() function.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nThe code to do this is below:\nrename(origID = `Participant Private ID`,\n         age = Age,\n         gender = Gender,\n         gender_text = `Gender-text`,\n         bilingual_text = `bilingual-text`,\n         handedness = categorical_hand) %&gt;%\n\n\n\n\n\nStep 6: Extract variables\n\nTASK: Extract the following variables: age, gender (as well the associated text variable), language background (as well as the associated text variable) and handedness.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the select() function.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nThe code to do this is below:\nselect(origID, age, gender, gender_text, bilingual, bilingual_text, handedness)\n\n\n\n\n\nStep 7: Recode gender and bilingual\nIf you look at the gender variable, you’ll see that it has 3 levels: ‘Male’, ‘Female’ and ‘Other (please specify)’. There are 2 issues with this: 1) Male and female are labels associated with biological sex, rather than gender. These labels should be recoded to ‘Man’ and ‘Woman’; 2) The third label reads ‘Other (please specify)’. While a necessary part of the question, we don’t really want the ‘(please specify)’ part to show in tables and figures. We will want to recode that to just ‘Other’. Similarly, for the bilingual variable, we have ‘Yes. My strongest language is:’ and ‘No’. We will want to recode that to a simpler ‘Yes’ and ‘No’.\n\nTASK: Recode the levels of the gender variable to ‘Man, ’Woman’ and ‘Other’, rather than ‘Male’, ‘Female and ’Other (please specify)’.\n\n\n\n\n\n\n\nHint 1\n\n\n\n\n\nUse the recode() function, in combination with the mutate() function.\n\n\n\n\n\n\n\n\n\nHint 2\n\n\n\n\n\nTry to fill in the template below:\nmutate (new variable = recode(old variable,\n                              `Old label1` = \"New label1\",\n                              `Old label2` = \"New label2\",\n                              `Old label3` = \"New label3\"))\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nThe code to do this is below:\nmutate (gender = dplyr::recode(gender,\n                               `Male` = \"Man\",\n                               `Female` = \"Woman\",\n                               `Other (please specify)` = \"Other\")) %&gt;%\nmutate(bilingual = dplyr::recode(bilingual,\n                                 `Yes. My strongest language is:` = \"Yes\"))\n\n\n\n\n\nStep 8: Change variable types\nSomething else to check at this stage is whether the variable types that R has automatically assigned to your variables are correct. An easy way to check what the variable type for each variable is, is by clicking on the white triangle in the blue circle next to the object name in the Environment. Using the glimpse() function will also show them. You will see that each variable is a ‘num’ (for ‘numerical’) or ‘chr’ (for ‘character’). Specifically, currently R thinks that the values stored in ‘origID’ are numerical values and that the values stored in all other variables are ‘character’ values (basically reading them as strings). This is not correct. Participant numbers, gender and bilingual are really factors with different levels (or categories) and the values in ‘age’ are numerical values. This is important, because certain functions expect variables of a certain type. For instance, you can’t compute the mean of a variable that consists of ‘characters’. So if you don’t change the variable types and try to compute the mean of ‘age’ it will not work. You can read more about variable types in R here.\n\nTASK: Change the variable types for participant number (factor), age (numerical), gender (factor), and bilingual (factor). Note that the variables ‘gender_text’ and ‘bilingual_text’ indeed contain string information, so are correctly identified as ‘character’ variables.\n\n\n\n\n\n\n\nHint 1\n\n\n\n\n\nUse the as.factor() and as.numeric() functions, in combination with the mutate() function.\n\n\n\n\n\n\n\n\n\nHint 2\n\n\n\n\n\nTry to fill in the template below:\nmutate(variable1 = as.factor(variable1),\n       variable2 = as.numeric(variable2),\n       variable3 = as.factor(variable3),\n       variable4 = as.factor(variable4))\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nThe code to do this is below:\nmutate(origID = as.factor(origID),\n       age = as.numeric(age),\n       gender = as.factor(gender),\n       handedness = as.factor(handedness))\n\n\n\n\n\nStep 9: Save processed file\nNow the data is in a much better shape to work with for data visualisation, combination with other data files and analysis. In my own work, I’d write a separate ‘cleaning’ script like this for each measure (questionnaire or experimental task) and store the cleaned data frame as a new data file. I can then easily read in those ‘cleaned’ data files when doing further visualisation or analysis, without having to touch the raw data files again.\n\nTASK: Store the data frame that contains the ‘cleaned’ or ‘processed’ data.\n\n\n\n\n\n\n\nHint 1\n\n\n\n\n\nUse the write_csv() function.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nThe code to do this is below:\nwrite_csv(demo_proc, (\"Week12-files/demo_proc.csv\"))\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nYou can download the R-script that combines all these bits of code into 1 pipe here. It is always a good idea to build a pipe like this 1 line at a time. Make sure to check that each line actually does what you expect it to do before you add the next line.\n\n\n\nWell done! The more you practise data wrangling, the easier it will become.\n\n\n\nPre-lab activity 2: Getting ready for the lab class\n\nGet your files ready\nDownload the 412_week12_lab.zip file.\n\n\nRemind yourself of how to access and work with the RStudio Server.\n\nSign in to the RStudio Server. Note that when you are not on campus you need to log into the VPN first (look on the portal if you need more information about that).\nCreate a new folder for this week’s work.\nUpload the zip-file to the folder you have created on the RStudio server. Note you can either upload a single file or a zip-file.\n\n\n\n\n\n\n\nIf you have difficulty uploading files to the server\n\n\n\nIf you get error messages when attempting to upload a file or a folder with files to the server, you can try the following steps:\n\nClose the R Studio server, close your browser and start afresh.\nOpen the R Studio server in a different browser.\nFollow a work around where you use code to directly download the file to the server. The code to do that will be available at the start of the lab activity where you need that particular file. The code to download the file you need to complete the quiz is below.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 12. Categorical predictors"
    ]
  },
  {
    "objectID": "PSYC412/part1/Week12.html#sec-wk12-labactivities",
    "href": "PSYC412/part1/Week12.html#sec-wk12-labactivities",
    "title": "Week 12. Categorical predictors",
    "section": "Lab activities",
    "text": "Lab activities\nIn this lab, you’ll gain understanding of and practice with:\n\nwhen and why to apply multiple regression to answer questions in psychological science\nconducting multiple regression in R when combining continuous and categorical predictors\ninterpreting the R output of multiple linear regression (when combining continuous and categorical predictors)\nreporting results for multiple linear regression (when combining continuous and categorical predictors), following APA guidelines\n\n\nLab activity 1: Combining a continuous and a categorical predictor in a regression model\n\nBackground: Smartphone screen-time and well-being\nThere is currently much debate (and hype) surrounding smartphones and their effects on well-being, especially with regard to children and teenagers. We’ll be looking at data from this recent study of English adolescents: Przybylski, A. & Weinstein, N. (2017). A Large-Scale Test of the Goldilocks Hypothesis. Psychological Science, 28, 204–215.\nThis was a large-scale study that found support for the “Goldilocks” hypothesis among adolescents: that there is a “just right” amount of screen-time, such that any amount more or less than this amount is associated with lower well-being. This was a huge survey study with data containing responses from over 120,000 participants! Fortunately, the authors made the data from this study openly available, which allows us to dig deeper into their results. And the question we want to expand on in this lab is whether the relationship between screen-time and well-being depends on the partcipant’s (self-reported) sex. In other words, our research question is: Does screen-time have a bigger impact on boys or girls, or is it the same for both?\nThe dependent measure used in the study was the Warwick-Edinburgh Mental Well-Being Scale (WEMWBS). This is a 14-item scale with 5 response categories, summed together to form a single score ranging from 14-70.\nOn Przybylski & Weinstein’s page for this study on the Open Science Framework, you can find the participant survey, which asks a large number of additional questions (see page 14 for the WEMWBS questions and pages 4-5 for the questions about screen-time). Within the same page you can also find the raw data, which some of you might want to consider using for your research report.\nHowever, for the purpose of this lab, you will be using local pre-processed copies of the data (participant_info.csv, screen_time.csv and `wellbeing.csv, which you downloaded as part of the ‘Pre-lab activities’.\nPrzybylski and Weinstein looked at multiple measures of screen-time, but again for the interests of this lab we will be focusing on smartphone use, but do feel free to expand your skills after by looking at different definitions of screen-time. Overall, Przybylski and Weinstein suggested that decrements in well-being started to appear when respondents reported more than one hour of daily smartphone use. So, bringing it back to our additional variable of sex, our research question is now: Does the negative association between hours of smartphone use and well-being (beyond the one-hour point) differ for boys and girls?\nLet’s think about this in terms of the variables. We have:\n\na continuous outcome variable: well-being;\na continuous∗ predictor variable: screen-time;\na categorical predictor variable: sex.\n\nPlease note that well-being and screen-time are technically only quasi-continuous inasmuch as that only discrete values are possible. However, there are a sufficient number of discrete categories in our data that we can treat the data as effectively continuous.\nNow, in terms of analysis, what we are effectively trying to do is to estimate two slopes relating screen-time to well-being, one for adolescent girls and one for adolescent boys, and then statistically compare these slopes. Sort of like running a correlation for boys, a correlation for girls, and comparing the two. Or alternatively, where you would run a regression (to estimate the slopes) but also one where you would need a t-test (to compare two groups). But the expressive power of regression allows us to do this all within a single model. Again, as we have seen building up to this lab, an independent groups t-test is just a special case of ordinary regression with a single categorical predictor; ANOVA is just a special case of regression where all predictors are categorical. But remember, although we can express any ANOVA design using regression, the converse is not true: we cannot express every regression design in ANOVA. As such people like regression, and the general linear model, as it allows us to have any combination of continuous and categorical predictors in the model. The only inconvenience with running ANOVA models as regression models is that you have to take care in how you numerically code the categorical predictors. We will use an approach called deviation coding which we will look at today later in this lab.\nTo complete this lab activity, you can use the R-script (402_wk12_labAct1_template.R) that you downloaded as part of the ‘Pre-lab activities’ as a template. Work through the activity below, adding relevant bits of code to your script as you go along.\n\n\nStep 1: Background and set up\n\n\n\n\n\n\nSet your working directory\n\n\n\nThe folder you were asked to download under ‘Pre-lab activity 3: Getting ready for the lab class’ contains the data files we’ll need. Make sure you have set your working directory to this folder by right-clicking on it and selecting ‘Set as working directory’.\n\n\n\n\n\n\n\n\nEmpty the R environment\n\n\n\nBefore you do anything else, when starting a new analysis, it is a good idea to empty the R environment. This prevents objects and variables from previous analyses interfering with the current one. To do this, you can click on the little broom icon in the top right of the Environment pane, or you can use rm(list=ls()).\n\n\nBefore we can get started we need to tell R which libraries to use. For this analysis we’ll need broom, car and tidyverse.\n\nTASK: Load the relevant libraries. If you are unsure how to do that, you can look at the ‘Hint’ below for a clue by expanding it. After that, if you are still unsure, you can view the code by expanding the ‘Code’ section below.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the library() function.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nThe code to do this is below.\nlibrary(broom)\nlibrary(car)\nlibrary(tidyverse)\n\n\n\n\n\n\n\n\n\nIf you couldn’t upload files to the server, do this:\n\n\n\nIf you experienced difficulties with uploading a folder or a file to the server, you can use the code below to directly download the file you need in this lab activity to the server (instead of first downloading it to you computer and then uploading it to the server). Remember that you can copy the code to your clipboard by clicking on the ‘clipboard’ in the top right corner.\n\n\n\ndownload.file(\"https://github.com/mg78/2324_PSYC402/blob/main/data/week12/participant_info.csv?raw=true\", destfile = \"participant_info.csv\")\n\n\ndownload.file(\"https://github.com/mg78/2324_PSYC402/blob/main/data/week12/screen_time.csv?raw=true\", destfile = \"screen_time.csv\")\n\n\ndownload.file(\"https://github.com/mg78/2324_PSYC402/blob/main/data/week12/wellbeing.csv?raw=true\", destfile = \"wellbeing.csv\")\n\n\nTASK: Finally, read in the three data files; call the participant info pinfo; call the screen_time data screen and the well-being data wellbeing.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the read_csv() function.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nThe code to do this is below.\npinfo &lt;- read_csv(\"participant_info.csv\")                                   \nscreen &lt;- read_csv(\"screen_time.csv\")\nwellbeing &lt;- read_csv(\"wellbeing.csv\")\n\n\n\n\n\nStep 2: Checking the formatting\nGiven our research question and the information you have about the scores, provided above under ‘Background’ and from the OSF-webpage, is the data ready for use?\n\nTASK: Add code to look at the first few lines of each data frame.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the head() function (or tail() function).\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nThe code to do this is below:\nhead(pinfo)                                  # Look at the data frames\nhead(screen)\nhead(wellbeing)\n\n\n\n\nQUESTION 2a: In which table is the variable corresponding to sex located and what is this variable called?\n\nThe ‘source and analysis code.sps’ file in the ‘Data and Code’ section on the OSF-webpage tells us how they coded the sex variable: 0 = female indicator and 1 = male indicator. It is worth exploring the OSF-webpage, to get used to foraging other files for these kinds of information, as they are not always clearly explained in a codebook or README. file.\n\nTASK: For ease, lets recode the sex variable to reflect word labels of ‘female’ and ‘male’. This doesn’t change the order: R will still see female as 0, and male as 1 because female occurs before male in the alphabet. Make sure to check your code does what you expect it to do.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the recode() function from the dplyr package to recode the labels. Use the head() and tail() functions to check the new data frame.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nThe code to do this is below:\n\npinfo &lt;- pinfo %&gt;%\n  mutate (sex = dplyr::recode(sex,\n                              `1` = \"male\",\n                              `0` = \"female\"))\n# Let's check it has done it:\nhead(pinfo)\ntail(pinfo)\n\n\n\n\nQUESTION 2b: In what format is the well-being data (long or wide)? On how many participants does it include observations? And on how many items for each participant?\n\n\nQUESTION 2c: What is the name of the variable that identifies individual participants in this dataset? It is important to work this out as this variable will allow us to link information across the three data files.\n\n\n\nStep 3: Data preparation - Aggregating the total well-being scores\nWe need to sum the 14 items of the well-being scale for each participant to create one well-being score per participant.\n\nTASK: To create one well-being score per participant, add code to the script to do the following: first, transform the well-being data frame from wide to long (using pivot_longer()); then, use group_by() to get scores for each participant and finally use summarise() to calculate a total well-being score, calling the new variable tot_wellbeing. Save all of this to an object called wb_tot.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nBelow is a code template. Make sure to add the relevant sections.\nwb_tot &lt;- DATA %&gt;%\n  pivot_longer(-Serial, names_to = \"\", values_to = \"\") %&gt;%\n  group_by(?) %&gt;%\n  summarise(tot_wellbeing = sum(?))\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nwb_tot &lt;- wellbeing %&gt;%\n  pivot_longer(-Serial, names_to = \"question\", values_to = \"score\") %&gt;%\n  group_by(Serial) %&gt;%\n  summarise(tot_wellbeing = sum(score))\n\n\n\nIt is useful to calculate some descriptive statistics for the new variable tot_wellbeing.\n\nTASK: Calculate some descriptive statistics for tot_wellbeing.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse summarise() to calculate the mean, standard deviation, minimum and maximum values.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nwb_tot %&gt;% summarise(mean = mean(tot_wellbeing),\n                     sd = sd(tot_wellbeing),\n                     min = min(tot_wellbeing), \n                     max = max(tot_wellbeing))\n\n\n\nFinally, let’s get an idea of the distribution of the new variable tot_wellbeing.\n\nTASK: Visualise the distribution in a histogram.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse ggplot() and geom_historgram().\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nggplot(wb_tot, aes(tot_wellbeing)) +\n  geom_histogram() \n\n\n\n\nQUESTION 3a: Is the distribution of well-being scores symmetrical, negatively skewed or positively skewed?\n\n\n\nStep 4: Data preparation - Transforming screen time data\nGreat, so we have the well-being scores sorted out, we now need to think about the screen-time usage data and whether it is being used on a weekday or a weekend. As always, to get an idea of the data, it is often very useful to visualise the variables before proceeding with the analysis.\nBefore we can do this, we’ll need to tidy these data up. Have another look at the screen data by using the head() function. You’ll see that we have Serial in the first column (this is good), but in the following eight columns, we have columns for each type of activity (Comph, Comp, Smart, Watch) and the part of the week it took place (we and wk) combined. Instead, to be able to work with the data, we need two columns: one for the type of activity (we’ll call it variable) and one for the part of the week (we’ll call it day).\nA second issue is that we need to alter the abbreviations Comph, Comp, Smart and Watch to reflect more descriptive text for each in plots.\nBelow are two chunks of code that represent these steps. In the next tasks you’ll practise with taking a set of piped commands apart. The purpose of this is to get you used to “parsing” the code at the right places so that when you see piped commands in other people’s code, you know how to break it down and find the relevant parts that you can use.\n\nTASK: In the code chunk below we use the separate() function to split the character strings already in the dataset. You know that with piped commands, there are chunks of code. Run the code first in its entirety and then pull each line apart to see how each function works on the data. Write a descriptive sentence for each function’s role in the command. Don’t forget to copy the chunk to your script and run it.\n\n\nscreen_long &lt;- screen %&gt;%\n  pivot_longer(-Serial, names_to = \"var\", values_to = \"hours\") %&gt;%\n  separate(var, c(\"variable\", \"day\"), \"_\")\n\n\nTASK: In the next code chunk we use the dplyr::recode() function with mutate() to relabel the separated names into understandable names that will be clear in plots. Again, run the code first in its entirety and then pull each line apart to see how each function works on the data. Write a descriptive sentence for each function’s role in the command. Don’t forget to copy the chunk to your script and run it.\n\n\nscreen2 &lt;- screen_long %&gt;%\n  mutate(variable = dplyr::recode(variable,\n                                  \"Watch\" = \"Watching TV\",\n                                  \"Comp\" = \"Playing Video Games\",\n                                  \"Comph\" = \"Using Computers\",\n                                  \"Smart\" = \"Using Smartphone\"),\n         day = dplyr::recode(day,\n                             \"wk\" = \"Weekday\",\n                             \"we\" = \"Weekend\"))\n\n\n\n\n\n\n\nTip\n\n\n\nThe code above has a new feature: the dplyr::recode part. This syntax – using the double colon – happens when there are many versions of a function with the same name. You can imagine that a function called ‘recode’ is immensely useful at the data wrangling stage of analysis. By using the name of the package, a double set of colons, followed by a function name, you are ensuring that R uses a particular version of the function, at that point only. This avoids having two or more packages loaded in your environment that sometimes do not play nicely together!\n\n\nTo be able to monitor that your code is performing as you want it to, you need to have in your mind an idea of how the data should look at the end of a code chunk. So stop a moment and be clear, discuss with your lab-mates if you feel like it and answer the following question.\n\nQUESTION 4a: What are the variables and the levels or conditions within each variable of screen2?\n\n\nTASK:Now join wb_tot and screen_2 by participant and then group by the variables ‘variable’, ‘day’ and ‘hours’ and then calculate a ‘mean_wellbeing’ variable for each of the grouped levels. Save it in an object called ‘dat_means’.\n\n\n\n\n\n\n\nHint 1\n\n\n\n\n\nWrite separate lines of code for each action and then, when you know each of them works, reformat them as a piped command. You’ll need inner_join(), group_by() and summarise().\n\n\n\n\n\n\n\n\n\nHint 2\n\n\n\n\n\nBelow templates for separate lines of code for each action. You’ll need to replace the names of relevant data frames and variables.\njoined &lt;- inner_join(data1, data2, by=)\ngrouped &lt;- group_by(data, var1, var2, var3)\nmeans &lt;- summarise(data, mean = mean(variable))\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nBelow the code for a piped command.\ndat_means &lt;- inner_join(wb_tot, screen2, \"Serial\") %&gt;%\n  group_by(variable, day, hours) %&gt;%\n  summarise(mean_wellbeing = mean(tot_wellbeing))\n\n\n\n\nTASK: Now check that you have an object that is 72 observations of 4 variables. You should have a mean wellbeing score for every level of the hours, over weekdays and weekends for each level of the four types of screen time (4 x 2 x 9)\n\nNext, it is a good idea to visualise the mean well-being data as function of hours of screen-time for the different days (weekday vs. weekend) and types of screen (playing video games, using computers, using smartphone and watching tv). This is quite a complex graph. We’ll go through creating it step-by-step, but let’s first look at the end result:\n\nOk, that’s what we are working towards.\n\nTASK: Below, a chunk of code is presented. It is your task to fill in the x and y variables.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nGo back to the research question - which variable is for the x axis and for the y axis?\n\n\n\n\nggplot(dat_means, aes(x = , y = )) +\n  geom_line() +\n  geom_point() +\n  theme_bw()\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nggplot(dat_means, aes(x = hours, y = mean_wellbeing,)) +\n  geom_line() +\n  geom_point() +\n  theme_bw()\n\n\n\nQUESTION 4b: What research question does this plot describe? Is it appropriate for the levels within the data?\n\nTASK: Now, let’s add a different linetype for each day (weekday vs. weekend). Fill in the blanks in the code below.\n\n\nggplot(dat_means, aes(x = , y = , linetype = )) +\n  geom_line() +\n  geom_point() +\n  theme_bw()\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nggplot(dat_means, aes(x = hours, y = mean_wellbeing, linetype = day)) +\n  geom_line() +\n  geom_point() +\n  theme_bw()\n\n\n\nQUESTION 4c: What research question does this plot describe? Is it appropriate for the levels within the data?\nStill not quite there.\n\nTASK: Fill in the blanks (for x, y and linetype) as before. Now have a good look at the code below. What has changed? Copy the code to your script and run it. Then, for each line write a sentence as a comment to describe its effect on the plot.\n\n\nggplot(dat_means, aes(x = , y = , linetype = )) +\n  geom_line() +\n  geom_point() +\n  facet_wrap(~variable, nrow = 2) +\n  theme_bw()\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nggplot(dat_means, aes(hours, mean_wellbeing, linetype = day)) + # plot 'hours' on the x-axis, 'mean_wellbeing' on the y-axis and use a different type of line for the levels of 'day'\n  geom_line() + # add a line\n  geom_point() + # add point data\n  facet_wrap(~variable, nrow = 2) + # plot separate plots for each level of 'variable'\n  theme_bw() # use the 'black and white' theme\n\n\n\nWe add the facet_wrap() function here. You can check the ?facet_wrap() help page for more information.\nQUESTION 4d: c. What does the facet_wrap() function do? Is this plot appropriate for the levels in the data?\n\n\nStep 5: Calculating mean hours per day for smartphone use, for each participant\nAs mentioned at the beginning, in today’s lab we’ll focus on smartphone use. So looking at the bottom left of the figure we could suggest that smartphone use of more than 1 hour per day is associated with increasingly negative well-being the longer screen time people have. This looks to be a similar effect for Weekdays and Weekends, though perhaps overall well-being in Weekdays is marginally lower than in Weekends (the line for Weekday is lower on the y-axis than Weekends). This makes some sense as people tend to be happier on Weekends!\n\nTASK: Below is a set of comments that describe what the chunk of code that you need to write next does:\n\n\n\n#use ‘screen2’\n#and then filter out the observations for ‘Using Smartphone’,\n#and then group together each participant,\n#and then summarise the mean hours calling it ‘hours_per_day’,\n#save it in an object called ‘smarttot’\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nBelow is a template for the code you need. Try to fill in the relevant bits:\nNEW_OBJECT &lt;- DATA %&gt;%\n  filter(variable == \"?\") %&gt;%\n  group_by(VARIABLE) %&gt;%\n  summarise(new_measure = mean(VARIABLE))\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nThe code to do this is below:\nsmarttot &lt;- screen2 %&gt;% # use 'screen2'\n  filter(variable == \"Using Smartphone\") %&gt;% # filter out the observations for 'Using Smartphone'\n  group_by(Serial) %&gt;% # group together each participant\n  summarise(hours_per_day = mean(hours)) #summarise the mean hours calling it 'hours_per_day'\n\n\n\n\nTASK: Now let’s do it the other way around. Run the code below. Have a look at the structure of ‘smart_wb’.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou can use the str() function.\n\n\n\n\nsmart_wb &lt;- smarttot %&gt;%\n  filter(hours_per_day &gt; 1) %&gt;%\n  inner_join(wb_tot, \"Serial\") %&gt;%\n  inner_join(pinfo, \"Serial\") %&gt;%\n  mutate(sex = as.factor(sex))\n\n\nQUESTION 5a: What does the code do? Write a short paragraph, using the phrase “and then” to represent the pipes.\n\n\n\nStep 6: More visualisation\nWe are now using only one small part of the data - smartphone use and its relationship with well-being over different durations of time. Before formally testing our research question, we can visualise the data and enquire about sex differences on the same plot - run each chunk of code below:\n\nTASK To further group the data, copy the code below to your script and run it. Look at the ‘smart_wb_gen’ dataframe. What has the code above done? Write a couple of sentences of description.\n\n\nsmart_wb_gen &lt;- smart_wb %&gt;%\n  group_by(hours_per_day, sex) %&gt;%\n  summarise(mean_wellbeing = mean(tot_wellbeing))\n\n\nTASK: Let’s visualise these data.\n\n\nggplot(smart_wb_gen, aes(hours_per_day, mean_wellbeing, color = sex)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  scale_color_discrete(name = \"Sex\", labels = c(\"Girls\", \"Boys\"))+\n  scale_x_continuous(name = \"Total hours smartphone use\") +\n  scale_y_continuous(name = \"Mean well-being score\") +\n  theme_bw()\n\n\nQUESTION 6a: Write an interpretation of the above plot in plain English.\n\n\n\nStep 7: The regression model\nIn the steps 2 to 6 we’ve covered some pretty heavy-lifting data-wrangling. As it is so often the case that something like this is needed when working with real data, it is really important to practise this. However, to ensure you also spend time on fitting the regression model and interpreting the output, you can choose to use the data-file smart_wb.csv to get started with that. It contains the data in a format that is the result of all the data-wrangling we did in steps 2 to 6. So, download the smart_sb.csv data-file, put it in the folder that is your working directory and you’re all set for running the regression model.\n\n\n\n\n\n\nIf you couldn’t upload files to the server, do this:\n\n\n\nIf you experienced difficulties with uploading a folder or a file to the server, you can use the code below to directly download the file you need in this lab activity to the server (instead of first downloading it to you computer and then uploading it to the server). Remember that you can copy the code to your clipboard by clicking on the ‘clipboard’ in the top right corner.\n\n\n\ndownload.file(\"https://github.com/mg78/2324_PSYC402/blob/main/data/week12/smart_wb.csv?raw=true\", destfile = \"smart_wb.csv\")\n\n\nTASK: Let’s run the regression. Write code in your script in which you call your output ‘mod’, and use the data ‘smart_wb’ using the following formula, lm(y ~ x1 + x2 + x1:x2, data) to construct your regression model. Go back to the research question for your outcome and two predictor variables.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the following template:\nmod &lt;- lm(y ~ x1 + x2 + x1:x2, data)\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nmod &lt;- lm(tot_wellbeing ~ hours_per_day + sex + hours_per_day:sex, smart_wb)\n\n\n\n\nTASK: Call and save the summary of your model as ‘mod_summary’; then have a look at it.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nmod_summary &lt;- summary(mod)\nmod_summary\n\n\n\nLet’s first look at the model as a whole:\n\nQUESTION 7a: What is the p-value for the overall model? Is it significant? What does this mean?\n\n\nQUESTION 7b: To two decimal places, what percentage of the variance in well-being scores does the overall model explain?\n\nNow, lets look at the coefficients of our predictors:\n\nQUESTION 7c: Are the main effects of smartphone use and sex significant?\n\n\nQUESTION 7d: Which variable indicates the interaction between smartphone use and sex?\n\n\nQUESTION 7e: And is the interaction significant?\n\n\nQUESTION 7f: What is the most reasonable interpretation of these results?\n\nThe above model uses treatment coding (sometimes called dummy coding), for the sex variable. In a categorical variable with only two levels this means that one level is coded as 0 and the other level is coded as 1. In categorical variables with more than two levels, it works slightly differently.\n\nTASK: We can check that the sex variable is treatment with the following code:\n\n\ncontrasts(smart_wb$sex)\n\nBecause we have not explicitly told R about the labels for the sex variable, it has used level 0 as the reference level, hidden within the intercept term and level sex1 describes the difference (or slope) between level 0 and 1 or in this dataset from female to male.\n\nQUESTION 7g: Is being Male better for a person’s well-being in the context of smartphone use than being Female?\n\nNow let’s look at deviation coding. There are other ways to code your categorical variables. One of them is deviation coding (also sometimes called sum coding). This effectively divides the difference of the values between your categorical levels by the number of levels so that each level can be compared to one intercept that is central to them all rather than comparing levels to one reference level. It is like centering for a categorical level.\n\nTASK Use the code chunk below to: 1) Add a variable to the smart_wb data that is a deviation coding of sex; 2) Set the deviation coding (we’ll label it ‘Sum’ here for easy variable naming); and 3) Look at the output for the sum-coded sex variable.\n\n\nsmart_wb &lt;- mutate(smart_wb, sexSum = sex) # add a variable to the smart_wb data that is a deviation coding of sex\ncontrasts(smart_wb$sexSum) &lt;- contr.sum(2) # wet the deviation coding\ncontrasts(smart_wb$sexSum) # look at the output for the sum coded sex variable\n\nNext, we’ll run the regression again, using the sum-coded sex variable and we’ll compare the outputs.\n\n# Run the regression model again, using the sumcoded sex model and compare outputs\nmod_S &lt;- lm(tot_wellbeing ~ hours_per_day + sexSum + hours_per_day:sexSum, smart_wb)\nmod_S_summary &lt;- summary(mod_S)\n\n# Compare the two model summary outputs\nmod_summary\nmod_S_summary\n\n‘sexSum1’ is now the coefficient for sex and represents the change from the intercept value which now lies between the values for being Female and Male. Note how this coefficient is negative.\nThe earlier model had a positive coefficient because the intercept described the reference group of the Girls, who on average begin at a lower well-being level than Boys (refer back to the scatterplot to verify this). Because the sum-coding has moved the intercept to a point that is the center of the difference between Boys and Girls, sexSum1 now describes the distance between the centre and a level of Sex.\nValues for well-being in Girls are thus: \\[ Intercept + sexSum*+1 = 49.74 + (-1.61)*(+1) \\]\nValues for well-being in Boys are thus: \\[Intercept + sexSum*-1 = 49.74 + (-1.61)*(-1)\\]\nwith the Boys being higher in well-being…(remember a negative number multiplied by a negative number produces a positive number and a negative number multiplied by a positive number produces a negative number).\nThe interpretation of both model effects is the same, and if you look at the summary statistics, they are identical. Deviation coding effectively centers your categorical variables and helps with interpretation of interaction terms.\n\n\nStep 8: Checking assumptions\nNow that we’ve fit a model, let’s check whether it meets the assumptions of linearity, normality and homoscedasticity. With regression models, you do this after you’ve actually fit the model.\nLinearity Unlike when we did simple regression we can’t use crPlots() to test for linearity when there is an interaction, but we know from looking at the grouped scatterplot that this assumption has been met.\nNormality Normally we would test for normality with a QQ-plot and a Shapiro-Wilk test. However, because this dataset is so large, the Shapiro-Wilk is not appropriate (if you try to run the test it will produce a warning telling you that the sample size must be between 3 and 5000). This is because with extremely large sample sizes the Shapiro-Wilk test will find that any deviation from normality is significant. Therefore we should judge normality based upon the QQ-plot.\n\nTASK: Create a QQ-plot to check the residuals are normally distributed.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou can use the qqPlot() function. The residuals are stored in the ‘mod’ object you created earlier.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nqqPlot(mod$residuals)\n\n\n\n\nQUESTION 8a: What do you conclude from the QQ-plot?\n\nHomoscedasticity Here we have the same problem as with testing for normality: with such a large sample the ncvTest() will produce a significant result for any deviation from homoscedasticity. So we need to rely on plots again. To check for homoscedasticity we can use plot() from Base R that will produce a bunch of helpful plots (more information here:.\n\nTASK: Copy the code chunk below to your script and run it.\n\n\npar(mfrow=c(2,2))             # 4 charts in 1 panel\nplot(mod)                     # this may take a few seconds to run\n\nThe residuals vs leverage plot shows a flat red line so, whilst it isn’t perfect, we can assume that with such a large sample size regression is still an appropriate analysis.\nMulti-collinearity Finally, lets check for multicollinearity using the vif() function. Essentially, this function estimates how much the variance of a coefficient is “inflated” because of linear dependence with other predictors, i.e., that a predictor isn’t actually adding any unique variance to the model, it’s just really strongly related to other predictors. Thankfully, vif is not affected by large samples like the other tests. There are various rules of thumb, but most converge on a VIF of above 2 to 2.5 for any one predictor being problematic.\n\nTASK: Copy the code chunk below to your script and run it.\n\n\nvif(mod)                      # Check for multi-collinearity\n\n\nQUESTION 8b: Do any of the predictors show evidence of multicollinearity?\n\n\n\nStep 9: Write up\n\nQUESTION 9a: How would you write up the results following APA guidance? You can choose whether you do so for the model using treatment coding or for the model using deviation coding.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 12. Categorical predictors"
    ]
  },
  {
    "objectID": "PSYC412/part1/Week12.html#sec-wk12-answers",
    "href": "PSYC412/part1/Week12.html#sec-wk12-answers",
    "title": "Week 12. Categorical predictors",
    "section": "Answers",
    "text": "Answers\nWhen you have completed all of the lab content, you may want to check your answers with our completed version of the script for this week. Remember, looking at this script (studying/revising it) does not replace the process of working through the lab activities, trying them out for yourself, getting stuck, asking questions, finding solutions, adding your own comments, etc. Actively engaging with the material is the way to learn these analysis skills, not by looking at someone else’s completed code…\n\nYou can download the R-script that includes the relevant code here: 412_wk12_labAct1_withAnswers.R.\n\nLab activity 1: Combining a continuous and a categorical predictor in a regression model\n2a. In which table is the variable corresponding to sex located and what is this variable called? In pinfo; the variable is called sex.\n2b. In what format is the well-being data (long or wide)? On how many participants does it include observations? And on how many items for each participant? The well-being data are in ‘wide’ format. It contains observations on 102580 participants, on 14 items.\n2c. What is the name of the variable that identifies individual participants in this dataset? It is important to work this out as this variable will allow us to link information across the three data files. Serial\n3a. Is the distribution of well-being scores symmetrical, negatively skewed or positively skewed? Negatively skewed\n4a. What are the variables and the levels or conditions within each variable of screen2?\nParticipants plus: Levels: variables = 4: watching tv, playing video games, using computers, using smartphone day = 2 = weekdays, weekends hours = 9 = 0, 0.5, 1 - 7\n4b. What research question does this plot describe? Is it appropriate for the levels within the data? The plot describes how hours of use impact upon well-being? No, it is too broad a research question.\n4c. What research question would fit this visualisation?Is it appropriate for the levels in the data? The hours are now displayed by weekdays and weekends. How do hours of screen time impact on well-being on weekdays and at the weekend? No, it is still too broad.\n4d. What does the facet_wrap() function do? Is this plot appropriate for the levels in the data? The facet_wrap() function has split the types of screen time and shown how hours of use across weekdays and weekends impact upon well-being. This captures the levels of information within the dataset.\n5a. See the script\n6a. Write an interpretation of the above plot in plain English. Something along the lines of: Adolescent girls show lower overall well-being compared to adolescent boys. In addition, the slope for girls appears more negative than that for boys; the one for boys appears relatively flat. This suggests that the negative association between well-being and smartphone use is stronger for girls.\n7a. What is the p-value for the overall model? Is it significant? What does this mean? The p-value for the overall model fit is &lt; 2.2e-16. This significant. It means that together the predictors describe the variance in well-being better than a model without the predictors (the null model). So knowing something about smartphone use and sex of participants will allow us to predict their well-being to a degree.\n7b. To two decimal places, what percentage of the variance in well-being scores does the overall model explain? 9.38%\n7c. Are the main effects of smartphone use and sex significant? Yes.\n7d. Which variable indicates the interaction between smartphone use and sex? The interaction is indicated by the variable hours_per_day:sex.\n7e. And is the interaction significant? Yes.\n7f. What is the most reasonable interpretation of these results? Smartphone use was more negatively associated with well-being for girls than for boys.\n7g. Is being Male better for a person’s well-being in the context of smartphone use than being Female? Yes.\n8a. What do you conclude from the QQ-plot? The residuals are normally distributed.\n8b. Do any of the predictors show evidence of multicollinearity? Yes, Boys and the interaction do. We’ll talk about that more, later in the module.\n\nWrite up\n\nTreatment / dummy coded model Treatment coding was used for categorical predictors with the Girls level acting as the reference group. The results of the regression indicated that the model significantly predicted well-being (F(3, 71029) = 2450.89, p &lt; .001, Adjusted R2 = 0.09), accounting for 9% of the variance.Total hours of smart phone use was a significant negative predictor of well-being scores (β = -0.77, p &lt; .001, as was sex (β = 3.22, p &lt; .001), with girls having lower well-being scores than boys. Importantly, there was a significant interaction between screen time and sex (β = 0.45, p &lt; .001): smartphone use was more negatively associated with well-being for girls than for boys.\nDeviation / sum coded model Deviation coding was used for categorical predictors. The results of the regression indicated that the model significantly predicted well-being (F(3, 71029) = 2450.89, p &lt; .001, Adjusted R2 = 0.09), accounting for 9% of the variance. Total hours of smart phone use was a significant negative predictor of well-being scores (β = -0.55, p &lt; .001, as was sex (β = -1.61, p &lt; .001), with girls having lower well-being scores than boys. Importantly, there was a significant interaction between screen time and sex (β = -0.22, p &lt; .001), indicating that smartphone use was more negatively associated with well-being for girls than for boys.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 12. Categorical predictors"
    ]
  },
  {
    "objectID": "PSYC412/part2/report-preface.html",
    "href": "PSYC412/part2/report-preface.html",
    "title": "Week 00. The structured research report – Requirements",
    "section": "",
    "text": "The structured research report assignment: We want you to analyse previously collected data and write a report about your findings.\nHere, we present a guide in five parts:\n\nWhat you have to write.\nHow you can do the analysis work.\nWhy we are asking you to do this.\nWhere you can get in-depth information.\nHow you can expect us to grade your work.\n\n\n\n\n\n\n\nWarning\n\n\n\nThis structured research report assignment has the same requirements or targets — and coursework should be submitted in the same format — as the structured research report assignment for PSYC411, therefore, here, what we are doing is:\n\nproviding, for your convenience, a repeat presentation of the information you can find in the previous guide\nand making edits to PSYC412-specific information like the deadline.\n\nObviously, this means that sometimes (e.g., in the lecture) I am going to reference PSYC411 when the information remains relevant to PSYC412.\n\n\nWe provide a summary guide to What you have to write next, and then a quick guide to How you can do the analysis work you need to do, so that you can do the writing.\nWe share a lecture on Why we are asking you to do this work: what you will learn and how you will benefit.\nAt the end of this page, in How we will mark your work, we detail what work of different grades looks like, so that you can both understand what we want you to do, and how you can do it.\nIf you want more in-depth guidance or support, we provide detailed step-by-step notes on the work you can plan to do in the PSYC411 How chapter.\nIf you want an explanation of why we are asking you to do this work, so you can think about best practice in science, we have got that for you in the PSYC411 Why chapter.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 00. The structured research report -- Requirements"
    ]
  },
  {
    "objectID": "PSYC412/part2/report-preface.html#sec-report-intro",
    "href": "PSYC412/part2/report-preface.html#sec-report-intro",
    "title": "Week 00. The structured research report – Requirements",
    "section": "",
    "text": "The structured research report assignment: We want you to analyse previously collected data and write a report about your findings.\nHere, we present a guide in five parts:\n\nWhat you have to write.\nHow you can do the analysis work.\nWhy we are asking you to do this.\nWhere you can get in-depth information.\nHow you can expect us to grade your work.\n\n\n\n\n\n\n\nWarning\n\n\n\nThis structured research report assignment has the same requirements or targets — and coursework should be submitted in the same format — as the structured research report assignment for PSYC411, therefore, here, what we are doing is:\n\nproviding, for your convenience, a repeat presentation of the information you can find in the previous guide\nand making edits to PSYC412-specific information like the deadline.\n\nObviously, this means that sometimes (e.g., in the lecture) I am going to reference PSYC411 when the information remains relevant to PSYC412.\n\n\nWe provide a summary guide to What you have to write next, and then a quick guide to How you can do the analysis work you need to do, so that you can do the writing.\nWe share a lecture on Why we are asking you to do this work: what you will learn and how you will benefit.\nAt the end of this page, in How we will mark your work, we detail what work of different grades looks like, so that you can both understand what we want you to do, and how you can do it.\nIf you want more in-depth guidance or support, we provide detailed step-by-step notes on the work you can plan to do in the PSYC411 How chapter.\nIf you want an explanation of why we are asking you to do this work, so you can think about best practice in science, we have got that for you in the PSYC411 Why chapter.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 00. The structured research report -- Requirements"
    ]
  },
  {
    "objectID": "PSYC412/part2/report-preface.html#sec-report-quick-what",
    "href": "PSYC412/part2/report-preface.html#sec-report-quick-what",
    "title": "Week 00. The structured research report – Requirements",
    "section": "What you have to write",
    "text": "What you have to write\nLet’s begin with what you need to write to complete this coursework.\n\nSubmission deadline: On Wednesday 14th May 2025\nYou will submit a structured report.\nPresenting an analysis: what you did; what you found; the context; and the implications.\n\n\n\n\n\n\n\nTip\n\n\n\nYou will be able to find the submission point on Moodle here when it is revealed.\n\n\nWhat is a structured report?\nYou will submit short answers to a series of questions:\n\nDescribe the questions or the predictions you examine in your analysis.\nExplain the research background: you are analyzing previously collected data, so briefly explain why the people who originally collected the data did that work.\nExplain if you are attempting to repeat the analysis that the original researchers did or if you are doing something different.\nExplain the motivation for the questions or the predictions your analysis examines.\nSummarize the methods that were used to collect the data you analyzed.\nIdentify what variables are included in the analysis: what variable is the outcome (or dependent) variable, and what variables are the predictor (or independent) variables.\nDescribe the analysis method you use to address the question or test the predictions.\nExplain your analysis method choices: why are you using this method?\nIdentify the model you use in your analysis.\nPresent a summary of the analysis results: use text and plots to show what you found.\nExplain the implications of the results, in theoretical or in policy terms.\nCritically evaluate your findings: reflect on the strength or the limitations of the evidence you present, as answers to the questions or as tests of the predictions you outlined.\nPredict: how can future research build on the work you have done?\n\n\n\n\n\n\n\nTip\n\n\n\n\nYou will write each answer in sentences, organized in one or more paragraphs.\nYou will submit your analysis code in an appendix.\nWhere appropriate, you will include plots.\nSubmit your report as a series of answers to the questions listed in a single document.\nYou can use each question as a heading in the document.\nWord count limit: no more than 1500 words are allowed for all materials except references, appendices, and the content of tables.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 00. The structured research report -- Requirements"
    ]
  },
  {
    "objectID": "PSYC412/part2/report-preface.html#sec-report-quick-how",
    "href": "PSYC412/part2/report-preface.html#sec-report-quick-how",
    "title": "Week 00. The structured research report – Requirements",
    "section": "How you can do the analysis work",
    "text": "How you can do the analysis work\nReports will concern, usually, findings from analyses of data collected in previous studies or data accessed from online sources. These data will usually be associated with a published report in a journal like Psychological Science or a pre-print archive like PsyArXiv.\nWe expect students to use one of the analysis methods taught in the module.\n\n\n\n\n\n\nTip\n\n\n\nIf you want to know more about how to do this work, you can read in-depth step-by-step guidance in the PSYC411 How chapter.\n\n\nYou can see information on APA formatting of statistics and numbers in the OWL Purdue guide. Though the APA guidelines are the authoritative guide.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 00. The structured research report -- Requirements"
    ]
  },
  {
    "objectID": "PSYC412/part2/report-preface.html#sec-report-quick-why",
    "href": "PSYC412/part2/report-preface.html#sec-report-quick-why",
    "title": "Week 00. The structured research report – Requirements",
    "section": "Why we are asking you to do this",
    "text": "Why we are asking you to do this\nWe are asking you to do the structured research report because doing the work — the analysis and the writing — will build your awareness and understanding of how psychological science really works, and will strengthen your sense of what best practice looks like in modern science.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 00. The structured research report -- Requirements"
    ]
  },
  {
    "objectID": "PSYC412/part2/report-preface.html#sec-report-intro-lecture",
    "href": "PSYC412/part2/report-preface.html#sec-report-intro-lecture",
    "title": "Week 00. The structured research report – Requirements",
    "section": "Lectures and slides",
    "text": "Lectures and slides\n\nLecture recordings – videos\nClick on a link and your browser should open a tab showing the Panopto video for the lecture part. You should be able to access the videos anywhere; you should not need to be on campus or logged on to the university VPN to view the videos.\n\nOverview (20 minutes): the key ideas, the scientific context\n\n\n\nOverview (20 minutes): the research workflow, multiverse analyses\n\n\n\nOverview (20 minutes): kinds of reproducibility, open data, and doing better science.\n\n\n\n\nLecture recordings – slides\nYou can download the lecture slides as a single downloadable .html file that you can open in any browser: 411-research-report.html. This can be opened in a browser and presents the slides as they are delivered.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 00. The structured research report -- Requirements"
    ]
  },
  {
    "objectID": "PSYC412/part2/report-preface.html#sec-report-quick-how-mark",
    "href": "PSYC412/part2/report-preface.html#sec-report-quick-how-mark",
    "title": "Week 00. The structured research report – Requirements",
    "section": "How we will mark your work",
    "text": "How we will mark your work\nA distinction requires the following.\nA. Background and methods\n\nThere should be a coherent, logical, argument for how the background or context leads to the questions or predictions.\nThere should be a concise explanation of the background or context for the research: Why did the original study authors do the research? Why are you doing the analysis you report?\nThere should be a specific concrete statement of the research question that your analysis addresses, or the hypothesis that your analysis tests.\nIn the method information, there should be a concise summary of the properties of the variables — how data were collected, how variable values were coded or scored — so that the reader can understand the structure of the data.\n\nB. Analysis and results\n\nThere should be a clear account of which variables are included in the analyses, so that the reader can understand what outcome (dependent) variable is being analyzed using what predictor (independent) variables.\nThere should be a scholarly explanation of the analysis you did, including an account of why you chose to use the method you use, given possible alternatives and, if relevant, referring to justifications for your choice that are identified in the statistical literature.\nThere should be a clear identification of what analysis you did, using what method, outlining the elements and structure of the model whose results you present. -There should be a clear presentation of the results, presenting a correct interpretation of statistical information concerning the size, the direction (or sign), and the significance of any effect you estimate or difference you test. The presentation should explain how the outcome is expected to vary, on average, given the differences or the effects estimated using your analyses.\nThere should be full and correct use of APA conventions in the presentation of statistical results.\nWhere appropriate, results are presented using both text and relevant, informative, visualizations.\n\nC. Implications and critical reflection\n\nThere should be an accurate account of whether or how the analysis results address the questions, or how the analysis results support or disconfirm the predictions stated.\nThere should be a scholarly explanation of the implications of the results: what do the results suggest we now know that we did (or did not) know before?\nThere should be a critical analysis of the strengths or limitations of the evidence: how confident can we be, how uncertain should we be, that the results that are presented do or do not suggest what you think they suggest.\nThere should be a constructive analysis of how future research could build on your findings to further extend understanding or evaluate methodological concerns.\n\nA merit will be awarded, by comparison, if:\nA. Background and methods\n\nThere is a concise explanation of the background or context for the research. There is limited or partial information on why the original study authors did the research, or why you are you doing the analysis you report. We do not see an argument explaining the reasons motivating your research.\nThere is a clear statement of the research question, or the hypothesis or prediction that your analysis tests. But the question or the prediction may be vague, or stated in general not specific terms.\nThe summary of the properties of the variables may not be sufficiently clear or informative about the structure of the data that the reader can understand what data are involved in analyses.\n\nB. Analysis and results\n\nThere is a clear account of which variables are included in the analyses, so that the reader can understand what outcome (dependent) variable is being analyzed using what predictor (independent) variables.\nThere is an explanation of the analysis you did, but there is limited explanation of why you chose to use the method you use. There is no or there is limited awareness of possible alternatives. No informed justification is presented for analysis choices.\nThere is a clear identification of what analysis you did, using what method, outlining the elements and structure of the model whose results you present.\nThere is evidence that the analysis method you chose was used correctly, that the method was appropriate to the question and the data, and that the code you used could do the analysis you say it did.\nThere is a clear presentation of the results, presenting a correct interpretation of statistical information concerning the size, the direction (or sign), and the significance of any effect you estimate or difference you test. The presentation should explain how the outcome is expected to vary, on average, given the differences or the effects estimated using your analyses.\nThere is full and correct use of APA conventions in the presentation of statistical results.\nWhere appropriate, results are presented using both text and relevant, informative, visualizations.\n\nC. Implications and critical reflection\n\nThere is an accurate account of whether or how the analysis results address the questions, or how the analysis results support or disconfirm the predictions stated.\nThere is some explanation of the implications of the results but this account is not linked to previous research (in the literature).\nThere is limited reflection on the strengths or limitations of the evidence. We may see generic but not well informed discussion of the evidence specific to your analysis or your data.\nThere is limited discussion of future research.\n\nA pass will be awarded, by comparison, if:\nA. Background and methods\n\nThere is a concise explanation of the background or context for the research. There is limited or no information on why the original study authors did the research or why you are you doing the analysis you report.\nThere is a statement of the research question, or the hypothesis or prediction that your analysis tests. But the question or the prediction may be vague, or stated in general terms only.\nThe summary of the properties of the variables provides information on the structure of the data but it may be unclear how one or more variables were coded or scored. There is limited engagement with questions of measurement reliability or validity.\n\nB. Analysis and results\n\nThere is a clear account of which variables are included in the analyses, so that the reader can understand what outcome (dependent) variable is being analyzed using what predictor (independent) variables.\nThere is an explanation of the analysis you did, but there is no explanation of why you chose to use the method you use. Limited or no informed justification is presented for analysis choices.\nThere is a clear identification of what analysis you did, using what method, outlining the elements and structure of the model whose results you present.\nThere is evidence that the analysis method you chose was used correctly, that the method was appropriate to the question and the data, and that the code you used could do the analysis you say it did.\nThere is a clear presentation of the results, presenting a correct interpretation of statistical information concerning the direction (or sign) and the significance of any effect you estimate or difference you test. The presentation should explain how the outcome is expected to vary, on average, given the differences or the effects estimated using your analyses.\nThere may be partial use of APA conventions in the presentation of statistical results.\nAppropriate visualizations may be included but without comment or discussion.\n\nC. Implications and critical reflection\n\nThere is a summary account of the results but it may not clearly explain whether or how the analysis results address the questions, or how the analysis results support or do not support the predictions stated.\nThere is limited or no explanation of the implications of some of the results.\nThere is limited or no reflection on the strengths or limitations of the evidence.\nThere is no discussion of future research.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 00. The structured research report -- Requirements"
    ]
  },
  {
    "objectID": "PSYC412/part2/05-ordinal.html",
    "href": "PSYC412/part2/05-ordinal.html",
    "title": "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models",
    "section": "",
    "text": "Ordinal data are very common in psychological science.\nOften, we will encounter ordinal data recorded as responses to Likert-style items in which the participant is asked to indicate a response on an ordered scale ranging between two end points (Bürkner & Vuorre, 2019; Liddell & Kruschke, 2018). An example of a Likert item is: How well do you think you have understood this text? (Please check one response) where the participant must respond by producing a rating, by checking one option, given nine different response options ranging from 1 (not well at all) to 5 (very well).\nThe critical characteristics of ordinal data values (like the responses recorded to ratings scale, Likert-style, items) are that:\n\nThe responses are discrete or categorical — you must pick one (e.g., 1), you cannot pick more than one at the same time (e.g., 1 and 2), and you cannot have part or fractional values (e.g., you can’t choose the rating 1.5).\nThe responses are ordered — ranging from some minimum value up to some maximum value (e.g., 1 \\(\\rightarrow\\) 2 \\(\\rightarrow\\) 3 \\(\\rightarrow\\) 4 \\(\\rightarrow\\) 5).\n\nIn these materials, we are going to keep things simple by focusing on data comprising responses recorded to ratings scale Likert items. As we discuss, following, we often suppose that the response a participant produces to a rating question corresponds to a value on an underlying dimension that indirectly informs the participant response production process. But we should be aware that ordinal data can come from a variety of possible psychological mechanisms Ordinal data can also reflect processes in which participants have worked their way through a progression or sequence of decisions (see, e.g., Ricketts et al., 2021).\n\n\n\n\n\n\nNote\n\n\n\nOrdinal models: why do we need to do this?\n\nWe are aiming to develop skills and understanding of an approach to modeling that enables us to more accurately capture, in principle, the psychological processes that produce the data we analyse.\n\n\n\n\n\n\nAt first, we face two main challenges when working with ordinal data. The challenges are conceptual and social, more than statistical or computational.\nThe first challenge is conceptual because ordinal data values look like numbers but we cannot treat them like the numbers we are used to when we measure things like reaction time (RT), heat energy (temperature) or distance (e.g., height).\n\nWe typically encounter ordinal data (e.g., from Likert-style items, given rating scales) in contexts in which the response values that we observe look like numbers (e.g., 1) just like the numbers we handle in other contexts (e.g., RT or temperature or height). But those other numbers have what is known as metric properties (Liddell & Kruschke, 2018): Metric measurement scales assume that the numbers we observe are on interval or ratio scales.\n\n\nInterval scales define the distances between points somehow.\nRatio scales incorporate a zero point.\n\nWhereas ordinal data (e.g., responses to a rating question) have neither of these properties.\nThis means that, for ordinal data:\n\nWe cannot assume that the distances are equal between different values of the range of possible values, for example, for the rating scale: 1 \\(\\rightarrow\\) 2 \\(\\rightarrow\\) 3 \\(\\rightarrow\\) 4 \\(\\rightarrow\\) 5. This means that we do not know or cannot assume that the distance between, e.g., 1 \\(\\rightarrow\\) 2 is the same as the distance between, e.g., 3 \\(\\rightarrow\\) 4.\nWe do assume that all observed responses will take one of the categorical values between the minimum and the maximum values on the scale (e.g., between 1 and 5 on a 5-point rating scale ranging from 1-5).\n\nThe second challenge is social because, most of the time, Psychologists do in practice treat ordinal data like numeric data from measurement scales with metric properties. This is a mistake though it is very very common.\n\nLiddell & Kruschke (2018) analyzed a sample of papers from a set of top Psychology journals. They found that whenever an article mentioned Likert data (i.e., ratings data), the authors of that article analyzed the Likert-scale ordinal data as if they were metric, using analysis methods (like ANOVA, correlation, ordinary least squares regression (linear models), and t-tests) that assume metric properties.\n\nThis means that:\n\nMost Psychologists will assume that it is correct to analyse ordinal data using methods that assume metric properties.\nMost Psychologists will fail to look for, or to see, the problems associated with this conventional approach.\nThe results from many reports in the literature will be difficult to evaluate or interpret, where the authors have analysed ordinal data using methods assuming metric properties in outcome measurement scales.\n\nThe challenge we face is that we will aim to develop skills in using ordinal models when, in contrast, most psychological research articles will report analyses of ordinal data using conventional methods like ANOVA or linear regression. We will work to understand why ordinal models are better. We will learn that applying conventional methods to ordinal data will, in principle, involve a poor account of the data and, in practice, will create the risk of producing misleading results. And we will learn how to work with and interpret the results from ordinal models with or without random effects.\nIn our work in this chapter, we will rely extensively on the ideas set out by Bürkner & Vuorre (2019) and Liddell & Kruschke (2018), see Section 1.16.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe point is that observed outcome response values, given ordinal data, look like numbers but it is best to understand ordinal data values as numeric labels for ordered categories.\n\nWhat the different categories correspond to — what dimension, or what data generating processing mechanism — depends on your psychological theory for the processes that drive response production for the task you constructed to collect or observe your data.\n\n\n\n\n\n\nIn the context of a study in which we have asked participants to produce a rating response to a Likert-style item, we may assume that the ratings responses are produced (in participants’ minds) by psychological processes in which the participant, in response to the Likert question, divides some latent (unobserved) psychological continuum or dimension into categories (or cut-points or thresholds) so that they can select a response option.\nImagine, for example, that you are a participant in a study on reading comprehension, and that you have been asked to read a text (giving information on some health condition) and have then been asked to respond to the question:\nHow well do you think you have understood this text? (Please check one response) (on the scale 1-5)\nIn theory, to answer this question, you will have to choose a response based on where you think you are on your unobserved (internal to you) measure of your understanding of the information in the text you have been asked to read. You may be able to evaluate the cohesion, or some other internal measure, of your understanding of the text. Simplifying a bit, we might assume that your internal measure of understanding is associated with a normal probability distribution so that it peaks over some value (e.g., 3) of the strength of understanding, though other values are possible.\nAs Figure 1 suggests, a participant in this situation will have to map where they are on the internal measure (the unobserved latent dimension of their understanding) to a number from the response options they are given (e.g., rating scale values ranging 1-5). They must cut or divide into categories (using unobserved threshold values) where they are on this unobserved internal dimension (this sense of their own understanding). And there is no reason to suppose that their internal measure of their understanding is divided into an ordered metric scale: we cannot assume that in a participant’s mind, e.g., the distance 1 \\(\\rightarrow\\) 2 is the same as the distance between, e.g., 3 \\(\\rightarrow\\) 4.\n\n\n\n\n\n\n\n\nFigure 1: A latent scale on the horizontal axis is divided into intervals or bins divided by thresholds marked by dotted lines. The cumulative normal probability in the intervals is the probability of the ordinal values.\n\n\n\n\n\nIn conducting analyses of ordinal data with ordinal models, we often fit models that describe the cumulative probability that a rating response maps to some value (typically, understood in terms of thresholds) on an underlying latent continuum.\nThis is a very common situation, and that is why we focus in this chapter on the corresponding class of parametric ordinal models: cumulative models. Bürkner & Vuorre (2019) provide a nice account of cumulative models, and we walk through that account here but note, before we go on, that this account is spelled out in the context of an example data-set and an example model that supposes some simplifying assumptions: we are talking about one example from a class of possible models; other models, with alternate assumptions, are possible, and you can expand your understanding by reading the papers referenced in Section 1.16.\nLet’s assume we have observed a set of responses to the question:\nHow well do you think you have understood this text? (Please check one response) (on the scale 1-5)\nOur cumulative model assumes that the observed ordinal data — the column of values, the rating responses we recorded from participants — variable \\(Y\\) are produced (by the participants) given categorization of a latent (not observed) continuous variable \\(\\tilde{Y}\\) into a set of different categories, using a set of threshold values.\nIn our analysis, we model the categorization process.\nWe assume that there are \\(K\\) thresholds \\(\\tau_k\\) which cut \\(\\tilde{Y}\\) into \\(K+1\\) observable ordered categories of \\(Y\\), where \\(Y\\) comprises the values of the response a participant can make (given the rating scale response options we give them).\nFor the latent variable we see in Figure 1, there are five (\\(K+1 = 5\\)) response categories so we need \\(K = 4\\) thresholds.\nBecause we can assume that \\(\\tilde{Y}\\) values derive from a specific probability distribution (e.g., a normal distribution, though there are alternatives), we can suppose that the probability that \\(Y\\) (the observed rating) has some value (e.g., a rating of 5), that is, the probability that \\(Y\\) is equal to category \\(k\\) as:\n\\[\nPr(Y=k) = F(\\tau_k) - F(\\tau_k-1)\n\\]\nYou can understand this probability we are targeting, in our analysis, as the probability that a rating response (made by a participant to our Likert-style item) is equal to \\(k\\) (e.g., a rating of 5) rather than some other value (e.g., on our 5-point example scale, a rating of 4 or 3 or 2 or 1), given the probability function \\(F\\).\n\nIf you reflect: this resembles the target for analysis when we are working with accuracy outcome data, and we wish to estimate the effects of factors that may predict the probability that the response a participant makes is correct (not incorrect).\n\nIn our model, we can aim to predict \\(\\tilde{Y}\\), given information about predictor variables. Let’s say, for our example, that we think that people who know more about health (so: score higher on measures of health literacy), are more likely to produce a high rating of their understanding of the information in a health text (maybe because they do understand the text better). If so, our model might look like this:\n\\[\n\\tilde{Y} = \\beta_X + \\varepsilon\n\\]\nwhere\n\n\\(\\tilde{Y}\\) is our latent (not observed) continuous variable,\n\n\\(\\beta_X\\) is the coefficient \\(\\beta\\) corresponding the average rate of change in the outcome, given different values in the predictor \\(X\\),\nand \\(\\varepsilon\\) is the random error of the model, the variance that cannot be explained by the model.\n\nFor short, we can identify \\(\\eta = \\beta_X + \\varepsilon\\).\n(Broadly the same model structure will apply, as we will see, for multilevel data or for nonlinear relationships.)\nGiven the probability function \\(F\\) we assumed, we model the probability of (observed) \\(Y\\) being equal to category \\(k\\) (some rating value versus other values), based on our prediction model \\(\\eta\\).\n\\[\nPr(Y=k | \\eta) = F(\\tau_k - \\eta) - F(\\tau_k-1 - \\eta)\n\\]\nWhat cumulative models do is to take the observed ratings values \\(Y\\) and, for the prediction model \\(\\beta_X + \\varepsilon\\), use that information to estimate the thresholds \\(\\tau_k\\) (for our example, thresholds \\(tau_1\\) to \\(\\tau_4\\)) and the coefficient \\(\\beta\\).\nIn doing this, we assume for simplicity that the effect of the predictors in the model \\(\\eta = \\beta_X + \\varepsilon\\) is constant across response categories though we can relax this assumption (if a predictor has different impacts on different categories).\n\n\n\nAs I referenced earlier, we cannot just fit a linear model to ordinal data like ratings: we should not use methods like ANOVA, correlation, t-test, or regression.\n\nWhy not?\n\nThe short answer (Bürkner & Vuorre, 2019) is that we should not use methods like ANOVA, correlation, t-test, or regression to analyze ordinal data because:\n\nthese methods assume metric properties for outcome variables but ordinal data do not possess metric properties (equal intervals, zero points);\nthe distribution of ordinal responses may be non-normal (if low or high ratings are often chosen);\nand variances of the unobserved latent variable or dimension (\\(\\tilde{Y}\\)) that informs (we hope) observed ratings may be different for different groups or conditions.\n\nThe long answer Why not? is supplied by Liddell & Kruschke (2018) in a series of analyses of simulated and real data.\nThey argue that ordinal models supply a more accurate account of the data generating (psychological processes) that result in the ratings or the other ordinal data that we observe so, in principle, we should ordinal models anyway. We may differ in exactly what assumptions we want to make about things like the underlying probability distribution, or the impact of predictors on categories, but in general once we choose to employ ordinal models we are adopting a more realistic account of where our data come from.\nEven if we were to ignore this in principle justification, however, we should be aware of the problems associated with employing metric methods (ANOVA etc.) with ordinal data. These problems derive from the facts that observed ordinal values have fixed ranges (e.g., rating responses cannot exceed the minimum or maximum points on a rating scale), and while it is unknown how observed ratings map to (or correspond to categorization cuts or thresholds) of the underlying psychological dimension, it is also unknown but can be expected that variance in values on that latent dimension may vary between different groups. This means that mean observed ordinal data values may appear to differ, between groups or between conditions, when the values on the underlying latent dimension actually do not differ (we get false positives) or that mean observed ordinal data values may appear to be equal between groups or between conditions, though values on the underlying latent dimension actually do differ (false negatives). It also means that the power of analyses, or the capacity to detect effects that may in fact be present will be reduced where inappropriate modeling approaches are employed.\nLiddell & Kruschke (2018) note that common solutions — to the problem of using metric methods to analyze ordinal data — like ignoring the problem or like averaging ordinal responses will not work because the mis-estimation will be there whether we ignore the problem, we just will not know where, and because analyses of averaged ordinal outcomes will only result in mis-estimates of the underlying effects of interest.\nFortunately, we do not have to employ inappropriate methods. And, fortunately, these methods are relatively straightforward to apply. We look at the practical application of ordinal modeling methods to ordinal data, next.\n\n\n\n\nUnderstand through practical experience the reasons for using ordinal models when we analyze ordinal outcome variables.\nPractice running ordinal models with varying random effects structures.\nPractice reporting the results of ordinal models, including through the use of prediction plots.\n\n\n\n\nI have provided a collection of materials you can use. Here, I explain what they are and how I suggest you use them.\n1. Chapter: 05-ordinal\n1.1. I have written this chapter to discuss the main ideas and set out the practical steps you can follow to start to develop the skills required to work with ordered categorical outcomes i.e. ordinal data using ordinal models.\n1.2. The practical elements include data tidying, visualization and analysis steps.\n1.3. You can read the chapter and run the code to gain experience and encounter code you can adapt for your own purposes.\n\nRead in the example dataset.\nExperiment with the .R code used to work with the example data.\nRun ordinal models of demonstration data.\nRun ordinal models of alternate data sets (see links in Section 1.12).\nReview the recommended readings (Section 1.16).\n\n2. Practical materials\n2.1 In the following sections, I describe the practical steps, and associated resources, you can use for your learning. I set out the data tidying, analysis and visualization steps you can follow, working with the example dataset, described next.\n\n\n\nWe will be working, at first, with a sample of data collected as part of work in progress, undertaken for Clearly understood: health comprehension project (Davies and colleagues).\n\n\n\n\n\n\nWarning\n\n\n\nThese data are unpublished so should not be shared without permission.\n\n\n\n\n\n\nOur interest, in conducting the project, lies in identifying what factors make it easy or difficult to understand written health information. In part, we are concerned about the processes that health providers or clinicians apply to assure the effectiveness of the text they produce to guide patients or carers, for example, in taking medication, in making treatment decisions, or in order to follow therapeutic programmes.\nIt is common, in the quality assurance process in the production of health information texts, that text producers ask participants in patient review panels to evaluate draft texts. In such reviews, a participant may be asked a question like “How well do you understand this text?” This kind of question presents a metacognitive task: we are asking a participant to think about their thinking. But it is unclear that people can do this well or, indeed, what factors determine the responses to such questions (Dunlosky & Lipko, 2007).\nFor these reasons, we conducted studies in which we presented adult participants with sampled health information texts (taken from health service webpages) and, critically, asked them to respond to the question:\n\nHow well do you think you have understood this text?\n\nFor each text, in response to this question, participants were asked to click on one option from an array of response options ranging from (1) Not well at all to (9) Extremely well. The data we collected in this element of our studies comprise, clearly, ordinal responses. Thus, we may use these data to address the following research question.\n\n\n\n\n\n\nNote\n\n\n\n\nWhat factors predict self-evaluated rated understanding of health information.\n\n\n\n\n\n\nWe will work with a sample of participant data drawn from a series of Lancaster University undergraduate dissertation studies connected to the Clearly understood project. In these studies, we collected data from 202 participants on a series of measures (Section 1.8.1.3) of vocabulary knowledge, health literacy, reading strategy, as well as responses to health information texts. The distributions of participants’ scores on each of a range of attribute variables\n\n\n\n\n\n\n\n\nFigure 2: Grid of plots showing the distribution of participant attributes. The grid includes histograms of the distributions of: self-rated accuracy; vocabulary (SHIPLEY); health literacy (HLVA); reading strategy (FACTOR3); and age (years). We also see dot plots presenting counts of numbers of participants of different self-reported gender, education, and ethnicity categories.\n\n\n\n\n\nThe plots indicate:\n\nmost self-rated accuracy scores are high (over 6);\nmany participants with vocabulary scores greater than 30, a few present lower scores;\nhealth literacy scores centered on 8 or some, with lower and higher scores;\na skewed distribution of reading strategy scores, with many around 20-40, and a tail of higher scores;\nmost participants are 20-40 years of age, some older;\nmany more female than male participants, very few non-binary reported;\nmany more participants with higher education than further, very few with secondary;\nand many White participants (Office of National Statistics categories), far fewer Asian or Mixed or Black ethnicity participants.\n\n\n\n\nWe collected data through an online survey administered through Qualtrics.\nWe used the Shipley vocabulary sub-test (Shipley et al., 2009) to estimate vocabulary knowledge.\nWe used the Health Literacy Vocabulary Assessment (HLVA, Chadwick, 2023; Ratajczak, 2020) to estimate health literacy.\nWe used an instrument drawn from unpublished work by Calloway (2019) to assess the approach participants took to reading and understanding written information.\nWe presented participants with a sample of 20 health information texts. In the data collection process for this dataset, participants were recruited in multiple different studies. In each study, any one participant was presented with a randomly selected subset of the total of 20 texts.\nWe asked participants to rate their level of understanding of the health-related texts that we presented in the study. We used a nine-point judgment scales because they have been found to outperform alternative scales with fewer categories in terms of criterion validity, internal consistency, test-retest reliability, and discriminating power (Preston & Colman, 2000).\nWe recorded participants’ demographic characteristics: gender (coded: Male, Female, non-binary, prefer not to say); education (coded: Secondary, Further, Higher); and ethnicity (coded: White, Black, Asian, Mixed, Other).\n\n\n\n\nYou can download the 2021-22_PSYC304-health-comprehension.csv file holding the data we analyse in this chapter by clicking on the link.\n\n\n\nI am going to assume you have downloaded the data file, and that you know where it is. We use read_csv to read the data file into R.\n\nhealth &lt;- read_csv(\"2021-22_PSYC304-health-comprehension.csv\", \n                                 na = \"-999\",\n                                 col_types = cols(\n                                   ResponseId = col_factor(),\n                                   rating = col_factor(),\n                                   GENDER = col_factor(),\n                                   EDUCATION = col_factor(),\n                                   ETHNICITY = col_factor(),\n                                   NATIVE.LANGUAGE = col_factor(),\n                                   OTHER.LANGUAGE = col_factor(),\n                                   text.id = col_factor(),\n                                   text.question.id = col_factor(),\n                                   study = col_factor()\n                                 )\n                               )\n\nNotice that we use col_types = cols(...) to require read_csv() to class some columns as factors.\nImportantly, we ask R to treat the rating variable as a factor with rating = col_factor().\n\n\n\n\n\n\nTip\n\n\n\nIn the practical work we do, we will be using functions from the {ordinal} library to model ordinal data.\n\nIn using these functions, we need ask R to treat the ordinal outcome variable as a factor.\n\n\n\n\n\n\nIt is always a good to inspect what you have got when you read a data file in to R. Here, what may most concern us is the distribution of observed responses on the rating scale (responses to the “How well do you understand?” question). Figure 3 is a dot plot showing the distribution of ratings responses. The Likert-style questions in the surveys asked participants to rate their level of understanding of the texts they saw on a scale from 1 (not well) to 9 (extremely well). The plot shows the number of responses recorded for each response option, over all participants and all texts.\n\nhealth &lt;- health %&gt;% mutate(rating = fct_relevel(rating, sort))\n\nhealth %&gt;%\n  group_by(rating) %&gt;%\n  summarise(count = n()) %&gt;%\n  ggplot(aes(x = rating, y = count, colour = rating)) + \n  geom_point(size = 3) +\n  scale_color_viridis(discrete=TRUE, option = \"mako\") + theme_bw() +\n  theme(\n    panel.grid.major.y = element_blank()  # No horizontal grid lines\n  ) +\n  coord_flip()\n\n\n\n\n\n\n\nFigure 3: Dot plot showing the distribution of ratings responses. The Likert-style questions in the surveys asked participants to rate their level of understanding of the texts they saw on a scale from 1 (not well) to 9 (extremely well). The plot shows the number of responses recorded for each response option, over all participants and all texts.\n\n\n\n\n\nThe plot indicates that most participants chose response options 5-9, while very few rated their understanding at the lowest levels (options 1-4). Interestingly, many ratings responses around 7-8 were recorded: many more than responses at 5-6.\nIn analyzing these data, we will seek to estimate what information available to us can be used to predict whether a participant’s rating of their understanding is more likely to be, say, 1 or 2, 2 or 3 … 7 or 8, 8 or 9.\n\nOne practical way to think about the estimation problem when working with ratings-style ordinal data is this:\n\nWhat factors move or how do influential factors move the probability that the ordinal response is a relatively low or relatively high order response option?\nIn doing this, we do not have to assume that rating scale points map to equal sized intervals on the underlying latent scale where the scale may be an unobserved psychological continuum (like understanding).\n\n\nHere, we mostly have information on participant attributes and some information on text properties to do our prediction analyses. In other studies, we may be using information about experimental conditions, or selected groups of participants to estimate effects on variation in ratings responses.\n\n\n\n\nThe Clearly understood health comprehension project dataset is tidy (?@sec-intro-mixed-data-tidy):\n\nEach variable has its own column.\nEach observation has its own row.\nEach value has its own cell.\n\nHowever, there are aspects of the data structure or properties of the dataset variables that will cause inefficiencies or problems in later data analysis if we do not fix them first.\nYou can see what we have if you look at the results we get from using summary() and str() to inspect the dataset.\n\nsummary(health)\n\n             ResponseId        AGE                     GENDER    \n R_sKW4OJnOlidPxrH:  20   Min.   :18.0   Female           :2900  \n R_27paPJzIutLoqk8:  20   1st Qu.:20.0   Male             :1120  \n R_1nW0lpFdfumlI1p:  20   Median :27.0   Prefer-not-to-say:  20  \n R_31ZqPQpNEEapoW8:  20   Mean   :34.3                           \n R_2whvE2IW90nj2P7:  20   3rd Qu.:50.0                           \n R_3CAxrri9clBT7sl:  20   Max.   :81.0                           \n (Other)          :3920                                          \n     EDUCATION    ETHNICITY    NATIVE.LANGUAGE    OTHER.LANGUAGE\n Further  :1780   Asian: 680   English:2720    NA        :2720  \n Higher   :1800   White:3260   Other  :1320    Polish    : 580  \n Secondary: 460   Other:  40                   Cantonese : 280  \n                  Mixed:  60                   Chinese   : 120  \n                                               Portuguese:  60  \n                                               polish    :  60  \n                                               (Other)   : 220  \n ENGLISH.PROFICIENCY    SHIPLEY           HLVA           FACTOR3     \n Length:4040         Min.   :15.00   Min.   : 3.000   Min.   :17.00  \n Class :character    1st Qu.:30.00   1st Qu.: 7.000   1st Qu.:45.00  \n Mode  :character    Median :34.00   Median : 9.000   Median :49.00  \n                     Mean   :32.97   Mean   : 8.564   Mean   :49.03  \n                     3rd Qu.:37.00   3rd Qu.:10.000   3rd Qu.:55.00  \n                     Max.   :40.00   Max.   :13.000   Max.   :63.00  \n                                                                     \n     rating        response          RDFKGL       study    \n 8      :1044   Min.   :0.0000   Min.   : 4.552   cs: 480  \n 7      : 916   1st Qu.:1.0000   1st Qu.: 6.358   jg:1120  \n 9      : 824   Median :1.0000   Median : 8.116   ml: 720  \n 6      : 500   Mean   :0.8064   Mean   : 7.930   rw:1720  \n 5      : 352   3rd Qu.:1.0000   3rd Qu.: 9.413            \n 4      : 176   Max.   :1.0000   Max.   :13.278            \n (Other): 228                                              \n             text.id                  text.question.id\n studyone.TEXT.37: 344   studyone.TEXT.37.CQ.1:  86   \n studyone.TEXT.39: 344   studyone.TEXT.37.CQ.2:  86   \n studyone.TEXT.72: 344   studyone.TEXT.37.CQ.3:  86   \n studyone.TEXT.14: 344   studyone.TEXT.37.CQ.4:  86   \n studyone.TEXT.50: 344   studyone.TEXT.39.CQ.1:  86   \n studyone.TEXT.10: 224   studyone.TEXT.39.CQ.2:  86   \n (Other)         :2096   (Other)              :3524   \n\n\nYou should be used to seeing the summary() of a dataset, showing summary statistics of numeric variables and counts of the numbers of observations of data coded at different levels for each categorical or nominal variable classed as a factor.\nUsing the str() function may be new to you and, as you can see, the output from the function call gives you a bit more information on how R interprets the data in the variable columns. You can see that each variable is listed alongside information about how the data in the column are interpreted (as Factor or num numeric). Where we have columns holding information on factors there we see information about the levels.\nRecall that for a categorical or nominal variable e.g. ETHNICITY, provided R interprets the variable as a factor, each data value in the column is coded as corresponding to one level i.e. group or class or category (e.g., we have ETHNICITY classes \"Asian\" etc.) Recall, also, that at the data read-in stage, we instructed R how we wanted it to interpret each column using col_types = cols().\n\nstr(health)\n\ntibble [4,040 × 17] (S3: tbl_df/tbl/data.frame)\n $ ResponseId         : Factor w/ 202 levels \"R_sKW4OJnOlidPxrH\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ AGE                : num [1:4040] 20 20 20 20 20 20 20 20 20 20 ...\n $ GENDER             : Factor w/ 3 levels \"Female\",\"Male\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ EDUCATION          : Factor w/ 3 levels \"Further\",\"Higher\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ ETHNICITY          : Factor w/ 4 levels \"Asian\",\"White\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ NATIVE.LANGUAGE    : Factor w/ 2 levels \"English\",\"Other\": 1 1 1 1 1 1 1 1 1 1 ...\n $ OTHER.LANGUAGE     : Factor w/ 17 levels \"NA\",\"Catonese\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ ENGLISH.PROFICIENCY: chr [1:4040] \"NA\" \"NA\" \"NA\" \"NA\" ...\n $ SHIPLEY            : num [1:4040] 26 26 26 26 26 26 26 26 26 26 ...\n $ HLVA               : num [1:4040] 8 8 8 8 8 8 8 8 8 8 ...\n $ FACTOR3            : num [1:4040] 59 59 59 59 59 59 59 59 59 59 ...\n $ rating             : Factor w/ 9 levels \"1\",\"2\",\"3\",\"4\",..: 8 8 8 8 7 7 7 7 7 7 ...\n $ response           : num [1:4040] 1 1 1 1 1 1 1 0 1 1 ...\n $ RDFKGL             : num [1:4040] 10.61 10.61 10.61 10.61 8.12 ...\n $ study              : Factor w/ 4 levels \"cs\",\"jg\",\"ml\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ text.id            : Factor w/ 20 levels \"studyone.TEXT.105\",..: 1 1 1 1 2 2 2 2 3 3 ...\n $ text.question.id   : Factor w/ 80 levels \"studyone.TEXT.105.CQ.1\",..: 1 2 3 4 5 6 7 8 9 10 ...\n\n\nOur specific concern, here, is that the rating response variable is treated as a factor because the {ordinal} library we are going to use to do the modeling must find the outcome variable is a factor.\nWe can focus str() on the rating variable. We see that it is being treated as a factor.\n\nstr(health$rating)\n\n Factor w/ 9 levels \"1\",\"2\",\"3\",\"4\",..: 8 8 8 8 7 7 7 7 7 7 ...\n\n\nHowever, we also need to make sure that the rating outcome variable is being treated as an ordered factor.\nWe can perform a check as follows. (I found how to do this here)\n\nis.ordered(factor(health$rating))\n\n[1] FALSE\n\n\nWe can see that the variable is not being treated as an ordered factor. We need to fix that.\nThe ordinal model estimates the locations (thresholds) for where to split the latent scale (the continuum underlying the ratings) corresponding to different ratings values. If we do not make sure that the outcome factor variable is split as it should be then there is no guarantee that {ordinal} functions will estimate the thresholds in the right order (i.e., 1,2,3 ... rather than 3,2,1...).\nWe can make sure that the confidence rating factor is ordered precisely as we wish using the ordered() function.\n\nhealth$rating &lt;- ordered(health$rating,\n                         levels = c(\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"))\n\nWe can then do a check to see that we have got what we want. We do not wantrating to be treated as numeric, we do want it to be treated as an ordered factor.\n\nis.numeric(health$rating)\n\n[1] FALSE\n\nis.factor(health$rating)\n\n[1] TRUE\n\nstr(health$rating)\n\n Ord.factor w/ 9 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 8 8 8 8 7 7 7 7 7 7 ...\n\nis.ordered(health$rating)\n\n[1] TRUE\n\n\nIt is.\nNext, before doing any modelling, it will be sensible to standardize potential predictors\n\nhealth &lt;- health %&gt;% \n  mutate(across(c(AGE, SHIPLEY, HLVA, FACTOR3, RDFKGL), \n                scale, center = TRUE, scale = TRUE,\n                .names = \"z_{.col}\"))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(...)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\nYou can see that in this chunk of code, we are doing a number of things:\n\nhealth &lt;- health %&gt;% recreates the health dataset from the following steps.\nmutate(...) do an operation which retains the existing variables in the dataset, to change the variables as further detailed.\nacross(...) work with the multple column variables that are named in the c(AGE, SHIPLEY, HLVA, FACTOR3, RDFKGL) set.\n...scale, center = TRUE, scale = TRUE... here is where we do the standardization work.\n\nWhat we are asking for is that R takes the variables we name and standardizes each of them.\n\n.names = \"z_{.col}\") creates the standardized variables under adapted names, adding z_ to the original column name so that we can distinguish between the standardized and original raw versions of the data columns.\n\nNote that the across() function is a useful function for applying a function across multiple column variables see information here There is a helpful discussion on how we can do this task here\nWe can then check that we have produced the standardized variables as required.\n\nsummary(health)\n\n             ResponseId        AGE                     GENDER    \n R_sKW4OJnOlidPxrH:  20   Min.   :18.0   Female           :2900  \n R_27paPJzIutLoqk8:  20   1st Qu.:20.0   Male             :1120  \n R_1nW0lpFdfumlI1p:  20   Median :27.0   Prefer-not-to-say:  20  \n R_31ZqPQpNEEapoW8:  20   Mean   :34.3                           \n R_2whvE2IW90nj2P7:  20   3rd Qu.:50.0                           \n R_3CAxrri9clBT7sl:  20   Max.   :81.0                           \n (Other)          :3920                                          \n     EDUCATION    ETHNICITY    NATIVE.LANGUAGE    OTHER.LANGUAGE\n Further  :1780   Asian: 680   English:2720    NA        :2720  \n Higher   :1800   White:3260   Other  :1320    Polish    : 580  \n Secondary: 460   Other:  40                   Cantonese : 280  \n                  Mixed:  60                   Chinese   : 120  \n                                               Portuguese:  60  \n                                               polish    :  60  \n                                               (Other)   : 220  \n ENGLISH.PROFICIENCY    SHIPLEY           HLVA           FACTOR3     \n Length:4040         Min.   :15.00   Min.   : 3.000   Min.   :17.00  \n Class :character    1st Qu.:30.00   1st Qu.: 7.000   1st Qu.:45.00  \n Mode  :character    Median :34.00   Median : 9.000   Median :49.00  \n                     Mean   :32.97   Mean   : 8.564   Mean   :49.03  \n                     3rd Qu.:37.00   3rd Qu.:10.000   3rd Qu.:55.00  \n                     Max.   :40.00   Max.   :13.000   Max.   :63.00  \n                                                                     \n     rating        response          RDFKGL       study    \n 8      :1044   Min.   :0.0000   Min.   : 4.552   cs: 480  \n 7      : 916   1st Qu.:1.0000   1st Qu.: 6.358   jg:1120  \n 9      : 824   Median :1.0000   Median : 8.116   ml: 720  \n 6      : 500   Mean   :0.8064   Mean   : 7.930   rw:1720  \n 5      : 352   3rd Qu.:1.0000   3rd Qu.: 9.413            \n 4      : 176   Max.   :1.0000   Max.   :13.278            \n (Other): 228                                              \n             text.id                  text.question.id\n studyone.TEXT.37: 344   studyone.TEXT.37.CQ.1:  86   \n studyone.TEXT.39: 344   studyone.TEXT.37.CQ.2:  86   \n studyone.TEXT.72: 344   studyone.TEXT.37.CQ.3:  86   \n studyone.TEXT.14: 344   studyone.TEXT.37.CQ.4:  86   \n studyone.TEXT.50: 344   studyone.TEXT.39.CQ.1:  86   \n studyone.TEXT.10: 224   studyone.TEXT.39.CQ.2:  86   \n (Other)         :2096   (Other)              :3524   \n           z_AGE.V1                   z_SHIPLEY.V1        \n Min.   :-9.82624656687e-01   Min.   :-3.294105320780000  \n 1st Qu.:-8.62035239524e-01   1st Qu.:-0.543722537102000  \n Median :-4.39972279452e-01   Median : 0.189712871877000  \n Mean   : 4.20000000000e-15   Mean   :-0.000000000000001  \n 3rd Qu.: 9.46806017926e-01   3rd Qu.: 0.739789428612000  \n Max.   : 2.81594198396e+00   Max.   : 1.289865985350000  \n                                                          \n          z_HLVA.V1                  z_FACTOR3.V1        \n Min.   :-2.60748865140e+00   Min.   :-4.20593553983000  \n 1st Qu.:-7.33066204487e-01   1st Qu.:-0.52915479589300  \n Median : 2.04145018971e-01   Median :-0.00390040390093  \n Mean   : 1.00000000000e-16   Mean   : 0.00000000000000  \n 3rd Qu.: 6.72750630700e-01   3rd Qu.: 0.78398118408700  \n Max.   : 2.07856746589e+00   Max.   : 1.83448996807000  \n                                                         \n         z_RDFKGL.V1         \n Min.   :-1.46446595688e+00  \n 1st Qu.:-6.81459422242e-01  \n Median : 8.07363074868e-02  \n Mean   : 6.99000000000e-14  \n 3rd Qu.: 6.43061598419e-01  \n Max.   : 2.31876495189e+00  \n                             \n\n\n\n\n\nIn our first analysis, we can begin by assuming no random effects. We keep things simple at this point so that we can focus on the key changes in model coding.\nThe model is conducted to examine what shapes the variation in rating responses that we see in Figure 3.\n\n\n\n\n\n\nNote\n\n\n\n\nWhat factors predict self-evaluated rated understanding of health information.\n\n\n\nIn our analysis, the outcome variable is the ordinal response variable rating. The predictors consist of the variables we standardized earlier. We use the clm() function from the {ordinal} library to do the analysis.\nI will give information in outline here, the interested reader can see more detailed information in Christensen (2022) and Christensen (2015).\nYou can also find the manual for the {ordinal} library functions here\nWe code the model as follows.\n\nhealth.clm &lt;- clm(rating ~\n                    \n                    z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL,\n                  \n                  Hess = TRUE, link = \"logit\",\n                  data = health)\n\nsummary(health.clm)\n\nThe code works as follows.\nFirst, we have a chunk of code mostly similar to what we have done before, but changing the function.\n\nclm() the function name changes because now we want a cumulative link model of the ordinal responses.\n\nThe model specification includes information about the fixed effects, the predictors: z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL.\nSecond, we have the bit that is specific to cumulative link models fitted using the clm() function.\n\nHess = TRUE is required if we want to get a summary of the model fit; the default is TRUE but it is worth being explicit about it.\nlink = \"logit\" specifies that we want to model the ordinal responses in terms of the log odds (hence, the probability) that a response is a low or a high rating value.\n\n\n\nIf you run the model code, it may take a few seconds to run. Then you will get the results shown in the output.\n\n\nformula: rating ~ z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL\ndata:    health\n\n link  threshold nobs logLik   AIC      niter max.grad cond.H \n logit flexible  4040 -6880.78 13787.55 5(0)  9.26e-07 7.3e+01\n\nCoefficients:\n          Estimate Std. Error z value Pr(&gt;|z|)    \nz_AGE     -0.17719    0.02966  -5.975 2.30e-09 ***\nz_SHIPLEY  0.34384    0.03393  10.135  &lt; 2e-16 ***\nz_HLVA     0.16174    0.03265   4.954 7.28e-07 ***\nz_FACTOR3  0.74535    0.03145  23.699  &lt; 2e-16 ***\nz_RDFKGL  -0.27220    0.02892  -9.412  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2 -4.65066    0.12978 -35.836\n2|3 -4.13902    0.10395 -39.817\n3|4 -3.26845    0.07390 -44.228\n4|5 -2.56826    0.05804 -44.248\n5|6 -1.69333    0.04437 -38.160\n6|7 -0.87651    0.03671 -23.876\n7|8  0.24214    0.03402   7.117\n8|9  1.63049    0.04252  38.346\n\n\nThe summary() output for the model is similar to the outputs you have seen for other model types.\n\nWe first get formula: information about the model you have specified.\nR will tell us what data: we are working with.\nWe then get Coefficients: estimates.\n\nThe table summary of coefficients arranges information in ways that will be familiar you:\n\nFor each predictor variable, we see Estimate, Std. Error, z value, and Pr(&gt;|z|) statistics.\nThe Pr(&gt;|z|) p-values are based on Wald tests of the null hypothesis that a predictor has null impact.\nThe coefficient estimates can be interpreted based on whether they are positive or negative.\n\n\nWe then get Threshold coefficients: indicating where the model fitted estimates the threshold locations: where the latent scale is cut, corresponding to different rating values.\n\n\n\n\n\n\n\nTip\n\n\n\nIn reporting ordinal (e.g., cumulative link) models, we typically focus on the coefficient estimates for the predictor variables.\n\nA positive coefficient estimate indicates that higher values of the predictor variable are associated with greater probability of higher rating values.\nA negative coefficient estimate indicates that higher values of the predictor variable are associated with greater probability of lower rating values.\n\n\n\n\n\n\n\nIn our analysis, we begin by assuming no random effects. However, this is unlikely to be appropriate given the data collection process deployed in the Clearly understood projects, where a sample of participants were asked to respond to a sample of texts:\n\nwe have multiple observations of responses for each participant;\nwe have multiple observations of responses for each stimulus text;\nparticipants were assigned to groups, and within a group all participants were asked to respond to the same stimulus texts.\n\nThese features ensure that the data have a multilevel structure and this structure requires us to fit a Cumulative Link Mixed-effects Model (CLMM).\nWe keep things simple at this point so that we can focus on the key changes in model coding. We can code a Cumulative Link Mixed-effects Model as follows.\n\nhealth.clmm &lt;- clmm(rating ~\n                      \n            z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL +\n                      \n            (1 | ResponseId),\n                    \n                    Hess = TRUE, link = \"logit\",\n                    data = health)\n\nsummary(health.clmm)\n\nIf you inspect the code chunk, you can see that we have made two changes.\nFirst, we have changed the function.\n\nclmm() the function name changes because now we want a Cumulative Linear Mixed-effects Model.\n\nSecondly, the model specification includes information about fixed effects and now about random effects.\n\nWith (1 | ResponseId) we include random effects of participants on on intercepts.\n\n\n\nIf you run the model code, you will see that the model may take several seconds, possibly a minute or two to complete. We will then get the results shown in the output.\n\n\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: rating ~ z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL +  \n    (1 | ResponseId)\ndata:    health\n\n link  threshold nobs logLik   AIC     niter       max.grad cond.H \n logit flexible  4040 -4978.08 9984.16 1490(19081) 3.86e-03 3.5e+02\n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ResponseId (Intercept) 9.825    3.134   \nNumber of groups:  ResponseId 202 \n\nCoefficients:\n          Estimate Std. Error z value Pr(&gt;|z|)    \nz_AGE     -0.44314    0.23473  -1.888  0.05905 .  \nz_SHIPLEY  0.77129    0.26498   2.911  0.00361 ** \nz_HLVA     0.20813    0.25608   0.813  0.41637    \nz_FACTOR3  1.68347    0.23830   7.065 1.61e-12 ***\nz_RDFKGL  -0.44346    0.03677 -12.060  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2  -9.3297     0.3452 -27.026\n2|3  -8.1413     0.2947 -27.629\n3|4  -6.5243     0.2631 -24.796\n4|5  -5.2159     0.2490 -20.947\n5|6  -3.4668     0.2370 -14.629\n6|7  -1.8481     0.2315  -7.984\n7|8   0.2966     0.2295   1.292\n8|9   3.0417     0.2346  12.966\n\n\nYou can see that the output summary presents the same structure. If you compare the output you see in Section 1.10.1, however, you will notice some similarities and some differences:\n\nIf you focus first on the estimates of the coefficients for the predictor variables, you will see that the estimates have the same sign (positive or negative) as they had before.\nHowever, you will see that the estimates have different magnitudes.\nYou will also see that the p-values are different.\n\nStudents often focus on p-values in reading model summaries. This is mistaken for multiple reasons.\n\nThe p-values correspond to the probabilities associated with the null hypothesis significance test: the test of the hypothesis that the effect of the predictor is null (i.e. the predictor has no impact).\nThis null assumption is made whichever model we are looking at. The p-values do not indicate whether an effect is more or less probable. (But you do get such posterior probabilities in Bayesian analyses.)\nSo it does not really mean much, though it is common, to talk about effects being highly significant.\n\nThus it should not worry us too much if the p-values are significant in one analysis but not significant in another.\nThat said, it is interesting, perhaps, that once we include random effects of participants on intercepts in our analysis then the effects of z_AGE and z_HLVA are no longer significant. I would be tempted to ask if the previously significant effects of these variables owed their impact to random differences between participants in their average or overall level of rating response.\n\n\n\nIt will be helpful for the interpretation of the estimates of the coefficients of these predictor variables if we visualize the predictions we can make, about how rating values vary, given differences in predictor variable values, given our model estimates. We can do this using functions from the {ggeffects} library. You can read more about the {ggeffects} library here where you will see a collection of articles explaining what you can do, and why, as well as technical information including some helpful tutorials.\nThe basic model prediction coding looks like this.\n\ndat &lt;- ggpredict(health.clmm, terms=\"z_FACTOR3 [all]\")\nplot(dat)\n\nFigure 4 shows you the marginal effect of variation in the reading strategy attribute, i.e., the effect of differences between individuals in how they score on the FACTOR3 measure of reading strategy. Note that the variable is listed as z_FACTOR3 because, as you will recall, we standardized numeric predictor variables before entering them in our model.\nYou can find a discussion of marginal effects in the context of working with the {ggeffects} library:\n\nhere and\nhere.\n\nYou can find an extensive, helpful (with examples) discussion of marginal effects by Andrew Heiss here.\nIn short, what we want to do is to take the model coefficient estimates, and generate predictions with these estimates, given different values of the predictor variable, while holding the other predictor variables at some constant or some level (or some series of values).\nIf you look at the code chunk, you can see that we first:\n\ndat &lt;- ggpredict(health.clmm, terms=\"z_FACTOR3 [all]\")\n\n\nIn this line, we use ggpredict() to work with some model information, assuming we previously fitted a model and gave it a name (here, health.clmm).\n\nNote that if you fit the model and call it health.clmm, as we did in Section 1.11, then an object of that name is created in the R workspace or environment. If you click on that object name in the environment window in R-Studio, you will see that there is a list of pieces of information about the model, including the coefficient estimates, the model formula etc. associated with that name.\n\nSo when we use ggpredict(), we ask R to take that model information and, for the term we specify, here, specify using terms=\"z_FACTOR3 [all]\", we ask R to generate some predictions.\ndat &lt;- ggpredict(...) asks R to put those predictions in an object called dat.\n\nIf you click on that object name in the environment window in R-Studio, you will see that it comprises a dataset. The dataset includes the columns:\n\nx giving different values of the predictor variable. ggpredict() will choose some ‘representative’ values for you but you can construct a set of values of the predictor for which you want predictions.\npredicted holds predicted values, given different predictor x values.\n\nIf you then run the line plot(dat) you can see what this gets us for these kinds of models. Figure 4 presents a grid of plots showing the model-predicted probabilities that a rating response will have one value for each of the 1-9 rating response values that are possible given the Likert rating scale used in data collection. In the grid, a different plot is shown for each possible response value, indicating how the probability varies that the rating response will take that value.\n\ndat &lt;- ggpredict(health.clmm, terms=\"z_FACTOR3 [all]\")\nplot(dat)\n\n\n\n\n\n\n\nFigure 4: A grid of plots showing marginal or conditional predicted probabilities that a rating response will have one value (among the 1-9 rating values possible), indicating how these predicted probabilities vary given variation in values of the standardized reading strategy (FACTOR3) variable.\n\n\n\n\n\nIf you examine Figure 4, you can recognize that we have one plot for each different value of the response options available for the Likert-scale rating items: 1-9. You can also see that in each plot we get a curve. In some cases – for rating response values 1-4 – the curve is flat or flattens very quickly, for higher levels of the z_FACTOR3 variable. In some cases – for rating response values 5-9 – the curve is more obvious, and resembles a normal distribution curve.\nIf you think about it, what these plots indicate are the ways in which the probability that a rating response is a low value (e.g., a rating of 1) or a high value (e.g., a rating of 9) rises or falls. Each possible rating response is associated with a probability distribution. For example, look at the plot labelled 6: that shows you the probability distribution indicating how the probability varies that a response will take the value 6. We can see that the distribution is normal in shape, a bell-shaped curve. We can see that the peak of the curve is over the z_FACTOR3 score (shown on the x-axis) of about 1.5. We can see that the probability represented by the height of the line showing the curve is lower for z_FACTOR3 scores lower than the score under the peak (e.g. scores less than z_FACTOR3 \\(=2\\)). The probability represented by the height of the line showing the curve is lower for z_FACTOR3 scores higher than the score under the peak (e.g. scores greater than z_FACTOR3 \\(=1\\)).\nWe can see that the peak of the normal curve, in the case of rating response values 5-9, is located at different places on the horizontal axis. Look at each of the plots labelled 5-9. Notice how the horizontal location of the curves shifts as z_FACTOR3 scores increase. If you go from left to right, i.e. from low to high values of z_FACTOR3, on each plot then you will see that the peak of the curve is located in different places: going from plot 5 to plot 9 the peak of the curve moves rightwards. These curves show how the probability that a rating response takes a high value (e.g. 9 instead of 8 or 8 instead of 7 etc.) is higher for higher values of z_FACTOR3. This idea might be a bit clearer if we draw the plot in a different way.\nFigure 5 shows the same model predictions but plots the predictions of the way that probability changes, for each rating response, by superimposing the plots for each response value, one on top of the other. I have drawn each probability curve in a different colour, and these colours match those used to present the counts of different response values shown in Figure 3.\n\ndat &lt;- ggpredict(health.clmm, terms=\"z_FACTOR3 [all]\")\nggplot(dat, aes(x, predicted, \n                colour = response.level)) + \n  geom_line(size = 1.5) +\n  scale_color_viridis(discrete=TRUE, option = \"mako\") + \n  labs(x = \"Reading strategy (z_FACTOR3)\", y = \"Predicted probability of a rating\") +\n  guides(colour = guide_legend(title = \"Rating\")) +\n  ylim(0, 1) +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 5: A plot showing marginal or conditional predicted probabilities that a rating response will have one value (among the 1-9 rating values possible), indicating how these predicted probabilities vary given variation in values of the standardized reading strategy (FACTOR3) variable\n\n\n\n\n\nYou can read Figure 5 by observing that:\n\nFor low value ratings e.g. for rating responses from 1-4, there is not much predicted probability that a response with such a value will be made (flat lines) but if they are going to be made they are likely to be made by people with low scores on the z_FACTOR3.\n\nYou can see this because you can see how the curves peak around low values of z_FACTOR3. This should make sense: people with low scores on reading strategy are maybe not doing reading effectively, are maybe as a result not doing well in understanding the texts they are given to read, and thus are not confident about their understanding. (This is a speculative causal theory but it will suffice for now.)\nRecall, also, that as Figure 3 indicated, in the Clearly understood health comprehension dataset, we saw that few rating responses were recorded for low value ratings of understanding. Few people in our sample made rating responses by choosing ratings of 1 or 2 to indicate low levels of understanding.\nFigure 5 also suggests that:\n\nFor higher value rating responses – responses representing ratings from 5 to 9 – there is variation in the probability that responses with such values will be made.\nThat variation in probability is shown by the probability distribution curves.\nFor these data, and this model, we can see that the probability shifts suggesting that participants in our sample were more likely to choose a higher value rating if they were also presenting high scores on the z_FACTOR3 measure of reading strategy.\n\n\n\n\n\nAs the review reported by Liddell & Kruschke (2018) suggests, we may have many many studies in which ordinal outcome data are analysed but very few published research reports that present analyses of ordinal data using ordinal models.\nYou can see two examples in the papers published by Ricketts et al. (2021) and by Rodríguez-Ferreiro et al. (2020). These papers are both published open accessible, so that they are freely available, and they are both associated with accessible data repositories.\n\nYou can find the repository for Ricketts et al. (2021) here.\nYou can find the repository for Rodríguez-Ferreiro et al. (2020) here.\n\nThe Rodríguez-Ferreiro et al. (2020) shares a data .csv only.\nThe Ricketts et al. (2021) repository shares data and analysis code as well as a fairly detailed guide to the analysis methods. Note that the core analysis approach taken in Ricketts et al. (2021) is based on Bayesian methods but that we also conduct clmm() models using the {ordinal} library functions discussed here; these models are labelled frequentist models and can be found under sensitivity analyses.\nFor what it’s worth, the Ricketts et al. (2021) is much more representative of the analysis approach I would recommend now.\nWhatever the specifics of your research question, dataset, analysis approach or model choices, I would recommend the following for your results report.\n\nExplain the model – the advice extended by Meteyard & Davies (2020) still apply: the reader will need to know:\n\n\nThe identity of the outcome and predictor variables;\nThe reason why you are using an ordinal approach, explaining the ordinal (ordered, categorical) nature of the outcome;\nThe structure of the fixed effects part of the model, i.e. the effects, in what form (main effects, interactions) you are seeking to estimate;\nAnd the structure of the random effects part of the model, i.e. what grouping variable (participants? items?), whether they encompass random intercepts or random slopes or covariances.\n\nYou can report or indicate some of this information by presenting a table summary of the effects estimated in your model (e.g., see Table 5, Rodríguez-Ferreiro et al., 2020; see tables 2 and 3, Ricketts et al., 2021). Journal formatting restrictions or other conventions may limit what information you can present.\nNotice that I do not present information on threshold estimates.\n\nExplain the results – I prefer to show and tell.\n\n\nPresent conditional or marginal effects plots (see figures 2 and 3, Ricketts et al., 2021) to indicate the predictions you can make given your model estimates.\nAnd explain what the estimates or what the prediction plots appear to show.\n\n\n\n\n\n\nAs I hint, when we discuss the concept that ordinal responses may map somehow to a latent unobserved underlying continuum (see Figure 1), there are other ways to think about ordinal data. Rather, there are other ways to think about the psychological mechanisms or the data generating mechanisms that give rise to the ordinal responses we analyse.\nIn Ricketts et al. (2021), we explain:\n\nIn the semantic post-test, participants worked their way through three steps, only progressing from one step to the next step if they provided an incorrect response or no response. Given the sequential nature of this task, we analysed data using sequential ratio ordinal models (Bürkner & Vuorre, 2019). In sequential models, we account for variation in the probability that a response falls into one response category (out of k ordered categories), equal to the probability that it did not fall into one of the foregoing categories, given the linear sum of predictors. We estimate the k-1 thresholds and the coefficients of the predictors.\n\nWhat this explanation refers to is the fact that, in our study:\n\nThe semantic post-test assessed knowledge for the meanings of newly trained words. We took a dynamic assessment or cuing hierarchy approach (Hasson & Joffe, 2007), providing children with increasing support to capture partial knowledge and the incremental nature of acquiring such knowledge (Dale, 1965). Each word was taken one at a time and children were given the op- portunity to demonstrate knowledge in three steps: definition, cued definition, recognition.\n\nWe follow advice set out by Bürkner & Vuorre (2019) in modeling the ordered categorical (i.e., ordinal) responses using a sequential ratio approach.\n\n\n\n\nYou will have noticed that the mixed-effects model coded in Section 1.11 incorporates a relatively simple random effect: a term specified to estimate the variance associated with the random effect of differences between participants in intercepts.\nAs we we have seen, more complex random effects structures may be warranted Matuschek et al. (2017). When we attempt to fit models with more complex structures, as we have discussed, for example, in ?@sec-dev-mixed-convergence-problems and ?@sec-glmm-bad-signs, we may run into convergence problems. (Such convergence problems are one reason why I tend to favour Bayesian methods; see, for exampe, the discussions in Bürkner & Vuorre (2019) and Liddell & Kruschke (2018).) There are ways to resolve these problems by changing the control parameters of the {ordinal} functions (see e.g. this discussion or see the information here) or by simplfying the model.\n\n\n\nWe discussed ordinal data and the reasons why we are motivated to analyze ordinal data using ordinal models.\nWe examine the coding required to fit ordinal models.\nWe look at the results outputs from ordinal models, and visualizations representing the predictions that can be generated given ordinal model estimates.\nWe consider the kinds of information that results reports should include.\nWe examine possible extensions to ordinal models.\n\n\nWe used two functions from the {ordinal} library to fit and evaluate ordinal models.\n\nWe used clm() to fit an ordinal model without random effects.\nWe used clmm() to fit an ordinal mixed-effects model with fixed effects and random effects.\n\n\n\n\n\nThe published example studies referred to in this chapter are published in (Ricketts et al., 2021; Rodríguez-Ferreiro et al., 2020).\nLiddell & Kruschke (2018) present a clear account of the problems associated with treating ordinal data as metric, and explain how we can better account for ordinal data.\nBürkner & Vuorre (2019) present a clear tutorial on cumulative and sequential ratio models.\nBoth Liddell & Kruschke (2018) and Bürkner & Vuorre (2019) work from a Bayesian perspective but the insights are generally applicable.\nGuides to the {ordinal} model functions clm() and clmm() are presented in (Christensen, 2015; Christensen, 2022).\n\n\n\n\n\n\nBaayen, R. H., Davidson, D. J., & Bates, D. M. (2008). Mixed-effects modeling with crossed random effects for subjects and items. Journal of Memory and Language, 59(4), 390–412. https://doi.org/10.1016/j.jml.2007.12.005\n\n\nBarr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of Memory and Language, 68, 255–278.\n\n\nBates, D., Kliegl, R., Vasishth, S., & Baayen, H. (2015). Parsimonious mixed models. arXiv Preprint arXiv:1506.04967.\n\n\nBürkner, P.-C., & Vuorre, M. (2019). Ordinal Regression Models in Psychology: A Tutorial. Advances in Methods and Practices in Psychological Science, 2(1), 77–101. https://doi.org/10.1177/2515245918823199\n\n\nCalloway, R. C. (2019). Why do you read? Toward a more comprehensive model of reading comprehension: The role of standards of coherence, reading goals, and interest [PhD thesis]. University of Pittsburg.\n\n\nChadwick, S. (2023). Metacomprehension accuracy of health-related information [PhD thesis]. Lancaster University.\n\n\nChristensen, R. H. B. (2015). Ordinal package for r. Version 3.4.2. 1–22. http://www.cran.r-project.org/package=ordinal/\n\n\nChristensen, R. H. B. (2022). Ordinal: Regression models for ordinal data. https://CRAN.R-project.org/package=ordinal\n\n\nDunlosky, J., & Lipko, A. R. (2007). Metacomprehension. Current Directions in Psychological Science, 16(4), 228–232. https://doi.org/10.1111/j.1467-8721.2007.00509.x\n\n\nLiddell, T. M., & Kruschke, J. K. (2018). Analyzing ordinal data with metric models: What could possibly go wrong? Journal of Experimental Social Psychology, 79, 328–348. https://doi.org/10.1016/j.jesp.2018.08.009\n\n\nMatuschek, H., Kliegl, R., Vasishth, S., Baayen, H., & Bates, D. (2017). Balancing type i error and power in linear mixed models. Journal of Memory and Language, 94, 305–315. https://doi.org/10.1016/j.jml.2017.01.001\n\n\nMeteyard, L., & Davies, R. A. I. (2020). Best practice guidance for linear mixed-effects models in psychological science. Journal of Memory and Language, 112, 104092. https://doi.org/10.1016/j.jml.2020.104092\n\n\nPreston, C. C., & Colman, A. M. (2000). Optimal number of response categories in rating scales: Reliability, validity, discriminating power, and respondent preferences. Acta Psychologica, 104(1), 1–15. https://doi.org/10.1016/S0001-6918(99)00050-5\n\n\nRatajczak, M. (2020). The effects of individual differences and linguistic features on reading comprehension of health-related texts [PhD thesis]. Lancaster University.\n\n\nRicketts, J., Dawson, N., & Davies, R. (2021). The hidden depths of new word knowledge: Using graded measures of orthographic and semantic learning to measure vocabulary acquisition. Learning and Instruction, 74, 101468. https://doi.org/10.1016/j.learninstruc.2021.101468\n\n\nRodríguez-Ferreiro, J., Aguilera, M., & Davies, R. (2020). Positive schizotypy increases the acceptance of unpresented materials in false memory tasks in non-clinical individuals. Frontiers in Psychology, 11. https://doi.org/10.3389/fpsyg.2020.00262\n\n\nShipley, W. C., Gruber, C. P., Martin, T. A., & Klein, A. M. (2009). Shipley-2 manual western psychological services. Western Psychological Services, 65.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/05-ordinal.html#sec-ordinal-motivations",
    "href": "PSYC412/part2/05-ordinal.html#sec-ordinal-motivations",
    "title": "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models",
    "section": "",
    "text": "Ordinal data are very common in psychological science.\nOften, we will encounter ordinal data recorded as responses to Likert-style items in which the participant is asked to indicate a response on an ordered scale ranging between two end points (Bürkner & Vuorre, 2019; Liddell & Kruschke, 2018). An example of a Likert item is: How well do you think you have understood this text? (Please check one response) where the participant must respond by producing a rating, by checking one option, given nine different response options ranging from 1 (not well at all) to 5 (very well).\nThe critical characteristics of ordinal data values (like the responses recorded to ratings scale, Likert-style, items) are that:\n\nThe responses are discrete or categorical — you must pick one (e.g., 1), you cannot pick more than one at the same time (e.g., 1 and 2), and you cannot have part or fractional values (e.g., you can’t choose the rating 1.5).\nThe responses are ordered — ranging from some minimum value up to some maximum value (e.g., 1 \\(\\rightarrow\\) 2 \\(\\rightarrow\\) 3 \\(\\rightarrow\\) 4 \\(\\rightarrow\\) 5).\n\nIn these materials, we are going to keep things simple by focusing on data comprising responses recorded to ratings scale Likert items. As we discuss, following, we often suppose that the response a participant produces to a rating question corresponds to a value on an underlying dimension that indirectly informs the participant response production process. But we should be aware that ordinal data can come from a variety of possible psychological mechanisms Ordinal data can also reflect processes in which participants have worked their way through a progression or sequence of decisions (see, e.g., Ricketts et al., 2021).\n\n\n\n\n\n\nNote\n\n\n\nOrdinal models: why do we need to do this?\n\nWe are aiming to develop skills and understanding of an approach to modeling that enables us to more accurately capture, in principle, the psychological processes that produce the data we analyse.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/05-ordinal.html#sec-ordinal-challenges",
    "href": "PSYC412/part2/05-ordinal.html#sec-ordinal-challenges",
    "title": "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models",
    "section": "",
    "text": "At first, we face two main challenges when working with ordinal data. The challenges are conceptual and social, more than statistical or computational.\nThe first challenge is conceptual because ordinal data values look like numbers but we cannot treat them like the numbers we are used to when we measure things like reaction time (RT), heat energy (temperature) or distance (e.g., height).\n\nWe typically encounter ordinal data (e.g., from Likert-style items, given rating scales) in contexts in which the response values that we observe look like numbers (e.g., 1) just like the numbers we handle in other contexts (e.g., RT or temperature or height). But those other numbers have what is known as metric properties (Liddell & Kruschke, 2018): Metric measurement scales assume that the numbers we observe are on interval or ratio scales.\n\n\nInterval scales define the distances between points somehow.\nRatio scales incorporate a zero point.\n\nWhereas ordinal data (e.g., responses to a rating question) have neither of these properties.\nThis means that, for ordinal data:\n\nWe cannot assume that the distances are equal between different values of the range of possible values, for example, for the rating scale: 1 \\(\\rightarrow\\) 2 \\(\\rightarrow\\) 3 \\(\\rightarrow\\) 4 \\(\\rightarrow\\) 5. This means that we do not know or cannot assume that the distance between, e.g., 1 \\(\\rightarrow\\) 2 is the same as the distance between, e.g., 3 \\(\\rightarrow\\) 4.\nWe do assume that all observed responses will take one of the categorical values between the minimum and the maximum values on the scale (e.g., between 1 and 5 on a 5-point rating scale ranging from 1-5).\n\nThe second challenge is social because, most of the time, Psychologists do in practice treat ordinal data like numeric data from measurement scales with metric properties. This is a mistake though it is very very common.\n\nLiddell & Kruschke (2018) analyzed a sample of papers from a set of top Psychology journals. They found that whenever an article mentioned Likert data (i.e., ratings data), the authors of that article analyzed the Likert-scale ordinal data as if they were metric, using analysis methods (like ANOVA, correlation, ordinary least squares regression (linear models), and t-tests) that assume metric properties.\n\nThis means that:\n\nMost Psychologists will assume that it is correct to analyse ordinal data using methods that assume metric properties.\nMost Psychologists will fail to look for, or to see, the problems associated with this conventional approach.\nThe results from many reports in the literature will be difficult to evaluate or interpret, where the authors have analysed ordinal data using methods assuming metric properties in outcome measurement scales.\n\nThe challenge we face is that we will aim to develop skills in using ordinal models when, in contrast, most psychological research articles will report analyses of ordinal data using conventional methods like ANOVA or linear regression. We will work to understand why ordinal models are better. We will learn that applying conventional methods to ordinal data will, in principle, involve a poor account of the data and, in practice, will create the risk of producing misleading results. And we will learn how to work with and interpret the results from ordinal models with or without random effects.\nIn our work in this chapter, we will rely extensively on the ideas set out by Bürkner & Vuorre (2019) and Liddell & Kruschke (2018), see Section 1.16.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/05-ordinal.html#sec-ordinal-ideas",
    "href": "PSYC412/part2/05-ordinal.html#sec-ordinal-ideas",
    "title": "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models",
    "section": "",
    "text": "Important\n\n\n\nThe point is that observed outcome response values, given ordinal data, look like numbers but it is best to understand ordinal data values as numeric labels for ordered categories.\n\nWhat the different categories correspond to — what dimension, or what data generating processing mechanism — depends on your psychological theory for the processes that drive response production for the task you constructed to collect or observe your data.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/05-ordinal.html#sec-ordinal-ideas-ratings",
    "href": "PSYC412/part2/05-ordinal.html#sec-ordinal-ideas-ratings",
    "title": "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models",
    "section": "",
    "text": "In the context of a study in which we have asked participants to produce a rating response to a Likert-style item, we may assume that the ratings responses are produced (in participants’ minds) by psychological processes in which the participant, in response to the Likert question, divides some latent (unobserved) psychological continuum or dimension into categories (or cut-points or thresholds) so that they can select a response option.\nImagine, for example, that you are a participant in a study on reading comprehension, and that you have been asked to read a text (giving information on some health condition) and have then been asked to respond to the question:\nHow well do you think you have understood this text? (Please check one response) (on the scale 1-5)\nIn theory, to answer this question, you will have to choose a response based on where you think you are on your unobserved (internal to you) measure of your understanding of the information in the text you have been asked to read. You may be able to evaluate the cohesion, or some other internal measure, of your understanding of the text. Simplifying a bit, we might assume that your internal measure of understanding is associated with a normal probability distribution so that it peaks over some value (e.g., 3) of the strength of understanding, though other values are possible.\nAs Figure 1 suggests, a participant in this situation will have to map where they are on the internal measure (the unobserved latent dimension of their understanding) to a number from the response options they are given (e.g., rating scale values ranging 1-5). They must cut or divide into categories (using unobserved threshold values) where they are on this unobserved internal dimension (this sense of their own understanding). And there is no reason to suppose that their internal measure of their understanding is divided into an ordered metric scale: we cannot assume that in a participant’s mind, e.g., the distance 1 \\(\\rightarrow\\) 2 is the same as the distance between, e.g., 3 \\(\\rightarrow\\) 4.\n\n\n\n\n\n\n\n\nFigure 1: A latent scale on the horizontal axis is divided into intervals or bins divided by thresholds marked by dotted lines. The cumulative normal probability in the intervals is the probability of the ordinal values.\n\n\n\n\n\nIn conducting analyses of ordinal data with ordinal models, we often fit models that describe the cumulative probability that a rating response maps to some value (typically, understood in terms of thresholds) on an underlying latent continuum.\nThis is a very common situation, and that is why we focus in this chapter on the corresponding class of parametric ordinal models: cumulative models. Bürkner & Vuorre (2019) provide a nice account of cumulative models, and we walk through that account here but note, before we go on, that this account is spelled out in the context of an example data-set and an example model that supposes some simplifying assumptions: we are talking about one example from a class of possible models; other models, with alternate assumptions, are possible, and you can expand your understanding by reading the papers referenced in Section 1.16.\nLet’s assume we have observed a set of responses to the question:\nHow well do you think you have understood this text? (Please check one response) (on the scale 1-5)\nOur cumulative model assumes that the observed ordinal data — the column of values, the rating responses we recorded from participants — variable \\(Y\\) are produced (by the participants) given categorization of a latent (not observed) continuous variable \\(\\tilde{Y}\\) into a set of different categories, using a set of threshold values.\nIn our analysis, we model the categorization process.\nWe assume that there are \\(K\\) thresholds \\(\\tau_k\\) which cut \\(\\tilde{Y}\\) into \\(K+1\\) observable ordered categories of \\(Y\\), where \\(Y\\) comprises the values of the response a participant can make (given the rating scale response options we give them).\nFor the latent variable we see in Figure 1, there are five (\\(K+1 = 5\\)) response categories so we need \\(K = 4\\) thresholds.\nBecause we can assume that \\(\\tilde{Y}\\) values derive from a specific probability distribution (e.g., a normal distribution, though there are alternatives), we can suppose that the probability that \\(Y\\) (the observed rating) has some value (e.g., a rating of 5), that is, the probability that \\(Y\\) is equal to category \\(k\\) as:\n\\[\nPr(Y=k) = F(\\tau_k) - F(\\tau_k-1)\n\\]\nYou can understand this probability we are targeting, in our analysis, as the probability that a rating response (made by a participant to our Likert-style item) is equal to \\(k\\) (e.g., a rating of 5) rather than some other value (e.g., on our 5-point example scale, a rating of 4 or 3 or 2 or 1), given the probability function \\(F\\).\n\nIf you reflect: this resembles the target for analysis when we are working with accuracy outcome data, and we wish to estimate the effects of factors that may predict the probability that the response a participant makes is correct (not incorrect).\n\nIn our model, we can aim to predict \\(\\tilde{Y}\\), given information about predictor variables. Let’s say, for our example, that we think that people who know more about health (so: score higher on measures of health literacy), are more likely to produce a high rating of their understanding of the information in a health text (maybe because they do understand the text better). If so, our model might look like this:\n\\[\n\\tilde{Y} = \\beta_X + \\varepsilon\n\\]\nwhere\n\n\\(\\tilde{Y}\\) is our latent (not observed) continuous variable,\n\n\\(\\beta_X\\) is the coefficient \\(\\beta\\) corresponding the average rate of change in the outcome, given different values in the predictor \\(X\\),\nand \\(\\varepsilon\\) is the random error of the model, the variance that cannot be explained by the model.\n\nFor short, we can identify \\(\\eta = \\beta_X + \\varepsilon\\).\n(Broadly the same model structure will apply, as we will see, for multilevel data or for nonlinear relationships.)\nGiven the probability function \\(F\\) we assumed, we model the probability of (observed) \\(Y\\) being equal to category \\(k\\) (some rating value versus other values), based on our prediction model \\(\\eta\\).\n\\[\nPr(Y=k | \\eta) = F(\\tau_k - \\eta) - F(\\tau_k-1 - \\eta)\n\\]\nWhat cumulative models do is to take the observed ratings values \\(Y\\) and, for the prediction model \\(\\beta_X + \\varepsilon\\), use that information to estimate the thresholds \\(\\tau_k\\) (for our example, thresholds \\(tau_1\\) to \\(\\tau_4\\)) and the coefficient \\(\\beta\\).\nIn doing this, we assume for simplicity that the effect of the predictors in the model \\(\\eta = \\beta_X + \\varepsilon\\) is constant across response categories though we can relax this assumption (if a predictor has different impacts on different categories).",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/05-ordinal.html#sec-nonordinal-ratings-problems",
    "href": "PSYC412/part2/05-ordinal.html#sec-nonordinal-ratings-problems",
    "title": "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models",
    "section": "",
    "text": "As I referenced earlier, we cannot just fit a linear model to ordinal data like ratings: we should not use methods like ANOVA, correlation, t-test, or regression.\n\nWhy not?\n\nThe short answer (Bürkner & Vuorre, 2019) is that we should not use methods like ANOVA, correlation, t-test, or regression to analyze ordinal data because:\n\nthese methods assume metric properties for outcome variables but ordinal data do not possess metric properties (equal intervals, zero points);\nthe distribution of ordinal responses may be non-normal (if low or high ratings are often chosen);\nand variances of the unobserved latent variable or dimension (\\(\\tilde{Y}\\)) that informs (we hope) observed ratings may be different for different groups or conditions.\n\nThe long answer Why not? is supplied by Liddell & Kruschke (2018) in a series of analyses of simulated and real data.\nThey argue that ordinal models supply a more accurate account of the data generating (psychological processes) that result in the ratings or the other ordinal data that we observe so, in principle, we should ordinal models anyway. We may differ in exactly what assumptions we want to make about things like the underlying probability distribution, or the impact of predictors on categories, but in general once we choose to employ ordinal models we are adopting a more realistic account of where our data come from.\nEven if we were to ignore this in principle justification, however, we should be aware of the problems associated with employing metric methods (ANOVA etc.) with ordinal data. These problems derive from the facts that observed ordinal values have fixed ranges (e.g., rating responses cannot exceed the minimum or maximum points on a rating scale), and while it is unknown how observed ratings map to (or correspond to categorization cuts or thresholds) of the underlying psychological dimension, it is also unknown but can be expected that variance in values on that latent dimension may vary between different groups. This means that mean observed ordinal data values may appear to differ, between groups or between conditions, when the values on the underlying latent dimension actually do not differ (we get false positives) or that mean observed ordinal data values may appear to be equal between groups or between conditions, though values on the underlying latent dimension actually do differ (false negatives). It also means that the power of analyses, or the capacity to detect effects that may in fact be present will be reduced where inappropriate modeling approaches are employed.\nLiddell & Kruschke (2018) note that common solutions — to the problem of using metric methods to analyze ordinal data — like ignoring the problem or like averaging ordinal responses will not work because the mis-estimation will be there whether we ignore the problem, we just will not know where, and because analyses of averaged ordinal outcomes will only result in mis-estimates of the underlying effects of interest.\nFortunately, we do not have to employ inappropriate methods. And, fortunately, these methods are relatively straightforward to apply. We look at the practical application of ordinal modeling methods to ordinal data, next.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/05-ordinal.html#sec-ordinal-targets",
    "href": "PSYC412/part2/05-ordinal.html#sec-ordinal-targets",
    "title": "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models",
    "section": "",
    "text": "Understand through practical experience the reasons for using ordinal models when we analyze ordinal outcome variables.\nPractice running ordinal models with varying random effects structures.\nPractice reporting the results of ordinal models, including through the use of prediction plots.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/05-ordinal.html#sec-ordinal-study-guide",
    "href": "PSYC412/part2/05-ordinal.html#sec-ordinal-study-guide",
    "title": "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models",
    "section": "",
    "text": "I have provided a collection of materials you can use. Here, I explain what they are and how I suggest you use them.\n1. Chapter: 05-ordinal\n1.1. I have written this chapter to discuss the main ideas and set out the practical steps you can follow to start to develop the skills required to work with ordered categorical outcomes i.e. ordinal data using ordinal models.\n1.2. The practical elements include data tidying, visualization and analysis steps.\n1.3. You can read the chapter and run the code to gain experience and encounter code you can adapt for your own purposes.\n\nRead in the example dataset.\nExperiment with the .R code used to work with the example data.\nRun ordinal models of demonstration data.\nRun ordinal models of alternate data sets (see links in Section 1.12).\nReview the recommended readings (Section 1.16).\n\n2. Practical materials\n2.1 In the following sections, I describe the practical steps, and associated resources, you can use for your learning. I set out the data tidying, analysis and visualization steps you can follow, working with the example dataset, described next.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/05-ordinal.html#sec-ordinal-data",
    "href": "PSYC412/part2/05-ordinal.html#sec-ordinal-data",
    "title": "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models",
    "section": "",
    "text": "We will be working, at first, with a sample of data collected as part of work in progress, undertaken for Clearly understood: health comprehension project (Davies and colleagues).\n\n\n\n\n\n\nWarning\n\n\n\nThese data are unpublished so should not be shared without permission.\n\n\n\n\n\n\nOur interest, in conducting the project, lies in identifying what factors make it easy or difficult to understand written health information. In part, we are concerned about the processes that health providers or clinicians apply to assure the effectiveness of the text they produce to guide patients or carers, for example, in taking medication, in making treatment decisions, or in order to follow therapeutic programmes.\nIt is common, in the quality assurance process in the production of health information texts, that text producers ask participants in patient review panels to evaluate draft texts. In such reviews, a participant may be asked a question like “How well do you understand this text?” This kind of question presents a metacognitive task: we are asking a participant to think about their thinking. But it is unclear that people can do this well or, indeed, what factors determine the responses to such questions (Dunlosky & Lipko, 2007).\nFor these reasons, we conducted studies in which we presented adult participants with sampled health information texts (taken from health service webpages) and, critically, asked them to respond to the question:\n\nHow well do you think you have understood this text?\n\nFor each text, in response to this question, participants were asked to click on one option from an array of response options ranging from (1) Not well at all to (9) Extremely well. The data we collected in this element of our studies comprise, clearly, ordinal responses. Thus, we may use these data to address the following research question.\n\n\n\n\n\n\nNote\n\n\n\n\nWhat factors predict self-evaluated rated understanding of health information.\n\n\n\n\n\n\nWe will work with a sample of participant data drawn from a series of Lancaster University undergraduate dissertation studies connected to the Clearly understood project. In these studies, we collected data from 202 participants on a series of measures (Section 1.8.1.3) of vocabulary knowledge, health literacy, reading strategy, as well as responses to health information texts. The distributions of participants’ scores on each of a range of attribute variables\n\n\n\n\n\n\n\n\nFigure 2: Grid of plots showing the distribution of participant attributes. The grid includes histograms of the distributions of: self-rated accuracy; vocabulary (SHIPLEY); health literacy (HLVA); reading strategy (FACTOR3); and age (years). We also see dot plots presenting counts of numbers of participants of different self-reported gender, education, and ethnicity categories.\n\n\n\n\n\nThe plots indicate:\n\nmost self-rated accuracy scores are high (over 6);\nmany participants with vocabulary scores greater than 30, a few present lower scores;\nhealth literacy scores centered on 8 or some, with lower and higher scores;\na skewed distribution of reading strategy scores, with many around 20-40, and a tail of higher scores;\nmost participants are 20-40 years of age, some older;\nmany more female than male participants, very few non-binary reported;\nmany more participants with higher education than further, very few with secondary;\nand many White participants (Office of National Statistics categories), far fewer Asian or Mixed or Black ethnicity participants.\n\n\n\n\nWe collected data through an online survey administered through Qualtrics.\nWe used the Shipley vocabulary sub-test (Shipley et al., 2009) to estimate vocabulary knowledge.\nWe used the Health Literacy Vocabulary Assessment (HLVA, Chadwick, 2023; Ratajczak, 2020) to estimate health literacy.\nWe used an instrument drawn from unpublished work by Calloway (2019) to assess the approach participants took to reading and understanding written information.\nWe presented participants with a sample of 20 health information texts. In the data collection process for this dataset, participants were recruited in multiple different studies. In each study, any one participant was presented with a randomly selected subset of the total of 20 texts.\nWe asked participants to rate their level of understanding of the health-related texts that we presented in the study. We used a nine-point judgment scales because they have been found to outperform alternative scales with fewer categories in terms of criterion validity, internal consistency, test-retest reliability, and discriminating power (Preston & Colman, 2000).\nWe recorded participants’ demographic characteristics: gender (coded: Male, Female, non-binary, prefer not to say); education (coded: Secondary, Further, Higher); and ethnicity (coded: White, Black, Asian, Mixed, Other).\n\n\n\n\nYou can download the 2021-22_PSYC304-health-comprehension.csv file holding the data we analyse in this chapter by clicking on the link.\n\n\n\nI am going to assume you have downloaded the data file, and that you know where it is. We use read_csv to read the data file into R.\n\nhealth &lt;- read_csv(\"2021-22_PSYC304-health-comprehension.csv\", \n                                 na = \"-999\",\n                                 col_types = cols(\n                                   ResponseId = col_factor(),\n                                   rating = col_factor(),\n                                   GENDER = col_factor(),\n                                   EDUCATION = col_factor(),\n                                   ETHNICITY = col_factor(),\n                                   NATIVE.LANGUAGE = col_factor(),\n                                   OTHER.LANGUAGE = col_factor(),\n                                   text.id = col_factor(),\n                                   text.question.id = col_factor(),\n                                   study = col_factor()\n                                 )\n                               )\n\nNotice that we use col_types = cols(...) to require read_csv() to class some columns as factors.\nImportantly, we ask R to treat the rating variable as a factor with rating = col_factor().\n\n\n\n\n\n\nTip\n\n\n\nIn the practical work we do, we will be using functions from the {ordinal} library to model ordinal data.\n\nIn using these functions, we need ask R to treat the ordinal outcome variable as a factor.\n\n\n\n\n\n\nIt is always a good to inspect what you have got when you read a data file in to R. Here, what may most concern us is the distribution of observed responses on the rating scale (responses to the “How well do you understand?” question). Figure 3 is a dot plot showing the distribution of ratings responses. The Likert-style questions in the surveys asked participants to rate their level of understanding of the texts they saw on a scale from 1 (not well) to 9 (extremely well). The plot shows the number of responses recorded for each response option, over all participants and all texts.\n\nhealth &lt;- health %&gt;% mutate(rating = fct_relevel(rating, sort))\n\nhealth %&gt;%\n  group_by(rating) %&gt;%\n  summarise(count = n()) %&gt;%\n  ggplot(aes(x = rating, y = count, colour = rating)) + \n  geom_point(size = 3) +\n  scale_color_viridis(discrete=TRUE, option = \"mako\") + theme_bw() +\n  theme(\n    panel.grid.major.y = element_blank()  # No horizontal grid lines\n  ) +\n  coord_flip()\n\n\n\n\n\n\n\nFigure 3: Dot plot showing the distribution of ratings responses. The Likert-style questions in the surveys asked participants to rate their level of understanding of the texts they saw on a scale from 1 (not well) to 9 (extremely well). The plot shows the number of responses recorded for each response option, over all participants and all texts.\n\n\n\n\n\nThe plot indicates that most participants chose response options 5-9, while very few rated their understanding at the lowest levels (options 1-4). Interestingly, many ratings responses around 7-8 were recorded: many more than responses at 5-6.\nIn analyzing these data, we will seek to estimate what information available to us can be used to predict whether a participant’s rating of their understanding is more likely to be, say, 1 or 2, 2 or 3 … 7 or 8, 8 or 9.\n\nOne practical way to think about the estimation problem when working with ratings-style ordinal data is this:\n\nWhat factors move or how do influential factors move the probability that the ordinal response is a relatively low or relatively high order response option?\nIn doing this, we do not have to assume that rating scale points map to equal sized intervals on the underlying latent scale where the scale may be an unobserved psychological continuum (like understanding).\n\n\nHere, we mostly have information on participant attributes and some information on text properties to do our prediction analyses. In other studies, we may be using information about experimental conditions, or selected groups of participants to estimate effects on variation in ratings responses.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/05-ordinal.html#sec-ordinal-data-tidy",
    "href": "PSYC412/part2/05-ordinal.html#sec-ordinal-data-tidy",
    "title": "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models",
    "section": "",
    "text": "The Clearly understood health comprehension project dataset is tidy (?@sec-intro-mixed-data-tidy):\n\nEach variable has its own column.\nEach observation has its own row.\nEach value has its own cell.\n\nHowever, there are aspects of the data structure or properties of the dataset variables that will cause inefficiencies or problems in later data analysis if we do not fix them first.\nYou can see what we have if you look at the results we get from using summary() and str() to inspect the dataset.\n\nsummary(health)\n\n             ResponseId        AGE                     GENDER    \n R_sKW4OJnOlidPxrH:  20   Min.   :18.0   Female           :2900  \n R_27paPJzIutLoqk8:  20   1st Qu.:20.0   Male             :1120  \n R_1nW0lpFdfumlI1p:  20   Median :27.0   Prefer-not-to-say:  20  \n R_31ZqPQpNEEapoW8:  20   Mean   :34.3                           \n R_2whvE2IW90nj2P7:  20   3rd Qu.:50.0                           \n R_3CAxrri9clBT7sl:  20   Max.   :81.0                           \n (Other)          :3920                                          \n     EDUCATION    ETHNICITY    NATIVE.LANGUAGE    OTHER.LANGUAGE\n Further  :1780   Asian: 680   English:2720    NA        :2720  \n Higher   :1800   White:3260   Other  :1320    Polish    : 580  \n Secondary: 460   Other:  40                   Cantonese : 280  \n                  Mixed:  60                   Chinese   : 120  \n                                               Portuguese:  60  \n                                               polish    :  60  \n                                               (Other)   : 220  \n ENGLISH.PROFICIENCY    SHIPLEY           HLVA           FACTOR3     \n Length:4040         Min.   :15.00   Min.   : 3.000   Min.   :17.00  \n Class :character    1st Qu.:30.00   1st Qu.: 7.000   1st Qu.:45.00  \n Mode  :character    Median :34.00   Median : 9.000   Median :49.00  \n                     Mean   :32.97   Mean   : 8.564   Mean   :49.03  \n                     3rd Qu.:37.00   3rd Qu.:10.000   3rd Qu.:55.00  \n                     Max.   :40.00   Max.   :13.000   Max.   :63.00  \n                                                                     \n     rating        response          RDFKGL       study    \n 8      :1044   Min.   :0.0000   Min.   : 4.552   cs: 480  \n 7      : 916   1st Qu.:1.0000   1st Qu.: 6.358   jg:1120  \n 9      : 824   Median :1.0000   Median : 8.116   ml: 720  \n 6      : 500   Mean   :0.8064   Mean   : 7.930   rw:1720  \n 5      : 352   3rd Qu.:1.0000   3rd Qu.: 9.413            \n 4      : 176   Max.   :1.0000   Max.   :13.278            \n (Other): 228                                              \n             text.id                  text.question.id\n studyone.TEXT.37: 344   studyone.TEXT.37.CQ.1:  86   \n studyone.TEXT.39: 344   studyone.TEXT.37.CQ.2:  86   \n studyone.TEXT.72: 344   studyone.TEXT.37.CQ.3:  86   \n studyone.TEXT.14: 344   studyone.TEXT.37.CQ.4:  86   \n studyone.TEXT.50: 344   studyone.TEXT.39.CQ.1:  86   \n studyone.TEXT.10: 224   studyone.TEXT.39.CQ.2:  86   \n (Other)         :2096   (Other)              :3524   \n\n\nYou should be used to seeing the summary() of a dataset, showing summary statistics of numeric variables and counts of the numbers of observations of data coded at different levels for each categorical or nominal variable classed as a factor.\nUsing the str() function may be new to you and, as you can see, the output from the function call gives you a bit more information on how R interprets the data in the variable columns. You can see that each variable is listed alongside information about how the data in the column are interpreted (as Factor or num numeric). Where we have columns holding information on factors there we see information about the levels.\nRecall that for a categorical or nominal variable e.g. ETHNICITY, provided R interprets the variable as a factor, each data value in the column is coded as corresponding to one level i.e. group or class or category (e.g., we have ETHNICITY classes \"Asian\" etc.) Recall, also, that at the data read-in stage, we instructed R how we wanted it to interpret each column using col_types = cols().\n\nstr(health)\n\ntibble [4,040 × 17] (S3: tbl_df/tbl/data.frame)\n $ ResponseId         : Factor w/ 202 levels \"R_sKW4OJnOlidPxrH\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ AGE                : num [1:4040] 20 20 20 20 20 20 20 20 20 20 ...\n $ GENDER             : Factor w/ 3 levels \"Female\",\"Male\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ EDUCATION          : Factor w/ 3 levels \"Further\",\"Higher\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ ETHNICITY          : Factor w/ 4 levels \"Asian\",\"White\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ NATIVE.LANGUAGE    : Factor w/ 2 levels \"English\",\"Other\": 1 1 1 1 1 1 1 1 1 1 ...\n $ OTHER.LANGUAGE     : Factor w/ 17 levels \"NA\",\"Catonese\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ ENGLISH.PROFICIENCY: chr [1:4040] \"NA\" \"NA\" \"NA\" \"NA\" ...\n $ SHIPLEY            : num [1:4040] 26 26 26 26 26 26 26 26 26 26 ...\n $ HLVA               : num [1:4040] 8 8 8 8 8 8 8 8 8 8 ...\n $ FACTOR3            : num [1:4040] 59 59 59 59 59 59 59 59 59 59 ...\n $ rating             : Factor w/ 9 levels \"1\",\"2\",\"3\",\"4\",..: 8 8 8 8 7 7 7 7 7 7 ...\n $ response           : num [1:4040] 1 1 1 1 1 1 1 0 1 1 ...\n $ RDFKGL             : num [1:4040] 10.61 10.61 10.61 10.61 8.12 ...\n $ study              : Factor w/ 4 levels \"cs\",\"jg\",\"ml\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ text.id            : Factor w/ 20 levels \"studyone.TEXT.105\",..: 1 1 1 1 2 2 2 2 3 3 ...\n $ text.question.id   : Factor w/ 80 levels \"studyone.TEXT.105.CQ.1\",..: 1 2 3 4 5 6 7 8 9 10 ...\n\n\nOur specific concern, here, is that the rating response variable is treated as a factor because the {ordinal} library we are going to use to do the modeling must find the outcome variable is a factor.\nWe can focus str() on the rating variable. We see that it is being treated as a factor.\n\nstr(health$rating)\n\n Factor w/ 9 levels \"1\",\"2\",\"3\",\"4\",..: 8 8 8 8 7 7 7 7 7 7 ...\n\n\nHowever, we also need to make sure that the rating outcome variable is being treated as an ordered factor.\nWe can perform a check as follows. (I found how to do this here)\n\nis.ordered(factor(health$rating))\n\n[1] FALSE\n\n\nWe can see that the variable is not being treated as an ordered factor. We need to fix that.\nThe ordinal model estimates the locations (thresholds) for where to split the latent scale (the continuum underlying the ratings) corresponding to different ratings values. If we do not make sure that the outcome factor variable is split as it should be then there is no guarantee that {ordinal} functions will estimate the thresholds in the right order (i.e., 1,2,3 ... rather than 3,2,1...).\nWe can make sure that the confidence rating factor is ordered precisely as we wish using the ordered() function.\n\nhealth$rating &lt;- ordered(health$rating,\n                         levels = c(\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"))\n\nWe can then do a check to see that we have got what we want. We do not wantrating to be treated as numeric, we do want it to be treated as an ordered factor.\n\nis.numeric(health$rating)\n\n[1] FALSE\n\nis.factor(health$rating)\n\n[1] TRUE\n\nstr(health$rating)\n\n Ord.factor w/ 9 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 8 8 8 8 7 7 7 7 7 7 ...\n\nis.ordered(health$rating)\n\n[1] TRUE\n\n\nIt is.\nNext, before doing any modelling, it will be sensible to standardize potential predictors\n\nhealth &lt;- health %&gt;% \n  mutate(across(c(AGE, SHIPLEY, HLVA, FACTOR3, RDFKGL), \n                scale, center = TRUE, scale = TRUE,\n                .names = \"z_{.col}\"))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(...)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\nYou can see that in this chunk of code, we are doing a number of things:\n\nhealth &lt;- health %&gt;% recreates the health dataset from the following steps.\nmutate(...) do an operation which retains the existing variables in the dataset, to change the variables as further detailed.\nacross(...) work with the multple column variables that are named in the c(AGE, SHIPLEY, HLVA, FACTOR3, RDFKGL) set.\n...scale, center = TRUE, scale = TRUE... here is where we do the standardization work.\n\nWhat we are asking for is that R takes the variables we name and standardizes each of them.\n\n.names = \"z_{.col}\") creates the standardized variables under adapted names, adding z_ to the original column name so that we can distinguish between the standardized and original raw versions of the data columns.\n\nNote that the across() function is a useful function for applying a function across multiple column variables see information here There is a helpful discussion on how we can do this task here\nWe can then check that we have produced the standardized variables as required.\n\nsummary(health)\n\n             ResponseId        AGE                     GENDER    \n R_sKW4OJnOlidPxrH:  20   Min.   :18.0   Female           :2900  \n R_27paPJzIutLoqk8:  20   1st Qu.:20.0   Male             :1120  \n R_1nW0lpFdfumlI1p:  20   Median :27.0   Prefer-not-to-say:  20  \n R_31ZqPQpNEEapoW8:  20   Mean   :34.3                           \n R_2whvE2IW90nj2P7:  20   3rd Qu.:50.0                           \n R_3CAxrri9clBT7sl:  20   Max.   :81.0                           \n (Other)          :3920                                          \n     EDUCATION    ETHNICITY    NATIVE.LANGUAGE    OTHER.LANGUAGE\n Further  :1780   Asian: 680   English:2720    NA        :2720  \n Higher   :1800   White:3260   Other  :1320    Polish    : 580  \n Secondary: 460   Other:  40                   Cantonese : 280  \n                  Mixed:  60                   Chinese   : 120  \n                                               Portuguese:  60  \n                                               polish    :  60  \n                                               (Other)   : 220  \n ENGLISH.PROFICIENCY    SHIPLEY           HLVA           FACTOR3     \n Length:4040         Min.   :15.00   Min.   : 3.000   Min.   :17.00  \n Class :character    1st Qu.:30.00   1st Qu.: 7.000   1st Qu.:45.00  \n Mode  :character    Median :34.00   Median : 9.000   Median :49.00  \n                     Mean   :32.97   Mean   : 8.564   Mean   :49.03  \n                     3rd Qu.:37.00   3rd Qu.:10.000   3rd Qu.:55.00  \n                     Max.   :40.00   Max.   :13.000   Max.   :63.00  \n                                                                     \n     rating        response          RDFKGL       study    \n 8      :1044   Min.   :0.0000   Min.   : 4.552   cs: 480  \n 7      : 916   1st Qu.:1.0000   1st Qu.: 6.358   jg:1120  \n 9      : 824   Median :1.0000   Median : 8.116   ml: 720  \n 6      : 500   Mean   :0.8064   Mean   : 7.930   rw:1720  \n 5      : 352   3rd Qu.:1.0000   3rd Qu.: 9.413            \n 4      : 176   Max.   :1.0000   Max.   :13.278            \n (Other): 228                                              \n             text.id                  text.question.id\n studyone.TEXT.37: 344   studyone.TEXT.37.CQ.1:  86   \n studyone.TEXT.39: 344   studyone.TEXT.37.CQ.2:  86   \n studyone.TEXT.72: 344   studyone.TEXT.37.CQ.3:  86   \n studyone.TEXT.14: 344   studyone.TEXT.37.CQ.4:  86   \n studyone.TEXT.50: 344   studyone.TEXT.39.CQ.1:  86   \n studyone.TEXT.10: 224   studyone.TEXT.39.CQ.2:  86   \n (Other)         :2096   (Other)              :3524   \n           z_AGE.V1                   z_SHIPLEY.V1        \n Min.   :-9.82624656687e-01   Min.   :-3.294105320780000  \n 1st Qu.:-8.62035239524e-01   1st Qu.:-0.543722537102000  \n Median :-4.39972279452e-01   Median : 0.189712871877000  \n Mean   : 4.20000000000e-15   Mean   :-0.000000000000001  \n 3rd Qu.: 9.46806017926e-01   3rd Qu.: 0.739789428612000  \n Max.   : 2.81594198396e+00   Max.   : 1.289865985350000  \n                                                          \n          z_HLVA.V1                  z_FACTOR3.V1        \n Min.   :-2.60748865140e+00   Min.   :-4.20593553983000  \n 1st Qu.:-7.33066204487e-01   1st Qu.:-0.52915479589300  \n Median : 2.04145018971e-01   Median :-0.00390040390093  \n Mean   : 1.00000000000e-16   Mean   : 0.00000000000000  \n 3rd Qu.: 6.72750630700e-01   3rd Qu.: 0.78398118408700  \n Max.   : 2.07856746589e+00   Max.   : 1.83448996807000  \n                                                         \n         z_RDFKGL.V1         \n Min.   :-1.46446595688e+00  \n 1st Qu.:-6.81459422242e-01  \n Median : 8.07363074868e-02  \n Mean   : 6.99000000000e-14  \n 3rd Qu.: 6.43061598419e-01  \n Max.   : 2.31876495189e+00",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/05-ordinal.html#sec-ordinal-working-models-clm",
    "href": "PSYC412/part2/05-ordinal.html#sec-ordinal-working-models-clm",
    "title": "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models",
    "section": "",
    "text": "In our first analysis, we can begin by assuming no random effects. We keep things simple at this point so that we can focus on the key changes in model coding.\nThe model is conducted to examine what shapes the variation in rating responses that we see in Figure 3.\n\n\n\n\n\n\nNote\n\n\n\n\nWhat factors predict self-evaluated rated understanding of health information.\n\n\n\nIn our analysis, the outcome variable is the ordinal response variable rating. The predictors consist of the variables we standardized earlier. We use the clm() function from the {ordinal} library to do the analysis.\nI will give information in outline here, the interested reader can see more detailed information in Christensen (2022) and Christensen (2015).\nYou can also find the manual for the {ordinal} library functions here\nWe code the model as follows.\n\nhealth.clm &lt;- clm(rating ~\n                    \n                    z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL,\n                  \n                  Hess = TRUE, link = \"logit\",\n                  data = health)\n\nsummary(health.clm)\n\nThe code works as follows.\nFirst, we have a chunk of code mostly similar to what we have done before, but changing the function.\n\nclm() the function name changes because now we want a cumulative link model of the ordinal responses.\n\nThe model specification includes information about the fixed effects, the predictors: z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL.\nSecond, we have the bit that is specific to cumulative link models fitted using the clm() function.\n\nHess = TRUE is required if we want to get a summary of the model fit; the default is TRUE but it is worth being explicit about it.\nlink = \"logit\" specifies that we want to model the ordinal responses in terms of the log odds (hence, the probability) that a response is a low or a high rating value.\n\n\n\nIf you run the model code, it may take a few seconds to run. Then you will get the results shown in the output.\n\n\nformula: rating ~ z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL\ndata:    health\n\n link  threshold nobs logLik   AIC      niter max.grad cond.H \n logit flexible  4040 -6880.78 13787.55 5(0)  9.26e-07 7.3e+01\n\nCoefficients:\n          Estimate Std. Error z value Pr(&gt;|z|)    \nz_AGE     -0.17719    0.02966  -5.975 2.30e-09 ***\nz_SHIPLEY  0.34384    0.03393  10.135  &lt; 2e-16 ***\nz_HLVA     0.16174    0.03265   4.954 7.28e-07 ***\nz_FACTOR3  0.74535    0.03145  23.699  &lt; 2e-16 ***\nz_RDFKGL  -0.27220    0.02892  -9.412  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2 -4.65066    0.12978 -35.836\n2|3 -4.13902    0.10395 -39.817\n3|4 -3.26845    0.07390 -44.228\n4|5 -2.56826    0.05804 -44.248\n5|6 -1.69333    0.04437 -38.160\n6|7 -0.87651    0.03671 -23.876\n7|8  0.24214    0.03402   7.117\n8|9  1.63049    0.04252  38.346\n\n\nThe summary() output for the model is similar to the outputs you have seen for other model types.\n\nWe first get formula: information about the model you have specified.\nR will tell us what data: we are working with.\nWe then get Coefficients: estimates.\n\nThe table summary of coefficients arranges information in ways that will be familiar you:\n\nFor each predictor variable, we see Estimate, Std. Error, z value, and Pr(&gt;|z|) statistics.\nThe Pr(&gt;|z|) p-values are based on Wald tests of the null hypothesis that a predictor has null impact.\nThe coefficient estimates can be interpreted based on whether they are positive or negative.\n\n\nWe then get Threshold coefficients: indicating where the model fitted estimates the threshold locations: where the latent scale is cut, corresponding to different rating values.\n\n\n\n\n\n\n\nTip\n\n\n\nIn reporting ordinal (e.g., cumulative link) models, we typically focus on the coefficient estimates for the predictor variables.\n\nA positive coefficient estimate indicates that higher values of the predictor variable are associated with greater probability of higher rating values.\nA negative coefficient estimate indicates that higher values of the predictor variable are associated with greater probability of lower rating values.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/05-ordinal.html#sec-ordinal-working-models-clmm",
    "href": "PSYC412/part2/05-ordinal.html#sec-ordinal-working-models-clmm",
    "title": "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models",
    "section": "",
    "text": "In our analysis, we begin by assuming no random effects. However, this is unlikely to be appropriate given the data collection process deployed in the Clearly understood projects, where a sample of participants were asked to respond to a sample of texts:\n\nwe have multiple observations of responses for each participant;\nwe have multiple observations of responses for each stimulus text;\nparticipants were assigned to groups, and within a group all participants were asked to respond to the same stimulus texts.\n\nThese features ensure that the data have a multilevel structure and this structure requires us to fit a Cumulative Link Mixed-effects Model (CLMM).\nWe keep things simple at this point so that we can focus on the key changes in model coding. We can code a Cumulative Link Mixed-effects Model as follows.\n\nhealth.clmm &lt;- clmm(rating ~\n                      \n            z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL +\n                      \n            (1 | ResponseId),\n                    \n                    Hess = TRUE, link = \"logit\",\n                    data = health)\n\nsummary(health.clmm)\n\nIf you inspect the code chunk, you can see that we have made two changes.\nFirst, we have changed the function.\n\nclmm() the function name changes because now we want a Cumulative Linear Mixed-effects Model.\n\nSecondly, the model specification includes information about fixed effects and now about random effects.\n\nWith (1 | ResponseId) we include random effects of participants on on intercepts.\n\n\n\nIf you run the model code, you will see that the model may take several seconds, possibly a minute or two to complete. We will then get the results shown in the output.\n\n\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: rating ~ z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL +  \n    (1 | ResponseId)\ndata:    health\n\n link  threshold nobs logLik   AIC     niter       max.grad cond.H \n logit flexible  4040 -4978.08 9984.16 1490(19081) 3.86e-03 3.5e+02\n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ResponseId (Intercept) 9.825    3.134   \nNumber of groups:  ResponseId 202 \n\nCoefficients:\n          Estimate Std. Error z value Pr(&gt;|z|)    \nz_AGE     -0.44314    0.23473  -1.888  0.05905 .  \nz_SHIPLEY  0.77129    0.26498   2.911  0.00361 ** \nz_HLVA     0.20813    0.25608   0.813  0.41637    \nz_FACTOR3  1.68347    0.23830   7.065 1.61e-12 ***\nz_RDFKGL  -0.44346    0.03677 -12.060  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2  -9.3297     0.3452 -27.026\n2|3  -8.1413     0.2947 -27.629\n3|4  -6.5243     0.2631 -24.796\n4|5  -5.2159     0.2490 -20.947\n5|6  -3.4668     0.2370 -14.629\n6|7  -1.8481     0.2315  -7.984\n7|8   0.2966     0.2295   1.292\n8|9   3.0417     0.2346  12.966\n\n\nYou can see that the output summary presents the same structure. If you compare the output you see in Section 1.10.1, however, you will notice some similarities and some differences:\n\nIf you focus first on the estimates of the coefficients for the predictor variables, you will see that the estimates have the same sign (positive or negative) as they had before.\nHowever, you will see that the estimates have different magnitudes.\nYou will also see that the p-values are different.\n\nStudents often focus on p-values in reading model summaries. This is mistaken for multiple reasons.\n\nThe p-values correspond to the probabilities associated with the null hypothesis significance test: the test of the hypothesis that the effect of the predictor is null (i.e. the predictor has no impact).\nThis null assumption is made whichever model we are looking at. The p-values do not indicate whether an effect is more or less probable. (But you do get such posterior probabilities in Bayesian analyses.)\nSo it does not really mean much, though it is common, to talk about effects being highly significant.\n\nThus it should not worry us too much if the p-values are significant in one analysis but not significant in another.\nThat said, it is interesting, perhaps, that once we include random effects of participants on intercepts in our analysis then the effects of z_AGE and z_HLVA are no longer significant. I would be tempted to ask if the previously significant effects of these variables owed their impact to random differences between participants in their average or overall level of rating response.\n\n\n\nIt will be helpful for the interpretation of the estimates of the coefficients of these predictor variables if we visualize the predictions we can make, about how rating values vary, given differences in predictor variable values, given our model estimates. We can do this using functions from the {ggeffects} library. You can read more about the {ggeffects} library here where you will see a collection of articles explaining what you can do, and why, as well as technical information including some helpful tutorials.\nThe basic model prediction coding looks like this.\n\ndat &lt;- ggpredict(health.clmm, terms=\"z_FACTOR3 [all]\")\nplot(dat)\n\nFigure 4 shows you the marginal effect of variation in the reading strategy attribute, i.e., the effect of differences between individuals in how they score on the FACTOR3 measure of reading strategy. Note that the variable is listed as z_FACTOR3 because, as you will recall, we standardized numeric predictor variables before entering them in our model.\nYou can find a discussion of marginal effects in the context of working with the {ggeffects} library:\n\nhere and\nhere.\n\nYou can find an extensive, helpful (with examples) discussion of marginal effects by Andrew Heiss here.\nIn short, what we want to do is to take the model coefficient estimates, and generate predictions with these estimates, given different values of the predictor variable, while holding the other predictor variables at some constant or some level (or some series of values).\nIf you look at the code chunk, you can see that we first:\n\ndat &lt;- ggpredict(health.clmm, terms=\"z_FACTOR3 [all]\")\n\n\nIn this line, we use ggpredict() to work with some model information, assuming we previously fitted a model and gave it a name (here, health.clmm).\n\nNote that if you fit the model and call it health.clmm, as we did in Section 1.11, then an object of that name is created in the R workspace or environment. If you click on that object name in the environment window in R-Studio, you will see that there is a list of pieces of information about the model, including the coefficient estimates, the model formula etc. associated with that name.\n\nSo when we use ggpredict(), we ask R to take that model information and, for the term we specify, here, specify using terms=\"z_FACTOR3 [all]\", we ask R to generate some predictions.\ndat &lt;- ggpredict(...) asks R to put those predictions in an object called dat.\n\nIf you click on that object name in the environment window in R-Studio, you will see that it comprises a dataset. The dataset includes the columns:\n\nx giving different values of the predictor variable. ggpredict() will choose some ‘representative’ values for you but you can construct a set of values of the predictor for which you want predictions.\npredicted holds predicted values, given different predictor x values.\n\nIf you then run the line plot(dat) you can see what this gets us for these kinds of models. Figure 4 presents a grid of plots showing the model-predicted probabilities that a rating response will have one value for each of the 1-9 rating response values that are possible given the Likert rating scale used in data collection. In the grid, a different plot is shown for each possible response value, indicating how the probability varies that the rating response will take that value.\n\ndat &lt;- ggpredict(health.clmm, terms=\"z_FACTOR3 [all]\")\nplot(dat)\n\n\n\n\n\n\n\nFigure 4: A grid of plots showing marginal or conditional predicted probabilities that a rating response will have one value (among the 1-9 rating values possible), indicating how these predicted probabilities vary given variation in values of the standardized reading strategy (FACTOR3) variable.\n\n\n\n\n\nIf you examine Figure 4, you can recognize that we have one plot for each different value of the response options available for the Likert-scale rating items: 1-9. You can also see that in each plot we get a curve. In some cases – for rating response values 1-4 – the curve is flat or flattens very quickly, for higher levels of the z_FACTOR3 variable. In some cases – for rating response values 5-9 – the curve is more obvious, and resembles a normal distribution curve.\nIf you think about it, what these plots indicate are the ways in which the probability that a rating response is a low value (e.g., a rating of 1) or a high value (e.g., a rating of 9) rises or falls. Each possible rating response is associated with a probability distribution. For example, look at the plot labelled 6: that shows you the probability distribution indicating how the probability varies that a response will take the value 6. We can see that the distribution is normal in shape, a bell-shaped curve. We can see that the peak of the curve is over the z_FACTOR3 score (shown on the x-axis) of about 1.5. We can see that the probability represented by the height of the line showing the curve is lower for z_FACTOR3 scores lower than the score under the peak (e.g. scores less than z_FACTOR3 \\(=2\\)). The probability represented by the height of the line showing the curve is lower for z_FACTOR3 scores higher than the score under the peak (e.g. scores greater than z_FACTOR3 \\(=1\\)).\nWe can see that the peak of the normal curve, in the case of rating response values 5-9, is located at different places on the horizontal axis. Look at each of the plots labelled 5-9. Notice how the horizontal location of the curves shifts as z_FACTOR3 scores increase. If you go from left to right, i.e. from low to high values of z_FACTOR3, on each plot then you will see that the peak of the curve is located in different places: going from plot 5 to plot 9 the peak of the curve moves rightwards. These curves show how the probability that a rating response takes a high value (e.g. 9 instead of 8 or 8 instead of 7 etc.) is higher for higher values of z_FACTOR3. This idea might be a bit clearer if we draw the plot in a different way.\nFigure 5 shows the same model predictions but plots the predictions of the way that probability changes, for each rating response, by superimposing the plots for each response value, one on top of the other. I have drawn each probability curve in a different colour, and these colours match those used to present the counts of different response values shown in Figure 3.\n\ndat &lt;- ggpredict(health.clmm, terms=\"z_FACTOR3 [all]\")\nggplot(dat, aes(x, predicted, \n                colour = response.level)) + \n  geom_line(size = 1.5) +\n  scale_color_viridis(discrete=TRUE, option = \"mako\") + \n  labs(x = \"Reading strategy (z_FACTOR3)\", y = \"Predicted probability of a rating\") +\n  guides(colour = guide_legend(title = \"Rating\")) +\n  ylim(0, 1) +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 5: A plot showing marginal or conditional predicted probabilities that a rating response will have one value (among the 1-9 rating values possible), indicating how these predicted probabilities vary given variation in values of the standardized reading strategy (FACTOR3) variable\n\n\n\n\n\nYou can read Figure 5 by observing that:\n\nFor low value ratings e.g. for rating responses from 1-4, there is not much predicted probability that a response with such a value will be made (flat lines) but if they are going to be made they are likely to be made by people with low scores on the z_FACTOR3.\n\nYou can see this because you can see how the curves peak around low values of z_FACTOR3. This should make sense: people with low scores on reading strategy are maybe not doing reading effectively, are maybe as a result not doing well in understanding the texts they are given to read, and thus are not confident about their understanding. (This is a speculative causal theory but it will suffice for now.)\nRecall, also, that as Figure 3 indicated, in the Clearly understood health comprehension dataset, we saw that few rating responses were recorded for low value ratings of understanding. Few people in our sample made rating responses by choosing ratings of 1 or 2 to indicate low levels of understanding.\nFigure 5 also suggests that:\n\nFor higher value rating responses – responses representing ratings from 5 to 9 – there is variation in the probability that responses with such values will be made.\nThat variation in probability is shown by the probability distribution curves.\nFor these data, and this model, we can see that the probability shifts suggesting that participants in our sample were more likely to choose a higher value rating if they were also presenting high scores on the z_FACTOR3 measure of reading strategy.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/05-ordinal.html#sec-ordinal-reporting-results",
    "href": "PSYC412/part2/05-ordinal.html#sec-ordinal-reporting-results",
    "title": "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models",
    "section": "",
    "text": "As the review reported by Liddell & Kruschke (2018) suggests, we may have many many studies in which ordinal outcome data are analysed but very few published research reports that present analyses of ordinal data using ordinal models.\nYou can see two examples in the papers published by Ricketts et al. (2021) and by Rodríguez-Ferreiro et al. (2020). These papers are both published open accessible, so that they are freely available, and they are both associated with accessible data repositories.\n\nYou can find the repository for Ricketts et al. (2021) here.\nYou can find the repository for Rodríguez-Ferreiro et al. (2020) here.\n\nThe Rodríguez-Ferreiro et al. (2020) shares a data .csv only.\nThe Ricketts et al. (2021) repository shares data and analysis code as well as a fairly detailed guide to the analysis methods. Note that the core analysis approach taken in Ricketts et al. (2021) is based on Bayesian methods but that we also conduct clmm() models using the {ordinal} library functions discussed here; these models are labelled frequentist models and can be found under sensitivity analyses.\nFor what it’s worth, the Ricketts et al. (2021) is much more representative of the analysis approach I would recommend now.\nWhatever the specifics of your research question, dataset, analysis approach or model choices, I would recommend the following for your results report.\n\nExplain the model – the advice extended by Meteyard & Davies (2020) still apply: the reader will need to know:\n\n\nThe identity of the outcome and predictor variables;\nThe reason why you are using an ordinal approach, explaining the ordinal (ordered, categorical) nature of the outcome;\nThe structure of the fixed effects part of the model, i.e. the effects, in what form (main effects, interactions) you are seeking to estimate;\nAnd the structure of the random effects part of the model, i.e. what grouping variable (participants? items?), whether they encompass random intercepts or random slopes or covariances.\n\nYou can report or indicate some of this information by presenting a table summary of the effects estimated in your model (e.g., see Table 5, Rodríguez-Ferreiro et al., 2020; see tables 2 and 3, Ricketts et al., 2021). Journal formatting restrictions or other conventions may limit what information you can present.\nNotice that I do not present information on threshold estimates.\n\nExplain the results – I prefer to show and tell.\n\n\nPresent conditional or marginal effects plots (see figures 2 and 3, Ricketts et al., 2021) to indicate the predictions you can make given your model estimates.\nAnd explain what the estimates or what the prediction plots appear to show.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/05-ordinal.html#sec-ordinal-extensions",
    "href": "PSYC412/part2/05-ordinal.html#sec-ordinal-extensions",
    "title": "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models",
    "section": "",
    "text": "As I hint, when we discuss the concept that ordinal responses may map somehow to a latent unobserved underlying continuum (see Figure 1), there are other ways to think about ordinal data. Rather, there are other ways to think about the psychological mechanisms or the data generating mechanisms that give rise to the ordinal responses we analyse.\nIn Ricketts et al. (2021), we explain:\n\nIn the semantic post-test, participants worked their way through three steps, only progressing from one step to the next step if they provided an incorrect response or no response. Given the sequential nature of this task, we analysed data using sequential ratio ordinal models (Bürkner & Vuorre, 2019). In sequential models, we account for variation in the probability that a response falls into one response category (out of k ordered categories), equal to the probability that it did not fall into one of the foregoing categories, given the linear sum of predictors. We estimate the k-1 thresholds and the coefficients of the predictors.\n\nWhat this explanation refers to is the fact that, in our study:\n\nThe semantic post-test assessed knowledge for the meanings of newly trained words. We took a dynamic assessment or cuing hierarchy approach (Hasson & Joffe, 2007), providing children with increasing support to capture partial knowledge and the incremental nature of acquiring such knowledge (Dale, 1965). Each word was taken one at a time and children were given the op- portunity to demonstrate knowledge in three steps: definition, cued definition, recognition.\n\nWe follow advice set out by Bürkner & Vuorre (2019) in modeling the ordered categorical (i.e., ordinal) responses using a sequential ratio approach.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/05-ordinal.html#richly-parameterized-mixed-effects-models",
    "href": "PSYC412/part2/05-ordinal.html#richly-parameterized-mixed-effects-models",
    "title": "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models",
    "section": "",
    "text": "You will have noticed that the mixed-effects model coded in Section 1.11 incorporates a relatively simple random effect: a term specified to estimate the variance associated with the random effect of differences between participants in intercepts.\nAs we we have seen, more complex random effects structures may be warranted Matuschek et al. (2017). When we attempt to fit models with more complex structures, as we have discussed, for example, in ?@sec-dev-mixed-convergence-problems and ?@sec-glmm-bad-signs, we may run into convergence problems. (Such convergence problems are one reason why I tend to favour Bayesian methods; see, for exampe, the discussions in Bürkner & Vuorre (2019) and Liddell & Kruschke (2018).) There are ways to resolve these problems by changing the control parameters of the {ordinal} functions (see e.g. this discussion or see the information here) or by simplfying the model.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/05-ordinal.html#sec-ordinal-summary",
    "href": "PSYC412/part2/05-ordinal.html#sec-ordinal-summary",
    "title": "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models",
    "section": "",
    "text": "We discussed ordinal data and the reasons why we are motivated to analyze ordinal data using ordinal models.\nWe examine the coding required to fit ordinal models.\nWe look at the results outputs from ordinal models, and visualizations representing the predictions that can be generated given ordinal model estimates.\nWe consider the kinds of information that results reports should include.\nWe examine possible extensions to ordinal models.\n\n\nWe used two functions from the {ordinal} library to fit and evaluate ordinal models.\n\nWe used clm() to fit an ordinal model without random effects.\nWe used clmm() to fit an ordinal mixed-effects model with fixed effects and random effects.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/05-ordinal.html#sec-ordinal-recommended-reading",
    "href": "PSYC412/part2/05-ordinal.html#sec-ordinal-recommended-reading",
    "title": "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models",
    "section": "",
    "text": "The published example studies referred to in this chapter are published in (Ricketts et al., 2021; Rodríguez-Ferreiro et al., 2020).\nLiddell & Kruschke (2018) present a clear account of the problems associated with treating ordinal data as metric, and explain how we can better account for ordinal data.\nBürkner & Vuorre (2019) present a clear tutorial on cumulative and sequential ratio models.\nBoth Liddell & Kruschke (2018) and Bürkner & Vuorre (2019) work from a Bayesian perspective but the insights are generally applicable.\nGuides to the {ordinal} model functions clm() and clmm() are presented in (Christensen, 2015; Christensen, 2022).\n\n\n\n\n\n\nBaayen, R. H., Davidson, D. J., & Bates, D. M. (2008). Mixed-effects modeling with crossed random effects for subjects and items. Journal of Memory and Language, 59(4), 390–412. https://doi.org/10.1016/j.jml.2007.12.005\n\n\nBarr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of Memory and Language, 68, 255–278.\n\n\nBates, D., Kliegl, R., Vasishth, S., & Baayen, H. (2015). Parsimonious mixed models. arXiv Preprint arXiv:1506.04967.\n\n\nBürkner, P.-C., & Vuorre, M. (2019). Ordinal Regression Models in Psychology: A Tutorial. Advances in Methods and Practices in Psychological Science, 2(1), 77–101. https://doi.org/10.1177/2515245918823199\n\n\nCalloway, R. C. (2019). Why do you read? Toward a more comprehensive model of reading comprehension: The role of standards of coherence, reading goals, and interest [PhD thesis]. University of Pittsburg.\n\n\nChadwick, S. (2023). Metacomprehension accuracy of health-related information [PhD thesis]. Lancaster University.\n\n\nChristensen, R. H. B. (2015). Ordinal package for r. Version 3.4.2. 1–22. http://www.cran.r-project.org/package=ordinal/\n\n\nChristensen, R. H. B. (2022). Ordinal: Regression models for ordinal data. https://CRAN.R-project.org/package=ordinal\n\n\nDunlosky, J., & Lipko, A. R. (2007). Metacomprehension. Current Directions in Psychological Science, 16(4), 228–232. https://doi.org/10.1111/j.1467-8721.2007.00509.x\n\n\nLiddell, T. M., & Kruschke, J. K. (2018). Analyzing ordinal data with metric models: What could possibly go wrong? Journal of Experimental Social Psychology, 79, 328–348. https://doi.org/10.1016/j.jesp.2018.08.009\n\n\nMatuschek, H., Kliegl, R., Vasishth, S., Baayen, H., & Bates, D. (2017). Balancing type i error and power in linear mixed models. Journal of Memory and Language, 94, 305–315. https://doi.org/10.1016/j.jml.2017.01.001\n\n\nMeteyard, L., & Davies, R. A. I. (2020). Best practice guidance for linear mixed-effects models in psychological science. Journal of Memory and Language, 112, 104092. https://doi.org/10.1016/j.jml.2020.104092\n\n\nPreston, C. C., & Colman, A. M. (2000). Optimal number of response categories in rating scales: Reliability, validity, discriminating power, and respondent preferences. Acta Psychologica, 104(1), 1–15. https://doi.org/10.1016/S0001-6918(99)00050-5\n\n\nRatajczak, M. (2020). The effects of individual differences and linguistic features on reading comprehension of health-related texts [PhD thesis]. Lancaster University.\n\n\nRicketts, J., Dawson, N., & Davies, R. (2021). The hidden depths of new word knowledge: Using graded measures of orthographic and semantic learning to measure vocabulary acquisition. Learning and Instruction, 74, 101468. https://doi.org/10.1016/j.learninstruc.2021.101468\n\n\nRodríguez-Ferreiro, J., Aguilera, M., & Davies, R. (2020). Positive schizotypy increases the acceptance of unpresented materials in false memory tasks in non-clinical individuals. Frontiers in Psychology, 11. https://doi.org/10.3389/fpsyg.2020.00262\n\n\nShipley, W. C., Gruber, C. P., Martin, T. A., & Klein, A. M. (2009). Shipley-2 manual western psychological services. Western Psychological Services, 65.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 20. Conceptual introduction to Ordinal (Mixed-effects) Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/02-mixed-workbook.html",
    "href": "PSYC412/part2/02-mixed-workbook.html",
    "title": "Week 17. Workbook introduction to mixed-effects models",
    "section": "",
    "text": "Welcome to your overview of the work we will do together in Week 17.\nMany Psychologists conduct studies with repeated-measures designs where the experimenter presents a sample of multiple stimuli, for response, to each participant in a sample of multiple participants. Studies with repeated-measures designs will produce data with a structure that requires the use of mixed-effects models.\nWe are going to learn about this kind of data, and build on the analysis methods we learned about in the conceptual introduction to multilevel data and the workbook introduction to multilevel data.\nWe use the same procedure we did for multilevel data but with one significant change which we shall explore and seek to understand in-depth.\n\n\nOur learning objectives again include the development of both concepts and skills.\n\nskills – practice how to tidy experimental data for mixed-effects analysis.\nconcepts – begin to develop an understanding of crossed random effects of participants and stimuli.\nskills and concepts – practice fitting linear mixed-effects models incorporating random effects of participants and stimuli.\n\n\n\n\nYou will see, next, the lectures we share to explain the concepts you will learn about, and the practical data analysis skills you will develop. Then you will see information about the practical materials you can use to build and practise your skills.\nEvery week, you will learn best if you first watch the lectures then do the practical exercises.\n\n\n\n\n\n\nLinked resources\n\n\n\nTo help your learning, you can read about the ideas and the practical coding required for analyses in the chapters I wrote for this course.\n\nThe chapter: Introduction to linear mixed-effects models\n\n\n\n\n\nThe lecture materials for this week are presented in three short parts.\nClick on a link and your browser should open a tab showing the Panopto video for the lecture part.\n\nPart 1 (17 minutes) Mixed-effects models: Repeated measures data and the what, when and how of mixed-effects models.\n\n\n\nPart 2 (21 minutes): Working with an example from a study with a repeated measures design, recognizing how people or how stimuli vary, and identifying the motivation for using mixed-effects models, compared to alternative methods.\n\n\n\nPart 3 (15 minutes): What mixed-effects models can do for us, how we can describe the logic, how we code mixed-effects models, what the bits of code do, what the results look like, what the results tell us, and how we should report our models and our findings.\n\n\n\n\n\n\n\n\n\n\n\nDownload the lecture slides\n\n\n\nThe slides presented in the videos can be downloaded here:\n\n402-week-18-LME-2.pdf: high resolution .pdf, exactly as delivered [6 MB];\n402-week-18-LME-2_1pp-lo.pdf: lower resolution .pdf, printable version, one-slide-per-page [about 900 KB];\n402-week-18-LME-2_6pp-lo.pdf: lower resolution .pdf, printable version, six-slides-per-page [about 900 KB].\n\nThe high resolution version is the version delivered for the lecture recordings. Because the images are produced to be high resolution, the file size is quite big (6 MB) so, to make the slides easier to download, I produced low resolution versions: 1pp and 6pp. These should be easier to download and print out if that is what you want to do.\n\n\n\n\n\nWe will be working with the CP reading study data-set. CP tested 62 children (aged 116-151 months) on reading aloud in English. In the experimental reading task, CP presented 160 words as stimuli. The same 160 words were presented to all children. The words were presented one at a time on a computer screen. Each time a word was shown, each child had to read the word out loud and their response was recorded. Thus, the CP reading study data-set comprised observations about the responses made by 62 children to 160 words.\nYou can read more about these data in the chapter: Introduction to linear mixed-effects models.\nThe critical features of the study are that:\n\nWe have an outcome measure – the reading response – observed multiple times.\n\n\nWe have multiple responses recorded for each participant: they make one response to each stimulus (here, each stimulus word), for the multiple stimuli that they see in the experimental reading task.\nAnd we have multiple responses recorded for each stimulus: one response is made to each stimulus by each participant, for all the participants who completed the task, in a sample of multiple participants.\n\nThe presence of these features is the reason why we need to use mixed-effects models in our analysis.\n\n\n\n\n\n\nImportant\n\n\n\nGet the data: get the data file and the .R script you can use to do the exercises that will support your learning.\n\nYou can download the files folder for this chapter by clicking on the link 02-mixed.zip.\n\n\n\nThe practical materials folder includes data files and an .R script:\n\nCP study word naming rt 180211.dat\nCP study word naming acc 180211.dat\nwords.items.5 120714 150916.csv\nall.subjects 110614-050316-290518.csv\nlong.all.noNAs.csv\n402-02-mixed-workbook.R the workbook you will need to do the practical exercises.\n\nThe .dat files are tab delimited files holding behavioural data: the latency or reaction time rt (in milliseconds) and the accuracy acc of response made by each participant to each stimulus.\nThe .csv files are comma separated values files. The words.items file holds information about the 160 stimulus words presented in the experimental reading (word naming) task. The all.subjects file holds information about the 62 participants who volunteered to take part in the experiment.\nIn the following, I will describe a series of steps through which we get the data ready for analysis. However, as we shall see, you can avoid these steps by using the pre-tidied data-set:\n\nlong.all.noNAs.csv\n\nThe data files are collected together through the steps set out in the .R script:\n\n402-02-mixed-workbook.R.\n\n\n\n\n\n\n\nImportant\n\n\n\nYou can access the sign-in page for R-Studio Server here\n\n\n\n\nHere, our learning targets are to:\n\nPractice how to tidy experimental data for mixed-effects analysis.\nBegin to develop an understanding of crossed random effects of subjects and stimuli.\nPractice fitting linear mixed-effects models incorporating random effects of subjects and stimuli.\n\nThe aims of the lab session work are to:\n\nGet practice running the code required to tidy and prepare experimental data for analysis.\nExercise skills by varying model code – changing variables, changing options – so that you can see how the code works.\nUse the opportunity to reflect on and evaluate results – so that we can support growth in development of understanding of main ideas.\nOptional – get practice running the code: so that you can reproduce the figures and results from the lecture and in the book chapter.\n\n\n\n\nNow you will progress through a series of tasks, and challenges, to aid your learning.\n\n\n\n\n\n\nWarning\n\n\n\nWe will work with the data files:\n\nCP study word naming rt 180211.dat\nCP study word naming acc 180211.dat\nwords.items.5 120714 150916.csv\nall.subjects 110614-050316-290518.csv\nlong.all.noNAs.csv\n\n\n\nWe again split the steps into into parts, tasks and questions.\nWe are going to work through the following workflow steps: each step is labelled as a practical part.\n\nSet-up\nLoad the data\nTidy the data\nRead in pre-tidied data\nAnalyze data with lm\nAnalyze data with lmer\nOptional: reproduce the plots in the chapter and slides\n\nIn the following, we will guide you through the tasks and questions step by step.\n\n\n\n\n\n\nImportant\n\n\n\nAn answers version of the workbook will be provided after the practical class.\n\n\n\n\n\nTo begin, we set up our environment in R.\n\n\nUse the library() function to make the functions we need available to you.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlibrary(broom)\nlibrary(gridExtra)\nlibrary(here)\n\nhere() starts at /Users/padraic/MSc_411_412_2025-26\n\nlibrary(lme4)\n\nLoading required package: Matrix\n\nlibrary(patchwork)\nlibrary(sjPlot)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::combine()     masks gridExtra::combine()\n✖ tidyr::expand()      masks Matrix::expand()\n✖ dplyr::filter()      masks stats::filter()\n✖ dplyr::lag()         masks stats::lag()\n✖ tidyr::pack()        masks Matrix::pack()\n✖ ggplot2::set_theme() masks sjPlot::set_theme()\n✖ tidyr::unpack()      masks Matrix::unpack()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\n\n\n\n\nRead the data files into R:\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nbehaviour.rt &lt;- read_tsv(\"CP study word naming rt 180211.dat\", na = \"-999\")\n\nRows: 160 Columns: 62\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (1): item_name\ndbl (61): AislingoC, AlexB, AllanaD, AmyR, AndyD, AnnaF, AoifeH, ChloeBergin...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbehaviour.acc &lt;- read_tsv(\"CP study word naming acc 180211.dat\", na = \"-999\")\n\nRows: 160 Columns: 62\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (1): item_name\ndbl (61): AislingoC, AlexB, AllanaD, AmyR, AndyD, AnnaF, AoifeH, ChloeBergin...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsubjects &lt;- read_csv(\"all.subjects 110614-050316-290518.csv\", na = \"-999\")\n\nRows: 62 Columns: 26\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (12): subjectID, WAIS_blocks, ART_truepos, ART_falsepos, handedness, gen...\ndbl (14): age.months, TOWREW_acc, TOWREW_time, TOWRENW_acc, TOWRENW_time, sp...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nwords &lt;- read_csv(\"words.items.5 120714 150916.csv\", na = \"-999\")\n\nRows: 160 Columns: 50\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): item_name, corteseIMG, corteseAOA\ndbl (47): Phono_N, OLD, OLDF, PLD, PLDF, NPhon, NMorph, regularity, Length, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\nPract.Q.1. Can you identify the differences between the functions used to read different kinds of data files?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThese different functions respect the different ways in which the .dat and .csv file formats work.\n\nWe need read_tsv() when data files consist of tab separated values.\nWe need read_csv() when data files consist of comma separated values.\n\nYou can read more about the {tidyverse} {readr} library of helpful functions here.\nIt is very common to get experimental data in all sorts of different formats. Learning to use tidyverse functions will make it easier to cope with this when you do research.\n\n\n\n\nPract.Q.2. Can you explain what the read_ functions are doing, or what you are doing with them when you code them?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nIt will help your understanding to examine an example. Take a look at what this line of code includes, element by element.\n\nbehaviour.rt &lt;- read_tsv(\"CP study word naming rt 180211.dat\", na = \"-999\")\n\n\nWe write behaviour.rt &lt;- read_tsv(...) to create an object in the R environment, which we call behaviour.rt: the object with this name is the data-set we read into R using read_tsv(...).\nWhen we write the function read_tsv(...) we include two arguments inside it.\nread_tsv(\"CP study word naming rt 180211.dat\", ... first, the name of the file, given in quotes \"\" and then a comma.\nread_tsv(..., na = \"-999\") second, we tell R that there are some missing values na which are coded with the value \"-999\".\n\n\n\n\n\n\n\n\nTidying the data involves a number of tasks, some essential and some things we do for our convenience.\n\n\nTo get ready for later analyses, we need to ensure that reaction time (rt) and accuracy (acc) observations are in tidy format.\nWhen we first encounter the data files, the data are untidy because, here, the reaction time (rt) and accuracy (acc) observations are in separate multiple columns. We pivot the data to solve this problem.\n\nYou can read more about what we are doing and why, here, in the chapter: Introduction to linear mixed-effects models.\n\n\nPract.Q.3. How do we do this?\n\n\nYou can try out some solutions, based on what you have done before.\nYou can open the Hint box for a detailed walk-through of the process, and then adapt the hint information.\nAnd you can open the Code box for the example code.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe use a function you may have seen before: pivot_longer().\n\nrt.long &lt;- behaviour.rt %&gt;%\n             pivot_longer(2:62, names_to = \"subjectID\", values_to = \"RT\")\n\nThe name of the function comes from the fact that we are starting with data in wide format e.g. behaviour.rt where we have what should be a single variable of observations (RTs) arranged in a wide series of multiple columns, side-by-side (one column for each participant). But we want to take those wide data and lengthen the data-set, increasing the number of rows and decreasing the number of columns.\nLet’s look at this line of code bit by bit.\n\nrt.long &lt;- behaviour.rt %&gt;%\n\n\nAt the start, I tell R that I am going to create a new longer data-set (more rows, fewer columns) that I shall call rt.long.\nI will create this longer data-set from &lt;- the original wide data-set behaviour.rt.\nand I will create the new longer data-set by taking the original wide data-set and piping it %&gt;% to the pivot function coded on the next line:\n\n\npivot_longer(2:62, names_to = \"subjectID\", values_to = \"RT\")\n\n\nOn this next line, I tell R how to do the pivoting by using three arguments.\n\n\npivot_longer(2:62...)\n\n\nFirst, I tell R that I want to re-arrange all the columns that can be found in the data-set from the second column to the sixty-second column.\nIn a spreadsheet, we have a number of columns.\nColumns can be identified by their position in the spreadsheet.\nThe position of a column in a spreadsheet can be identified by number, from the leftmost column (column number 1) to the rightmost column (here, column number 62) in our data-set.\nSo this argument tells R exactly which columns I want to pivot.\n\n\npivot_longer(..., names_to = \"subjectID\", ...)\n\n\nSecond, I tell R that I want it to take the column labels and put them into a new column, called subjectID.\nIn the wide data-set behaviour.rt, each column holds a list of numbers (RTs) but begins with a word in the topmost cell, the name code for a participant, in the column label position.\nWe want to keep the information about which participant produces which response when we pivot the wide data to a longer structure.\nWe do this by asking R to take the column labels (the participant names) and listing them in a new column, called subjectID which now holds the names as participant ID codes.\n\n\npivot_longer(...values_to = \"RT\")\n\n\nThird, we tell R that all the RT values should be put in a single column.\nWe can understand that this new column RT will hold RT observations in a vertical stack, one cell for each response by a person to a word, with rows ordered by subjectID.\n\nThere are 61 columns of data listed by participant though 62 children were tested because we lost one child’s data through an administrative error. As a result, in the wide data sets there are 62 columns, with the first column holding item_name data.\nYou can find more information about pivoting data here\nAnd you can find more information specifically about the pivot_longer() operation here\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nrt.long &lt;- behaviour.rt %&gt;%\n             pivot_longer(2:62, names_to = \"subjectID\", values_to = \"RT\")\n\nacc.long &lt;- behaviour.acc %&gt;%\n              pivot_longer(2:62, names_to = \"subjectID\", values_to = \"accuracy\")\n\n\n\n\n\n\n\nTo answer our research question, we next need to combine the RT with the accuracy data, and then the combined behavioural data with participant information and with stimulus information.\nIn common practice, in psychological research, we have to deal with the fact that information about behavioural responses, and about participant attributes or stimulus word properties, are located in separate files.\nThis is a problem where we need, as here, to analyse outcome behavioural responses using information about participant attributes or stimulus word properties.\n\nPract.Q.4. How do we solve this problem?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe need to join the data from the different sources:\n\njoin RT with accuracy then\njoin those response data with data about subject attributes\nthen join those response plus subjects data\nwith item properties data\n\nWe can combine the data-sets, in the way that we need, using the {tidyverse} full_join() function.\n\n\n\nFirst, we join RT and accuracy data together.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlong &lt;- rt.long %&gt;% \n          full_join(acc.long)\n\n\n\n\nThen, we join subject and item information to the behavioural data.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlong.subjects &lt;- long %&gt;% \n                   full_join(subjects, by = \"subjectID\")\n\nlong.all &lt;- long.subjects %&gt;%\n              full_join(words, by = \"item_name\")\n\n\n\n\n\nPract.Q.4. Can you explain what you are doing, or what the code you use is doing, to solve the problem of putting the separate sets of data together?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nHere, in a series of steps, we take one data-set and join it (merge it) with the second data-set. Let’s look at an example element by element to better understand how this is accomplished.\n\nlong &lt;- rt.long %&gt;% \n           full_join(acc.long)\n\nThe code work as follows.\n\nlong &lt;- rt.long %&gt;%\n\n\nWe create a new data-set we call long.\nWe do this by taking one original data-set rt.long and %&gt;% piping it to the operation defined in the second step.\n\n\nfull_join(acc.long)\n\n\nIn this second step, we use the function full_join() to add observations from a second original data-set acc.long to those already from rt.long\n\nThe addition of observations from one data-set joining to those from another happens through a matching process.\n\nR looks at the data-sets being merged.\nIt identifies if the two data-sets have columns in common. Here, the data-sets have subjectID and item_name in common).\nR can use these common columns to identify rows of data. Here, each row of data will be identified by both subjectID and item_name i.e. as data about the response made by a participant to a word.\nR will then do a series of identity checks, comparing one data-set with the other and, row by row, looking for matching values in the columns that are common to both data-sets.\nIf there is a match then R joins the corresponding rows of data together.\nIf there isn’t a match then it creates NAs where there are missing entries in one row for one data-set which cannot be matched to a row from the joining data-set.\n\n\n\n\n\nYou can read more about what we are doing here in the Introduction to linear mixed-effects models chapter section on relational data and the same chapter section on join functions.\n\n\n\n\nIf the pivoting and joining steps have gone according to plan then we should now be looking at a big, long and wide, data-set. But we do not actually require all of the data-set for the analyses we are going to do.\nFor our own convenience, we are going to want to select just the variables we need.\nWe want the variables:\n\nitem_name, subjectID, RT, accuracy\nLg.UK.CDcount, brookesIMG, AoA_Kup_lem\nOrtho_N, regularity, Length, BG_Mean\nVoice,   Nasal,  Fricative,  Liquid_SV\n\nBilabials,   Labiodentals,   Alveolars\nPalatals,    Velars, Glottals, age.months\nTOWREW_skill, TOWRENW_skill, spoonerisms, CART_score\n\n\nPract.Q.5. How do we get to a data-set with just these variables in it?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe are going to select just the variables we need using the select() function.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlong.all.select &lt;- long.all %&gt;% \n                        select(item_name, subjectID, RT, accuracy, \n                               Lg.UK.CDcount, brookesIMG, AoA_Kup_lem, \n                               Ortho_N, regularity, Length, BG_Mean, \n                               Voice,   Nasal,  Fricative,  Liquid_SV,\n                               Bilabials,   Labiodentals,   Alveolars,\n                               Palatals,    Velars, Glottals, \n                               age.months, TOWREW_skill, TOWRENW_skill, \n                               spoonerisms, CART_score)\n\n\n\n\n\nPract.Q.6. What if you wanted to analyze a different set of variables, could you select different variables?\n\n\n\n\nThe data-set includes missing values, designated NA.\n\nHere, every error (coded 0, in accuracy) corresponds to an NA in the RT column.\n\nThe data-set also includes outlier data.\n\nIn this context, \\(RT &lt; 200\\) are probably response errors or equipment failures. We will want to analyse accuracy later, so we shall need to be careful about getting rid of NAs.\n\n\nPract.Q.7. How do we exclude NAs and outliers so that we can do our analysis?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe can exclude two sets of observations:\n\nobservations corresponding to correct response reaction times that are too short: \\(RT &lt; 200\\).\nplus observations corresponding to the word false which (because of an Excel formatting problem) dropped item attribute data.\n\nWe can do this using the filter() function, setting conditions on rows, as arguments.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\n# step 1\nlong.all.select.filter &lt;- long.all.select %&gt;% \n                            filter(item_name != 'FALSE')\n\n# step 2\nlong.all.select.filter &lt;- long.all.select.filter %&gt;%\n                            filter(RT &gt;= 200)\n\n\n\n\n\nPract.Q.8. Can you explain what is going on when you code R to exclude observations?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe can do the exclusions we need to do using the filter() function, setting conditions on rows, as arguments.\n\n# step 1\nlong.all.select.filter &lt;- long.all.select %&gt;% \n                            filter(item_name != 'FALSE')\n\n# step 2\nlong.all.select.filter &lt;- long.all.select.filter %&gt;%\n                            filter(RT &gt;= 200)\n\nHere, I am using the function filter() to …\n\nCreate a new data-set long.all.select.filter &lt;- ... by\nUsing functions to work on the data named immediately to the right of the assignment arrow: long.all.select\nAn observation is included in the new data-set if it matches the condition specified as an argument in the filter() function call, thus:\n\n\nfilter(item_name !='FALSE') means: include in the new data-set long.all.select.filter all observations from the old data-set long.all.select that are not != (! not = equal to) the value FALSE in the variable item_name,\nthen recreate the long.all.select.filter as a version of itself (with no name change) by including in the new version only those observations where RT was greater than or equal to 200ms using RT &gt;= 200.\n\n\n\n\n\nPract.Q.9. Do you understand what the difference is between = and ==.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou need to be careful to distinguish these signs.\n\n= assigns a value, so x = 2 means “x equals 2”\n== tests a match so x == 2 means: “is x equal to 2?”\n\n\n\n\n\nPract.Q.10. Can you vary the filter conditions: in different ways?\n\n\nChange the threshold for including RTs from RT &gt;= 200 to something else?\nCan you assess what impact the change has? Note that you can count the number of observations (rows) in a data-set using e.g. length(data.set.name$variable.name)??\n\n\n\n\nThe data-set includes missing values, designated NA. We have not yet dealt with these.\n\nPract.Q.11. How do you remove missing values?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe can remove missing values before we go any further, using the na.omit() function.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlong.all.noNAs &lt;- na.omit(long.all.select.filter)\n\n\n\n\n\nPract.Q.12. Can you explain how the code function works, when we exclude missing NA values?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe na.omit() function is powerful.\nLook at the example code:\n\nlong.all.noNAs &lt;- na.omit(long.all.select.filter)\n\n\nIn using this function, I am asking R to create a new data-set long.all.noNAs from the old data-set long.all.select.filter in a process in which the new data-set will have no rows in which there is a missing value NA in any column.\nYou need to be reasonably sure, when you use this function, where your NAs may be because, otherwise, you may end the process with a new filtered data-set that has many fewer rows in it than you expected.\n\n\n\n\n\n\n\n\nWe have a learnt in following the process of tidying data, step-by-step.\nIf we wanted to, we could complete the process, check that what we have done is correct, and then write the final version of the data-set out to a new .csv using the write_csv() function.\n\nwrite_csv(long.all.noNAs, \"long.all.noNAs.csv\")\n\n\n\n\nPract.Q.13. If you want to skip the tidying process steps, assuming you have written the tidied data-set out to a new .csv file, what would you do?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlong.all.noNAs &lt;- read_csv(\"long.all.noNAs.csv\", \n                           col_types = cols(\n                             subjectID = col_factor(),\n                             item_name = col_factor()\n                           )\n                          ) \n\n\n\n\n\nPract.Q.14. Take a look at the example code, can you explain what it is doing?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nlong.all.noNAs &lt;- read_csv(\"long.all.noNAs.csv\", \n                           col_types = cols(\n                             subjectID = col_factor(),\n                             item_name = col_factor()\n                           )\n                          ) \n\nNotice that I am using read_csv() with an additional argument col_types = cols(...).\n\nHere, I am requesting that read_csv() treats subjectID and item_name as factors.\nWe use col_types = cols(...) to control how read_csv() interprets specific column variables in the data.\n\nControlling the way that read_csv() handles variables is a very useful capacity, and a more efficient way to work than, say, first reading in the data and then using coercion to ensure that variables are assigned appropriate types.\nYou can read more about it here.\n\n\n\n\n\n\n\nWe begin our data analysis by asking if reading reaction time RT varies in association with the estimated frequency of occurrence in language experience of the words we ask participants to read.\n\nWe ignore, at first, the multilevel structure or clustering in the data.\n\n\n\nYou need to do three things, here:\n\nDraw a scatterplot.\nRemember to map the outcome reaction time variable to the heights of points.\nRemember in this data-set, word frequency information is located in the Lg.UK.CDcount variable.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlong.all.noNAs %&gt;%\nggplot(aes(x = Lg.UK.CDcount, y = RT)) +\n  geom_point(alpha = .2) + \n  geom_smooth(method = \"lm\", se = FALSE, size = 1.5, colour=\"red\") + \n  theme_bw() + \n  xlab(\"Word frequency: log context distinctiveness (CD) count\")\n\n\n\n\n\n\n\nFigure 1: Reading reaction time compared to word frequency, all data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou can use lm() to do this.\n\nRemember, we are ignoring random differences between participants or stimuli, at this point.\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlm.all.freq &lt;- lm(RT ~  Lg.UK.CDcount, data = long.all.noNAs)\n\nsummary(lm.all.freq)\n\n\nCall:\nlm(formula = RT ~ Lg.UK.CDcount, data = long.all.noNAs)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-346.62 -116.03  -38.37   62.05 1981.58 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    882.983     11.901   74.19   &lt;2e-16 ***\nLg.UK.CDcount  -53.375      3.067  -17.40   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 185.9 on 9083 degrees of freedom\nMultiple R-squared:  0.03227,   Adjusted R-squared:  0.03216 \nF-statistic: 302.8 on 1 and 9083 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\nVary the linear model using different outcomes or predictors: now fit a model in which the outcome RT and the predictor is word length.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlm.all.length &lt;- lm(RT ~  Length,\n\n                 data = long.all.noNAs)\n\nsummary(lm.all.length)\n\n\nCall:\nlm(formula = RT ~ Length, data = long.all.noNAs)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-358.99 -117.89  -39.99   61.17 1944.07 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  600.149     11.734  51.145  &lt; 2e-16 ***\nLength        18.369      2.706   6.789  1.2e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 188.5 on 9083 degrees of freedom\nMultiple R-squared:  0.005049,  Adjusted R-squared:  0.00494 \nF-statistic: 46.09 on 1 and 9083 DF,  p-value: 1.198e-11\n\n\n\n\n\n\nPract.Q.15. What is the estimate of the effect of Length on RT?\n\n\nPract.A.15. The estimate for the Length effect is 18.369.\n\n\nPract.Q.16. What does the estimate tell you about how RT varies in relation to word length?\n\n\nPract.A.16. This tells us that RT increases by about 18ms for each increase in word length by one letter.\n\n\nPract.Q.17. What is the R-squared for the model?\n\n\nPract.A.17. The R-sq is .01.\n\nChange the predictor from frequency to something else: you choose.\n\nPract.Q.18. Produce a scatterplot to visualize the relationship between the two variables: does the relationship you see in the plot match the coefficient you see in the model estimates?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou need to look at the model summary to do this. Specifically, you need to think about:\n\nThe coefficients for the effect of the predictor variable you include in your model: focus on the sign and the magnitude of the effect coefficient.\nWhether or how that sign and that magnitude do or do not match the slopes you see in the plot.\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou can Use facet_wrap() to do this.\n\nWe can visualize the frequency effect for each child in a grid of plots, with each plot representing the \\(\\text{RT} \\sim \\text{frequency}\\) relationship for the data for a child (see the lattice or trellis plot, following).\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlong.all.noNAs %&gt;%\n  ggplot(aes(x = Lg.UK.CDcount, y = RT)) +\n    geom_point(alpha = .2) + \n    geom_smooth(method = \"lm\", se = FALSE, size = 1.25, colour = \"red\") + \n    theme_bw() + \n    xlab(\"Word frequency (log10 UK SUBTLEX CD count)\") + \n    facet_wrap(~ subjectID)\n\n\n\n\n\n\n\nFigure 2: RT vs. word frequency, considered separately for data for each child\n\n\n\n\n\n\n\n\n\nPract.Q.19. Can you visualize the relationship between RT and Length, also, separately for each participant?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlong.all.noNAs %&gt;%\n  ggplot(aes(x = Length, y = RT)) +\n  geom_point(alpha = .2) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 1.25, colour = \"red\") +\n  theme_bw() +\n  facet_wrap(~ subjectID)\n\n\n\n\n\n\n\nFigure 3: RT vs. word length, considered separately for data for each child\n\n\n\n\n\n\n\n\n\nPract.Q.19. What do you conclude from this plot about the Length effect, and about how the effect varies?\n\n\nPract.A.19. We can conclude that the length effect is small and really only apparent for some participants.\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou need to specify:\n\nthe lmer() function;\nRT as the outcome;\nfrequency (Lg.UK.CDcount) as the fixed effects predictor;\nplus random effects including the effect of participants (subjectID) on intercepts and on the slopes of the frequency effect (Lg.UK.CDcount).\n\nCheck the chapter code examples if you need help, see also the explanations and examples in chapter: Introduction to linear mixed-effects models.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlmer.all.1 &lt;- lmer(RT ~  Lg.UK.CDcount + (Lg.UK.CDcount + 1||subjectID),\n             \n             data = long.all.noNAs)\n\nsummary(lmer.all.1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: RT ~ Lg.UK.CDcount + ((1 | subjectID) + (0 + Lg.UK.CDcount |  \n    subjectID))\n   Data: long.all.noNAs\n\nREML criterion at convergence: 117805.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.7839 -0.5568 -0.1659  0.3040 12.4850 \n\nRandom effects:\n Groups      Name          Variance Std.Dev.\n subjectID   (Intercept)   87575    295.93  \n subjectID.1 Lg.UK.CDcount  2657     51.55  \n Residual                  23734    154.06  \nNumber of obs: 9085, groups:  subjectID, 61\n\nFixed effects:\n              Estimate Std. Error t value\n(Intercept)    950.913     39.216  24.248\nLg.UK.CDcount  -67.980      7.092  -9.586\n\nCorrelation of Fixed Effects:\n            (Intr)\nLg.UK.CDcnt -0.093\n\n\n\n\n\n\nPract.Q.20. What are the differences between the lm() and the lmer() model results?\n\n\nPract.A.20. The differences include:\n\n\nThe estimate for the coefficient of the frequency (Lg.UK.CDcount) effect changes from (lm) -53.375 to (lmer) -67.980.\nThe (lm) model includes just an estimate of residuals.\nThe (lmer) model includes estimates of the Residual variance plus the subjectID (Intercept) variance and the subjectID (Lg.UK.CDcount) variance.\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlmer.all.si &lt;- lmer(RT ~  Lg.UK.CDcount + \n                         (Lg.UK.CDcount + 1||subjectID) +\n                         (1|item_name),\n             \n             data = long.all.noNAs)\n\nsummary(lmer.all.si)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: RT ~ Lg.UK.CDcount + ((1 | subjectID) + (0 + Lg.UK.CDcount |  \n    subjectID)) + (1 | item_name)\n   Data: long.all.noNAs\n\nREML criterion at convergence: 116976.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.1795 -0.5474 -0.1646  0.3058 12.9485 \n\nRandom effects:\n Groups      Name          Variance Std.Dev.\n item_name   (Intercept)     3397    58.29  \n subjectID   Lg.UK.CDcount   3624    60.20  \n subjectID.1 (Intercept)   112313   335.13  \n Residual                   20704   143.89  \nNumber of obs: 9085, groups:  item_name, 159; subjectID, 61\n\nFixed effects:\n              Estimate Std. Error t value\n(Intercept)     971.07      51.87  18.723\nLg.UK.CDcount   -72.33      10.79  -6.703\n\nCorrelation of Fixed Effects:\n            (Intr)\nLg.UK.CDcnt -0.388\n\n\n\n\n\n\nPract.Q.21. Can you describe the differences between the models with vs. without the item term?\n\n\nHow do the random effects differ?\nHow do the fixed effects estimates differ?\n\n\nPract.A.21. We can compare models:\n\nlmer.all.1 &lt;- lmer(RT ~  Lg.UK.CDcount + (Lg.UK.CDcount + 1||subjectID),\n\n                    data = long.all.noNAs)\nlmer.all.si &lt;- lmer(RT ~  Lg.UK.CDcount +\n                      (Lg.UK.CDcount + 1||subjectID) +\n                      (1|item_name),\n\n                    data = long.all.noNAs)\n\nThe models differ, obviously, in the inclusion of a random term corresponding to the effect of items on intercepts. We can also see that the residual variance is smaller where we include the item random effect.\nThe estimate for the Lg.UK.CDcount is now -72.33.\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlmer.all.2 &lt;- lmer(RT ~  Length + \n                     (Length + 1||subjectID) +\n                     (1|item_name),\n                   \n                   data = long.all.noNAs)\n\nsummary(lmer.all.2)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: RT ~ Length + ((1 | subjectID) + (0 + Length | subjectID)) +  \n    (1 | item_name)\n   Data: long.all.noNAs\n\nREML criterion at convergence: 117114.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.6205 -0.5443 -0.1641  0.3055 12.9620 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n item_name   (Intercept)  4751.3   68.93  \n subjectID   Length        757.9   27.53  \n subjectID.1 (Intercept)  4994.3   70.67  \n Residual                21440.3  146.43  \nNumber of obs: 9085, groups:  item_name, 159; subjectID, 61\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  572.709     35.034  16.347\nLength        28.361      8.534   3.323\n\nCorrelation of Fixed Effects:\n       (Intr)\nLength -0.867\n\n\n\n\n\n\nPract.Q.22. What are the random effects estimates: the variances?\n\n\nPract.A.22. We see: items/intercepts variance of 4751, subjects/Length of 757, subjects/intercepts of 4994 and residuals of 21440.\n\n\nPract.Q.23. What is the estimate of the effect of length?\n\n\nPract.A.23. The estimate is 28.361.\n\n\nPract.Q.24. What does the estimate tell you about how RT varies as Length varies?\n\n\nPract.A.24. That RT increases by about 28ms for unit increase in word length.\n\n\n\n\n\nReproduce the plots in the chapter and slides\nThis part is optional for you to view and work with.\n\nRun the code, and consider the code steps only if you are interested in how the materials for the book chapter were created.\n\n\n\nWhat we aim to do is:\n\nModel the effect of word frequency for each child, for their data considered separately.\nExtract the coefficient estimate for the frequency effect for each separate model (per child).\nThen plot these estimates, together with\n\nCan you figure out how to code these steps?\n\n\n\n\n\n\nCode\n\n\n\n\n\n\n# -- First fit the model for each child\nfreqperchildlm &lt;- long.all.noNAs %&gt;%\n  group_by(subjectID) %&gt;% \n  do(tidy(lm(RT ~ Lg.UK.CDcount, data=.)))\n\n# -- What does freqperchildlm hold: take a look?\n\n# -- Make sure you can treat term as a factor\nfreqperchildlm$term &lt;- as.factor(freqperchildlm$term)\n\n# -- Get the intercept and frequency effect estimates\nfreqperchildlmint &lt;- filter(freqperchildlm, term == '(Intercept)')\nfreqperchildlmfreq &lt;- filter(freqperchildlm, term == 'Lg.UK.CDcount')\n\n# -- Make the intercepts plots: order by size\npfreqperchildlmint &lt;- freqperchildlmint %&gt;%\n  ggplot(aes(x = fct_reorder(subjectID, estimate), y = estimate, ymin = estimate - std.error, ymax = estimate + std.error)) + \n  geom_point() + geom_linerange() + \n  theme_bw() + \n  xlab(\"Intercepts in order of size\") +\n  ylab(\"Estimated intercept +/- SE\") + \n  theme(axis.title.y = element_text(size = 10), \n        axis.text.y = element_text(size = 5), \n        axis.text.x = element_blank(), \n        panel.grid = element_blank())\n\n# -- Make the slopes plots: order by size\npfreqperchildlmfreq &lt;- freqperchildlmfreq %&gt;%\n  ggplot(aes(x = fct_reorder(subjectID, estimate), y = estimate, ymin = estimate - std.error, ymax = estimate + std.error)) + \n  geom_point() + geom_linerange() + \n  theme_bw() + \n  xlab(\"Frequency effect coefficient in order of size\") +\n  ylab(\"Estimated coefficient for the frequency effect +/- SE\") + \n  theme(axis.title.y = element_text(size = 10), \n        axis.text.y = element_text(size = 5), \n        axis.text.x = element_blank(), \n        panel.grid = element_blank())\n\n# -- Make a grid to show the plots side by side: in 2 columns\ngrid.arrange(pfreqperchildlmint, pfreqperchildlmfreq,\n             ncol = 2) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat we aim to do is:\n\nModel the effect of word frequency for all the children, including random effects terms to take into account random differences between children in intercepts or in slopes.\nGet the random effects estimates – the between-children differences – and plot them.\n\nCan you figure out how to code these steps?\nFirst, fit the model.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlmer.all.1 &lt;- lmer(RT ~  Lg.UK.CDcount + (Lg.UK.CDcount + 1||subjectID),\n                   \n                   data = long.all.noNAs)\n\n\n\n\nSecond, extract the random effects and plot them.\n\n\n\n\n\n\nHint\n\n\n\n\n\nPlot the distribution of random effects – usually called conditional modes.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nBLUPS &lt;- ranef(lmer.all.1)$subjectID\n# summary(BLUPS)\n\nBLUPS.df &lt;- data.frame(subjectID = rownames(BLUPS), \n                       intercepts = BLUPS[,1], \n                       frequency = BLUPS[,2])\n# summary(BLUPS.df)\n# str(BLUPS.df)\n\np.BLUPS.intercepts &lt;- BLUPS.df %&gt;%\n  ggplot(aes(x = intercepts)) +\n  geom_histogram() + \n  ggtitle(\"(a.) Adjustment by child\") +\n  xlab(\"In the intercept\") +\n  geom_vline(xintercept = 0, colour = \"red\", size = 1.5) +\n  theme_bw()\n\np.BLUPS.slopes &lt;- BLUPS.df %&gt;%\n  ggplot(aes(x = frequency)) +\n  geom_histogram() + \n  ggtitle(\"(b.) Adjustment by child\") +\n  xlab(\"In the frequency slope (coefficient)\") +\n  geom_vline(xintercept = 0, colour = \"red\", size = 1.5) +\n  theme_bw()\n\ngrid.arrange(p.BLUPS.intercepts, p.BLUPS.slopes,\n             ncol = 2\n)\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat we aim to do is:\n\nModel the effect of word frequency for all the children, including (in different models) different random effects terms to take into account (in different ways) random differences between children in intercepts or in slopes.\nGet the random effects estimates – the between-children differences – and plot the model predictions.\n\nCan you figure out how to code these steps?\nFirst, fit the models. We want to compare models with:\n\nRandom effects of participants on intercepts and on slopes.\nRandom effects of participants on intercepts.\nRandom effects of participants on slopes.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\n# -- Fit different kinds of mixed-effects model, then plot the resulting differences in model predictions\n\nlmer.all.1 &lt;- lmer(RT ~  Lg.UK.CDcount + (Lg.UK.CDcount + 1||subjectID),\n                   \n                   data = long.all.noNAs)\n\n# summary(lmer.all.1)\n\nlmer.all.2 &lt;- lmer(RT ~  Lg.UK.CDcount + (1|subjectID),\n                   \n                   data = long.all.noNAs)\n\n# summary(lmer.all.2)\n\nlmer.all.3 &lt;- lmer(RT ~  Lg.UK.CDcount + (Lg.UK.CDcount + 0|subjectID),\n                   \n                   data = long.all.noNAs)\n\n# summary(lmer.all.3)\n\n\n\n\nThen derive predictions of the change in outcome, given variation in predictor values, assuming different random effects structures.\n\n\n\n\n\n\nHint\n\n\n\n\n\nPlot random effect – per person predictions – following:\nhttps://bbolker.github.io/morelia_2018/notes/mixedlab.html\nIn lmer, predict has a re.form argument that specifies which random effects should be included (NA or ~0=none, population level; NULL (=all) or ~subject=prediction at the subject level; more complex models, might have additional nested levels).\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\n# -- get predictions\n\nlong.all.noNAs$pred1 &lt;- predict(lmer.all.1) ## individual level\nlong.all.noNAs$pred2 &lt;- predict(lmer.all.2) ## individual level\nlong.all.noNAs$pred3 &lt;- predict(lmer.all.3) ## individual level\n\n# -- show predictions as slopes on top of raw data\n\np.slopes.intercepts &lt;- long.all.noNAs %&gt;%\n  ggplot(aes(x = Lg.UK.CDcount, y = RT)) +\n  geom_point(alpha = .25, colour = \"darkgrey\") +\n  geom_line(aes(y = pred1, group = subjectID), colour=\"red\", alpha = .4) +\n  ggtitle(\"(c.)\\n(Lg.UK.CDcount + 1||subjectID)\") +\n  xlab(\"Frequency (Lg.UK.CDcount)\") +\n  xlim(0,5) +\n  theme_bw()\n\np.intercepts &lt;- long.all.noNAs %&gt;%\n  ggplot(aes(x = Lg.UK.CDcount, y = RT)) +\n  geom_point(alpha = .25, colour = \"darkgrey\") +\n  geom_line(aes(y = pred2, group = subjectID), colour=\"red\", alpha = .4) +\n  ggtitle(\"(a.)\\n(1|subjectID)\") +\n  xlab(\"Frequency (Lg.UK.CDcount)\") +\n  xlim(0,5) +\n  theme_bw()\n\np.slopes &lt;- long.all.noNAs %&gt;%\n  ggplot(aes(x = Lg.UK.CDcount, y = RT)) +\n  geom_point(alpha = .25, colour = \"darkgrey\") +\n  geom_line(aes(y = pred3, group = subjectID), colour=\"red\", alpha = .4) +\n  ggtitle(\"(b.)\\n(Lg.UK.CDcount + 0|subjectID)\") +\n  xlab(\"Frequency (Lg.UK.CDcount)\") +\n  xlim(0,5) +\n  theme_bw()\n\ngrid.arrange(p.intercepts, p.slopes, p.slopes.intercepts, \n             ncol = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat we aim to do is:\n\nCreate a model for each item, including just the intercept.\nPlot intercepts by-items – ordering items by item_name by estimated intercept size.\n\nCan you figure out how to code these steps?\nFirst, create a model for each item, including just the intercept.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nwperitemlm &lt;- long.all.noNAs %&gt;% \n  group_by(item_name) %&gt;% \n  do(tidy(lm(RT ~ 1, data=.)))\nwperitemlm$term &lt;- as.factor(wperitemlm$term)\n# wperitemlm\n\n\n\n\nThen plot intercepts by-items – ordering items by item_name by estimated intercept size.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\npwperitemlmint &lt;- ggplot(wperitemlm, aes(x = fct_reorder(item_name, estimate), y = estimate, ymin = estimate - std.error, ymax = estimate + std.error))\npwperitemlmint + \n  geom_point() +\n  geom_linerange() + \n  theme_bw() + \n  ylab(\"Estimated coefficient +/- SE\") + \n  xlab(\"Per-item estimates of intercept, ordered by intercept estimate size\") +\n  theme(axis.title.y = element_text(size = 10), \n        axis.text.y = element_text(size = 5), \n        axis.text.x = element_blank(), panel.grid = element_blank())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfter the practical class, we will reveal the answers that are currently hidden.\nThe answers version of the webpage will present my answers for questions, and some extra information where that is helpful.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 17. Workbook introduction to mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/02-mixed-workbook.html#sec-mixed-effects-workbook-targets",
    "href": "PSYC412/part2/02-mixed-workbook.html#sec-mixed-effects-workbook-targets",
    "title": "Week 17. Workbook introduction to mixed-effects models",
    "section": "",
    "text": "Our learning objectives again include the development of both concepts and skills.\n\nskills – practice how to tidy experimental data for mixed-effects analysis.\nconcepts – begin to develop an understanding of crossed random effects of participants and stimuli.\nskills and concepts – practice fitting linear mixed-effects models incorporating random effects of participants and stimuli.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 17. Workbook introduction to mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/02-mixed-workbook.html#sec-mixed-effects-workbook-resources",
    "href": "PSYC412/part2/02-mixed-workbook.html#sec-mixed-effects-workbook-resources",
    "title": "Week 17. Workbook introduction to mixed-effects models",
    "section": "",
    "text": "You will see, next, the lectures we share to explain the concepts you will learn about, and the practical data analysis skills you will develop. Then you will see information about the practical materials you can use to build and practise your skills.\nEvery week, you will learn best if you first watch the lectures then do the practical exercises.\n\n\n\n\n\n\nLinked resources\n\n\n\nTo help your learning, you can read about the ideas and the practical coding required for analyses in the chapters I wrote for this course.\n\nThe chapter: Introduction to linear mixed-effects models\n\n\n\n\n\nThe lecture materials for this week are presented in three short parts.\nClick on a link and your browser should open a tab showing the Panopto video for the lecture part.\n\nPart 1 (17 minutes) Mixed-effects models: Repeated measures data and the what, when and how of mixed-effects models.\n\n\n\nPart 2 (21 minutes): Working with an example from a study with a repeated measures design, recognizing how people or how stimuli vary, and identifying the motivation for using mixed-effects models, compared to alternative methods.\n\n\n\nPart 3 (15 minutes): What mixed-effects models can do for us, how we can describe the logic, how we code mixed-effects models, what the bits of code do, what the results look like, what the results tell us, and how we should report our models and our findings.\n\n\n\n\n\n\n\n\n\n\n\nDownload the lecture slides\n\n\n\nThe slides presented in the videos can be downloaded here:\n\n402-week-18-LME-2.pdf: high resolution .pdf, exactly as delivered [6 MB];\n402-week-18-LME-2_1pp-lo.pdf: lower resolution .pdf, printable version, one-slide-per-page [about 900 KB];\n402-week-18-LME-2_6pp-lo.pdf: lower resolution .pdf, printable version, six-slides-per-page [about 900 KB].\n\nThe high resolution version is the version delivered for the lecture recordings. Because the images are produced to be high resolution, the file size is quite big (6 MB) so, to make the slides easier to download, I produced low resolution versions: 1pp and 6pp. These should be easier to download and print out if that is what you want to do.\n\n\n\n\n\nWe will be working with the CP reading study data-set. CP tested 62 children (aged 116-151 months) on reading aloud in English. In the experimental reading task, CP presented 160 words as stimuli. The same 160 words were presented to all children. The words were presented one at a time on a computer screen. Each time a word was shown, each child had to read the word out loud and their response was recorded. Thus, the CP reading study data-set comprised observations about the responses made by 62 children to 160 words.\nYou can read more about these data in the chapter: Introduction to linear mixed-effects models.\nThe critical features of the study are that:\n\nWe have an outcome measure – the reading response – observed multiple times.\n\n\nWe have multiple responses recorded for each participant: they make one response to each stimulus (here, each stimulus word), for the multiple stimuli that they see in the experimental reading task.\nAnd we have multiple responses recorded for each stimulus: one response is made to each stimulus by each participant, for all the participants who completed the task, in a sample of multiple participants.\n\nThe presence of these features is the reason why we need to use mixed-effects models in our analysis.\n\n\n\n\n\n\nImportant\n\n\n\nGet the data: get the data file and the .R script you can use to do the exercises that will support your learning.\n\nYou can download the files folder for this chapter by clicking on the link 02-mixed.zip.\n\n\n\nThe practical materials folder includes data files and an .R script:\n\nCP study word naming rt 180211.dat\nCP study word naming acc 180211.dat\nwords.items.5 120714 150916.csv\nall.subjects 110614-050316-290518.csv\nlong.all.noNAs.csv\n402-02-mixed-workbook.R the workbook you will need to do the practical exercises.\n\nThe .dat files are tab delimited files holding behavioural data: the latency or reaction time rt (in milliseconds) and the accuracy acc of response made by each participant to each stimulus.\nThe .csv files are comma separated values files. The words.items file holds information about the 160 stimulus words presented in the experimental reading (word naming) task. The all.subjects file holds information about the 62 participants who volunteered to take part in the experiment.\nIn the following, I will describe a series of steps through which we get the data ready for analysis. However, as we shall see, you can avoid these steps by using the pre-tidied data-set:\n\nlong.all.noNAs.csv\n\nThe data files are collected together through the steps set out in the .R script:\n\n402-02-mixed-workbook.R.\n\n\n\n\n\n\n\nImportant\n\n\n\nYou can access the sign-in page for R-Studio Server here\n\n\n\n\nHere, our learning targets are to:\n\nPractice how to tidy experimental data for mixed-effects analysis.\nBegin to develop an understanding of crossed random effects of subjects and stimuli.\nPractice fitting linear mixed-effects models incorporating random effects of subjects and stimuli.\n\nThe aims of the lab session work are to:\n\nGet practice running the code required to tidy and prepare experimental data for analysis.\nExercise skills by varying model code – changing variables, changing options – so that you can see how the code works.\nUse the opportunity to reflect on and evaluate results – so that we can support growth in development of understanding of main ideas.\nOptional – get practice running the code: so that you can reproduce the figures and results from the lecture and in the book chapter.\n\n\n\n\nNow you will progress through a series of tasks, and challenges, to aid your learning.\n\n\n\n\n\n\nWarning\n\n\n\nWe will work with the data files:\n\nCP study word naming rt 180211.dat\nCP study word naming acc 180211.dat\nwords.items.5 120714 150916.csv\nall.subjects 110614-050316-290518.csv\nlong.all.noNAs.csv\n\n\n\nWe again split the steps into into parts, tasks and questions.\nWe are going to work through the following workflow steps: each step is labelled as a practical part.\n\nSet-up\nLoad the data\nTidy the data\nRead in pre-tidied data\nAnalyze data with lm\nAnalyze data with lmer\nOptional: reproduce the plots in the chapter and slides\n\nIn the following, we will guide you through the tasks and questions step by step.\n\n\n\n\n\n\nImportant\n\n\n\nAn answers version of the workbook will be provided after the practical class.\n\n\n\n\n\nTo begin, we set up our environment in R.\n\n\nUse the library() function to make the functions we need available to you.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlibrary(broom)\nlibrary(gridExtra)\nlibrary(here)\n\nhere() starts at /Users/padraic/MSc_411_412_2025-26\n\nlibrary(lme4)\n\nLoading required package: Matrix\n\nlibrary(patchwork)\nlibrary(sjPlot)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::combine()     masks gridExtra::combine()\n✖ tidyr::expand()      masks Matrix::expand()\n✖ dplyr::filter()      masks stats::filter()\n✖ dplyr::lag()         masks stats::lag()\n✖ tidyr::pack()        masks Matrix::pack()\n✖ ggplot2::set_theme() masks sjPlot::set_theme()\n✖ tidyr::unpack()      masks Matrix::unpack()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\n\n\n\n\nRead the data files into R:\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nbehaviour.rt &lt;- read_tsv(\"CP study word naming rt 180211.dat\", na = \"-999\")\n\nRows: 160 Columns: 62\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (1): item_name\ndbl (61): AislingoC, AlexB, AllanaD, AmyR, AndyD, AnnaF, AoifeH, ChloeBergin...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbehaviour.acc &lt;- read_tsv(\"CP study word naming acc 180211.dat\", na = \"-999\")\n\nRows: 160 Columns: 62\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (1): item_name\ndbl (61): AislingoC, AlexB, AllanaD, AmyR, AndyD, AnnaF, AoifeH, ChloeBergin...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsubjects &lt;- read_csv(\"all.subjects 110614-050316-290518.csv\", na = \"-999\")\n\nRows: 62 Columns: 26\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (12): subjectID, WAIS_blocks, ART_truepos, ART_falsepos, handedness, gen...\ndbl (14): age.months, TOWREW_acc, TOWREW_time, TOWRENW_acc, TOWRENW_time, sp...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nwords &lt;- read_csv(\"words.items.5 120714 150916.csv\", na = \"-999\")\n\nRows: 160 Columns: 50\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): item_name, corteseIMG, corteseAOA\ndbl (47): Phono_N, OLD, OLDF, PLD, PLDF, NPhon, NMorph, regularity, Length, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\nPract.Q.1. Can you identify the differences between the functions used to read different kinds of data files?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThese different functions respect the different ways in which the .dat and .csv file formats work.\n\nWe need read_tsv() when data files consist of tab separated values.\nWe need read_csv() when data files consist of comma separated values.\n\nYou can read more about the {tidyverse} {readr} library of helpful functions here.\nIt is very common to get experimental data in all sorts of different formats. Learning to use tidyverse functions will make it easier to cope with this when you do research.\n\n\n\n\nPract.Q.2. Can you explain what the read_ functions are doing, or what you are doing with them when you code them?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nIt will help your understanding to examine an example. Take a look at what this line of code includes, element by element.\n\nbehaviour.rt &lt;- read_tsv(\"CP study word naming rt 180211.dat\", na = \"-999\")\n\n\nWe write behaviour.rt &lt;- read_tsv(...) to create an object in the R environment, which we call behaviour.rt: the object with this name is the data-set we read into R using read_tsv(...).\nWhen we write the function read_tsv(...) we include two arguments inside it.\nread_tsv(\"CP study word naming rt 180211.dat\", ... first, the name of the file, given in quotes \"\" and then a comma.\nread_tsv(..., na = \"-999\") second, we tell R that there are some missing values na which are coded with the value \"-999\".\n\n\n\n\n\n\n\n\nTidying the data involves a number of tasks, some essential and some things we do for our convenience.\n\n\nTo get ready for later analyses, we need to ensure that reaction time (rt) and accuracy (acc) observations are in tidy format.\nWhen we first encounter the data files, the data are untidy because, here, the reaction time (rt) and accuracy (acc) observations are in separate multiple columns. We pivot the data to solve this problem.\n\nYou can read more about what we are doing and why, here, in the chapter: Introduction to linear mixed-effects models.\n\n\nPract.Q.3. How do we do this?\n\n\nYou can try out some solutions, based on what you have done before.\nYou can open the Hint box for a detailed walk-through of the process, and then adapt the hint information.\nAnd you can open the Code box for the example code.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe use a function you may have seen before: pivot_longer().\n\nrt.long &lt;- behaviour.rt %&gt;%\n             pivot_longer(2:62, names_to = \"subjectID\", values_to = \"RT\")\n\nThe name of the function comes from the fact that we are starting with data in wide format e.g. behaviour.rt where we have what should be a single variable of observations (RTs) arranged in a wide series of multiple columns, side-by-side (one column for each participant). But we want to take those wide data and lengthen the data-set, increasing the number of rows and decreasing the number of columns.\nLet’s look at this line of code bit by bit.\n\nrt.long &lt;- behaviour.rt %&gt;%\n\n\nAt the start, I tell R that I am going to create a new longer data-set (more rows, fewer columns) that I shall call rt.long.\nI will create this longer data-set from &lt;- the original wide data-set behaviour.rt.\nand I will create the new longer data-set by taking the original wide data-set and piping it %&gt;% to the pivot function coded on the next line:\n\n\npivot_longer(2:62, names_to = \"subjectID\", values_to = \"RT\")\n\n\nOn this next line, I tell R how to do the pivoting by using three arguments.\n\n\npivot_longer(2:62...)\n\n\nFirst, I tell R that I want to re-arrange all the columns that can be found in the data-set from the second column to the sixty-second column.\nIn a spreadsheet, we have a number of columns.\nColumns can be identified by their position in the spreadsheet.\nThe position of a column in a spreadsheet can be identified by number, from the leftmost column (column number 1) to the rightmost column (here, column number 62) in our data-set.\nSo this argument tells R exactly which columns I want to pivot.\n\n\npivot_longer(..., names_to = \"subjectID\", ...)\n\n\nSecond, I tell R that I want it to take the column labels and put them into a new column, called subjectID.\nIn the wide data-set behaviour.rt, each column holds a list of numbers (RTs) but begins with a word in the topmost cell, the name code for a participant, in the column label position.\nWe want to keep the information about which participant produces which response when we pivot the wide data to a longer structure.\nWe do this by asking R to take the column labels (the participant names) and listing them in a new column, called subjectID which now holds the names as participant ID codes.\n\n\npivot_longer(...values_to = \"RT\")\n\n\nThird, we tell R that all the RT values should be put in a single column.\nWe can understand that this new column RT will hold RT observations in a vertical stack, one cell for each response by a person to a word, with rows ordered by subjectID.\n\nThere are 61 columns of data listed by participant though 62 children were tested because we lost one child’s data through an administrative error. As a result, in the wide data sets there are 62 columns, with the first column holding item_name data.\nYou can find more information about pivoting data here\nAnd you can find more information specifically about the pivot_longer() operation here\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nrt.long &lt;- behaviour.rt %&gt;%\n             pivot_longer(2:62, names_to = \"subjectID\", values_to = \"RT\")\n\nacc.long &lt;- behaviour.acc %&gt;%\n              pivot_longer(2:62, names_to = \"subjectID\", values_to = \"accuracy\")\n\n\n\n\n\n\n\nTo answer our research question, we next need to combine the RT with the accuracy data, and then the combined behavioural data with participant information and with stimulus information.\nIn common practice, in psychological research, we have to deal with the fact that information about behavioural responses, and about participant attributes or stimulus word properties, are located in separate files.\nThis is a problem where we need, as here, to analyse outcome behavioural responses using information about participant attributes or stimulus word properties.\n\nPract.Q.4. How do we solve this problem?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe need to join the data from the different sources:\n\njoin RT with accuracy then\njoin those response data with data about subject attributes\nthen join those response plus subjects data\nwith item properties data\n\nWe can combine the data-sets, in the way that we need, using the {tidyverse} full_join() function.\n\n\n\nFirst, we join RT and accuracy data together.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlong &lt;- rt.long %&gt;% \n          full_join(acc.long)\n\n\n\n\nThen, we join subject and item information to the behavioural data.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlong.subjects &lt;- long %&gt;% \n                   full_join(subjects, by = \"subjectID\")\n\nlong.all &lt;- long.subjects %&gt;%\n              full_join(words, by = \"item_name\")\n\n\n\n\n\nPract.Q.4. Can you explain what you are doing, or what the code you use is doing, to solve the problem of putting the separate sets of data together?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nHere, in a series of steps, we take one data-set and join it (merge it) with the second data-set. Let’s look at an example element by element to better understand how this is accomplished.\n\nlong &lt;- rt.long %&gt;% \n           full_join(acc.long)\n\nThe code work as follows.\n\nlong &lt;- rt.long %&gt;%\n\n\nWe create a new data-set we call long.\nWe do this by taking one original data-set rt.long and %&gt;% piping it to the operation defined in the second step.\n\n\nfull_join(acc.long)\n\n\nIn this second step, we use the function full_join() to add observations from a second original data-set acc.long to those already from rt.long\n\nThe addition of observations from one data-set joining to those from another happens through a matching process.\n\nR looks at the data-sets being merged.\nIt identifies if the two data-sets have columns in common. Here, the data-sets have subjectID and item_name in common).\nR can use these common columns to identify rows of data. Here, each row of data will be identified by both subjectID and item_name i.e. as data about the response made by a participant to a word.\nR will then do a series of identity checks, comparing one data-set with the other and, row by row, looking for matching values in the columns that are common to both data-sets.\nIf there is a match then R joins the corresponding rows of data together.\nIf there isn’t a match then it creates NAs where there are missing entries in one row for one data-set which cannot be matched to a row from the joining data-set.\n\n\n\n\n\nYou can read more about what we are doing here in the Introduction to linear mixed-effects models chapter section on relational data and the same chapter section on join functions.\n\n\n\n\nIf the pivoting and joining steps have gone according to plan then we should now be looking at a big, long and wide, data-set. But we do not actually require all of the data-set for the analyses we are going to do.\nFor our own convenience, we are going to want to select just the variables we need.\nWe want the variables:\n\nitem_name, subjectID, RT, accuracy\nLg.UK.CDcount, brookesIMG, AoA_Kup_lem\nOrtho_N, regularity, Length, BG_Mean\nVoice,   Nasal,  Fricative,  Liquid_SV\n\nBilabials,   Labiodentals,   Alveolars\nPalatals,    Velars, Glottals, age.months\nTOWREW_skill, TOWRENW_skill, spoonerisms, CART_score\n\n\nPract.Q.5. How do we get to a data-set with just these variables in it?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe are going to select just the variables we need using the select() function.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlong.all.select &lt;- long.all %&gt;% \n                        select(item_name, subjectID, RT, accuracy, \n                               Lg.UK.CDcount, brookesIMG, AoA_Kup_lem, \n                               Ortho_N, regularity, Length, BG_Mean, \n                               Voice,   Nasal,  Fricative,  Liquid_SV,\n                               Bilabials,   Labiodentals,   Alveolars,\n                               Palatals,    Velars, Glottals, \n                               age.months, TOWREW_skill, TOWRENW_skill, \n                               spoonerisms, CART_score)\n\n\n\n\n\nPract.Q.6. What if you wanted to analyze a different set of variables, could you select different variables?\n\n\n\n\nThe data-set includes missing values, designated NA.\n\nHere, every error (coded 0, in accuracy) corresponds to an NA in the RT column.\n\nThe data-set also includes outlier data.\n\nIn this context, \\(RT &lt; 200\\) are probably response errors or equipment failures. We will want to analyse accuracy later, so we shall need to be careful about getting rid of NAs.\n\n\nPract.Q.7. How do we exclude NAs and outliers so that we can do our analysis?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe can exclude two sets of observations:\n\nobservations corresponding to correct response reaction times that are too short: \\(RT &lt; 200\\).\nplus observations corresponding to the word false which (because of an Excel formatting problem) dropped item attribute data.\n\nWe can do this using the filter() function, setting conditions on rows, as arguments.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\n# step 1\nlong.all.select.filter &lt;- long.all.select %&gt;% \n                            filter(item_name != 'FALSE')\n\n# step 2\nlong.all.select.filter &lt;- long.all.select.filter %&gt;%\n                            filter(RT &gt;= 200)\n\n\n\n\n\nPract.Q.8. Can you explain what is going on when you code R to exclude observations?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe can do the exclusions we need to do using the filter() function, setting conditions on rows, as arguments.\n\n# step 1\nlong.all.select.filter &lt;- long.all.select %&gt;% \n                            filter(item_name != 'FALSE')\n\n# step 2\nlong.all.select.filter &lt;- long.all.select.filter %&gt;%\n                            filter(RT &gt;= 200)\n\nHere, I am using the function filter() to …\n\nCreate a new data-set long.all.select.filter &lt;- ... by\nUsing functions to work on the data named immediately to the right of the assignment arrow: long.all.select\nAn observation is included in the new data-set if it matches the condition specified as an argument in the filter() function call, thus:\n\n\nfilter(item_name !='FALSE') means: include in the new data-set long.all.select.filter all observations from the old data-set long.all.select that are not != (! not = equal to) the value FALSE in the variable item_name,\nthen recreate the long.all.select.filter as a version of itself (with no name change) by including in the new version only those observations where RT was greater than or equal to 200ms using RT &gt;= 200.\n\n\n\n\n\nPract.Q.9. Do you understand what the difference is between = and ==.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou need to be careful to distinguish these signs.\n\n= assigns a value, so x = 2 means “x equals 2”\n== tests a match so x == 2 means: “is x equal to 2?”\n\n\n\n\n\nPract.Q.10. Can you vary the filter conditions: in different ways?\n\n\nChange the threshold for including RTs from RT &gt;= 200 to something else?\nCan you assess what impact the change has? Note that you can count the number of observations (rows) in a data-set using e.g. length(data.set.name$variable.name)??\n\n\n\n\nThe data-set includes missing values, designated NA. We have not yet dealt with these.\n\nPract.Q.11. How do you remove missing values?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe can remove missing values before we go any further, using the na.omit() function.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlong.all.noNAs &lt;- na.omit(long.all.select.filter)\n\n\n\n\n\nPract.Q.12. Can you explain how the code function works, when we exclude missing NA values?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe na.omit() function is powerful.\nLook at the example code:\n\nlong.all.noNAs &lt;- na.omit(long.all.select.filter)\n\n\nIn using this function, I am asking R to create a new data-set long.all.noNAs from the old data-set long.all.select.filter in a process in which the new data-set will have no rows in which there is a missing value NA in any column.\nYou need to be reasonably sure, when you use this function, where your NAs may be because, otherwise, you may end the process with a new filtered data-set that has many fewer rows in it than you expected.\n\n\n\n\n\n\n\n\nWe have a learnt in following the process of tidying data, step-by-step.\nIf we wanted to, we could complete the process, check that what we have done is correct, and then write the final version of the data-set out to a new .csv using the write_csv() function.\n\nwrite_csv(long.all.noNAs, \"long.all.noNAs.csv\")\n\n\n\n\nPract.Q.13. If you want to skip the tidying process steps, assuming you have written the tidied data-set out to a new .csv file, what would you do?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlong.all.noNAs &lt;- read_csv(\"long.all.noNAs.csv\", \n                           col_types = cols(\n                             subjectID = col_factor(),\n                             item_name = col_factor()\n                           )\n                          ) \n\n\n\n\n\nPract.Q.14. Take a look at the example code, can you explain what it is doing?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nlong.all.noNAs &lt;- read_csv(\"long.all.noNAs.csv\", \n                           col_types = cols(\n                             subjectID = col_factor(),\n                             item_name = col_factor()\n                           )\n                          ) \n\nNotice that I am using read_csv() with an additional argument col_types = cols(...).\n\nHere, I am requesting that read_csv() treats subjectID and item_name as factors.\nWe use col_types = cols(...) to control how read_csv() interprets specific column variables in the data.\n\nControlling the way that read_csv() handles variables is a very useful capacity, and a more efficient way to work than, say, first reading in the data and then using coercion to ensure that variables are assigned appropriate types.\nYou can read more about it here.\n\n\n\n\n\n\n\nWe begin our data analysis by asking if reading reaction time RT varies in association with the estimated frequency of occurrence in language experience of the words we ask participants to read.\n\nWe ignore, at first, the multilevel structure or clustering in the data.\n\n\n\nYou need to do three things, here:\n\nDraw a scatterplot.\nRemember to map the outcome reaction time variable to the heights of points.\nRemember in this data-set, word frequency information is located in the Lg.UK.CDcount variable.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlong.all.noNAs %&gt;%\nggplot(aes(x = Lg.UK.CDcount, y = RT)) +\n  geom_point(alpha = .2) + \n  geom_smooth(method = \"lm\", se = FALSE, size = 1.5, colour=\"red\") + \n  theme_bw() + \n  xlab(\"Word frequency: log context distinctiveness (CD) count\")\n\n\n\n\n\n\n\nFigure 1: Reading reaction time compared to word frequency, all data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou can use lm() to do this.\n\nRemember, we are ignoring random differences between participants or stimuli, at this point.\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlm.all.freq &lt;- lm(RT ~  Lg.UK.CDcount, data = long.all.noNAs)\n\nsummary(lm.all.freq)\n\n\nCall:\nlm(formula = RT ~ Lg.UK.CDcount, data = long.all.noNAs)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-346.62 -116.03  -38.37   62.05 1981.58 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    882.983     11.901   74.19   &lt;2e-16 ***\nLg.UK.CDcount  -53.375      3.067  -17.40   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 185.9 on 9083 degrees of freedom\nMultiple R-squared:  0.03227,   Adjusted R-squared:  0.03216 \nF-statistic: 302.8 on 1 and 9083 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\nVary the linear model using different outcomes or predictors: now fit a model in which the outcome RT and the predictor is word length.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlm.all.length &lt;- lm(RT ~  Length,\n\n                 data = long.all.noNAs)\n\nsummary(lm.all.length)\n\n\nCall:\nlm(formula = RT ~ Length, data = long.all.noNAs)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-358.99 -117.89  -39.99   61.17 1944.07 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  600.149     11.734  51.145  &lt; 2e-16 ***\nLength        18.369      2.706   6.789  1.2e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 188.5 on 9083 degrees of freedom\nMultiple R-squared:  0.005049,  Adjusted R-squared:  0.00494 \nF-statistic: 46.09 on 1 and 9083 DF,  p-value: 1.198e-11\n\n\n\n\n\n\nPract.Q.15. What is the estimate of the effect of Length on RT?\n\n\nPract.A.15. The estimate for the Length effect is 18.369.\n\n\nPract.Q.16. What does the estimate tell you about how RT varies in relation to word length?\n\n\nPract.A.16. This tells us that RT increases by about 18ms for each increase in word length by one letter.\n\n\nPract.Q.17. What is the R-squared for the model?\n\n\nPract.A.17. The R-sq is .01.\n\nChange the predictor from frequency to something else: you choose.\n\nPract.Q.18. Produce a scatterplot to visualize the relationship between the two variables: does the relationship you see in the plot match the coefficient you see in the model estimates?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou need to look at the model summary to do this. Specifically, you need to think about:\n\nThe coefficients for the effect of the predictor variable you include in your model: focus on the sign and the magnitude of the effect coefficient.\nWhether or how that sign and that magnitude do or do not match the slopes you see in the plot.\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou can Use facet_wrap() to do this.\n\nWe can visualize the frequency effect for each child in a grid of plots, with each plot representing the \\(\\text{RT} \\sim \\text{frequency}\\) relationship for the data for a child (see the lattice or trellis plot, following).\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlong.all.noNAs %&gt;%\n  ggplot(aes(x = Lg.UK.CDcount, y = RT)) +\n    geom_point(alpha = .2) + \n    geom_smooth(method = \"lm\", se = FALSE, size = 1.25, colour = \"red\") + \n    theme_bw() + \n    xlab(\"Word frequency (log10 UK SUBTLEX CD count)\") + \n    facet_wrap(~ subjectID)\n\n\n\n\n\n\n\nFigure 2: RT vs. word frequency, considered separately for data for each child\n\n\n\n\n\n\n\n\n\nPract.Q.19. Can you visualize the relationship between RT and Length, also, separately for each participant?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlong.all.noNAs %&gt;%\n  ggplot(aes(x = Length, y = RT)) +\n  geom_point(alpha = .2) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 1.25, colour = \"red\") +\n  theme_bw() +\n  facet_wrap(~ subjectID)\n\n\n\n\n\n\n\nFigure 3: RT vs. word length, considered separately for data for each child\n\n\n\n\n\n\n\n\n\nPract.Q.19. What do you conclude from this plot about the Length effect, and about how the effect varies?\n\n\nPract.A.19. We can conclude that the length effect is small and really only apparent for some participants.\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou need to specify:\n\nthe lmer() function;\nRT as the outcome;\nfrequency (Lg.UK.CDcount) as the fixed effects predictor;\nplus random effects including the effect of participants (subjectID) on intercepts and on the slopes of the frequency effect (Lg.UK.CDcount).\n\nCheck the chapter code examples if you need help, see also the explanations and examples in chapter: Introduction to linear mixed-effects models.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlmer.all.1 &lt;- lmer(RT ~  Lg.UK.CDcount + (Lg.UK.CDcount + 1||subjectID),\n             \n             data = long.all.noNAs)\n\nsummary(lmer.all.1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: RT ~ Lg.UK.CDcount + ((1 | subjectID) + (0 + Lg.UK.CDcount |  \n    subjectID))\n   Data: long.all.noNAs\n\nREML criterion at convergence: 117805.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.7839 -0.5568 -0.1659  0.3040 12.4850 \n\nRandom effects:\n Groups      Name          Variance Std.Dev.\n subjectID   (Intercept)   87575    295.93  \n subjectID.1 Lg.UK.CDcount  2657     51.55  \n Residual                  23734    154.06  \nNumber of obs: 9085, groups:  subjectID, 61\n\nFixed effects:\n              Estimate Std. Error t value\n(Intercept)    950.913     39.216  24.248\nLg.UK.CDcount  -67.980      7.092  -9.586\n\nCorrelation of Fixed Effects:\n            (Intr)\nLg.UK.CDcnt -0.093\n\n\n\n\n\n\nPract.Q.20. What are the differences between the lm() and the lmer() model results?\n\n\nPract.A.20. The differences include:\n\n\nThe estimate for the coefficient of the frequency (Lg.UK.CDcount) effect changes from (lm) -53.375 to (lmer) -67.980.\nThe (lm) model includes just an estimate of residuals.\nThe (lmer) model includes estimates of the Residual variance plus the subjectID (Intercept) variance and the subjectID (Lg.UK.CDcount) variance.\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlmer.all.si &lt;- lmer(RT ~  Lg.UK.CDcount + \n                         (Lg.UK.CDcount + 1||subjectID) +\n                         (1|item_name),\n             \n             data = long.all.noNAs)\n\nsummary(lmer.all.si)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: RT ~ Lg.UK.CDcount + ((1 | subjectID) + (0 + Lg.UK.CDcount |  \n    subjectID)) + (1 | item_name)\n   Data: long.all.noNAs\n\nREML criterion at convergence: 116976.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.1795 -0.5474 -0.1646  0.3058 12.9485 \n\nRandom effects:\n Groups      Name          Variance Std.Dev.\n item_name   (Intercept)     3397    58.29  \n subjectID   Lg.UK.CDcount   3624    60.20  \n subjectID.1 (Intercept)   112313   335.13  \n Residual                   20704   143.89  \nNumber of obs: 9085, groups:  item_name, 159; subjectID, 61\n\nFixed effects:\n              Estimate Std. Error t value\n(Intercept)     971.07      51.87  18.723\nLg.UK.CDcount   -72.33      10.79  -6.703\n\nCorrelation of Fixed Effects:\n            (Intr)\nLg.UK.CDcnt -0.388\n\n\n\n\n\n\nPract.Q.21. Can you describe the differences between the models with vs. without the item term?\n\n\nHow do the random effects differ?\nHow do the fixed effects estimates differ?\n\n\nPract.A.21. We can compare models:\n\nlmer.all.1 &lt;- lmer(RT ~  Lg.UK.CDcount + (Lg.UK.CDcount + 1||subjectID),\n\n                    data = long.all.noNAs)\nlmer.all.si &lt;- lmer(RT ~  Lg.UK.CDcount +\n                      (Lg.UK.CDcount + 1||subjectID) +\n                      (1|item_name),\n\n                    data = long.all.noNAs)\n\nThe models differ, obviously, in the inclusion of a random term corresponding to the effect of items on intercepts. We can also see that the residual variance is smaller where we include the item random effect.\nThe estimate for the Lg.UK.CDcount is now -72.33.\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlmer.all.2 &lt;- lmer(RT ~  Length + \n                     (Length + 1||subjectID) +\n                     (1|item_name),\n                   \n                   data = long.all.noNAs)\n\nsummary(lmer.all.2)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: RT ~ Length + ((1 | subjectID) + (0 + Length | subjectID)) +  \n    (1 | item_name)\n   Data: long.all.noNAs\n\nREML criterion at convergence: 117114.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.6205 -0.5443 -0.1641  0.3055 12.9620 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n item_name   (Intercept)  4751.3   68.93  \n subjectID   Length        757.9   27.53  \n subjectID.1 (Intercept)  4994.3   70.67  \n Residual                21440.3  146.43  \nNumber of obs: 9085, groups:  item_name, 159; subjectID, 61\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  572.709     35.034  16.347\nLength        28.361      8.534   3.323\n\nCorrelation of Fixed Effects:\n       (Intr)\nLength -0.867\n\n\n\n\n\n\nPract.Q.22. What are the random effects estimates: the variances?\n\n\nPract.A.22. We see: items/intercepts variance of 4751, subjects/Length of 757, subjects/intercepts of 4994 and residuals of 21440.\n\n\nPract.Q.23. What is the estimate of the effect of length?\n\n\nPract.A.23. The estimate is 28.361.\n\n\nPract.Q.24. What does the estimate tell you about how RT varies as Length varies?\n\n\nPract.A.24. That RT increases by about 28ms for unit increase in word length.\n\n\n\n\n\nReproduce the plots in the chapter and slides\nThis part is optional for you to view and work with.\n\nRun the code, and consider the code steps only if you are interested in how the materials for the book chapter were created.\n\n\n\nWhat we aim to do is:\n\nModel the effect of word frequency for each child, for their data considered separately.\nExtract the coefficient estimate for the frequency effect for each separate model (per child).\nThen plot these estimates, together with\n\nCan you figure out how to code these steps?\n\n\n\n\n\n\nCode\n\n\n\n\n\n\n# -- First fit the model for each child\nfreqperchildlm &lt;- long.all.noNAs %&gt;%\n  group_by(subjectID) %&gt;% \n  do(tidy(lm(RT ~ Lg.UK.CDcount, data=.)))\n\n# -- What does freqperchildlm hold: take a look?\n\n# -- Make sure you can treat term as a factor\nfreqperchildlm$term &lt;- as.factor(freqperchildlm$term)\n\n# -- Get the intercept and frequency effect estimates\nfreqperchildlmint &lt;- filter(freqperchildlm, term == '(Intercept)')\nfreqperchildlmfreq &lt;- filter(freqperchildlm, term == 'Lg.UK.CDcount')\n\n# -- Make the intercepts plots: order by size\npfreqperchildlmint &lt;- freqperchildlmint %&gt;%\n  ggplot(aes(x = fct_reorder(subjectID, estimate), y = estimate, ymin = estimate - std.error, ymax = estimate + std.error)) + \n  geom_point() + geom_linerange() + \n  theme_bw() + \n  xlab(\"Intercepts in order of size\") +\n  ylab(\"Estimated intercept +/- SE\") + \n  theme(axis.title.y = element_text(size = 10), \n        axis.text.y = element_text(size = 5), \n        axis.text.x = element_blank(), \n        panel.grid = element_blank())\n\n# -- Make the slopes plots: order by size\npfreqperchildlmfreq &lt;- freqperchildlmfreq %&gt;%\n  ggplot(aes(x = fct_reorder(subjectID, estimate), y = estimate, ymin = estimate - std.error, ymax = estimate + std.error)) + \n  geom_point() + geom_linerange() + \n  theme_bw() + \n  xlab(\"Frequency effect coefficient in order of size\") +\n  ylab(\"Estimated coefficient for the frequency effect +/- SE\") + \n  theme(axis.title.y = element_text(size = 10), \n        axis.text.y = element_text(size = 5), \n        axis.text.x = element_blank(), \n        panel.grid = element_blank())\n\n# -- Make a grid to show the plots side by side: in 2 columns\ngrid.arrange(pfreqperchildlmint, pfreqperchildlmfreq,\n             ncol = 2) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat we aim to do is:\n\nModel the effect of word frequency for all the children, including random effects terms to take into account random differences between children in intercepts or in slopes.\nGet the random effects estimates – the between-children differences – and plot them.\n\nCan you figure out how to code these steps?\nFirst, fit the model.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlmer.all.1 &lt;- lmer(RT ~  Lg.UK.CDcount + (Lg.UK.CDcount + 1||subjectID),\n                   \n                   data = long.all.noNAs)\n\n\n\n\nSecond, extract the random effects and plot them.\n\n\n\n\n\n\nHint\n\n\n\n\n\nPlot the distribution of random effects – usually called conditional modes.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nBLUPS &lt;- ranef(lmer.all.1)$subjectID\n# summary(BLUPS)\n\nBLUPS.df &lt;- data.frame(subjectID = rownames(BLUPS), \n                       intercepts = BLUPS[,1], \n                       frequency = BLUPS[,2])\n# summary(BLUPS.df)\n# str(BLUPS.df)\n\np.BLUPS.intercepts &lt;- BLUPS.df %&gt;%\n  ggplot(aes(x = intercepts)) +\n  geom_histogram() + \n  ggtitle(\"(a.) Adjustment by child\") +\n  xlab(\"In the intercept\") +\n  geom_vline(xintercept = 0, colour = \"red\", size = 1.5) +\n  theme_bw()\n\np.BLUPS.slopes &lt;- BLUPS.df %&gt;%\n  ggplot(aes(x = frequency)) +\n  geom_histogram() + \n  ggtitle(\"(b.) Adjustment by child\") +\n  xlab(\"In the frequency slope (coefficient)\") +\n  geom_vline(xintercept = 0, colour = \"red\", size = 1.5) +\n  theme_bw()\n\ngrid.arrange(p.BLUPS.intercepts, p.BLUPS.slopes,\n             ncol = 2\n)\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat we aim to do is:\n\nModel the effect of word frequency for all the children, including (in different models) different random effects terms to take into account (in different ways) random differences between children in intercepts or in slopes.\nGet the random effects estimates – the between-children differences – and plot the model predictions.\n\nCan you figure out how to code these steps?\nFirst, fit the models. We want to compare models with:\n\nRandom effects of participants on intercepts and on slopes.\nRandom effects of participants on intercepts.\nRandom effects of participants on slopes.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\n# -- Fit different kinds of mixed-effects model, then plot the resulting differences in model predictions\n\nlmer.all.1 &lt;- lmer(RT ~  Lg.UK.CDcount + (Lg.UK.CDcount + 1||subjectID),\n                   \n                   data = long.all.noNAs)\n\n# summary(lmer.all.1)\n\nlmer.all.2 &lt;- lmer(RT ~  Lg.UK.CDcount + (1|subjectID),\n                   \n                   data = long.all.noNAs)\n\n# summary(lmer.all.2)\n\nlmer.all.3 &lt;- lmer(RT ~  Lg.UK.CDcount + (Lg.UK.CDcount + 0|subjectID),\n                   \n                   data = long.all.noNAs)\n\n# summary(lmer.all.3)\n\n\n\n\nThen derive predictions of the change in outcome, given variation in predictor values, assuming different random effects structures.\n\n\n\n\n\n\nHint\n\n\n\n\n\nPlot random effect – per person predictions – following:\nhttps://bbolker.github.io/morelia_2018/notes/mixedlab.html\nIn lmer, predict has a re.form argument that specifies which random effects should be included (NA or ~0=none, population level; NULL (=all) or ~subject=prediction at the subject level; more complex models, might have additional nested levels).\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\n# -- get predictions\n\nlong.all.noNAs$pred1 &lt;- predict(lmer.all.1) ## individual level\nlong.all.noNAs$pred2 &lt;- predict(lmer.all.2) ## individual level\nlong.all.noNAs$pred3 &lt;- predict(lmer.all.3) ## individual level\n\n# -- show predictions as slopes on top of raw data\n\np.slopes.intercepts &lt;- long.all.noNAs %&gt;%\n  ggplot(aes(x = Lg.UK.CDcount, y = RT)) +\n  geom_point(alpha = .25, colour = \"darkgrey\") +\n  geom_line(aes(y = pred1, group = subjectID), colour=\"red\", alpha = .4) +\n  ggtitle(\"(c.)\\n(Lg.UK.CDcount + 1||subjectID)\") +\n  xlab(\"Frequency (Lg.UK.CDcount)\") +\n  xlim(0,5) +\n  theme_bw()\n\np.intercepts &lt;- long.all.noNAs %&gt;%\n  ggplot(aes(x = Lg.UK.CDcount, y = RT)) +\n  geom_point(alpha = .25, colour = \"darkgrey\") +\n  geom_line(aes(y = pred2, group = subjectID), colour=\"red\", alpha = .4) +\n  ggtitle(\"(a.)\\n(1|subjectID)\") +\n  xlab(\"Frequency (Lg.UK.CDcount)\") +\n  xlim(0,5) +\n  theme_bw()\n\np.slopes &lt;- long.all.noNAs %&gt;%\n  ggplot(aes(x = Lg.UK.CDcount, y = RT)) +\n  geom_point(alpha = .25, colour = \"darkgrey\") +\n  geom_line(aes(y = pred3, group = subjectID), colour=\"red\", alpha = .4) +\n  ggtitle(\"(b.)\\n(Lg.UK.CDcount + 0|subjectID)\") +\n  xlab(\"Frequency (Lg.UK.CDcount)\") +\n  xlim(0,5) +\n  theme_bw()\n\ngrid.arrange(p.intercepts, p.slopes, p.slopes.intercepts, \n             ncol = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat we aim to do is:\n\nCreate a model for each item, including just the intercept.\nPlot intercepts by-items – ordering items by item_name by estimated intercept size.\n\nCan you figure out how to code these steps?\nFirst, create a model for each item, including just the intercept.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nwperitemlm &lt;- long.all.noNAs %&gt;% \n  group_by(item_name) %&gt;% \n  do(tidy(lm(RT ~ 1, data=.)))\nwperitemlm$term &lt;- as.factor(wperitemlm$term)\n# wperitemlm\n\n\n\n\nThen plot intercepts by-items – ordering items by item_name by estimated intercept size.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\npwperitemlmint &lt;- ggplot(wperitemlm, aes(x = fct_reorder(item_name, estimate), y = estimate, ymin = estimate - std.error, ymax = estimate + std.error))\npwperitemlmint + \n  geom_point() +\n  geom_linerange() + \n  theme_bw() + \n  ylab(\"Estimated coefficient +/- SE\") + \n  xlab(\"Per-item estimates of intercept, ordered by intercept estimate size\") +\n  theme(axis.title.y = element_text(size = 10), \n        axis.text.y = element_text(size = 5), \n        axis.text.x = element_blank(), panel.grid = element_blank())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfter the practical class, we will reveal the answers that are currently hidden.\nThe answers version of the webpage will present my answers for questions, and some extra information where that is helpful.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 17. Workbook introduction to mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/05-ordinal-workbook.html",
    "href": "PSYC412/part2/05-ordinal-workbook.html",
    "title": "Week 20. Workbook introduction to Ordinal (Mixed-effects) Models",
    "section": "",
    "text": "Welcome to your overview of the work we will do together in Week 20.\nWe extend our understanding and skills by moving to examine data where the outcome variable is categorical and ordered: this is a context that requires the use of a class of Generalized Linear (Mixed-effects) Models (GLMMs) usually known as ordinal models.\nOrdinal data are very common in psychological science.\nOften, we will encounter ordinal data recorded as responses to Likert-style items in which the participant is asked to indicate a response on an ordered scale ranging between two end points (Bürkner & Vuorre, 2019; Liddell & Kruschke, 2018). An example of a Likert item is: How well do you think you have understood this text? (Please check one response) where the participant must respond by producing a rating, by checking one option, given nine different response options ranging from 1 (not well at all) to 5 (very well).\nThe critical characteristics of ordinal data values (like the responses recorded to ratings scale, Likert-style, items) are that:\n\nThe responses are discrete or categorical — you must pick one (e.g., 1), you cannot pick more than one at the same time (e.g., 1 and 2), and you cannot have part or fractional values (e.g., you can’t choose the rating 1.5).\nThe responses are ordered — ranging from some minimum value up to some maximum value (e.g., 1 \\(\\rightarrow\\) 2 \\(\\rightarrow\\) 3 \\(\\rightarrow\\) 4 \\(\\rightarrow\\) 5).\n\nOrdinal data can come from a variety of possible psychological mechanisms or processes. We are going to focus on data comprising responses recorded to ratings scale Likert-style items. But ordinal data can also reflect processes in which participants have worked their way through a progression or sequence of decisions (see, e.g., Ricketts et al., 2021).\n\n\n\n\n\n\nImportant\n\n\n\nObserved ordinal outcome values, responses like ratings, look like numbers but it is best to understand these ordinal data values as numeric labels for ordered categorizations.\n\nWhat the different categories correspond to — what dimension, or what data generating processing mechanism — depends on your psychological theory for the processes that drive response production for the task you constructed to collect or observe your data.\n\n\n\nThe challenge we face is that we will aim to develop skills in using ordinal models when, in contrast, most psychological research articles will report analyses of ordinal data using conventional methods like ANOVA or linear regression. We will work to understand why ordinal models are better. We will learn that applying conventional methods to ordinal data will, in principle, involve a poor account of the data and, in practice, will create the risk of producing misleading results. And we will learn how to work with and interpret the results from ordinal models with or without random effects.\nIn our work, we will rely extensively on the ideas set out by Bürkner & Vuorre (2019) and Liddell & Kruschke (2018).\n\n\nHere, we look at ordinal models: we can use these models to analyze outcome variables of different kinds, including outcome variables like ratings responses that are coded using discrete and ordered categories (e.g., how well you think you have understood something, on a scale from 1 to 9).\nIn this workbook, and in our conceptual introduction, our aims are to:\n\nUnderstand through practical experience the reasons for using ordinal models when we analyze ordinal outcome variables.\nPractice running ordinal models with varying random effects structures.\nPractice reporting the results of ordinal models, including through the use of prediction plots.\n\n\n\n\nHere, we will present information about the practical materials you can use to build and practice your skills.\n\n\n\n\n\n\nLinked resources\n\n\n\n\nWe learned about multilevel structured data in the conceptual introduction to multilevel data and the workbook introduction to multilevel data.\nWe then deepened our understanding by looking at the analysis of data from studies with repeated-measures designs in the conceptual introduction to linear mixed-effects models and the workbook introduction to mixed-effects models.\nWe extended our understanding and practice skills in the chapter on developing linear mixed-effects models and the corresponding workbook.\nWe further extended our understanding and practice skills in the conceptual introduction to Generalized Linear Mixed-effects Models (GLMMs) and the workbook introduction to GLMMs.\n\nThis workbook introduction on ordinal models is linked to the corresponding conceptual introduction chapter where the explanation of ideas or of practical analysis steps is set out more extensively or in more depth.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe will be working with a sample of data collected as part of work in progress, undertaken for Clearly understood: health comprehension project (Davies and colleagues).\n\n\n\n\n\n\nWarning\n\n\n\nThese data are unpublished so should not be shared without permission.\n\n\nYou can read more about these data in the conceptual introduction chapter on ordinal models.\nOur interest, in conducting the project, lies in identifying what factors make it easy or difficult to understand written health information (health texts).\nIt is common, in the quality assurance process in the production of health information texts, that text producers ask participants in patient review panels to evaluate draft texts. In such reviews, a participant may be asked a question like “How well do you understand this text?” This kind of question presents a metacognitive task: we are asking a participant to think about their thinking. But it is unclear that people can do this well or, indeed, what factors determine the responses to such questions (Dunlosky & Lipko, 2007).\nWe conducted studies in which we presented adult participants with sampled health information texts and asked them to respond to the question:\n\nHow well do you think you have understood this text?\n\nFor each text, in response to this question, participants were asked to click on one option from an array of response options ranging from 1 (Not well at all) to 9 (Extremely well).\nThe data we collected in this element of our studies comprise, clearly, ordinal responses:\n\nWe are asking participants to reflect on their understanding of the information in health texts.\nWe are asking them to translate their evaluation of their own understanding into a response.\nThe permitted response options are discrete or categorical — you must pick one (e.g., 1), you cannot pick more than one at the same time (e.g., 1 and 2), and you cannot have part or fractional values (e.g., you can’t choose the rating 1.5).\nThe permitted responses are ordered, ranging from some minimum value up to some maximum value (e.g., 1 \\(\\rightarrow\\) 2 \\(\\rightarrow\\) 3 \\(\\rightarrow\\) 4 \\(\\rightarrow\\) 5 \\(\\rightarrow\\) ...), so that if a participant picks a higher response value, they are (in our theory of the task) choosing to signal a higher level of understanding.\n\n\n\n\n\n\n\nImportant\n\n\n\nGet the data: get the data file you can use to do the exercises that will support your learning.\n\nYou can download the 2021-22_PSYC304-health-comprehension.csv file holding the data we analyse in the practical exercises, shown following, by clicking on the link.\n\n\n\nwe collected these data to address the following research question.\n\nWhat factors predict self-evaluated rated understanding of health information.\n\n\n\n\n\n\n\nImportant\n\n\n\nYou can access the sign-in page for R-Studio Server here\n\n\n\n\nThe aims of the practical work are to:\n\nUnderstand through practical experience the reasons for using ordinal models when we analyze ordinal outcome variables.\nPractice running ordinal models with varying random effects structures.\nPractice reporting the results of ordinal models, including through the use of prediction plots.\n\nWe are going to focus on working with Cumulative Link Models (CLMs) and Cumulative Link Mixed-effects Models (CLMMs) in R (Christensen, 2015; Christensen, 2022).\nMy recommendations for learning are that you should aim to:\n\nrun CLMs or CLMMs of demonstration data;\nrun CLMs or CLMMs of the alternate data sets that I reference, or with your own ratings data;\nplay with the .R code used to create practical exercises;\nand edit example code to create alternate visualizations.\n\n\n\n\nNow you will progress through a series of tasks, and challenges, to aid your learning.\n\n\n\n\n\n\nWarning\n\n\n\nWe will work with the data file:\n\n2021-22_PSYC304-health-comprehension.csv\n\n\n\nWe again split the steps into into parts, tasks and questions.\nWe are going to work through the following workflow steps: each step is labelled as a practical part.\n\nSet-up\nLoad the data\nTidy data\nAnalyze the data: working with Cumulative Link Models\nAnalyze the data: working with Cumulative Link Mixed-effects Models\nPresenting and visualizing the effects\nDifferent kinds of ordinal data\n\nIn the following, we will guide you through the tasks and questions step by step.\n\n\n\n\n\n\nImportant\n\n\n\nAn answers version of the workbook will be provided after the practical class.\n\n\n\n\n\nTo begin, we set up our environment in R.\n\n\nUse the library() function to make the functions we need available to you.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlibrary(ggdist)\nlibrary(ggeffects)\nlibrary(here)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nlibrary(lme4)\nlibrary(memisc)\nlibrary(ordinal)\nlibrary(patchwork)\nlibrary(tidyverse)\nlibrary(viridis)\n\n\n\n\nNote that the key library here is {ordinal} (Christensen, 2015; Christensen, 2022). You can find the manual for the {ordinal} library functions in the CRAN webpages.\n\n\n\n\n\n\nRead the data file into R:\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nhealth &lt;- read_csv(\"2021-22_PSYC304-health-comprehension.csv\", \n            na = \"-999\",\n            col_types = cols(\n              ResponseId = col_factor(),\n              rating = col_factor(),\n              GENDER = col_factor(),\n              EDUCATION = col_factor(),\n              ETHNICITY = col_factor(),\n              NATIVE.LANGUAGE = col_factor(),\n              OTHER.LANGUAGE = col_factor(),\n              text.id = col_factor(),\n              text.question.id = col_factor(),\n              study = col_factor()\n            )\n          )\n\n\n\n\nYou can see, here, that within the read_csv() function call, I specify col_types, instructing R how to treat a number of different variables.\n\nYou can read more about this convenient way to control the read-in process here.\n\n\n\n\n\nThe data are already tidy: each column in long.orth_2020-08-11.csv corresponds to a variable and each row corresponds to an observation. However, we need to do a bit of work, before we can run any analyses, to fix the coding of the categorical predictor (or independent) variables: the factors Orthography, Instructions, and Time.\n\n\nIt is always a good to inspect what you have got when you read a data file in to R.\nIf you open the data-set .csv, you will see:\n\n\n\n\n\nResponseId\nAGE\nGENDER\nEDUCATION\nETHNICITY\nNATIVE.LANGUAGE\nOTHER.LANGUAGE\nENGLISH.PROFICIENCY\nSHIPLEY\nHLVA\nFACTOR3\nrating\nresponse\nRDFKGL\nstudy\ntext.id\ntext.question.id\n\n\n\n\nR_sKW4OJnOlidPxrH\n20\nFemale\nFurther\nAsian\nEnglish\nNA\nNA\n26\n8\n59\n8\n1\n10.608\ncs\nstudyone.TEXT.105\nstudyone.TEXT.105.CQ.1\n\n\nR_sKW4OJnOlidPxrH\n20\nFemale\nFurther\nAsian\nEnglish\nNA\nNA\n26\n8\n59\n8\n1\n10.608\ncs\nstudyone.TEXT.105\nstudyone.TEXT.105.CQ.2\n\n\nR_sKW4OJnOlidPxrH\n20\nFemale\nFurther\nAsian\nEnglish\nNA\nNA\n26\n8\n59\n8\n1\n10.608\ncs\nstudyone.TEXT.105\nstudyone.TEXT.105.CQ.3\n\n\nR_sKW4OJnOlidPxrH\n20\nFemale\nFurther\nAsian\nEnglish\nNA\nNA\n26\n8\n59\n8\n1\n10.608\ncs\nstudyone.TEXT.105\nstudyone.TEXT.105.CQ.4\n\n\nR_sKW4OJnOlidPxrH\n20\nFemale\nFurther\nAsian\nEnglish\nNA\nNA\n26\n8\n59\n7\n1\n8.116\ncs\nstudyone.TEXT.94\nstudyone.TEXT.94.CQ.1\n\n\nR_sKW4OJnOlidPxrH\n20\nFemale\nFurther\nAsian\nEnglish\nNA\nNA\n26\n8\n59\n7\n1\n8.116\ncs\nstudyone.TEXT.94\nstudyone.TEXT.94.CQ.2\n\n\nR_sKW4OJnOlidPxrH\n20\nFemale\nFurther\nAsian\nEnglish\nNA\nNA\n26\n8\n59\n7\n1\n8.116\ncs\nstudyone.TEXT.94\nstudyone.TEXT.94.CQ.3\n\n\nR_sKW4OJnOlidPxrH\n20\nFemale\nFurther\nAsian\nEnglish\nNA\nNA\n26\n8\n59\n7\n0\n8.116\ncs\nstudyone.TEXT.94\nstudyone.TEXT.94.CQ.4\n\n\nR_sKW4OJnOlidPxrH\n20\nFemale\nFurther\nAsian\nEnglish\nNA\nNA\n26\n8\n59\n7\n1\n13.278\ncs\nstudyone.TEXT.28\nstudyone.TEXT.28.CQ.1\n\n\nR_sKW4OJnOlidPxrH\n20\nFemale\nFurther\nAsian\nEnglish\nNA\nNA\n26\n8\n59\n7\n1\n13.278\ncs\nstudyone.TEXT.28\nstudyone.TEXT.28.CQ.2\n\n\n\n\n\n\n\nYou can use the scroll bar at the bottom of the data window to view different columns.\nYou can see the columns:\n\nResponseId participant code\nAGE age in years\nGENDER gender code\nEDUCATION education level code\nETHNICITY ethnicity (Office National Statistics categories) code\nNATIVE.LANGUAGE code whether English the native language\nOTHER.LANGUAGE text indicating native language if not English\nENGLISH.PROFICIENCY self-rated English proficiency if native language not English\nSHIPLEY vocabulary knowledge test score\nHLVA health literacy test score\nFACTOR3 reading strategy survey score\nRDFKGL health text (Flesch-Kincaid Grade Level) readability\nstudy study identity code\ntext.id health information text identity code\ntext.question.id health information text question identity code\n\nThe data are structured this way because:\n\nEach participant was asked to read and respond to a sample of health information texts. Each health text is coded text.id. For each text, a participant was asked to respond to a set of multiple choice questions (MCQs) designed to probe their understanding. Each question is coded text.question.id. The accuracy of the response made by each participant to each queston about each text is coded correct (1) or incorrect (0) in the response column.\n\n\nThis data collection process means that, for each participant ID, you are going to see multiple rows of data, one row for each participant, each text they read, and each question they respond to.\n\n\nIn addition to the MCQs, each participant was asked to rate their understanding of the information in each text that they saw.\n\n\nHow well do you think you have understood this text? (Please check one response) where the participant must respond by producing a rating, by checking one option, given nine different response options ranging from 1 (not well at all) to 5 (very well).\nNote that each participant was asked to produce only one rating for each health text that they saw. This means that in the data-set there is one rating response value, for each participant, for each text. This also means that the rating is repeated for each row, where different rows correspond to different MCQs, for each text.\n\nAs we have said, we are going to focus our analyses on the ordered ratings data. And, as noted, these rating responses are categorical and ordered: ranging across potential values, from 1 \\(\\rightarrow\\) 2 \\(\\rightarrow\\) 3 \\(\\rightarrow\\) 4 \\(\\rightarrow\\) 5 \\(\\rightarrow\\) 6 \\(\\rightarrow\\) 7 \\(\\rightarrow\\) 8 \\(\\rightarrow\\) 9.\n\n\n\nThe most important task is to assess the nature of the outcome ratings variable: what have we got?\n\nPract.Q.1. What is the frequency distribution of responses under each possible rating response value (or category): can you visualize the distribution?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThere are different ways to assess the distribution of ratings responses.\n\nWe are going to ignore the fact that ratings are repeated within participants across question-rows for each text because we are primarily interested in the relative proportion of ratings responses recorded for different possible ratings.\nWe can focus on getting a count of the number of ratings responses recorded, for this sample of participants and texts, for each possible rating response value.\nWe can visualize the counts in different ways: you will have seen bar plots but we focus here on dot plots.\n\nHow would you de-duplicate the ratings data to get counts showing just the number of ratings (made by participants to texts) for each possible rating response value?\n\n\n\n\nProduce a plot to visualize the distribution of ratings responses.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nhealth &lt;- health %&gt;% mutate(rating = fct_relevel(rating, sort))\n\nhealth %&gt;%\n  group_by(rating) %&gt;%\n  summarise(count = n()) %&gt;%\n  ggplot(aes(x = rating, y = count, colour = rating)) + \n  geom_point(size = 3) +\n  scale_color_viridis(discrete=TRUE, option = \"mako\") + theme_bw() +\n  theme(\n    panel.grid.major.y = element_blank()  # No horizontal grid lines\n  ) +\n  coord_flip()\n\n\n\n\n\n\n\nFigure 1: Dot plot showing the distribution of ratings responses. The Likert-style questions in the surveys asked participants to rate their level of understanding of the texts they saw on a scale from 1 (not well) to 9 (extremely well). The plot shows the number of responses recorded for each response option, over all participants and all texts.\n\n\n\n\n\n\n\n\n\nPract.Q.2. What does the plot show us?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nIn analyzing these data, we will seek to estimate what information available to us can be used to predict whether a participant’s rating of their understanding is more likely to be, say, 1 or 2, 2 or 3 … 7 or 8, 8 or 9.\n\n\n\n\n\n\n\nAs explained, when we do analyses using ordinal models, we expect to work with categorical and ordered outcome response values. We know that, given the study and task design, that the ratings responses should be treated as ordered and categorical data but we need to check that R agrees that the ratings data are ordered and categorical.\n\nPract.Q.3. How can we tell that R treats values in the outcome rating column as ordered and categorical data?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nJust because the data-set looks to us like values in the outcome rating column are ordered and categorical does not mean that R sees the values as we would prefer it to.\n\nWe can try to identify how R classifies values in the rating column using the summary() function we have been using.\nWe can try a different function: str().\n\n\n\n\n\nPract.Q.4. What does a summary of the data-set show us?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nsummary(health)\n\n             ResponseId        AGE                     GENDER    \n R_sKW4OJnOlidPxrH:  20   Min.   :18.0   Female           :2900  \n R_27paPJzIutLoqk8:  20   1st Qu.:20.0   Male             :1120  \n R_1nW0lpFdfumlI1p:  20   Median :27.0   Prefer-not-to-say:  20  \n R_31ZqPQpNEEapoW8:  20   Mean   :34.3                           \n R_2whvE2IW90nj2P7:  20   3rd Qu.:50.0                           \n R_3CAxrri9clBT7sl:  20   Max.   :81.0                           \n (Other)          :3920                                          \n     EDUCATION    ETHNICITY    NATIVE.LANGUAGE    OTHER.LANGUAGE\n Further  :1780   Asian: 680   English:2720    NA        :2720  \n Higher   :1800   White:3260   Other  :1320    Polish    : 580  \n Secondary: 460   Other:  40                   Cantonese : 280  \n                  Mixed:  60                   Chinese   : 120  \n                                               Portuguese:  60  \n                                               polish    :  60  \n                                               (Other)   : 220  \n ENGLISH.PROFICIENCY    SHIPLEY           HLVA           FACTOR3     \n Length:4040         Min.   :15.00   Min.   : 3.000   Min.   :17.00  \n Class :character    1st Qu.:30.00   1st Qu.: 7.000   1st Qu.:45.00  \n Mode  :character    Median :34.00   Median : 9.000   Median :49.00  \n                     Mean   :32.97   Mean   : 8.564   Mean   :49.03  \n                     3rd Qu.:37.00   3rd Qu.:10.000   3rd Qu.:55.00  \n                     Max.   :40.00   Max.   :13.000   Max.   :63.00  \n                                                                     \n     rating        response          RDFKGL       study    \n 8      :1044   Min.   :0.0000   Min.   : 4.552   cs: 480  \n 7      : 916   1st Qu.:1.0000   1st Qu.: 6.358   jg:1120  \n 9      : 824   Median :1.0000   Median : 8.116   ml: 720  \n 6      : 500   Mean   :0.8064   Mean   : 7.930   rw:1720  \n 5      : 352   3rd Qu.:1.0000   3rd Qu.: 9.413            \n 4      : 176   Max.   :1.0000   Max.   :13.278            \n (Other): 228                                              \n             text.id                  text.question.id\n studyone.TEXT.37: 344   studyone.TEXT.37.CQ.1:  86   \n studyone.TEXT.39: 344   studyone.TEXT.37.CQ.2:  86   \n studyone.TEXT.72: 344   studyone.TEXT.37.CQ.3:  86   \n studyone.TEXT.14: 344   studyone.TEXT.37.CQ.4:  86   \n studyone.TEXT.50: 344   studyone.TEXT.39.CQ.1:  86   \n studyone.TEXT.10: 224   studyone.TEXT.39.CQ.2:  86   \n (Other)         :2096   (Other)              :3524   \n\n\n\n\n\n\n\n\nPract.Q.5. What does a structural summary of the rating variable show us?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe can use the str() function to get a concise summary of the rating variable, showing if or how R classifies the variable values as categorical and ordered.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nstr(health$rating)\n\n Factor w/ 9 levels \"1\",\"2\",\"3\",\"4\",..: 8 8 8 8 7 7 7 7 7 7 ...\n\n\n\n\n\n\n\nPract.Q.6. How can we check if R thinks that the rating variable values are categorical and ordered?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou have seen, previously, that you can check how R classifies the variables like the rating variable using is._() type functions.\n\nYou can do the same here using is.ordered().\n\nYou can read more about these functions here.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nis.ordered(factor(health$rating))\n\n[1] FALSE\n\n\n\n\n\n\nPract.Q.7. If you can work out how to check if R thinks that the rating variable values are categorical and ordered, what does that check tell us?\n\n\n\n\n\nThe ordinal model estimates the locations (thresholds) for where to split the latent scale (the continuum underlying the ratings) corresponding to different ratings values. If we do not make sure that the outcome factor variable is split as it should be then there is no guarantee that {ordinal} functions will estimate the thresholds in the right order (i.e., 1,2,3 ... rather than 3,2,1...).\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe can make sure that the confidence rating factor is ordered precisely as we wish using the ordered() function.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nhealth$rating &lt;- ordered(health$rating,\n      levels = c(\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"))\n\n\n\n\n\nPract.Q.8. How can we check if R thinks that the rating variable values are categorical and ordered?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou have seen how to do this: we need to do checks using a combination of is._() function calls.\n\n\n\nWe can do a check to see that rating is treated as an ordered factor.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nis.numeric(health$rating)\n\n[1] FALSE\n\nis.factor(health$rating)\n\n[1] TRUE\n\nstr(health$rating)\n\n Ord.factor w/ 9 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 8 8 8 8 7 7 7 7 7 7 ...\n\nis.ordered(health$rating)\n\n[1] TRUE\n\n\n\n\n\n\nPract.Q.9. If you can work out how to check if R thinks that the rating variable values are categorical and ordered, what do the checks tell us?\n\n\n\n\n\n\n\n\nBefore doing any modelling, it will be sensible to standardize potential predictors.\n\nCan you work out how to do this?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThere are different ways to do this. Usually, we want to use the scale() function to do the work.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nhealth &lt;- health %&gt;% \n  mutate(across(c(AGE, SHIPLEY, HLVA, FACTOR3, RDFKGL), \n                scale, center = TRUE, scale = TRUE,\n                .names = \"z_{.col}\"))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(...)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\nYou can see that in this chunk of code, we are doing a number of things:\n\nhealth &lt;- health %&gt;% recreates the health dataset from the following steps.\nmutate(...) do an operation which retains the existing variables in the dataset, to change the variables as further detailed.\nacross(...) work with the multple column variables that are named in the c(AGE, SHIPLEY, HLVA, FACTOR3, RDFKGL) set.\n...scale, center = TRUE, scale = TRUE... here is where we do the standardization work.\n\nWhat we are asking for is that R takes the variables we name and standardizes each of them.\n\n.names = \"z_{.col}\") creates the standardized variables under adapted names, adding z_ to the original column name so that we can distinguish between the standardized and original raw versions of the data columns.\n\nNote that the across() function is a useful function for applying a function across multiple column variables see information here There is a helpful discussion on how we can do this task here\nWe can then check that we have produced the standardized variables as required.\n\nsummary(health)\n\n             ResponseId        AGE                     GENDER    \n R_sKW4OJnOlidPxrH:  20   Min.   :18.0   Female           :2900  \n R_27paPJzIutLoqk8:  20   1st Qu.:20.0   Male             :1120  \n R_1nW0lpFdfumlI1p:  20   Median :27.0   Prefer-not-to-say:  20  \n R_31ZqPQpNEEapoW8:  20   Mean   :34.3                           \n R_2whvE2IW90nj2P7:  20   3rd Qu.:50.0                           \n R_3CAxrri9clBT7sl:  20   Max.   :81.0                           \n (Other)          :3920                                          \n     EDUCATION    ETHNICITY    NATIVE.LANGUAGE    OTHER.LANGUAGE\n Further  :1780   Asian: 680   English:2720    NA        :2720  \n Higher   :1800   White:3260   Other  :1320    Polish    : 580  \n Secondary: 460   Other:  40                   Cantonese : 280  \n                  Mixed:  60                   Chinese   : 120  \n                                               Portuguese:  60  \n                                               polish    :  60  \n                                               (Other)   : 220  \n ENGLISH.PROFICIENCY    SHIPLEY           HLVA           FACTOR3     \n Length:4040         Min.   :15.00   Min.   : 3.000   Min.   :17.00  \n Class :character    1st Qu.:30.00   1st Qu.: 7.000   1st Qu.:45.00  \n Mode  :character    Median :34.00   Median : 9.000   Median :49.00  \n                     Mean   :32.97   Mean   : 8.564   Mean   :49.03  \n                     3rd Qu.:37.00   3rd Qu.:10.000   3rd Qu.:55.00  \n                     Max.   :40.00   Max.   :13.000   Max.   :63.00  \n                                                                     \n     rating        response          RDFKGL       study    \n 8      :1044   Min.   :0.0000   Min.   : 4.552   cs: 480  \n 7      : 916   1st Qu.:1.0000   1st Qu.: 6.358   jg:1120  \n 9      : 824   Median :1.0000   Median : 8.116   ml: 720  \n 6      : 500   Mean   :0.8064   Mean   : 7.930   rw:1720  \n 5      : 352   3rd Qu.:1.0000   3rd Qu.: 9.413            \n 4      : 176   Max.   :1.0000   Max.   :13.278            \n (Other): 228                                              \n             text.id                  text.question.id\n studyone.TEXT.37: 344   studyone.TEXT.37.CQ.1:  86   \n studyone.TEXT.39: 344   studyone.TEXT.37.CQ.2:  86   \n studyone.TEXT.72: 344   studyone.TEXT.37.CQ.3:  86   \n studyone.TEXT.14: 344   studyone.TEXT.37.CQ.4:  86   \n studyone.TEXT.50: 344   studyone.TEXT.39.CQ.1:  86   \n studyone.TEXT.10: 224   studyone.TEXT.39.CQ.2:  86   \n (Other)         :2096   (Other)              :3524   \n           z_AGE.V1                   z_SHIPLEY.V1        \n Min.   :-9.82624656687e-01   Min.   :-3.294105320780000  \n 1st Qu.:-8.62035239524e-01   1st Qu.:-0.543722537102000  \n Median :-4.39972279452e-01   Median : 0.189712871877000  \n Mean   : 4.20000000000e-15   Mean   :-0.000000000000001  \n 3rd Qu.: 9.46806017926e-01   3rd Qu.: 0.739789428612000  \n Max.   : 2.81594198396e+00   Max.   : 1.289865985350000  \n                                                          \n          z_HLVA.V1                  z_FACTOR3.V1        \n Min.   :-2.60748865140e+00   Min.   :-4.20593553983000  \n 1st Qu.:-7.33066204487e-01   1st Qu.:-0.52915479589300  \n Median : 2.04145018971e-01   Median :-0.00390040390093  \n Mean   : 1.00000000000e-16   Mean   : 0.00000000000000  \n 3rd Qu.: 6.72750630700e-01   3rd Qu.: 0.78398118408700  \n Max.   : 2.07856746589e+00   Max.   : 1.83448996807000  \n                                                         \n         z_RDFKGL.V1         \n Min.   :-1.46446595688e+00  \n 1st Qu.:-6.81459422242e-01  \n Median : 8.07363074868e-02  \n Mean   : 6.99000000000e-14  \n 3rd Qu.: 6.43061598419e-01  \n Max.   : 2.31876495189e+00  \n                             \n\n\n\n\n\n\n\n\n\nIn our first analysis, we can begin by assuming no random effects. We keep things simple at this point so that we can focus on the key changes in model coding.\nThe model is fitted to examine what shapes the variation in rating responses that we see in Figure 1.\n\n\nOur research question is:\n\n\n\n\n\n\nNote\n\n\n\n\nWhat factors predict self-evaluated rated understanding of health information.\n\n\n\nIn this first analysis:\n\nThe outcome variable is the ordinal response variable rating.\nThe predictors consist of the variables we standardized earlier.\n\nWe use the clm() function from the {ordinal} library to do the analysis.\n\nCan you work out how to code the analysis?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe key idea — and you may already have guessed this — is that the structure of model analysis code in R is often similar across the different kinds of modeling methods we may employ, in the context of working with different kinds of data structure, different outcome variables.\n\n\n\nWhat we want is a cumulative link model with:\n\nrating as the outcome variable;\nstandardized age, vocabulary, health literacy, reading strategy, and text readability as factors.\n\n\n\n\n\n\n\nTip\n\n\n\nHere, we ignore any data clustering we may think or know structures the data at multiple levels: we do not specify random effects.\n\n\nYou can check out the information on the modeling methods in Christensen (2022) and Christensen (2015).\nYou can also find the manual for the {ordinal} library functions that we need to use here.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nhealth.clm &lt;- clm(rating ~\n                    \n  z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL,\n                  \n  Hess = TRUE, link = \"logit\",\n  data = health)\n\nsummary(health.clm)\n\nformula: rating ~ z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL\ndata:    health\n\n link  threshold nobs logLik   AIC      niter max.grad cond.H \n logit flexible  4040 -6880.78 13787.55 5(0)  9.26e-07 7.3e+01\n\nCoefficients:\n          Estimate Std. Error z value Pr(&gt;|z|)    \nz_AGE     -0.17719    0.02966  -5.975 2.30e-09 ***\nz_SHIPLEY  0.34384    0.03393  10.135  &lt; 2e-16 ***\nz_HLVA     0.16174    0.03265   4.954 7.28e-07 ***\nz_FACTOR3  0.74535    0.03145  23.699  &lt; 2e-16 ***\nz_RDFKGL  -0.27220    0.02892  -9.412  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2 -4.65066    0.12978 -35.836\n2|3 -4.13902    0.10395 -39.817\n3|4 -3.26845    0.07390 -44.228\n4|5 -2.56826    0.05804 -44.248\n5|6 -1.69333    0.04437 -38.160\n6|7 -0.87651    0.03671 -23.876\n7|8  0.24214    0.03402   7.117\n8|9  1.63049    0.04252  38.346\n\n\nThe code works as follows.\nFirst, we have a chunk of code mostly similar to what we have done before, but changing the function.\n\nclm() the function name changes because now we want a cumulative link model of the ordinal responses.\n\nThe model specification includes information about the fixed effects, the predictors: z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL.\nSecond, we have the bit that is specific to cumulative link models fitted using the clm() function.\n\nHess = TRUE is required if we want to get a summary of the model fit; the default is TRUE but it is worth being explicit about it.\nlink = \"logit\" specifies that we want to model the ordinal responses in terms of the log odds (hence, the probability) that a response is a low or a high rating value.\n\n\n\n\n\nPract.Q.10. If you can work out how to fit the ordinal model of rating variable values, what does the results summary show about the estimated effects of the predictors entered into the model?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe summary() output for the model is similar to the outputs you have seen for other model types.\n\nWe first get formula: information about the model you have specified.\nR will tell us what data: we are working with.\nWe then get Coefficients: estimates.\n\nThe table summary of coefficients arranges information in ways that will be familiar you:\n\nFor each predictor variable, we see Estimate, Std. Error, z value, and Pr(&gt;|z|) statistics.\nThe Pr(&gt;|z|) p-values are based on Wald tests of the null hypothesis that a predictor has null impact.\nThe coefficient estimates can be interpreted based on whether they are positive or negative.\n\n\nWe then get Threshold coefficients: indicating where the model fitted estimates the threshold locations: where the latent scale is cut, corresponding to different rating values.\n\n\n\n\n\n\n–&gt;  –&gt;\n\n–&gt;\n\n\n\n\n\n\nHint\n\n\n\n\n\nIn reporting ordinal (e.g., cumulative link) models, we typically focus on the coefficient estimates for the predictor variables.\n\nA positive coefficient estimate indicates that higher values of the predictor variable are associated with greater probability of higher rating values.\nA negative coefficient estimate indicates that higher values of the predictor variable are associated with greater probability of lower rating values.\n\n\n\n\n\n\n\nIt will be a good idea for you to get some practice specifying cumulative link models.\nFit a new model:\n\nUse a different set of predictors.\nExperiment with the arguments used to control how the model fitting function works.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nYou can choose whatever set of predictors you like, given the health data-set.\nOne way to understand how model functions work (in addition to reading manual information) is to experiment with what you do or do not specify, among the function arguments, in controlling the function. Even causing errors can be helpul to your undestanding.\n\n\n\n\nWhat we want is a cumulative link model with:\n\nrating as the outcome variable;\nsome combination of (one or all of) standardized age, vocabulary, health literacy, reading strategy, and text readability as factors.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nTry changing one element at a time, for example, here, simply removing the arguments entered to control how the clm() function works.\n\nhealth.clm &lt;- clm(rating ~\n                    \n  z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL,\n                  \n  data = health)\n\nsummary(health.clm)\n\nformula: rating ~ z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL\ndata:    health\n\n link  threshold nobs logLik   AIC      niter max.grad cond.H \n logit flexible  4040 -6880.78 13787.55 5(0)  9.26e-07 7.3e+01\n\nCoefficients:\n          Estimate Std. Error z value Pr(&gt;|z|)    \nz_AGE     -0.17719    0.02966  -5.975 2.30e-09 ***\nz_SHIPLEY  0.34384    0.03393  10.135  &lt; 2e-16 ***\nz_HLVA     0.16174    0.03265   4.954 7.28e-07 ***\nz_FACTOR3  0.74535    0.03145  23.699  &lt; 2e-16 ***\nz_RDFKGL  -0.27220    0.02892  -9.412  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2 -4.65066    0.12978 -35.836\n2|3 -4.13902    0.10395 -39.817\n3|4 -3.26845    0.07390 -44.228\n4|5 -2.56826    0.05804 -44.248\n5|6 -1.69333    0.04437 -38.160\n6|7 -0.87651    0.03671 -23.876\n7|8  0.24214    0.03402   7.117\n8|9  1.63049    0.04252  38.346\n\n\nHere, we add in another variable:\n\nhealth.clm &lt;- clm(rating ~\n                    \n  NATIVE.LANGUAGE + z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL,\n                  \n  data = health)\n\nsummary(health.clm)\n\nformula: \nrating ~ NATIVE.LANGUAGE + z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL\ndata:    health\n\n link  threshold nobs logLik   AIC      niter max.grad cond.H \n logit flexible  4040 -6874.81 13777.63 6(0)  1.23e-12 7.8e+01\n\nCoefficients:\n                     Estimate Std. Error z value Pr(&gt;|z|)    \nNATIVE.LANGUAGEOther  0.22901    0.06637   3.450  0.00056 ***\nz_AGE                -0.17238    0.02966  -5.812 6.18e-09 ***\nz_SHIPLEY             0.35725    0.03413  10.466  &lt; 2e-16 ***\nz_HLVA                0.18076    0.03306   5.468 4.55e-08 ***\nz_FACTOR3             0.73665    0.03157  23.336  &lt; 2e-16 ***\nz_RDFKGL             -0.30030    0.03003 -10.001  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2 -4.57653    0.13135 -34.843\n2|3 -4.06620    0.10584 -38.418\n3|4 -3.19670    0.07648 -41.795\n4|5 -2.49814    0.06125 -40.787\n5|6 -1.62541    0.04839 -33.591\n6|7 -0.80807    0.04165 -19.400\n7|8  0.31593    0.04025   7.849\n8|9  1.71140    0.04878  35.086\n\n\n\n\n\n\n\n\n\nIn our analysis, we began by assuming no random effects. However, this is unlikely to be appropriate given the data collection process deployed in the Clearly understood project, where a sample of participants was asked to respond to a sample of texts, because that data collection process means that:\n\nwe have multiple observations of responses for each participant;\nwe have multiple observations of responses for each stimulus text;\nparticipants were assigned to groups, and within a group all participants were asked to respond to the same stimulus texts.\n\nThese features ensure that the data have a multilevel structure and this structure requires us to fit a Cumulative Link Mixed-effects Model (CLMM).\n\n\nOur research question is:\n\n\n\n\n\n\nNote\n\n\n\n\nWhat factors predict self-evaluated rated understanding of health information.\n\n\n\nYou can guess that fitting a CLMM is not going to be very different from specifying and running a GLMM.\n\nCan you work out how to code the model?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe key idea — and you may already have guessed this — is again that the structure of model analysis code in R is often similar across the different kinds of modeling methods we may employ.\n\nHere, the main insight for your development is that we are going to want to change things one thing at a time so take the clm() code you used earlier and add the element (a random effect) that we now need to take into account the multilevel structure we know is there.\n\n\n\n\nWhat we want is a cumulative link mixed-effects model with:\n\nrating as the outcome variable;\nstandardized age, vocabulary, health literacy, reading strategy, and text readability as factors;\na random effect to take into account the fact that each participant is producing ratings to multiple different texts.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nWe can code a Cumulative Link Mixed-effects Model as follows.\n\nhealth.clmm &lt;- clmm(rating ~\n                      \n                      z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL +\n                      \n                      (1 | ResponseId),\n                    \n                    Hess = TRUE, link = \"logit\",\n                    data = health)\n\nsummary(health.clmm)\n\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: rating ~ z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL +  \n    (1 | ResponseId)\ndata:    health\n\n link  threshold nobs logLik   AIC     niter       max.grad cond.H \n logit flexible  4040 -4978.08 9984.16 1490(19081) 2.78e-03 3.5e+02\n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ResponseId (Intercept) 9.825    3.134   \nNumber of groups:  ResponseId 202 \n\nCoefficients:\n          Estimate Std. Error z value Pr(&gt;|z|)    \nz_AGE     -0.44313    0.23474  -1.888   0.0591 .  \nz_SHIPLEY  0.77130    0.26497   2.911   0.0036 ** \nz_HLVA     0.20812    0.25608   0.813   0.4164    \nz_FACTOR3  1.68345    0.23822   7.067 1.58e-12 ***\nz_RDFKGL  -0.44345    0.03677 -12.060  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2  -9.3297     0.3452 -27.029\n2|3  -8.1413     0.2946 -27.633\n3|4  -6.5243     0.2631 -24.799\n4|5  -5.2159     0.2490 -20.949\n5|6  -3.4668     0.2369 -14.631\n6|7  -1.8481     0.2314  -7.985\n7|8   0.2966     0.2294   1.293\n8|9   3.0417     0.2346  12.967\n\n\nIf you inspect the code chunk, you can see that we have made two changes.\nFirst, we have changed the function.\n\nclmm() the function name changes because now we want a Cumulative Linear Mixed-effects Model.\n\nSecondly, the model specification includes information about fixed effects and now about random effects.\n\nWith (1 | ResponseId) we include include a random effect of participants on categorization thresholds.\n\n\n\n\n\nPract.Q.11. If you can work out how to fit the CLMM of rating variable values, what does the results summary show about the estimated effects of the predictors entered into the model? Compare the results for the CLMM with the results for the CLM.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe summary() output for the model is similar to the outputs you have seen for other mixed-effects model types. Having included a random effect (of participants on intercepts, strictly, on categorization thresholds):\n\nWe now get a Random effects variance estimates.\nWe again get Coefficients: estimates.\n\nThe coefficients summary information can be interpreted in the same way for a CLMM as for a CLM.\n\n\n\n\n\n\n\n\nIt will be helpful for the interpretation of the estimates of the coefficients of these predictor variables if we visualize the predictions we can make, about how rating values vary, given differences in predictor variable values, given our model estimates.\n\n\nWe can produce a visualization of the predictions from the ordinal models we fit, in order to be able to understand and to explain the model results.\n\nCan you work out how to get and visualize model predictions?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou have already had some experience in doing visualizations of model predictions. The steps you need to complete are steps you have made before:\n\nFit a model;\nGet predictions about outcome variation, given the model;\nShow those predictions in a plot.\n\nWe can do this using functions from the {ggeffects} library.\n\nYou can read more about the {ggeffects} library here where you will see a collection of articles explaining what you can do, and why, as well as technical information including some helpful tutorials.\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nSo: we can complete the steps identified, as we have before, for other kinds of model, other data-sets:\n\nFit a model\n\n\nhealth.clmm &lt;- clmm(rating ~\n                      \n                      z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL +\n                      \n                      (1 | ResponseId),\n                    \n                    Hess = TRUE, link = \"logit\",\n                    data = health)\n\nsummary(health.clmm)\n\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: rating ~ z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL +  \n    (1 | ResponseId)\ndata:    health\n\n link  threshold nobs logLik   AIC     niter       max.grad cond.H \n logit flexible  4040 -4978.08 9984.16 1490(19081) 2.78e-03 3.5e+02\n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ResponseId (Intercept) 9.825    3.134   \nNumber of groups:  ResponseId 202 \n\nCoefficients:\n          Estimate Std. Error z value Pr(&gt;|z|)    \nz_AGE     -0.44313    0.23474  -1.888   0.0591 .  \nz_SHIPLEY  0.77130    0.26497   2.911   0.0036 ** \nz_HLVA     0.20812    0.25608   0.813   0.4164    \nz_FACTOR3  1.68345    0.23822   7.067 1.58e-12 ***\nz_RDFKGL  -0.44345    0.03677 -12.060  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2  -9.3297     0.3452 -27.029\n2|3  -8.1413     0.2946 -27.633\n3|4  -6.5243     0.2631 -24.799\n4|5  -5.2159     0.2490 -20.949\n5|6  -3.4668     0.2369 -14.631\n6|7  -1.8481     0.2314  -7.985\n7|8   0.2966     0.2294   1.293\n8|9   3.0417     0.2346  12.967\n\n\n\nGet predictions about outcome variation, given the model\n\n\ndat &lt;- ggpredict(health.clmm, terms=\"z_FACTOR3 [all]\")\n\nYou are calculating adjusted predictions on the population-level (i.e.\n  `type = \"fixed\"`) for a *generalized* linear mixed model.\n  This may produce biased estimates due to Jensen's inequality. Consider\n  setting `bias_correction = TRUE` to correct for this bias.\n  See also the documentation of the `bias_correction` argument.\n\n\n\nIn this line, we use ggpredict() to work with some model information, assuming we previously fitted a model and gave it a name (here, health.clmm).\nWhen we use ggpredict(), we ask R to take that model information and, for the term we specify, here, specify using terms=\"z_FACTOR3 [all]\", we ask R to generate some predictions.\ndat &lt;- ggpredict(...) asks R to put those predictions in an object called dat.\n\n\nShow those predictions in a plot\n\n\nplot(dat)\n\n\n\n\n\n\n\n\n\n\n\n\nPract.Q.12. What does the plot show us?\n\n\nFit the model, get the predictions, and visualize them; see Figure 2.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\ndat &lt;- ggpredict(health.clmm, terms=\"z_FACTOR3 [all]\")\nplot(dat)\n\n\n\n\n\n\n\nFigure 2: A grid of plots showing marginal or conditional predicted probabilities that a rating response will have one value (among the 1-9 rating values possible), indicating how these predicted probabilities vary given variation in values of the standardized reading strategy (FACTOR3) variable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote that in our conceptual introduction, we examine visualizations of ordinal predictions in more depth, with a discussion of alternate visualization methods.\n\n\n\n\n\n\nAs the review reported by Liddell & Kruschke (2018) suggests, we may have many many studies in which ordinal outcome data are analysed but very few published research reports that present analyses of ordinal data using ordinal models.\nYou can see two examples in the papers published by Ricketts et al. (2021) and by Rodríguez-Ferreiro et al. (2020).\nThese papers are both published open accessible, so that they are freely available, and they are both associated with accessible data repositories.\n\nYou can find the repository for Ricketts et al. (2021) here.\nYou can find the repository for Rodríguez-Ferreiro et al. (2020) here.\n\nThe Rodríguez-Ferreiro et al. (2020) shares a data .csv only.\nThe Ricketts et al. (2021) repository shares data and analysis code as well as a fairly detailed guide to the analysis methods. Note that the core analysis approach taken in Ricketts et al. (2021) is based on Bayesian methods but that we also conduct clmm() models using the {ordinal} library functions discussed here; these models are labelled frequentist models and can be found under sensitivity analyses.\nFor what it’s worth, the Ricketts et al. (2021) is much more representative of the analysis approach I would recommend now.\n\n\n\n\n\n\nTip\n\n\n\nGo ahead and take a look at the repositories.\n\nExperiment with attempting to reproduce the reported analyses.\n\n\n\n\n\n\n\nAfter the practical class, we will reveal the answers that are currently hidden.\nThe answers version of the webpage will present my answers for questions, and some extra information where that is helpful.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 20. Workbook introduction to Ordinal (Mixed-effects) Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/05-ordinal-workbook.html#sec-ordinal-workbook-targets",
    "href": "PSYC412/part2/05-ordinal-workbook.html#sec-ordinal-workbook-targets",
    "title": "Week 20. Workbook introduction to Ordinal (Mixed-effects) Models",
    "section": "",
    "text": "Here, we look at ordinal models: we can use these models to analyze outcome variables of different kinds, including outcome variables like ratings responses that are coded using discrete and ordered categories (e.g., how well you think you have understood something, on a scale from 1 to 9).\nIn this workbook, and in our conceptual introduction, our aims are to:\n\nUnderstand through practical experience the reasons for using ordinal models when we analyze ordinal outcome variables.\nPractice running ordinal models with varying random effects structures.\nPractice reporting the results of ordinal models, including through the use of prediction plots.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 20. Workbook introduction to Ordinal (Mixed-effects) Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/05-ordinal-workbook.html#sec-ordinal-workbook-resources",
    "href": "PSYC412/part2/05-ordinal-workbook.html#sec-ordinal-workbook-resources",
    "title": "Week 20. Workbook introduction to Ordinal (Mixed-effects) Models",
    "section": "",
    "text": "Here, we will present information about the practical materials you can use to build and practice your skills.\n\n\n\n\n\n\nLinked resources\n\n\n\n\nWe learned about multilevel structured data in the conceptual introduction to multilevel data and the workbook introduction to multilevel data.\nWe then deepened our understanding by looking at the analysis of data from studies with repeated-measures designs in the conceptual introduction to linear mixed-effects models and the workbook introduction to mixed-effects models.\nWe extended our understanding and practice skills in the chapter on developing linear mixed-effects models and the corresponding workbook.\nWe further extended our understanding and practice skills in the conceptual introduction to Generalized Linear Mixed-effects Models (GLMMs) and the workbook introduction to GLMMs.\n\nThis workbook introduction on ordinal models is linked to the corresponding conceptual introduction chapter where the explanation of ideas or of practical analysis steps is set out more extensively or in more depth.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe will be working with a sample of data collected as part of work in progress, undertaken for Clearly understood: health comprehension project (Davies and colleagues).\n\n\n\n\n\n\nWarning\n\n\n\nThese data are unpublished so should not be shared without permission.\n\n\nYou can read more about these data in the conceptual introduction chapter on ordinal models.\nOur interest, in conducting the project, lies in identifying what factors make it easy or difficult to understand written health information (health texts).\nIt is common, in the quality assurance process in the production of health information texts, that text producers ask participants in patient review panels to evaluate draft texts. In such reviews, a participant may be asked a question like “How well do you understand this text?” This kind of question presents a metacognitive task: we are asking a participant to think about their thinking. But it is unclear that people can do this well or, indeed, what factors determine the responses to such questions (Dunlosky & Lipko, 2007).\nWe conducted studies in which we presented adult participants with sampled health information texts and asked them to respond to the question:\n\nHow well do you think you have understood this text?\n\nFor each text, in response to this question, participants were asked to click on one option from an array of response options ranging from 1 (Not well at all) to 9 (Extremely well).\nThe data we collected in this element of our studies comprise, clearly, ordinal responses:\n\nWe are asking participants to reflect on their understanding of the information in health texts.\nWe are asking them to translate their evaluation of their own understanding into a response.\nThe permitted response options are discrete or categorical — you must pick one (e.g., 1), you cannot pick more than one at the same time (e.g., 1 and 2), and you cannot have part or fractional values (e.g., you can’t choose the rating 1.5).\nThe permitted responses are ordered, ranging from some minimum value up to some maximum value (e.g., 1 \\(\\rightarrow\\) 2 \\(\\rightarrow\\) 3 \\(\\rightarrow\\) 4 \\(\\rightarrow\\) 5 \\(\\rightarrow\\) ...), so that if a participant picks a higher response value, they are (in our theory of the task) choosing to signal a higher level of understanding.\n\n\n\n\n\n\n\nImportant\n\n\n\nGet the data: get the data file you can use to do the exercises that will support your learning.\n\nYou can download the 2021-22_PSYC304-health-comprehension.csv file holding the data we analyse in the practical exercises, shown following, by clicking on the link.\n\n\n\nwe collected these data to address the following research question.\n\nWhat factors predict self-evaluated rated understanding of health information.\n\n\n\n\n\n\n\nImportant\n\n\n\nYou can access the sign-in page for R-Studio Server here\n\n\n\n\nThe aims of the practical work are to:\n\nUnderstand through practical experience the reasons for using ordinal models when we analyze ordinal outcome variables.\nPractice running ordinal models with varying random effects structures.\nPractice reporting the results of ordinal models, including through the use of prediction plots.\n\nWe are going to focus on working with Cumulative Link Models (CLMs) and Cumulative Link Mixed-effects Models (CLMMs) in R (Christensen, 2015; Christensen, 2022).\nMy recommendations for learning are that you should aim to:\n\nrun CLMs or CLMMs of demonstration data;\nrun CLMs or CLMMs of the alternate data sets that I reference, or with your own ratings data;\nplay with the .R code used to create practical exercises;\nand edit example code to create alternate visualizations.\n\n\n\n\nNow you will progress through a series of tasks, and challenges, to aid your learning.\n\n\n\n\n\n\nWarning\n\n\n\nWe will work with the data file:\n\n2021-22_PSYC304-health-comprehension.csv\n\n\n\nWe again split the steps into into parts, tasks and questions.\nWe are going to work through the following workflow steps: each step is labelled as a practical part.\n\nSet-up\nLoad the data\nTidy data\nAnalyze the data: working with Cumulative Link Models\nAnalyze the data: working with Cumulative Link Mixed-effects Models\nPresenting and visualizing the effects\nDifferent kinds of ordinal data\n\nIn the following, we will guide you through the tasks and questions step by step.\n\n\n\n\n\n\nImportant\n\n\n\nAn answers version of the workbook will be provided after the practical class.\n\n\n\n\n\nTo begin, we set up our environment in R.\n\n\nUse the library() function to make the functions we need available to you.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlibrary(ggdist)\nlibrary(ggeffects)\nlibrary(here)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nlibrary(lme4)\nlibrary(memisc)\nlibrary(ordinal)\nlibrary(patchwork)\nlibrary(tidyverse)\nlibrary(viridis)\n\n\n\n\nNote that the key library here is {ordinal} (Christensen, 2015; Christensen, 2022). You can find the manual for the {ordinal} library functions in the CRAN webpages.\n\n\n\n\n\n\nRead the data file into R:\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nhealth &lt;- read_csv(\"2021-22_PSYC304-health-comprehension.csv\", \n            na = \"-999\",\n            col_types = cols(\n              ResponseId = col_factor(),\n              rating = col_factor(),\n              GENDER = col_factor(),\n              EDUCATION = col_factor(),\n              ETHNICITY = col_factor(),\n              NATIVE.LANGUAGE = col_factor(),\n              OTHER.LANGUAGE = col_factor(),\n              text.id = col_factor(),\n              text.question.id = col_factor(),\n              study = col_factor()\n            )\n          )\n\n\n\n\nYou can see, here, that within the read_csv() function call, I specify col_types, instructing R how to treat a number of different variables.\n\nYou can read more about this convenient way to control the read-in process here.\n\n\n\n\n\nThe data are already tidy: each column in long.orth_2020-08-11.csv corresponds to a variable and each row corresponds to an observation. However, we need to do a bit of work, before we can run any analyses, to fix the coding of the categorical predictor (or independent) variables: the factors Orthography, Instructions, and Time.\n\n\nIt is always a good to inspect what you have got when you read a data file in to R.\nIf you open the data-set .csv, you will see:\n\n\n\n\n\nResponseId\nAGE\nGENDER\nEDUCATION\nETHNICITY\nNATIVE.LANGUAGE\nOTHER.LANGUAGE\nENGLISH.PROFICIENCY\nSHIPLEY\nHLVA\nFACTOR3\nrating\nresponse\nRDFKGL\nstudy\ntext.id\ntext.question.id\n\n\n\n\nR_sKW4OJnOlidPxrH\n20\nFemale\nFurther\nAsian\nEnglish\nNA\nNA\n26\n8\n59\n8\n1\n10.608\ncs\nstudyone.TEXT.105\nstudyone.TEXT.105.CQ.1\n\n\nR_sKW4OJnOlidPxrH\n20\nFemale\nFurther\nAsian\nEnglish\nNA\nNA\n26\n8\n59\n8\n1\n10.608\ncs\nstudyone.TEXT.105\nstudyone.TEXT.105.CQ.2\n\n\nR_sKW4OJnOlidPxrH\n20\nFemale\nFurther\nAsian\nEnglish\nNA\nNA\n26\n8\n59\n8\n1\n10.608\ncs\nstudyone.TEXT.105\nstudyone.TEXT.105.CQ.3\n\n\nR_sKW4OJnOlidPxrH\n20\nFemale\nFurther\nAsian\nEnglish\nNA\nNA\n26\n8\n59\n8\n1\n10.608\ncs\nstudyone.TEXT.105\nstudyone.TEXT.105.CQ.4\n\n\nR_sKW4OJnOlidPxrH\n20\nFemale\nFurther\nAsian\nEnglish\nNA\nNA\n26\n8\n59\n7\n1\n8.116\ncs\nstudyone.TEXT.94\nstudyone.TEXT.94.CQ.1\n\n\nR_sKW4OJnOlidPxrH\n20\nFemale\nFurther\nAsian\nEnglish\nNA\nNA\n26\n8\n59\n7\n1\n8.116\ncs\nstudyone.TEXT.94\nstudyone.TEXT.94.CQ.2\n\n\nR_sKW4OJnOlidPxrH\n20\nFemale\nFurther\nAsian\nEnglish\nNA\nNA\n26\n8\n59\n7\n1\n8.116\ncs\nstudyone.TEXT.94\nstudyone.TEXT.94.CQ.3\n\n\nR_sKW4OJnOlidPxrH\n20\nFemale\nFurther\nAsian\nEnglish\nNA\nNA\n26\n8\n59\n7\n0\n8.116\ncs\nstudyone.TEXT.94\nstudyone.TEXT.94.CQ.4\n\n\nR_sKW4OJnOlidPxrH\n20\nFemale\nFurther\nAsian\nEnglish\nNA\nNA\n26\n8\n59\n7\n1\n13.278\ncs\nstudyone.TEXT.28\nstudyone.TEXT.28.CQ.1\n\n\nR_sKW4OJnOlidPxrH\n20\nFemale\nFurther\nAsian\nEnglish\nNA\nNA\n26\n8\n59\n7\n1\n13.278\ncs\nstudyone.TEXT.28\nstudyone.TEXT.28.CQ.2\n\n\n\n\n\n\n\nYou can use the scroll bar at the bottom of the data window to view different columns.\nYou can see the columns:\n\nResponseId participant code\nAGE age in years\nGENDER gender code\nEDUCATION education level code\nETHNICITY ethnicity (Office National Statistics categories) code\nNATIVE.LANGUAGE code whether English the native language\nOTHER.LANGUAGE text indicating native language if not English\nENGLISH.PROFICIENCY self-rated English proficiency if native language not English\nSHIPLEY vocabulary knowledge test score\nHLVA health literacy test score\nFACTOR3 reading strategy survey score\nRDFKGL health text (Flesch-Kincaid Grade Level) readability\nstudy study identity code\ntext.id health information text identity code\ntext.question.id health information text question identity code\n\nThe data are structured this way because:\n\nEach participant was asked to read and respond to a sample of health information texts. Each health text is coded text.id. For each text, a participant was asked to respond to a set of multiple choice questions (MCQs) designed to probe their understanding. Each question is coded text.question.id. The accuracy of the response made by each participant to each queston about each text is coded correct (1) or incorrect (0) in the response column.\n\n\nThis data collection process means that, for each participant ID, you are going to see multiple rows of data, one row for each participant, each text they read, and each question they respond to.\n\n\nIn addition to the MCQs, each participant was asked to rate their understanding of the information in each text that they saw.\n\n\nHow well do you think you have understood this text? (Please check one response) where the participant must respond by producing a rating, by checking one option, given nine different response options ranging from 1 (not well at all) to 5 (very well).\nNote that each participant was asked to produce only one rating for each health text that they saw. This means that in the data-set there is one rating response value, for each participant, for each text. This also means that the rating is repeated for each row, where different rows correspond to different MCQs, for each text.\n\nAs we have said, we are going to focus our analyses on the ordered ratings data. And, as noted, these rating responses are categorical and ordered: ranging across potential values, from 1 \\(\\rightarrow\\) 2 \\(\\rightarrow\\) 3 \\(\\rightarrow\\) 4 \\(\\rightarrow\\) 5 \\(\\rightarrow\\) 6 \\(\\rightarrow\\) 7 \\(\\rightarrow\\) 8 \\(\\rightarrow\\) 9.\n\n\n\nThe most important task is to assess the nature of the outcome ratings variable: what have we got?\n\nPract.Q.1. What is the frequency distribution of responses under each possible rating response value (or category): can you visualize the distribution?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThere are different ways to assess the distribution of ratings responses.\n\nWe are going to ignore the fact that ratings are repeated within participants across question-rows for each text because we are primarily interested in the relative proportion of ratings responses recorded for different possible ratings.\nWe can focus on getting a count of the number of ratings responses recorded, for this sample of participants and texts, for each possible rating response value.\nWe can visualize the counts in different ways: you will have seen bar plots but we focus here on dot plots.\n\nHow would you de-duplicate the ratings data to get counts showing just the number of ratings (made by participants to texts) for each possible rating response value?\n\n\n\n\nProduce a plot to visualize the distribution of ratings responses.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nhealth &lt;- health %&gt;% mutate(rating = fct_relevel(rating, sort))\n\nhealth %&gt;%\n  group_by(rating) %&gt;%\n  summarise(count = n()) %&gt;%\n  ggplot(aes(x = rating, y = count, colour = rating)) + \n  geom_point(size = 3) +\n  scale_color_viridis(discrete=TRUE, option = \"mako\") + theme_bw() +\n  theme(\n    panel.grid.major.y = element_blank()  # No horizontal grid lines\n  ) +\n  coord_flip()\n\n\n\n\n\n\n\nFigure 1: Dot plot showing the distribution of ratings responses. The Likert-style questions in the surveys asked participants to rate their level of understanding of the texts they saw on a scale from 1 (not well) to 9 (extremely well). The plot shows the number of responses recorded for each response option, over all participants and all texts.\n\n\n\n\n\n\n\n\n\nPract.Q.2. What does the plot show us?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nIn analyzing these data, we will seek to estimate what information available to us can be used to predict whether a participant’s rating of their understanding is more likely to be, say, 1 or 2, 2 or 3 … 7 or 8, 8 or 9.\n\n\n\n\n\n\n\nAs explained, when we do analyses using ordinal models, we expect to work with categorical and ordered outcome response values. We know that, given the study and task design, that the ratings responses should be treated as ordered and categorical data but we need to check that R agrees that the ratings data are ordered and categorical.\n\nPract.Q.3. How can we tell that R treats values in the outcome rating column as ordered and categorical data?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nJust because the data-set looks to us like values in the outcome rating column are ordered and categorical does not mean that R sees the values as we would prefer it to.\n\nWe can try to identify how R classifies values in the rating column using the summary() function we have been using.\nWe can try a different function: str().\n\n\n\n\n\nPract.Q.4. What does a summary of the data-set show us?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nsummary(health)\n\n             ResponseId        AGE                     GENDER    \n R_sKW4OJnOlidPxrH:  20   Min.   :18.0   Female           :2900  \n R_27paPJzIutLoqk8:  20   1st Qu.:20.0   Male             :1120  \n R_1nW0lpFdfumlI1p:  20   Median :27.0   Prefer-not-to-say:  20  \n R_31ZqPQpNEEapoW8:  20   Mean   :34.3                           \n R_2whvE2IW90nj2P7:  20   3rd Qu.:50.0                           \n R_3CAxrri9clBT7sl:  20   Max.   :81.0                           \n (Other)          :3920                                          \n     EDUCATION    ETHNICITY    NATIVE.LANGUAGE    OTHER.LANGUAGE\n Further  :1780   Asian: 680   English:2720    NA        :2720  \n Higher   :1800   White:3260   Other  :1320    Polish    : 580  \n Secondary: 460   Other:  40                   Cantonese : 280  \n                  Mixed:  60                   Chinese   : 120  \n                                               Portuguese:  60  \n                                               polish    :  60  \n                                               (Other)   : 220  \n ENGLISH.PROFICIENCY    SHIPLEY           HLVA           FACTOR3     \n Length:4040         Min.   :15.00   Min.   : 3.000   Min.   :17.00  \n Class :character    1st Qu.:30.00   1st Qu.: 7.000   1st Qu.:45.00  \n Mode  :character    Median :34.00   Median : 9.000   Median :49.00  \n                     Mean   :32.97   Mean   : 8.564   Mean   :49.03  \n                     3rd Qu.:37.00   3rd Qu.:10.000   3rd Qu.:55.00  \n                     Max.   :40.00   Max.   :13.000   Max.   :63.00  \n                                                                     \n     rating        response          RDFKGL       study    \n 8      :1044   Min.   :0.0000   Min.   : 4.552   cs: 480  \n 7      : 916   1st Qu.:1.0000   1st Qu.: 6.358   jg:1120  \n 9      : 824   Median :1.0000   Median : 8.116   ml: 720  \n 6      : 500   Mean   :0.8064   Mean   : 7.930   rw:1720  \n 5      : 352   3rd Qu.:1.0000   3rd Qu.: 9.413            \n 4      : 176   Max.   :1.0000   Max.   :13.278            \n (Other): 228                                              \n             text.id                  text.question.id\n studyone.TEXT.37: 344   studyone.TEXT.37.CQ.1:  86   \n studyone.TEXT.39: 344   studyone.TEXT.37.CQ.2:  86   \n studyone.TEXT.72: 344   studyone.TEXT.37.CQ.3:  86   \n studyone.TEXT.14: 344   studyone.TEXT.37.CQ.4:  86   \n studyone.TEXT.50: 344   studyone.TEXT.39.CQ.1:  86   \n studyone.TEXT.10: 224   studyone.TEXT.39.CQ.2:  86   \n (Other)         :2096   (Other)              :3524   \n\n\n\n\n\n\n\n\nPract.Q.5. What does a structural summary of the rating variable show us?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe can use the str() function to get a concise summary of the rating variable, showing if or how R classifies the variable values as categorical and ordered.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nstr(health$rating)\n\n Factor w/ 9 levels \"1\",\"2\",\"3\",\"4\",..: 8 8 8 8 7 7 7 7 7 7 ...\n\n\n\n\n\n\n\nPract.Q.6. How can we check if R thinks that the rating variable values are categorical and ordered?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou have seen, previously, that you can check how R classifies the variables like the rating variable using is._() type functions.\n\nYou can do the same here using is.ordered().\n\nYou can read more about these functions here.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nis.ordered(factor(health$rating))\n\n[1] FALSE\n\n\n\n\n\n\nPract.Q.7. If you can work out how to check if R thinks that the rating variable values are categorical and ordered, what does that check tell us?\n\n\n\n\n\nThe ordinal model estimates the locations (thresholds) for where to split the latent scale (the continuum underlying the ratings) corresponding to different ratings values. If we do not make sure that the outcome factor variable is split as it should be then there is no guarantee that {ordinal} functions will estimate the thresholds in the right order (i.e., 1,2,3 ... rather than 3,2,1...).\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe can make sure that the confidence rating factor is ordered precisely as we wish using the ordered() function.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nhealth$rating &lt;- ordered(health$rating,\n      levels = c(\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"))\n\n\n\n\n\nPract.Q.8. How can we check if R thinks that the rating variable values are categorical and ordered?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou have seen how to do this: we need to do checks using a combination of is._() function calls.\n\n\n\nWe can do a check to see that rating is treated as an ordered factor.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nis.numeric(health$rating)\n\n[1] FALSE\n\nis.factor(health$rating)\n\n[1] TRUE\n\nstr(health$rating)\n\n Ord.factor w/ 9 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 8 8 8 8 7 7 7 7 7 7 ...\n\nis.ordered(health$rating)\n\n[1] TRUE\n\n\n\n\n\n\nPract.Q.9. If you can work out how to check if R thinks that the rating variable values are categorical and ordered, what do the checks tell us?\n\n\n\n\n\n\n\n\nBefore doing any modelling, it will be sensible to standardize potential predictors.\n\nCan you work out how to do this?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThere are different ways to do this. Usually, we want to use the scale() function to do the work.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nhealth &lt;- health %&gt;% \n  mutate(across(c(AGE, SHIPLEY, HLVA, FACTOR3, RDFKGL), \n                scale, center = TRUE, scale = TRUE,\n                .names = \"z_{.col}\"))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(...)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\nYou can see that in this chunk of code, we are doing a number of things:\n\nhealth &lt;- health %&gt;% recreates the health dataset from the following steps.\nmutate(...) do an operation which retains the existing variables in the dataset, to change the variables as further detailed.\nacross(...) work with the multple column variables that are named in the c(AGE, SHIPLEY, HLVA, FACTOR3, RDFKGL) set.\n...scale, center = TRUE, scale = TRUE... here is where we do the standardization work.\n\nWhat we are asking for is that R takes the variables we name and standardizes each of them.\n\n.names = \"z_{.col}\") creates the standardized variables under adapted names, adding z_ to the original column name so that we can distinguish between the standardized and original raw versions of the data columns.\n\nNote that the across() function is a useful function for applying a function across multiple column variables see information here There is a helpful discussion on how we can do this task here\nWe can then check that we have produced the standardized variables as required.\n\nsummary(health)\n\n             ResponseId        AGE                     GENDER    \n R_sKW4OJnOlidPxrH:  20   Min.   :18.0   Female           :2900  \n R_27paPJzIutLoqk8:  20   1st Qu.:20.0   Male             :1120  \n R_1nW0lpFdfumlI1p:  20   Median :27.0   Prefer-not-to-say:  20  \n R_31ZqPQpNEEapoW8:  20   Mean   :34.3                           \n R_2whvE2IW90nj2P7:  20   3rd Qu.:50.0                           \n R_3CAxrri9clBT7sl:  20   Max.   :81.0                           \n (Other)          :3920                                          \n     EDUCATION    ETHNICITY    NATIVE.LANGUAGE    OTHER.LANGUAGE\n Further  :1780   Asian: 680   English:2720    NA        :2720  \n Higher   :1800   White:3260   Other  :1320    Polish    : 580  \n Secondary: 460   Other:  40                   Cantonese : 280  \n                  Mixed:  60                   Chinese   : 120  \n                                               Portuguese:  60  \n                                               polish    :  60  \n                                               (Other)   : 220  \n ENGLISH.PROFICIENCY    SHIPLEY           HLVA           FACTOR3     \n Length:4040         Min.   :15.00   Min.   : 3.000   Min.   :17.00  \n Class :character    1st Qu.:30.00   1st Qu.: 7.000   1st Qu.:45.00  \n Mode  :character    Median :34.00   Median : 9.000   Median :49.00  \n                     Mean   :32.97   Mean   : 8.564   Mean   :49.03  \n                     3rd Qu.:37.00   3rd Qu.:10.000   3rd Qu.:55.00  \n                     Max.   :40.00   Max.   :13.000   Max.   :63.00  \n                                                                     \n     rating        response          RDFKGL       study    \n 8      :1044   Min.   :0.0000   Min.   : 4.552   cs: 480  \n 7      : 916   1st Qu.:1.0000   1st Qu.: 6.358   jg:1120  \n 9      : 824   Median :1.0000   Median : 8.116   ml: 720  \n 6      : 500   Mean   :0.8064   Mean   : 7.930   rw:1720  \n 5      : 352   3rd Qu.:1.0000   3rd Qu.: 9.413            \n 4      : 176   Max.   :1.0000   Max.   :13.278            \n (Other): 228                                              \n             text.id                  text.question.id\n studyone.TEXT.37: 344   studyone.TEXT.37.CQ.1:  86   \n studyone.TEXT.39: 344   studyone.TEXT.37.CQ.2:  86   \n studyone.TEXT.72: 344   studyone.TEXT.37.CQ.3:  86   \n studyone.TEXT.14: 344   studyone.TEXT.37.CQ.4:  86   \n studyone.TEXT.50: 344   studyone.TEXT.39.CQ.1:  86   \n studyone.TEXT.10: 224   studyone.TEXT.39.CQ.2:  86   \n (Other)         :2096   (Other)              :3524   \n           z_AGE.V1                   z_SHIPLEY.V1        \n Min.   :-9.82624656687e-01   Min.   :-3.294105320780000  \n 1st Qu.:-8.62035239524e-01   1st Qu.:-0.543722537102000  \n Median :-4.39972279452e-01   Median : 0.189712871877000  \n Mean   : 4.20000000000e-15   Mean   :-0.000000000000001  \n 3rd Qu.: 9.46806017926e-01   3rd Qu.: 0.739789428612000  \n Max.   : 2.81594198396e+00   Max.   : 1.289865985350000  \n                                                          \n          z_HLVA.V1                  z_FACTOR3.V1        \n Min.   :-2.60748865140e+00   Min.   :-4.20593553983000  \n 1st Qu.:-7.33066204487e-01   1st Qu.:-0.52915479589300  \n Median : 2.04145018971e-01   Median :-0.00390040390093  \n Mean   : 1.00000000000e-16   Mean   : 0.00000000000000  \n 3rd Qu.: 6.72750630700e-01   3rd Qu.: 0.78398118408700  \n Max.   : 2.07856746589e+00   Max.   : 1.83448996807000  \n                                                         \n         z_RDFKGL.V1         \n Min.   :-1.46446595688e+00  \n 1st Qu.:-6.81459422242e-01  \n Median : 8.07363074868e-02  \n Mean   : 6.99000000000e-14  \n 3rd Qu.: 6.43061598419e-01  \n Max.   : 2.31876495189e+00  \n                             \n\n\n\n\n\n\n\n\n\nIn our first analysis, we can begin by assuming no random effects. We keep things simple at this point so that we can focus on the key changes in model coding.\nThe model is fitted to examine what shapes the variation in rating responses that we see in Figure 1.\n\n\nOur research question is:\n\n\n\n\n\n\nNote\n\n\n\n\nWhat factors predict self-evaluated rated understanding of health information.\n\n\n\nIn this first analysis:\n\nThe outcome variable is the ordinal response variable rating.\nThe predictors consist of the variables we standardized earlier.\n\nWe use the clm() function from the {ordinal} library to do the analysis.\n\nCan you work out how to code the analysis?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe key idea — and you may already have guessed this — is that the structure of model analysis code in R is often similar across the different kinds of modeling methods we may employ, in the context of working with different kinds of data structure, different outcome variables.\n\n\n\nWhat we want is a cumulative link model with:\n\nrating as the outcome variable;\nstandardized age, vocabulary, health literacy, reading strategy, and text readability as factors.\n\n\n\n\n\n\n\nTip\n\n\n\nHere, we ignore any data clustering we may think or know structures the data at multiple levels: we do not specify random effects.\n\n\nYou can check out the information on the modeling methods in Christensen (2022) and Christensen (2015).\nYou can also find the manual for the {ordinal} library functions that we need to use here.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nhealth.clm &lt;- clm(rating ~\n                    \n  z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL,\n                  \n  Hess = TRUE, link = \"logit\",\n  data = health)\n\nsummary(health.clm)\n\nformula: rating ~ z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL\ndata:    health\n\n link  threshold nobs logLik   AIC      niter max.grad cond.H \n logit flexible  4040 -6880.78 13787.55 5(0)  9.26e-07 7.3e+01\n\nCoefficients:\n          Estimate Std. Error z value Pr(&gt;|z|)    \nz_AGE     -0.17719    0.02966  -5.975 2.30e-09 ***\nz_SHIPLEY  0.34384    0.03393  10.135  &lt; 2e-16 ***\nz_HLVA     0.16174    0.03265   4.954 7.28e-07 ***\nz_FACTOR3  0.74535    0.03145  23.699  &lt; 2e-16 ***\nz_RDFKGL  -0.27220    0.02892  -9.412  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2 -4.65066    0.12978 -35.836\n2|3 -4.13902    0.10395 -39.817\n3|4 -3.26845    0.07390 -44.228\n4|5 -2.56826    0.05804 -44.248\n5|6 -1.69333    0.04437 -38.160\n6|7 -0.87651    0.03671 -23.876\n7|8  0.24214    0.03402   7.117\n8|9  1.63049    0.04252  38.346\n\n\nThe code works as follows.\nFirst, we have a chunk of code mostly similar to what we have done before, but changing the function.\n\nclm() the function name changes because now we want a cumulative link model of the ordinal responses.\n\nThe model specification includes information about the fixed effects, the predictors: z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL.\nSecond, we have the bit that is specific to cumulative link models fitted using the clm() function.\n\nHess = TRUE is required if we want to get a summary of the model fit; the default is TRUE but it is worth being explicit about it.\nlink = \"logit\" specifies that we want to model the ordinal responses in terms of the log odds (hence, the probability) that a response is a low or a high rating value.\n\n\n\n\n\nPract.Q.10. If you can work out how to fit the ordinal model of rating variable values, what does the results summary show about the estimated effects of the predictors entered into the model?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe summary() output for the model is similar to the outputs you have seen for other model types.\n\nWe first get formula: information about the model you have specified.\nR will tell us what data: we are working with.\nWe then get Coefficients: estimates.\n\nThe table summary of coefficients arranges information in ways that will be familiar you:\n\nFor each predictor variable, we see Estimate, Std. Error, z value, and Pr(&gt;|z|) statistics.\nThe Pr(&gt;|z|) p-values are based on Wald tests of the null hypothesis that a predictor has null impact.\nThe coefficient estimates can be interpreted based on whether they are positive or negative.\n\n\nWe then get Threshold coefficients: indicating where the model fitted estimates the threshold locations: where the latent scale is cut, corresponding to different rating values.\n\n\n\n\n\n\n–&gt;  –&gt;\n\n–&gt;\n\n\n\n\n\n\nHint\n\n\n\n\n\nIn reporting ordinal (e.g., cumulative link) models, we typically focus on the coefficient estimates for the predictor variables.\n\nA positive coefficient estimate indicates that higher values of the predictor variable are associated with greater probability of higher rating values.\nA negative coefficient estimate indicates that higher values of the predictor variable are associated with greater probability of lower rating values.\n\n\n\n\n\n\n\nIt will be a good idea for you to get some practice specifying cumulative link models.\nFit a new model:\n\nUse a different set of predictors.\nExperiment with the arguments used to control how the model fitting function works.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nYou can choose whatever set of predictors you like, given the health data-set.\nOne way to understand how model functions work (in addition to reading manual information) is to experiment with what you do or do not specify, among the function arguments, in controlling the function. Even causing errors can be helpul to your undestanding.\n\n\n\n\nWhat we want is a cumulative link model with:\n\nrating as the outcome variable;\nsome combination of (one or all of) standardized age, vocabulary, health literacy, reading strategy, and text readability as factors.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nTry changing one element at a time, for example, here, simply removing the arguments entered to control how the clm() function works.\n\nhealth.clm &lt;- clm(rating ~\n                    \n  z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL,\n                  \n  data = health)\n\nsummary(health.clm)\n\nformula: rating ~ z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL\ndata:    health\n\n link  threshold nobs logLik   AIC      niter max.grad cond.H \n logit flexible  4040 -6880.78 13787.55 5(0)  9.26e-07 7.3e+01\n\nCoefficients:\n          Estimate Std. Error z value Pr(&gt;|z|)    \nz_AGE     -0.17719    0.02966  -5.975 2.30e-09 ***\nz_SHIPLEY  0.34384    0.03393  10.135  &lt; 2e-16 ***\nz_HLVA     0.16174    0.03265   4.954 7.28e-07 ***\nz_FACTOR3  0.74535    0.03145  23.699  &lt; 2e-16 ***\nz_RDFKGL  -0.27220    0.02892  -9.412  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2 -4.65066    0.12978 -35.836\n2|3 -4.13902    0.10395 -39.817\n3|4 -3.26845    0.07390 -44.228\n4|5 -2.56826    0.05804 -44.248\n5|6 -1.69333    0.04437 -38.160\n6|7 -0.87651    0.03671 -23.876\n7|8  0.24214    0.03402   7.117\n8|9  1.63049    0.04252  38.346\n\n\nHere, we add in another variable:\n\nhealth.clm &lt;- clm(rating ~\n                    \n  NATIVE.LANGUAGE + z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL,\n                  \n  data = health)\n\nsummary(health.clm)\n\nformula: \nrating ~ NATIVE.LANGUAGE + z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL\ndata:    health\n\n link  threshold nobs logLik   AIC      niter max.grad cond.H \n logit flexible  4040 -6874.81 13777.63 6(0)  1.23e-12 7.8e+01\n\nCoefficients:\n                     Estimate Std. Error z value Pr(&gt;|z|)    \nNATIVE.LANGUAGEOther  0.22901    0.06637   3.450  0.00056 ***\nz_AGE                -0.17238    0.02966  -5.812 6.18e-09 ***\nz_SHIPLEY             0.35725    0.03413  10.466  &lt; 2e-16 ***\nz_HLVA                0.18076    0.03306   5.468 4.55e-08 ***\nz_FACTOR3             0.73665    0.03157  23.336  &lt; 2e-16 ***\nz_RDFKGL             -0.30030    0.03003 -10.001  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2 -4.57653    0.13135 -34.843\n2|3 -4.06620    0.10584 -38.418\n3|4 -3.19670    0.07648 -41.795\n4|5 -2.49814    0.06125 -40.787\n5|6 -1.62541    0.04839 -33.591\n6|7 -0.80807    0.04165 -19.400\n7|8  0.31593    0.04025   7.849\n8|9  1.71140    0.04878  35.086\n\n\n\n\n\n\n\n\n\nIn our analysis, we began by assuming no random effects. However, this is unlikely to be appropriate given the data collection process deployed in the Clearly understood project, where a sample of participants was asked to respond to a sample of texts, because that data collection process means that:\n\nwe have multiple observations of responses for each participant;\nwe have multiple observations of responses for each stimulus text;\nparticipants were assigned to groups, and within a group all participants were asked to respond to the same stimulus texts.\n\nThese features ensure that the data have a multilevel structure and this structure requires us to fit a Cumulative Link Mixed-effects Model (CLMM).\n\n\nOur research question is:\n\n\n\n\n\n\nNote\n\n\n\n\nWhat factors predict self-evaluated rated understanding of health information.\n\n\n\nYou can guess that fitting a CLMM is not going to be very different from specifying and running a GLMM.\n\nCan you work out how to code the model?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe key idea — and you may already have guessed this — is again that the structure of model analysis code in R is often similar across the different kinds of modeling methods we may employ.\n\nHere, the main insight for your development is that we are going to want to change things one thing at a time so take the clm() code you used earlier and add the element (a random effect) that we now need to take into account the multilevel structure we know is there.\n\n\n\n\nWhat we want is a cumulative link mixed-effects model with:\n\nrating as the outcome variable;\nstandardized age, vocabulary, health literacy, reading strategy, and text readability as factors;\na random effect to take into account the fact that each participant is producing ratings to multiple different texts.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nWe can code a Cumulative Link Mixed-effects Model as follows.\n\nhealth.clmm &lt;- clmm(rating ~\n                      \n                      z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL +\n                      \n                      (1 | ResponseId),\n                    \n                    Hess = TRUE, link = \"logit\",\n                    data = health)\n\nsummary(health.clmm)\n\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: rating ~ z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL +  \n    (1 | ResponseId)\ndata:    health\n\n link  threshold nobs logLik   AIC     niter       max.grad cond.H \n logit flexible  4040 -4978.08 9984.16 1490(19081) 2.78e-03 3.5e+02\n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ResponseId (Intercept) 9.825    3.134   \nNumber of groups:  ResponseId 202 \n\nCoefficients:\n          Estimate Std. Error z value Pr(&gt;|z|)    \nz_AGE     -0.44313    0.23474  -1.888   0.0591 .  \nz_SHIPLEY  0.77130    0.26497   2.911   0.0036 ** \nz_HLVA     0.20812    0.25608   0.813   0.4164    \nz_FACTOR3  1.68345    0.23822   7.067 1.58e-12 ***\nz_RDFKGL  -0.44345    0.03677 -12.060  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2  -9.3297     0.3452 -27.029\n2|3  -8.1413     0.2946 -27.633\n3|4  -6.5243     0.2631 -24.799\n4|5  -5.2159     0.2490 -20.949\n5|6  -3.4668     0.2369 -14.631\n6|7  -1.8481     0.2314  -7.985\n7|8   0.2966     0.2294   1.293\n8|9   3.0417     0.2346  12.967\n\n\nIf you inspect the code chunk, you can see that we have made two changes.\nFirst, we have changed the function.\n\nclmm() the function name changes because now we want a Cumulative Linear Mixed-effects Model.\n\nSecondly, the model specification includes information about fixed effects and now about random effects.\n\nWith (1 | ResponseId) we include include a random effect of participants on categorization thresholds.\n\n\n\n\n\nPract.Q.11. If you can work out how to fit the CLMM of rating variable values, what does the results summary show about the estimated effects of the predictors entered into the model? Compare the results for the CLMM with the results for the CLM.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe summary() output for the model is similar to the outputs you have seen for other mixed-effects model types. Having included a random effect (of participants on intercepts, strictly, on categorization thresholds):\n\nWe now get a Random effects variance estimates.\nWe again get Coefficients: estimates.\n\nThe coefficients summary information can be interpreted in the same way for a CLMM as for a CLM.\n\n\n\n\n\n\n\n\nIt will be helpful for the interpretation of the estimates of the coefficients of these predictor variables if we visualize the predictions we can make, about how rating values vary, given differences in predictor variable values, given our model estimates.\n\n\nWe can produce a visualization of the predictions from the ordinal models we fit, in order to be able to understand and to explain the model results.\n\nCan you work out how to get and visualize model predictions?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou have already had some experience in doing visualizations of model predictions. The steps you need to complete are steps you have made before:\n\nFit a model;\nGet predictions about outcome variation, given the model;\nShow those predictions in a plot.\n\nWe can do this using functions from the {ggeffects} library.\n\nYou can read more about the {ggeffects} library here where you will see a collection of articles explaining what you can do, and why, as well as technical information including some helpful tutorials.\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nSo: we can complete the steps identified, as we have before, for other kinds of model, other data-sets:\n\nFit a model\n\n\nhealth.clmm &lt;- clmm(rating ~\n                      \n                      z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL +\n                      \n                      (1 | ResponseId),\n                    \n                    Hess = TRUE, link = \"logit\",\n                    data = health)\n\nsummary(health.clmm)\n\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: rating ~ z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL +  \n    (1 | ResponseId)\ndata:    health\n\n link  threshold nobs logLik   AIC     niter       max.grad cond.H \n logit flexible  4040 -4978.08 9984.16 1490(19081) 2.78e-03 3.5e+02\n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ResponseId (Intercept) 9.825    3.134   \nNumber of groups:  ResponseId 202 \n\nCoefficients:\n          Estimate Std. Error z value Pr(&gt;|z|)    \nz_AGE     -0.44313    0.23474  -1.888   0.0591 .  \nz_SHIPLEY  0.77130    0.26497   2.911   0.0036 ** \nz_HLVA     0.20812    0.25608   0.813   0.4164    \nz_FACTOR3  1.68345    0.23822   7.067 1.58e-12 ***\nz_RDFKGL  -0.44345    0.03677 -12.060  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2  -9.3297     0.3452 -27.029\n2|3  -8.1413     0.2946 -27.633\n3|4  -6.5243     0.2631 -24.799\n4|5  -5.2159     0.2490 -20.949\n5|6  -3.4668     0.2369 -14.631\n6|7  -1.8481     0.2314  -7.985\n7|8   0.2966     0.2294   1.293\n8|9   3.0417     0.2346  12.967\n\n\n\nGet predictions about outcome variation, given the model\n\n\ndat &lt;- ggpredict(health.clmm, terms=\"z_FACTOR3 [all]\")\n\nYou are calculating adjusted predictions on the population-level (i.e.\n  `type = \"fixed\"`) for a *generalized* linear mixed model.\n  This may produce biased estimates due to Jensen's inequality. Consider\n  setting `bias_correction = TRUE` to correct for this bias.\n  See also the documentation of the `bias_correction` argument.\n\n\n\nIn this line, we use ggpredict() to work with some model information, assuming we previously fitted a model and gave it a name (here, health.clmm).\nWhen we use ggpredict(), we ask R to take that model information and, for the term we specify, here, specify using terms=\"z_FACTOR3 [all]\", we ask R to generate some predictions.\ndat &lt;- ggpredict(...) asks R to put those predictions in an object called dat.\n\n\nShow those predictions in a plot\n\n\nplot(dat)\n\n\n\n\n\n\n\n\n\n\n\n\nPract.Q.12. What does the plot show us?\n\n\nFit the model, get the predictions, and visualize them; see Figure 2.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\ndat &lt;- ggpredict(health.clmm, terms=\"z_FACTOR3 [all]\")\nplot(dat)\n\n\n\n\n\n\n\nFigure 2: A grid of plots showing marginal or conditional predicted probabilities that a rating response will have one value (among the 1-9 rating values possible), indicating how these predicted probabilities vary given variation in values of the standardized reading strategy (FACTOR3) variable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote that in our conceptual introduction, we examine visualizations of ordinal predictions in more depth, with a discussion of alternate visualization methods.\n\n\n\n\n\n\nAs the review reported by Liddell & Kruschke (2018) suggests, we may have many many studies in which ordinal outcome data are analysed but very few published research reports that present analyses of ordinal data using ordinal models.\nYou can see two examples in the papers published by Ricketts et al. (2021) and by Rodríguez-Ferreiro et al. (2020).\nThese papers are both published open accessible, so that they are freely available, and they are both associated with accessible data repositories.\n\nYou can find the repository for Ricketts et al. (2021) here.\nYou can find the repository for Rodríguez-Ferreiro et al. (2020) here.\n\nThe Rodríguez-Ferreiro et al. (2020) shares a data .csv only.\nThe Ricketts et al. (2021) repository shares data and analysis code as well as a fairly detailed guide to the analysis methods. Note that the core analysis approach taken in Ricketts et al. (2021) is based on Bayesian methods but that we also conduct clmm() models using the {ordinal} library functions discussed here; these models are labelled frequentist models and can be found under sensitivity analyses.\nFor what it’s worth, the Ricketts et al. (2021) is much more representative of the analysis approach I would recommend now.\n\n\n\n\n\n\nTip\n\n\n\nGo ahead and take a look at the repositories.\n\nExperiment with attempting to reproduce the reported analyses.\n\n\n\n\n\n\n\nAfter the practical class, we will reveal the answers that are currently hidden.\nThe answers version of the webpage will present my answers for questions, and some extra information where that is helpful.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 20. Workbook introduction to Ordinal (Mixed-effects) Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/03-mixed-workbook.html",
    "href": "PSYC412/part2/03-mixed-workbook.html",
    "title": "Week 18. Developing linear mixed-effects models",
    "section": "",
    "text": "Welcome to your overview of the work we will do together in Week 18.\nWe have been learning to:\n\nTo recognize the situations where we shall see multilevel structured data and therefore where we will need to apply multilevel or mixed-effects models.\nTo understand the nature and the advantages of these models: what they are, and why they work better than other kinds of models, given multilevel data.\nTo practice how we code for mixed-effects models, and how we read or write about the results.\n\nWe now need to develop our understanding and skills further. And we now need to examine some of the complexities that we may face when we work with mixed-effects models.\n\n\n\n\n\n\nThe key idea\n\n\n\nShrinkage or regularization means that models of data should be excited by the data but not too excited.\n\n\n\n\nWe have three capacities that we are seeking to develop this week:\n\nto understand mixed-effects models;\nto work with these models practically or efficiently in R;\nand to communicate their results effectively (to ourselves and others).\n\n\n\nYou can test your development against this checklist of targets for learning.\n\nWe want to develop the capacity to understand mixed-effects models, the capacity to:\n\n\nrecognize where data have a multilevel structure;\nrecognize where multilevel or mixed-effects models are required;\ndistinguish the elements of a mixed-effects model, including fixed effects and random effects;\nexplain how random effects can be understood in terms of random differences (or deviations) between groups or classes or individuals, in intercepts or slopes;\nexplain how random effects can be understood in terms of variances, as a means to account for random differences between groups or classes or individuals in intercepts or slopes;\nexplain how mixed-effects models work better than linear models, for multilevel structured data;\nexplain how mixed-effects models work better because they allow partial-pooling of estimates.\n\n\nWe want to develop the capacity to work practically with mixed-effects models in R, the capacity to:\n\n\nspecify a mixed-effects model in lmer() code;\nidentify how the mixed-effects model code varies, depending on the kinds of random effects that are assumed;\nidentify the elements of the output or results that come from an lmer() mixed-effects analysis;\ninterpret the fixed-effects estimates;\ninterpret the random effects estimates, including both the variance and covariance estimates.\n\n\nWe want to develop the capacity to communicate the results of mixed-effects models effectively, to ourselves and to others, the capacity to:\n\n\ndescribe in words and summary tables the results of a mixed-effects model;\nvisualize the effects estimates or predictions from a mixed-effects model.\n\n\n\n\n\nYou will see, next, the lectures we share to explain the concepts you will learn about, and the practical data analysis skills you will develop. Then you will see information about the practical materials you can use to build and practise your skills.\nEvery week, you will learn best if you first watch the lectures then do the practical exercises.\n\n\n\n\n\n\nLinked resources\n\n\n\nTo support your learning, you can read about the ideas and the practical coding required for analyses in the chapters I wrote for this course.\n\nWe learned about multilevel structured data in the conceptual introduction to multilevel data and the workbook introduction to multilevel data.\nWe then deepened our understanding by looking at the analysis of data from studies with repeated-measures designs in the conceptual introduction to linear mixed-effects models and the workbook introduction to mixed-effects models.\nThis week, you can find more in-depth explanations of analysis methods and key ideas in the chapter on developing linear mixed-effects models.\n\n\n\n\n\nThe lecture materials for this week are presented in three short parts.\nClick on a link and your browser should open a tab showing the Panopto video for the lecture part.\n\nPart 1 (13 minutes) Developing the capacity to understand and work with mixed-effects models: what we want to be able to do, and why; our working example data, and the study.\n\n\n\nPart 2 (13 minutes): Data tidying, data transformation, and why we do this work; sample data, variation and the benefits of pooling.\n\n\n\nPart 3 (24 minutes): The elements and the logic of mixed-effects models; the impact of including random effects, pooling, shrinkage and regularisation; how mixed-effects models are estimated.\n\n\n\n\n\n\n\n\n\n\n\nDownload the lecture slides\n\n\n\nYou can download the lecture slides in three different versions:\n\n402-week-19-LME-3.pdf: high resolution .pdf, exactly as delivered [1.3 MB];\n402-week-19-LME-3_1pp.pdf: printable version, one-slide-per-page [1.2 MB];\n402-week-19-LME-3_6pp.pdf: printable version, six-slides-per-page [1.3 MB].\n\nThe high resolution version is the version delivered for the lecture recordings. To make the slides easier to download, I produced lower resolution versions: 1pp and 6pp. These should be easier to download and print out if that is what you want to do.\n\n\n\n\n\nWe will be working with the ML word recognition study data-set. ML examined visual word recognition in younger and older adults using the lexical decision task.\nIn lexical decision, participants are presented with a stimulus: a string of letters that is either a real word (e.g., ‘car’) or a made-up or non-word (e.g., ‘cas’). Participants are required to respond to the stimulus by pressing a button to indicate either that they think the stimulus is a word or that they think it is a non-word. Each complete sequence of events, in which a stimulus is presented and a response is recorded, is known as a trial. In the lexical decision task implemented by ML, all study participants were presented with a mix of 160 word stimuli and 160 non-word stimuli, in random order, in a total of 320 trials.\nEach stimulus was presented one at a time on the computer screen. The critical outcome measure was the reaction time (RT) or latency for each response. Observed RT represents the interval of time from the moment the stimulus was first presented (the stimulus onset) to the moment the response was made (the response onset).\nThe critical feature of the study, here, is that we have an outcome – a decision response – observed multiple times (for each stimulus) for each participant. We shall be analyzing the speed of response, reaction time (RT), measured in milliseconds (ms).\nYou can read more about these data in the chapter on developing linear mixed-effects models.\nInstead of posing a simple and general research question, we shall orient our work around a set of quite specific predictions. ML hypothesized:\n\nEffects of stimulus attributes\n\n\nPredicting that words that are shorter, that look like more other words, and that appear frequently in the language will be easier to recognize.\n\n\nEffects of participant attributes\n\n\nPredicting that older readers would be faster and more accurate than younger readers in word recognition.\n\n\nEffects of interactions between the effects of word attributes and person attributes.\n\n\nPredicting that better (older) readers will show smaller effects of word attributes.\n\n\n\n\n\n\n\nImportant\n\n\n\nGet the data: get the data file and the .R script you can use to do the exercises that will support your learning.\n\nYou can download the files folder for this chapter by clicking on the link 03-mixed.zip.\n\n\n\nThe practical materials folder includes a data file and an .R script:\n\nsubjects.behaviour.words-310114.csv which holds information about the (word) stimuli, participants, and the responses recorded in the ML study.\n\nThe .csv file is a comma separated values file and can be opened in Excel.\nThe data file is collected together with the .R script:\n\n402-03-mixed-workbook.R the workbook you will need to do the practical exercises.\n\n\n\n\n\n\n\nImportant\n\n\n\nYou can access the sign-in page for R-Studio Server here\n\n\n\n\nWe have reached the stage where our learning targets can be detailed – here, we are concerned with:\n\nbeing able to specify a mixed-effects model in lmer() code;\nbeing able to identify how the mixed-effects model code varies depending on the kinds of random effects;\nbeing able to identify the elements of the output or results that come from an lmer() mixed-effects analysis;\nbeing able to interpret the fixed-effects estimates;\nbeing able to interpret the random effects estimates, variance, covariance;\nbeing able to describe in words and summary tables the results of a mixed-effects model;\nbeing able to visualise the effects estimates from a mixed-effects model.\n\n\n\n\nNow you will progress through a series of tasks, and challenges, to aid your learning.\n\n\n\n\n\n\nWarning\n\n\n\nWe will work with the data file:\n\nsubjects.behaviour.words-310114.csv\n\n\n\nWe again split the steps into into parts, tasks and questions.\nWe are going to work through the following workflow steps: each step is labelled as a practical part.\n\nSet-up\nLoad the data\nTidy the data\nAnalyze data with lm\nAnalyze data with lmer\nCompare models with different random effects\np-values and significance using lmerTest\nRun your own sequence of lmer models\nOptional: reproduce the plots in the chapter and slides\n\nIn the following, we will guide you through the tasks and questions step by step.\n\n\n\n\n\n\nImportant\n\n\n\nAn answers version of the workbook will be provided after the practical class.\n\n\n\n\n\nTo begin, we set up our environment in R.\n\n\nUse the library() function to make the functions we need available to you.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlibrary(broom)\nlibrary(gridExtra)\nlibrary(here)\n\nhere() starts at /Users/padraic/MSc_411_412_2025-26\n\nlibrary(lme4)\n\nLoading required package: Matrix\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::combine() masks gridExtra::combine()\n✖ tidyr::expand()  masks Matrix::expand()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ tidyr::pack()    masks Matrix::pack()\n✖ tidyr::unpack()  masks Matrix::unpack()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\n\n\n\n\nRead the data files into R:\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all &lt;- read_csv(\"subjects.behaviour.words-310114.csv\", na = \"-999\")\n\nRows: 5440 Columns: 25\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (6): subjectID, Test, Gender, Subject, item_name, item_type\ndbl (18): item_number, Age, Years_in_education, TOWRE_wordacc, TOWRE_nonword...\nnum  (1): SUBTLWF\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nThe ML study data includes the following variables that we will work with (as well as some you can ignore):\n\nIdentifying variables\n\n\nsubjectID – identifying code for participants\nitem_name – words presented as stimuli\nitem_number – identifying code for words presented\n\n\nResponse variables\n\n\nRT – response reaction time (ms), for responses to words, i.e., word recognition time\n\n\nParticipant attribute variables\n\n\nAge – in years\nGender – coded M (male), F (female)\nTOWRE_wordacc – word reading skill, words read correctly (out of 104)\nTOWRE_nonwordacc – nonword reading skill, nonwords (made up words) read correctly (out of 63)\nART_HRminusFR – reading experience score\n\n\nStimulus property variables\n\n\nLength – word length, in letters\nOrtho_N – orthographic neighbourhood size, how many other words in English a stimulus word looks like\nOLD – orthographic Levenshtein distance, how many letter edits (addition, deletion or substitution) it would take to make a stimulus word look like another English word (a measure of orthographic neighbourhood) (Yarkoni et al., 2008)\nBG_Sum, BG_Mean, BG_Freq_By_Pos – measures of how common are pairs of letters that compose stimulus words\nSUBTLWF, LgSUBTLWF, SUBTLCD, LgSUBTLCD – measures of how common stimulus words are, taken from the SUBTLEX corpus analysis of word frequency (Brysbaert & New, 2009)\n\n\n\n\n\nTidying the data involves a number of tasks, some essential and some things we do for our convenience.\nWe are going to first filter the observations, then transform the outcome variable.\n\nWe do this work and use data visualization to examine the impacts of the actions.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is always a good idea to first inspect what you have got when you read a data file into R before you do anything more demanding.\n\nYou cannot assume that the data are what you think they are\nor that the data are structured or coded in the ways that you think (or have been told) they should be structured or coded.\n\n\n\n\n\nWe should examine the distribution of the outcome variable, lexical decision response reaction time (RT in ms). Observations about variable value distributions are a part of Exploratory Data Analysis and serve to catch errors in the data-set (e.g. incorrectly recorded scores) but also to inform the researcher’s understanding of their own data.\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe shall examine the distribution of the outcome variable, lexical decision response reaction time (RT in ms), using density plots. An alternative method would be to use histograms.\nA density plot shows a curve. You can say that the density corresponds to the height of the curve for a given value of the variable being depicted, and that it is related to the probability of observing values of the variable within some range of values (Howell, 2016).\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all %&gt;%\n  ggplot(aes(x = RT)) +\n  geom_density(size=1.5) +\n  geom_rug(alpha = .2) +\n  ggtitle(\"Raw RT\") +\n  theme_bw()  \n\n\n\n\n\n\n\nFigure 1: Density plot showing word recognition reaction time, correct and incorrect responses\n\n\n\n\n\nThe code to produce Figure 1 works in a series of steps.\n\nML.all %&gt;% takes the data-set, from the ML study, that we have read in to the R workspace and pipes it to the visualization code, next.\nggplot(aes(x = RT)) + creates a plot object in which the x-axis variable is specified as RT. The values of this variable will be mapped to geometric objects, i.e. plot features, that you can see, next.\ngeom_density(size=1.5) + first displays the distribution of values in the variable RT as a density curve. The argument size=1.5 tells R to make the line \\(1.5 \\times\\) the thickness of the line used by default to show variation in density.\n\nSome further information is added to the plot, next.\n\ngeom_rug(alpha = .2) + with a command that tells R to add a rug plot below the density curve.\nggtitle(\"Raw RT\") makes a plot title.\n\nNotice that beneath the curve of the density plot, you can see a series of vertical lines. Each line represents the x-axis location of an RT observation in the ML study data set. This rug plot represents the distribution of RT observations in one dimension.\n\ngeom_rug() draws a vertical line at each location on the x-axis that we observe a value of the variable, RT, named in aes(x = RT).\ngeom_rug(alpha = .2) reduces the opacity of each line, using alpha, to ensure the reader can see how the RT observations are denser in some places than others.\n\n\n\n\n\n\n\n\nTake a look at the geoms documented in the {ggplot2} library reference section here.\nExperiment with code to answer the following questions:\n\n\nWould a histogram or a frequency polygon provide a more informative view? Take a look here for advice.\nWhat about a dotplot? Take a look here for advice\n\n\n\n\nThe code example, shown above, delivers a plot (Figure 1) showing three peaks in the distribution of RT values. You can see that there is a peak of RT observations around 500-1000ms, another smaller peak around -500ms, and a third smaller peak around -2000ms.\nThe density plot shows the reaction times recorded for participants’ button press ‘yes’ responses to word stimuli in the lexical decision task. The peaks of negative RTs represent observations that are impossible.\n\nHow do we deal with the impossible RT values?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe density plot shows us that the raw ML lexical decision RT variable includes negative RT values corresponding to incorrect response. These have to be removed.\n\nWe can do this quite efficiently by creating a subset of the original “raw” data, defined according to the RT variable using the {dpyr} library filter() function.\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all.correct &lt;- filter(ML.all, RT &gt;= 200)\n\nThe filter code is written to subset the data by rows using a condition on the values of the RT variable.\nML.all.correct &lt;- filter(ML.all, RT &gt;= 200) works as follows.\n\nML.all.correct &lt;- filter(ML.all ...) creates a new data-set with a new name ML.all.correct from the old data-set ML.all using the filter() function.\nfilter(... RT &gt;= 200) specifies an argument for the filter() function.\n\nIn effect, we are asking R to check every value in the RT column.\n\nR will do a check through the ML.all data-set, row by row.\nIf a row includes an RT that is greater than or equal to 200 then that row will be included in the new data-set ML.all.correct. This is what I mean by using a condition.\nBut if a row includes an RT that is less than 200, then that row will not be included. We express this condition as RT &gt;= 200.\n\n\n\n\n\n\n\n\nHow can we check that the filter operation worked in the way we should expect it to?\n\nAfter we have removed negative (error) RTs, we can check that the size of the data-set – here, the number of rows – matches our expectations.\n\nHow do we do that?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe length() function will count the elements in whatever object is specified as an argument in the function call.\n\nThis means that if you put a variable name into the function as in length(data$variable) it will count how long that variable is – how many rows there are in the column.\nIf that variable happens to be, as here, part of a data-set, the same calculation will tell you how many rows there are in the data-set as a whole.\nIf you just enter length(data), naming some data-set, then the function will return a count of the number of columns in the data-set.\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlength(ML.all$RT)\n\n[1] 5440\n\nlength(ML.all.correct$RT)\n\n[1] 5257\n\n\nIf you run the length() function calls then you should see that the length or number of observations or rows in the ML.all.correct data-set should be smaller than the number of observations in the ML.all data-set.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIt is wise to check that the operations you perform to tidy, process or wrangle data actually do do what you mean them to do. Checks can be performed, for each processing stage, by:\n\nForming expectations or predictions about what the operation is supposed to do e.g. filter out some rows by some number;\nCheck what you get against these predictions e.g. count the number of rows before versus after filtering.\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all.correct %&gt;%\n  ggplot(aes(x = RT)) +\n  geom_density(size=1.5) + \n  geom_rug(alpha = .2) +\n  ggtitle(\"Correct RTs\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 2: Density plot showing word recognition reaction time, correct responses only\n\n\n\n\n\n\n\n\n\n\n\n\nChange the threshold for including RTs from RT &gt;= 200 to something else: you can change the number, or you can change the operator from &gt;= to a different comparison (try =, &lt;, &lt;=, &gt;.\nCan you assess what impact the change has?\n\n\n\n\nFigure 2 shows that we have successfully removed all errors (negative RTs) but now we see just how skewed the RT distribution is. Note the long tail of longer RTs.\nGenerally, we assume that departures from a model’s predictions about our observations (the linear model residuals) are normally distributed, and we often assume that the relationship between outcome and predictor variables is linear (Cohen et al., 2003). We can ensure that our data are compliant with both assumptions by transforming the RT distribution.\n\nWhat transformation should we use?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nPsychology researchers often take the log (often the log base 10) of RT values before performing an analysis. Transforming RTs to the log base 10 of RT values has the effect of correcting the skew – bringing the larger RTs ‘closer’ (e.g., \\(1000 = 3\\) in log10) to those near the middle which do not change as much (e.g. \\(500 = 2.7\\) in log10).\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all.correct$logrt &lt;- log10(ML.all.correct$RT)            \n\nThe log10() function works as follows:-\n\nML.all.correct$logrt &lt;- log10(...) creates a a new variable logrt, adding it to the ML.all.correct data-set. The variable is created using the transformation function log10().\nlog10(ML.all.correct$RT) creates a the new variable by transforming (to log10) the values of the old variable, RT.\n\n\n\n\n\n\n\nWe can see the effect of the transformation if we plot the log10 transformed RTs (see Figure 3).\n\nDo we arrive at a distribution that more closely approximates the normal distribution?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all.correct %&gt;%\n  ggplot(aes(x = logrt)) +\n  geom_density(size = 1.5) + \n  geom_rug(alpha = .2) +\n  ggtitle(\"Correct log10 RTs\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 3: Density plot showing log10 transformed reaction time, correct responses only\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThere are other log transformation functions and we often see researchers using the natural log instead of the log base 10 as discussed here\n\n\n\n\n\nML asked all participants in a sample of people to read a selection of words, a sample of words from the language.\nFor each participant, we will have multiple observations and these observations will not be independent. One participant will tend to be slower or less accurate compared to another.\n\nWhat does this look like?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all.correct %&gt;%\n  group_by(subjectID) %&gt;%\n  mutate(mean_logrt = mean(logrt, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(subjectID = fct_reorder(subjectID, mean_logrt)) %&gt;%\n  ggplot(aes(x = logrt)) +\n  geom_density(size = 1.25) +\n  facet_wrap(~ subjectID) +\n  geom_vline(xintercept = 2.778807, colour = \"red\", linetype = 2) +\n  scale_x_continuous(breaks = c(2.5,3)) +\n  ggtitle(\"Plot showing distribution of logRT for each participant; red line shows mean log10 RT\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 4: Density plot showing log10 transformed reaction time, correct responses, separately for each participant\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor each stimulus word, there are multiple observations and these observations will not be independent. One stimulus may prove to be more challenging to all participants compared to another, eliciting slower or less accurate responses on average.\n\nWhat does this look like?\n\n\n\n\n\n\n\nWe model the effects of interest, using all the data (hence, complete pooling) but ignoring the differences between participants. This means we can see something of the ‘true’ picture of our data through the linear model results though we should be aware that the linear model will miss important information, which the mixed-effects model will include, that would improve its performance.\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe linear model is fit in R using the lm() function.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all.correct.lm  &lt;- lm(logrt ~\n                             \n                             LgSUBTLCD,     \n                           \n                           data = ML.all.correct)\n\nsummary(ML.all.correct.lm)\n\n\nCall:\nlm(formula = logrt ~ LgSUBTLCD, data = ML.all.correct)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.41677 -0.07083 -0.01163  0.05489  0.53411 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.885383   0.007117  405.41   &lt;2e-16 ***\nLgSUBTLCD   -0.033850   0.002209  -15.32   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1095 on 5255 degrees of freedom\nMultiple R-squared:  0.04277,   Adjusted R-squared:  0.04259 \nF-statistic: 234.8 on 1 and 5255 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nVary the linear model using different outcomes or predictors.\n\nThe ML study data, like the CP study data, are rich with possibility. It would be useful to experiment with data like this because this will prepare you for similar analysis targets in future.\nChange the predictor from frequency to something else: what do you see when you visualize the relationship between outcome and predictor variables using scatterplots?\n\n\n\n\n\n\n\nTip\n\n\n\nSpecify linear models with different predictors: do the relationships you see in plots match the coefficients you see in the model estimates?\n\nYou should both estimate the effects of variables and visualize the relationships between variables using scatterplots.\nIf you combine reflection on the model estimates with evaluation of what the plots show you then you will be able to see how reading model results and reading plots can reveal the correspondences between the two ways of looking at your data.\n\n\n\n\n\n\n\n\n\nYou should specify, here, a model of the frequency effect that takes into account:\n\nthe random effect of participants on intercepts;\nthe random effect of participants on the slopes of the frequency (LgSUBTLCD) effect;\nas well as the random effect of items on intercepts.\n\nYou can specify the random effects of participants allowing for covariance between the random intercepts and the random slopes.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all.correct.lmer  &lt;- lmer(logrt ~\n\n                           LgSUBTLCD +\n\n                           (LgSUBTLCD + 1|subjectID) +\n\n                           (1|item_name),\n\n                         data = ML.all.correct)\n\nsummary(ML.all.correct.lmer)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: logrt ~ LgSUBTLCD + (LgSUBTLCD + 1 | subjectID) + (1 | item_name)\n   Data: ML.all.correct\n\nREML criterion at convergence: -9868.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.6307 -0.6324 -0.1483  0.4340  5.6132 \n\nRandom effects:\n Groups    Name        Variance  Std.Dev. Corr \n item_name (Intercept) 0.0003268 0.01808       \n subjectID (Intercept) 0.0054212 0.07363       \n           LgSUBTLCD   0.0002005 0.01416  -0.63\n Residual              0.0084333 0.09183       \nNumber of obs: 5257, groups:  item_name, 160; subjectID, 34\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)  2.887997   0.015479 186.577\nLgSUBTLCD   -0.034471   0.003693  -9.333\n\nCorrelation of Fixed Effects:\n          (Intr)\nLgSUBTLCD -0.764\n\n\nAs will now be getting familiar, the code works as follows:\n\nML.all.correct.lmer  &lt;- lmer(...) creates a linear mixed-effects model object using the lmer() function.\nlogrt ~ LgSUBTLCD the fixed effect in the model is expressed as a formula in which the outcome or dependent variable logrt is predicted ~ by the independent or predictor variable LgSUBTLCD word frequency.\n\nThe random effects part of the model is then specified as follows.\n\nWe first have the random effects associated with random differences between participants:\n\n\n(...|subjectID) adds random effects corresponding to random differences between sample groups (participants subjects) coded by the subjectID variable.\n(...1 |subjectID) including random differences between sample groups (subjectID) in intercepts coded 1.\n(LgSUBTLCD... |subjectID) and random differences between sample groups (subjectID) in the slopes of the frequency effect coded by using theLgSUBTLCD variable name.\n\n\nThen, we have the random effects associated with random differences between stimuli:\n\n\n(1|item_name) adds a random effect to account for random differences between sample groups (item_name) in intercepts coded 1.\n\n\n...(..., data = ML.all.correct) specifies the data-set in which you can find the variables named in the model fitting code.\nLastly, we can then specify summary(ML.all.correct.lmer) to get a summary of the fitted model.\n\n\n\n\n\nCan you explain the results shown in the model summary?\n\nYou can be guided by the in-depth explanation of how. to read the analysis results in the chapter on developing linear mixed-effects models.\n\n\n\n\n\n\nYou can specify models where the fixed effects part is the same (e.g., you assume $RT $ word frequency) but, for various reasons, you may suppose different random effects structures.\nIn this scenario — the model fixed effects part is the same but the random effects are different — you may be required to evaluate which model structure fits the data better, to the extent that that can be assessed, given your sample data.\n\nTo begin, fit a model with just the fixed effects of intercept and frequency, and the random effects of participants or items on intercepts only.\n\n\nWe exclude the (LgSUBTLCD + ...|subjectID) specification for the random effect of participants on the slope of the frequency LgSUBTLCD effect.\n\nWe use REML fitting, if we want to compare (as we will) models with the same fixed effects but different random effects.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all.correct.lmer.REML.si  &lt;- lmer(logrt ~ LgSUBTLCD + \n                                    \n                                          (1|subjectID) + (1|item_name),\n\n       data = ML.all.correct, REML = TRUE)\n\nsummary(ML.all.correct.lmer.REML.si)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: logrt ~ LgSUBTLCD + (1 | subjectID) + (1 | item_name)\n   Data: ML.all.correct\n\nREML criterion at convergence: -9845.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.5339 -0.6375 -0.1567  0.4364  5.5851 \n\nRandom effects:\n Groups    Name        Variance  Std.Dev.\n item_name (Intercept) 0.0003204 0.01790 \n subjectID (Intercept) 0.0032650 0.05714 \n Residual              0.0085285 0.09235 \nNumber of obs: 5257, groups:  item_name, 160; subjectID, 34\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)  2.887697   0.013253   217.9\nLgSUBTLCD   -0.034390   0.002774   -12.4\n\nCorrelation of Fixed Effects:\n          (Intr)\nLgSUBTLCD -0.658\n\n\nIf you look at the code chunk, you can see that:\n\nREML = TRUE is the only change to the code: it specifies the change in model fitting method;\nalso, I changed the model name to ML.all.correct.lmer.REML.si to be able to distinguish the maximum likelihood from the restricted maximum likelihood model.\n\n\n\n\nFollowing Baayen et al. (2008a), we can then run a series of models with just one random effect.\n\nNext, fit a model with just the random effect of items on intercepts.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all.correct.lmer.REML.i  &lt;- lmer(logrt ~\n\n       LgSUBTLCD + (1|item_name),\n\n       data = ML.all.correct, REML = TRUE)\n\nsummary(ML.all.correct.lmer.REML.i)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: logrt ~ LgSUBTLCD + (1 | item_name)\n   Data: ML.all.correct\n\nREML criterion at convergence: -8337\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.7324 -0.6455 -0.1053  0.4944  4.8970 \n\nRandom effects:\n Groups    Name        Variance  Std.Dev.\n item_name (Intercept) 0.0002364 0.01537 \n Residual              0.0117640 0.10846 \nNumber of obs: 5257, groups:  item_name, 160\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)  2.886765   0.009047  319.07\nLgSUBTLCD   -0.034206   0.002811  -12.17\n\nCorrelation of Fixed Effects:\n          (Intr)\nLgSUBTLCD -0.977\n\n\n\n\n\n\nNext, fit a model with just the random effect of participants on intercepts.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all.correct.lmer.REML.s  &lt;- lmer(logrt ~\n\n       LgSUBTLCD + (1|subjectID),\n\n       data = ML.all.correct, REML = TRUE)\n\nsummary(ML.all.correct.lmer.REML.s)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: logrt ~ LgSUBTLCD + (1 | subjectID)\n   Data: ML.all.correct\n\nREML criterion at convergence: -9786.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.5843 -0.6443 -0.1589  0.4434  5.5266 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n subjectID (Intercept) 0.003275 0.05723 \n Residual              0.008837 0.09401 \nNumber of obs: 5257, groups:  subjectID, 34\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)  2.885751   0.011561  249.60\nLgSUBTLCD   -0.033888   0.001897  -17.87\n\nCorrelation of Fixed Effects:\n          (Intr)\nLgSUBTLCD -0.517\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNotice how each different model has a distinct name.\n\nWe will use these different names to compare the models, next.\n\n\n\n\n\n\nIf we now run Likelihood Ratio Test comparisons of these models, we are effectively examining if one of the random effects can be dispensed with: if its inclusion makes no difference to the likelihood of the model then it is not needed.\n\nIs the random effect of subjects on intercepts justified?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nCompare models, first, with ML.all.correct.lmer.REML.si versus without ML.all.correct.lmer.REML.i the random effect of subjects on intercepts.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nanova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.i, refit = FALSE)\n\nData: ML.all.correct\nModels:\nML.all.correct.lmer.REML.i: logrt ~ LgSUBTLCD + (1 | item_name)\nML.all.correct.lmer.REML.si: logrt ~ LgSUBTLCD + (1 | subjectID) + (1 | item_name)\n                            npar     AIC     BIC logLik -2*log(L)  Chisq Df\nML.all.correct.lmer.REML.i     4 -8329.0 -8302.7 4168.5   -8337.0          \nML.all.correct.lmer.REML.si    5 -9835.1 -9802.3 4922.6   -9845.1 1508.1  1\n                            Pr(&gt;Chisq)    \nML.all.correct.lmer.REML.i                \nML.all.correct.lmer.REML.si  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\nIs the random effect of items on intercepts justified?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nCompare models with ML.all.correct.lmer.REML.si versus without ML.all.correct.lmer.REML.s the random effect of items on intercepts.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nanova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.s, refit = FALSE)\n\nData: ML.all.correct\nModels:\nML.all.correct.lmer.REML.s: logrt ~ LgSUBTLCD + (1 | subjectID)\nML.all.correct.lmer.REML.si: logrt ~ LgSUBTLCD + (1 | subjectID) + (1 | item_name)\n                            npar     AIC     BIC logLik -2*log(L)  Chisq Df\nML.all.correct.lmer.REML.s     4 -9778.3 -9752.0 4893.2   -9786.3          \nML.all.correct.lmer.REML.si    5 -9835.1 -9802.3 4922.6   -9845.1 58.825  1\n                            Pr(&gt;Chisq)    \nML.all.correct.lmer.REML.s                \nML.all.correct.lmer.REML.si  1.723e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nWe compare models using the anova() function.\n\nanova() does the model comparison, for the models named in the list in brackets.\n\nYou can do this for the foregoing series of models but notice that in the code we specify:\n\nrefit = FALSE\n\nWhat happens if we do not add that bit?\nWhat you will see if you run the anova() function call, without therefit = FALSE argument – try it – is that you will then get the warning refitting model(s) with ML (instead of REML).\nWhy? The immediate reason for this warning is that we have to specify refit = FALSE because otherwise R will compare ML fitted models. The refitting occurs by default.\nWhat is the reason for the imposition of this default?\nPinheiro & Bates (2000) advise that if one is fitting models with random effects the estimates are more accurate if the models are fitted using Restricted Maximum Likelihood (REML). That is achieved in the lmer() function call by adding the argument REML=TRUE. Pinheiro & Bates (2000) further recommend (see, e.g., pp.82-) that if you compare models:\n\nwith the same fixed effects\nbut with varying random effects\nthen the models should be fitted using Restricted Maximum Likelihood.\n\nRead a discussion of the issues, with references, in chapter on developing linear mixed-effects models. See that chapter, also, for an in-depth account of what the ’anova()` results tell you.\n\n\n\nIs it justified to add a random effect of participants on the slopes of the frequency effect?\n\nHow should you address this question?\n\nFirst, you will need to fit a model incorporating the random effect of participants on the slopes of the frequency effect.\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou will want to make sure that this random slopes model is comparable to the next-most complex model: ML.all.correct.lmer.REML.si.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all.correct.lmer.REML.slopes  &lt;- lmer(logrt ~ LgSUBTLCD + \n                                           \n                                          (LgSUBTLCD + 1|subjectID) +\n                                          (1|item_name),\n\n       data = ML.all.correct, REML = TRUE)\n\nLooking at the code:\n\nWith (LgSUBTLCD + 1 |subjectID) we specify a random effect of subjects on intercepts and on the slope of the frequency effects.\nWe do not specify – it happens by default – the estimation of the covariance of random differences among subjects in intercepts and random differences among subjects in the slope of the frequency effect.\n\n\n\n\nThen you will want to compare two models that are the same except for the incorporation of the the random effect of participants on the slopes of the frequency effect.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nanova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.slopes, \n      refit = FALSE)\n\nData: ML.all.correct\nModels:\nML.all.correct.lmer.REML.si: logrt ~ LgSUBTLCD + (1 | subjectID) + (1 | item_name)\nML.all.correct.lmer.REML.slopes: logrt ~ LgSUBTLCD + (LgSUBTLCD + 1 | subjectID) + (1 | item_name)\n                                npar     AIC     BIC logLik -2*log(L)  Chisq Df\nML.all.correct.lmer.REML.si        5 -9835.1 -9802.3 4922.6   -9845.1          \nML.all.correct.lmer.REML.slopes    7 -9854.1 -9808.1 4934.0   -9868.1 22.934  2\n                                Pr(&gt;Chisq)    \nML.all.correct.lmer.REML.si                   \nML.all.correct.lmer.REML.slopes  1.047e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\n\nIf you look at the fixed effects summary, you can see that we do not get p-values by default. To calculate p-values, we need to count residual degrees of freedom. The authors of the {lme4} library that furnishes the lmer() function do not (as e.g. Baayen et al., 2008b discuss) think that it is sensible to estimate the residual degrees of freedom for a model in terms of the number of observations. This is because the number of observations concerns one level of a multilevel data-set that might be structured with respect to some number of subjects, some number of items. This means that one cannot then accurately calculate p-values to go with the t-tests on the coefficients estimates; therefore they do not.\n\n\nWe can run mixed-effects models with p-values from significance tests on the estimates of the fixed effects coefficients using the library(lmerTest).\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlibrary(lmerTest)\n\nML.all.correct.lmer.REML.slopes  &lt;- lmer(logrt ~ LgSUBTLCD + \n                                           \n                                            (LgSUBTLCD + 1|subjectID) +\n                                            (1|item_name),\n\n                                          data = ML.all.correct, REML = TRUE)\n\nsummary(ML.all.correct.lmer.REML.slopes)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: logrt ~ LgSUBTLCD + (LgSUBTLCD + 1 | subjectID) + (1 | item_name)\n   Data: ML.all.correct\n\nREML criterion at convergence: -9868.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.6307 -0.6324 -0.1483  0.4340  5.6132 \n\nRandom effects:\n Groups    Name        Variance  Std.Dev. Corr \n item_name (Intercept) 0.0003268 0.01808       \n subjectID (Intercept) 0.0054212 0.07363       \n           LgSUBTLCD   0.0002005 0.01416  -0.63\n Residual              0.0084333 0.09183       \nNumber of obs: 5257, groups:  item_name, 160; subjectID, 34\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)  2.887997   0.015479 47.782839 186.577  &lt; 2e-16 ***\nLgSUBTLCD   -0.034471   0.003693 60.338786  -9.333 2.59e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n          (Intr)\nLgSUBTLCD -0.764\n\n\n\n\n\nBasically, the call to access the lmerTest library ensures that when we run the lmer() function we get a calculation of an approximation to the denominator degrees of freedom that enables the calculation of the p-value for the t-test for the fixed effects coefficient.\n\n\n\n\n\n\nTip\n\n\n\nAn alternative approach to doing significance tests of hypothesized effects, is to compare models with versus without the effect of interest.\n\n\n\n\n\n\n\n\nIt will be useful for you to examine model comparisons with a different set of models for the same data.\nYou could try to run a series of models in which the fixed effects variable is something different, for example, the effect of word Length or the effect of orthographic neighbourhood size Ortho_N.\n\nRun models with these variables or pick your own set of predictor variables to get estimates of fixed effects.\n\nIn this series of models, specify a set of models where:\n\nEach model has the same fixed effect e.g. logrt ~ Length;\nBut you vary the random effects components so that, at the end, you can assess what random effects are justified by better model fit to data.\n\nIn the following, I set out another series of models as an example of what you can do.\n\nRandom effects of participants on intercepts and slopes, random effect of items on intercepts\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all.correct.lmer.REML.slopes  &lt;- lmer(logrt ~ Length + \n                                           \n                                           (Length + 1|subjectID) +\n                                           (1|item_name),\n                                         \n                                         data = ML.all.correct, REML = TRUE)\n\nsummary(ML.all.correct.lmer.REML.slopes)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: logrt ~ Length + (Length + 1 | subjectID) + (1 | item_name)\n   Data: ML.all.correct\n\nREML criterion at convergence: -9749.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.4052 -0.6300 -0.1563  0.4364  5.5167 \n\nRandom effects:\n Groups    Name        Variance  Std.Dev. Corr \n item_name (Intercept) 9.043e-04 0.030072      \n subjectID (Intercept) 3.055e-03 0.055272      \n           Length      9.039e-05 0.009507 -0.33\n Residual              8.474e-03 0.092056      \nNumber of obs: 5257, groups:  item_name, 160; subjectID, 34\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept) 2.764e+00  1.870e-02 1.110e+02 147.782   &lt;2e-16 ***\nLength      3.740e-03  4.038e-03 1.200e+02   0.926    0.356    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n       (Intr)\nLength -0.844\n\n\n\n\n\n\nRandom effect of participants on intercepts, random effect of items on intercepts\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all.correct.lmer.REML.si  &lt;- lmer(logrt ~ Length + \n                                       \n                                       (1|subjectID) + \n                                       (1|item_name),\n                                     \n                                     data = ML.all.correct, REML = TRUE)\n\nsummary(ML.all.correct.lmer.REML.si)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: logrt ~ Length + (1 | subjectID) + (1 | item_name)\n   Data: ML.all.correct\n\nREML criterion at convergence: -9738.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.5428 -0.6289 -0.1535  0.4357  5.5149 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n item_name (Intercept) 0.000901 0.03002 \n subjectID (Intercept) 0.003259 0.05709 \n Residual              0.008523 0.09232 \nNumber of obs: 5257, groups:  item_name, 160; subjectID, 34\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept) 2.764e+00  1.885e-02 1.725e+02 146.625   &lt;2e-16 ***\nLength      3.723e-03  3.691e-03 1.513e+02   1.009    0.315    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n       (Intr)\nLength -0.843\n\n\n\n\n\n\nRandom effect of items on intercepts\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all.correct.lmer.REML.i  &lt;- lmer(logrt ~ Length + \n                                       \n                                       (1|item_name),\n                                     \n                                     data = ML.all.correct, REML = TRUE)\n\nsummary(ML.all.correct.lmer.REML.i)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: logrt ~ Length + (1 | item_name)\n   Data: ML.all.correct\n\nREML criterion at convergence: -8233.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.4940 -0.6411 -0.1042  0.4930  4.8136 \n\nRandom effects:\n Groups    Name        Variance  Std.Dev.\n item_name (Intercept) 0.0008171 0.02858 \n Residual              0.0117545 0.10842 \nNumber of obs: 5257, groups:  item_name, 160\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept) 2.765e+00  1.622e-02 1.489e+02 170.471   &lt;2e-16 ***\nLength      3.487e-03  3.716e-03 1.487e+02   0.938     0.35    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n       (Intr)\nLength -0.986\n\n\n\n\n\n\nRandom effect of participants on intercepts\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all.correct.lmer.REML.s  &lt;- lmer(logrt ~ Length + \n                                      \n                                      (1|subjectID),\n                                    \n                                    data = ML.all.correct, REML = TRUE)\n\nsummary(ML.all.correct.lmer.REML.s)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: logrt ~ Length + (1 | subjectID)\n   Data: ML.all.correct\n\nREML criterion at convergence: -9481.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.6945 -0.6475 -0.1608  0.4572  5.2837 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n subjectID (Intercept) 0.003272 0.05720 \n Residual              0.009367 0.09678 \nNumber of obs: 5257, groups:  subjectID, 34\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept) 2.760e+00  1.266e-02 8.792e+01 218.088   &lt;2e-16 ***\nLength      4.346e-03  1.832e-03 5.222e+03   2.372   0.0177 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n       (Intr)\nLength -0.623\n\n\n\n\n\n\nModel comparisons\n\nI would compare the models in the sequence, shown in the foregoing, one pair of models at a time, to keep it simple.\n\nWhen you look at the model comparison, ask: is the difference between the models a piece of complexity (an effect) whose inclusion in the more complex model is justified or warranted by improved model fit to data?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nIf we run model comparisons, we can see that random effects of subjects or of items on intercepts, as well as random effect of subjects on slopes, are justified by significantly improved model fit to data.\n\nanova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.s, refit=FALSE)\n\nData: ML.all.correct\nModels:\nML.all.correct.lmer.REML.s: logrt ~ Length + (1 | subjectID)\nML.all.correct.lmer.REML.si: logrt ~ Length + (1 | subjectID) + (1 | item_name)\n                            npar     AIC     BIC logLik -2*log(L)  Chisq Df\nML.all.correct.lmer.REML.s     4 -9473.9 -9447.7 4741.0   -9481.9          \nML.all.correct.lmer.REML.si    5 -9728.9 -9696.1 4869.4   -9738.9 256.97  1\n                            Pr(&gt;Chisq)    \nML.all.correct.lmer.REML.s                \nML.all.correct.lmer.REML.si  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.i, refit=FALSE)\n\nData: ML.all.correct\nModels:\nML.all.correct.lmer.REML.i: logrt ~ Length + (1 | item_name)\nML.all.correct.lmer.REML.si: logrt ~ Length + (1 | subjectID) + (1 | item_name)\n                            npar     AIC     BIC logLik -2*log(L)  Chisq Df\nML.all.correct.lmer.REML.i     4 -8225.5 -8199.2 4116.8   -8233.5          \nML.all.correct.lmer.REML.si    5 -9728.9 -9696.1 4869.4   -9738.9 1505.4  1\n                            Pr(&gt;Chisq)    \nML.all.correct.lmer.REML.i                \nML.all.correct.lmer.REML.si  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.slopes, refit=FALSE)\n\nData: ML.all.correct\nModels:\nML.all.correct.lmer.REML.si: logrt ~ Length + (1 | subjectID) + (1 | item_name)\nML.all.correct.lmer.REML.slopes: logrt ~ Length + (Length + 1 | subjectID) + (1 | item_name)\n                                npar     AIC     BIC logLik -2*log(L) Chisq Df\nML.all.correct.lmer.REML.si        5 -9728.9 -9696.1 4869.4   -9738.9         \nML.all.correct.lmer.REML.slopes    7 -9735.5 -9689.5 4874.8   -9749.5 10.61  2\n                                Pr(&gt;Chisq)   \nML.all.correct.lmer.REML.si                  \nML.all.correct.lmer.REML.slopes   0.004965 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\n\nReproduce the plots in the chapter and slides\nThis part is optional for you to view and work with.\n\nRun the code, and consider the code steps only if you are interested in how the materials for the book chapter were created.\n\n\n\nIn the chapter on developing linear mixed-effects models I talk about pooling, no pooling and partial pooling, and the impact on estimates.\nTo produce the plots I discuss there, I adapted Tristan Mahr’s code, here:\n\nhttps://www.tjmahr.com/plotting-partial-pooling-in-mixed-effects-models/\n\nWe work through a sequence of fitting models, getting estimates, and producing plots.\n\nGet no pooling estimates by running lm() for each participant.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\ndf_no_pooling &lt;- lmList(logrt ~\n                          \n                          LgSUBTLCD | subjectID, ML.all.correct) %&gt;% \n  coef() %&gt;% \n  # Subject IDs are stored as row-names. Make them an explicit column\n  rownames_to_column(\"subjectID\") %&gt;% \n  rename(Intercept = `(Intercept)`, Slope_frequency = LgSUBTLCD) %&gt;% \n  add_column(Model = \"No pooling\")\n\n# -- you can uncomment and inspect the tibble produced by these lines of code\n# summary(df_no_pooling)\n\n\n\n\n\nMake the subjectID and Model label vectors a factor.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\ndf_no_pooling$subjectID &lt;- as.factor(df_no_pooling$subjectID)\ndf_no_pooling$Model &lt;- as.factor(df_no_pooling$Model)\n\n\n\n\n\nIn contrast, we might consider a complete pooling model where all the information from the participants is combined together.\n\n\nFit a single mdoel for all the data pooled together.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nm_pooled &lt;- lm(logrt ~ LgSUBTLCD, ML.all.correct) \n\n\n\n\n\nRepeat the intercept and slope terms for each participant.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\ndf_pooled &lt;- data_frame(\n  Model = \"Complete pooling\",\n  subjectID = unique(ML.all.correct$subjectID),\n  Intercept = coef(m_pooled)[1], \n  Slope_frequency = coef(m_pooled)[2])\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nℹ Please use `tibble()` instead.\n\n# -- inspect\n# summary(df_pooled)\n\n\n\n\n\nMake the model label vector a factor.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\ndf_pooled$Model &lt;- as.factor(df_pooled$Model)\n\n\n\n\n\nWe can compare these two approaches: instead of calculating the regression lines with stat_smooth(), we can use geom_abline() to draw the lines from our dataframe of intercept and slope parameters.\n\nJoin the raw data so we can use plot the points and the lines.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\ndf_models &lt;- bind_rows(df_pooled, df_no_pooling) %&gt;% \n  left_join(ML.all.correct, by = \"subjectID\")\n\nWarning in left_join(., ML.all.correct, by = \"subjectID\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 3 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n\n\n\n\nProduce the plot:\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\np_model_comparison &lt;- ggplot(df_models) + \n  aes(x = LgSUBTLCD, y = logrt) + \n  # Set the color mapping in this layer so the points don't get a color\n  geom_point(alpha = .05) +\n  geom_abline(aes(intercept = Intercept, \n                  slope = Slope_frequency, \n                  color = Model),\n              size = .75) +\n  facet_wrap(~ subjectID) +\n  scale_x_continuous(breaks = 1.5:4 * 1) + \n  theme_bw() + \n  theme(legend.position = \"top\")\n\np_model_comparison\n\n\n\n\n\n\n\n\n\n\n\n\nCompare no vs. complete vs. partial pooling estimates\n\n\nMixed-effects models represent partial pooling of data.\n\nWe fit a separate line for each cluster of data, one for each participant.\n\nthe lmList() function in {lme4} automates this process.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\ndf_no_pooling &lt;- lmList(logrt ~\n                          \n                          LgSUBTLCD | subjectID, ML.all.correct) %&gt;% \n  coef() %&gt;% \n  # Subject IDs are stored as row-names. Make them an explicit column\n  rownames_to_column(\"subjectID\") %&gt;% \n  rename(Intercept = `(Intercept)`, Slope_frequency = LgSUBTLCD) %&gt;% \n  add_column(Model = \"No pooling\")\n\n# -- inspect the tibble produced by these lines of code\n# summary(df_no_pooling)\n\n# -- make the subjectID and model label vectors a factor\ndf_no_pooling$subjectID &lt;- as.factor(df_no_pooling$subjectID)\ndf_no_pooling$Model &lt;- as.factor(df_no_pooling$Model)\n\n\n\n\nIn contrast, we might consider a complete pooling model where all the information from the participants is combined together.\n\nWe fit a single model on all the data pooled together.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nm_pooled &lt;- lm(logrt ~ LgSUBTLCD, ML.all.correct) \n\n# -- repeat the intercept and slope terms for each participant\ndf_pooled &lt;- data_frame(\n  Model = \"Complete pooling\",\n  subjectID = unique(ML.all.correct$subjectID),\n  Intercept = coef(m_pooled)[1], \n  Slope_frequency = coef(m_pooled)[2])\n\n# -- inspect\n# summary(df_pooled)\n\n# -- make the model label vector a factor\ndf_pooled$Model &lt;- as.factor(df_pooled$Model)\n\n\n\n\nWe can fit a mixed-effects model including the fixed effects of the intercept and the frequency effect\n\nas well as the random effects due to differences between participants (subject ID) in intercepts or in the slope of the frequency effect, as well as differences between items (item_name) in intercepts.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all.correct.lmer  &lt;- lmer(logrt ~\n                               \n                               LgSUBTLCD + (LgSUBTLCD + 1|subjectID) + (1|item_name),     \n                             \n                             data = ML.all.correct, REML = FALSE)\n# summary(ML.all.correct.lmer)\n\n\n\n\nTo visualize these estimates, we extract each participant’s intercept and slope using coef().\n\n\n\n\n\n\nCode\n\n\n\n\n\n\n# -- make a dataframe with the fitted effects\ndf_partial_pooling &lt;- coef(ML.all.correct.lmer)[[\"subjectID\"]] %&gt;% \n  rownames_to_column(\"subjectID\") %&gt;% \n  as_tibble() %&gt;% \n  rename(Intercept = `(Intercept)`, Slope_frequency = LgSUBTLCD) %&gt;% \n  add_column(Model = \"Partial pooling\")\n\n# -- inspect the tibble produced by these lines of code\n# summary(df_partial_pooling)\n\n# -- make the subjectID and model label vectors both factors\ndf_partial_pooling$subjectID &lt;- as.factor(df_partial_pooling$subjectID)\ndf_partial_pooling$Model &lt;- as.factor(df_partial_pooling$Model)\n\n\n\n\nVisualize the comparison by producing a plot using a data-set consisting of of all three sets of model estimates.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\ndf_models &lt;- rbind(df_pooled, df_no_pooling, df_partial_pooling) %&gt;%\n  left_join(ML.all.correct, by = \"subjectID\")\n\nWarning in left_join(., ML.all.correct, by = \"subjectID\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 3 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n# -- inspect the differences between the data-sets\n# summary(df_pooled)\n# summary(df_no_pooling)\n# summary(df_partial_pooling)\n# summary(ML.all.correct)\n# summary(df_models)\n\n# -- we  produce a plot showing the no-pooling, complete-pooling and partial-pooling (mixed effects)\n# estimates\n\np_model_comparison &lt;- ggplot(df_models) +\n  aes(x = LgSUBTLCD, y = logrt) +\n  # Set the color mapping in this layer so the points don't get a color\n  geom_point(alpha = .05) +\n  geom_abline(aes(intercept = Intercept,\n                  slope = Slope_frequency,\n                  color = Model),\n              size = .75) +\n  facet_wrap(~ subjectID) +\n  scale_x_continuous(breaks = 1.5:4 * 1) +\n  theme_bw() +\n  theme(legend.position = \"top\")\n\np_model_comparison\n\n\n\n\n\n\n\n\n\n\n\n\nZoom in on a few participants:\n\n\nuse filter() to select a small number of participants.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\ndf_zoom &lt;- df_models %&gt;% \n  filter(subjectID %in% c(\"EB5\", \"JP3\", \"JL3\", \"AA1\"))\n\n# -- re-generate the plot with this zoomed-in selection data-set\n\np_model_comparison &lt;- ggplot(df_zoom) + \n  aes(x = LgSUBTLCD, y = logrt) + \n  # Set the color mapping in this layer so the points don't get a color\n  geom_point(alpha = .05) +\n  geom_abline(aes(intercept = Intercept, \n                  slope = Slope_frequency, \n                  color = Model),\n              size = .75) +\n  facet_wrap(~ subjectID) +\n  scale_x_continuous(breaks = 1.5:4 * 1) + \n  theme_bw() + \n  theme(legend.position = \"top\")\n\np_model_comparison\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe partial pooling model pulls (or shrinks) more extreme estimates towards an overall average. We can visualize this shrinkage effect by plotting a scatterplot of intercept and slope parameters from each model and connecting estimates for the same participant.\n\nWe use arrows to connect the different estimates for each participant, different estimates from no-pooling (per-participant) compared to partial-pooling (mixed-effects) models.\nThe plot shows how more extreme estimates are shrunk towards the global average estimate.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\ndf_fixef &lt;- data_frame(\n  Model = \"Partial pooling (average)\",\n  Intercept = fixef(ML.all.correct.lmer)[1],\n  Slope_frequency = fixef(ML.all.correct.lmer)[2])\n\n# Complete pooling / fixed effects are center of gravity in the plot\ndf_gravity &lt;- df_pooled %&gt;% \n  distinct(Model, Intercept, Slope_frequency) %&gt;% \n  rbind(df_fixef)\n# df_gravity\n#&gt; # A tibble: 2 x 3\n#&gt;                       Model Intercept Slope_Days\n#&gt;                       &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1          Complete pooling  252.3207   10.32766\n#&gt; 2 Partial pooling (average)  252.5426   10.45212\n\ndf_pulled &lt;- rbind(df_no_pooling, df_partial_pooling)\n\nggplot(df_pulled) + \n  aes(x = Intercept, y = Slope_frequency, color = Model) + \n  geom_point(size = 2) + \n  geom_point(data = df_gravity, size = 5) + \n  # Draw an arrow connecting the observations between models\n  geom_path(aes(group = subjectID, color = NULL), \n            arrow = arrow(length = unit(.02, \"npc\"))) + \n  # Use ggrepel to jitter the labels away from the points\n  ggrepel::geom_text_repel(\n    aes(label = subjectID, color = NULL),\n    data = df_no_pooling) +\n  # Don't forget 373\n  # ggrepel::geom_text_repel(\n  #   aes(label = subjectID, color = NULL), \n  #   data = filter(df_partial_pooling, Subject == \"373\")) + \n  theme(legend.position = \"right\") + \n  # ggtitle(\"Pooling of regression parameters\") + \n  xlab(\"Intercept estimate\") + \n  ylab(\"Slope estimate\") + \n  scale_color_brewer(palette = \"Dark2\") +\n  theme_bw()\n\nWarning: ggrepel: 6 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfter the practical class, we will reveal the answers that are currently hidden.\nThe answers version of the webpage will present my answers for questions, and some extra information where that is helpful.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 18. Developing linear mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/03-mixed-workbook.html#sec-dev-mixed-workbook-targets",
    "href": "PSYC412/part2/03-mixed-workbook.html#sec-dev-mixed-workbook-targets",
    "title": "Week 18. Developing linear mixed-effects models",
    "section": "",
    "text": "We have three capacities that we are seeking to develop this week:\n\nto understand mixed-effects models;\nto work with these models practically or efficiently in R;\nand to communicate their results effectively (to ourselves and others).\n\n\n\nYou can test your development against this checklist of targets for learning.\n\nWe want to develop the capacity to understand mixed-effects models, the capacity to:\n\n\nrecognize where data have a multilevel structure;\nrecognize where multilevel or mixed-effects models are required;\ndistinguish the elements of a mixed-effects model, including fixed effects and random effects;\nexplain how random effects can be understood in terms of random differences (or deviations) between groups or classes or individuals, in intercepts or slopes;\nexplain how random effects can be understood in terms of variances, as a means to account for random differences between groups or classes or individuals in intercepts or slopes;\nexplain how mixed-effects models work better than linear models, for multilevel structured data;\nexplain how mixed-effects models work better because they allow partial-pooling of estimates.\n\n\nWe want to develop the capacity to work practically with mixed-effects models in R, the capacity to:\n\n\nspecify a mixed-effects model in lmer() code;\nidentify how the mixed-effects model code varies, depending on the kinds of random effects that are assumed;\nidentify the elements of the output or results that come from an lmer() mixed-effects analysis;\ninterpret the fixed-effects estimates;\ninterpret the random effects estimates, including both the variance and covariance estimates.\n\n\nWe want to develop the capacity to communicate the results of mixed-effects models effectively, to ourselves and to others, the capacity to:\n\n\ndescribe in words and summary tables the results of a mixed-effects model;\nvisualize the effects estimates or predictions from a mixed-effects model.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 18. Developing linear mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/03-mixed-workbook.html#sec-dev-mixed-workbook-resources",
    "href": "PSYC412/part2/03-mixed-workbook.html#sec-dev-mixed-workbook-resources",
    "title": "Week 18. Developing linear mixed-effects models",
    "section": "",
    "text": "You will see, next, the lectures we share to explain the concepts you will learn about, and the practical data analysis skills you will develop. Then you will see information about the practical materials you can use to build and practise your skills.\nEvery week, you will learn best if you first watch the lectures then do the practical exercises.\n\n\n\n\n\n\nLinked resources\n\n\n\nTo support your learning, you can read about the ideas and the practical coding required for analyses in the chapters I wrote for this course.\n\nWe learned about multilevel structured data in the conceptual introduction to multilevel data and the workbook introduction to multilevel data.\nWe then deepened our understanding by looking at the analysis of data from studies with repeated-measures designs in the conceptual introduction to linear mixed-effects models and the workbook introduction to mixed-effects models.\nThis week, you can find more in-depth explanations of analysis methods and key ideas in the chapter on developing linear mixed-effects models.\n\n\n\n\n\nThe lecture materials for this week are presented in three short parts.\nClick on a link and your browser should open a tab showing the Panopto video for the lecture part.\n\nPart 1 (13 minutes) Developing the capacity to understand and work with mixed-effects models: what we want to be able to do, and why; our working example data, and the study.\n\n\n\nPart 2 (13 minutes): Data tidying, data transformation, and why we do this work; sample data, variation and the benefits of pooling.\n\n\n\nPart 3 (24 minutes): The elements and the logic of mixed-effects models; the impact of including random effects, pooling, shrinkage and regularisation; how mixed-effects models are estimated.\n\n\n\n\n\n\n\n\n\n\n\nDownload the lecture slides\n\n\n\nYou can download the lecture slides in three different versions:\n\n402-week-19-LME-3.pdf: high resolution .pdf, exactly as delivered [1.3 MB];\n402-week-19-LME-3_1pp.pdf: printable version, one-slide-per-page [1.2 MB];\n402-week-19-LME-3_6pp.pdf: printable version, six-slides-per-page [1.3 MB].\n\nThe high resolution version is the version delivered for the lecture recordings. To make the slides easier to download, I produced lower resolution versions: 1pp and 6pp. These should be easier to download and print out if that is what you want to do.\n\n\n\n\n\nWe will be working with the ML word recognition study data-set. ML examined visual word recognition in younger and older adults using the lexical decision task.\nIn lexical decision, participants are presented with a stimulus: a string of letters that is either a real word (e.g., ‘car’) or a made-up or non-word (e.g., ‘cas’). Participants are required to respond to the stimulus by pressing a button to indicate either that they think the stimulus is a word or that they think it is a non-word. Each complete sequence of events, in which a stimulus is presented and a response is recorded, is known as a trial. In the lexical decision task implemented by ML, all study participants were presented with a mix of 160 word stimuli and 160 non-word stimuli, in random order, in a total of 320 trials.\nEach stimulus was presented one at a time on the computer screen. The critical outcome measure was the reaction time (RT) or latency for each response. Observed RT represents the interval of time from the moment the stimulus was first presented (the stimulus onset) to the moment the response was made (the response onset).\nThe critical feature of the study, here, is that we have an outcome – a decision response – observed multiple times (for each stimulus) for each participant. We shall be analyzing the speed of response, reaction time (RT), measured in milliseconds (ms).\nYou can read more about these data in the chapter on developing linear mixed-effects models.\nInstead of posing a simple and general research question, we shall orient our work around a set of quite specific predictions. ML hypothesized:\n\nEffects of stimulus attributes\n\n\nPredicting that words that are shorter, that look like more other words, and that appear frequently in the language will be easier to recognize.\n\n\nEffects of participant attributes\n\n\nPredicting that older readers would be faster and more accurate than younger readers in word recognition.\n\n\nEffects of interactions between the effects of word attributes and person attributes.\n\n\nPredicting that better (older) readers will show smaller effects of word attributes.\n\n\n\n\n\n\n\nImportant\n\n\n\nGet the data: get the data file and the .R script you can use to do the exercises that will support your learning.\n\nYou can download the files folder for this chapter by clicking on the link 03-mixed.zip.\n\n\n\nThe practical materials folder includes a data file and an .R script:\n\nsubjects.behaviour.words-310114.csv which holds information about the (word) stimuli, participants, and the responses recorded in the ML study.\n\nThe .csv file is a comma separated values file and can be opened in Excel.\nThe data file is collected together with the .R script:\n\n402-03-mixed-workbook.R the workbook you will need to do the practical exercises.\n\n\n\n\n\n\n\nImportant\n\n\n\nYou can access the sign-in page for R-Studio Server here\n\n\n\n\nWe have reached the stage where our learning targets can be detailed – here, we are concerned with:\n\nbeing able to specify a mixed-effects model in lmer() code;\nbeing able to identify how the mixed-effects model code varies depending on the kinds of random effects;\nbeing able to identify the elements of the output or results that come from an lmer() mixed-effects analysis;\nbeing able to interpret the fixed-effects estimates;\nbeing able to interpret the random effects estimates, variance, covariance;\nbeing able to describe in words and summary tables the results of a mixed-effects model;\nbeing able to visualise the effects estimates from a mixed-effects model.\n\n\n\n\nNow you will progress through a series of tasks, and challenges, to aid your learning.\n\n\n\n\n\n\nWarning\n\n\n\nWe will work with the data file:\n\nsubjects.behaviour.words-310114.csv\n\n\n\nWe again split the steps into into parts, tasks and questions.\nWe are going to work through the following workflow steps: each step is labelled as a practical part.\n\nSet-up\nLoad the data\nTidy the data\nAnalyze data with lm\nAnalyze data with lmer\nCompare models with different random effects\np-values and significance using lmerTest\nRun your own sequence of lmer models\nOptional: reproduce the plots in the chapter and slides\n\nIn the following, we will guide you through the tasks and questions step by step.\n\n\n\n\n\n\nImportant\n\n\n\nAn answers version of the workbook will be provided after the practical class.\n\n\n\n\n\nTo begin, we set up our environment in R.\n\n\nUse the library() function to make the functions we need available to you.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlibrary(broom)\nlibrary(gridExtra)\nlibrary(here)\n\nhere() starts at /Users/padraic/MSc_411_412_2025-26\n\nlibrary(lme4)\n\nLoading required package: Matrix\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::combine() masks gridExtra::combine()\n✖ tidyr::expand()  masks Matrix::expand()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ tidyr::pack()    masks Matrix::pack()\n✖ tidyr::unpack()  masks Matrix::unpack()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\n\n\n\n\nRead the data files into R:\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all &lt;- read_csv(\"subjects.behaviour.words-310114.csv\", na = \"-999\")\n\nRows: 5440 Columns: 25\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (6): subjectID, Test, Gender, Subject, item_name, item_type\ndbl (18): item_number, Age, Years_in_education, TOWRE_wordacc, TOWRE_nonword...\nnum  (1): SUBTLWF\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nThe ML study data includes the following variables that we will work with (as well as some you can ignore):\n\nIdentifying variables\n\n\nsubjectID – identifying code for participants\nitem_name – words presented as stimuli\nitem_number – identifying code for words presented\n\n\nResponse variables\n\n\nRT – response reaction time (ms), for responses to words, i.e., word recognition time\n\n\nParticipant attribute variables\n\n\nAge – in years\nGender – coded M (male), F (female)\nTOWRE_wordacc – word reading skill, words read correctly (out of 104)\nTOWRE_nonwordacc – nonword reading skill, nonwords (made up words) read correctly (out of 63)\nART_HRminusFR – reading experience score\n\n\nStimulus property variables\n\n\nLength – word length, in letters\nOrtho_N – orthographic neighbourhood size, how many other words in English a stimulus word looks like\nOLD – orthographic Levenshtein distance, how many letter edits (addition, deletion or substitution) it would take to make a stimulus word look like another English word (a measure of orthographic neighbourhood) (Yarkoni et al., 2008)\nBG_Sum, BG_Mean, BG_Freq_By_Pos – measures of how common are pairs of letters that compose stimulus words\nSUBTLWF, LgSUBTLWF, SUBTLCD, LgSUBTLCD – measures of how common stimulus words are, taken from the SUBTLEX corpus analysis of word frequency (Brysbaert & New, 2009)\n\n\n\n\n\nTidying the data involves a number of tasks, some essential and some things we do for our convenience.\nWe are going to first filter the observations, then transform the outcome variable.\n\nWe do this work and use data visualization to examine the impacts of the actions.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is always a good idea to first inspect what you have got when you read a data file into R before you do anything more demanding.\n\nYou cannot assume that the data are what you think they are\nor that the data are structured or coded in the ways that you think (or have been told) they should be structured or coded.\n\n\n\n\n\nWe should examine the distribution of the outcome variable, lexical decision response reaction time (RT in ms). Observations about variable value distributions are a part of Exploratory Data Analysis and serve to catch errors in the data-set (e.g. incorrectly recorded scores) but also to inform the researcher’s understanding of their own data.\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe shall examine the distribution of the outcome variable, lexical decision response reaction time (RT in ms), using density plots. An alternative method would be to use histograms.\nA density plot shows a curve. You can say that the density corresponds to the height of the curve for a given value of the variable being depicted, and that it is related to the probability of observing values of the variable within some range of values (Howell, 2016).\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all %&gt;%\n  ggplot(aes(x = RT)) +\n  geom_density(size=1.5) +\n  geom_rug(alpha = .2) +\n  ggtitle(\"Raw RT\") +\n  theme_bw()  \n\n\n\n\n\n\n\nFigure 1: Density plot showing word recognition reaction time, correct and incorrect responses\n\n\n\n\n\nThe code to produce Figure 1 works in a series of steps.\n\nML.all %&gt;% takes the data-set, from the ML study, that we have read in to the R workspace and pipes it to the visualization code, next.\nggplot(aes(x = RT)) + creates a plot object in which the x-axis variable is specified as RT. The values of this variable will be mapped to geometric objects, i.e. plot features, that you can see, next.\ngeom_density(size=1.5) + first displays the distribution of values in the variable RT as a density curve. The argument size=1.5 tells R to make the line \\(1.5 \\times\\) the thickness of the line used by default to show variation in density.\n\nSome further information is added to the plot, next.\n\ngeom_rug(alpha = .2) + with a command that tells R to add a rug plot below the density curve.\nggtitle(\"Raw RT\") makes a plot title.\n\nNotice that beneath the curve of the density plot, you can see a series of vertical lines. Each line represents the x-axis location of an RT observation in the ML study data set. This rug plot represents the distribution of RT observations in one dimension.\n\ngeom_rug() draws a vertical line at each location on the x-axis that we observe a value of the variable, RT, named in aes(x = RT).\ngeom_rug(alpha = .2) reduces the opacity of each line, using alpha, to ensure the reader can see how the RT observations are denser in some places than others.\n\n\n\n\n\n\n\n\nTake a look at the geoms documented in the {ggplot2} library reference section here.\nExperiment with code to answer the following questions:\n\n\nWould a histogram or a frequency polygon provide a more informative view? Take a look here for advice.\nWhat about a dotplot? Take a look here for advice\n\n\n\n\nThe code example, shown above, delivers a plot (Figure 1) showing three peaks in the distribution of RT values. You can see that there is a peak of RT observations around 500-1000ms, another smaller peak around -500ms, and a third smaller peak around -2000ms.\nThe density plot shows the reaction times recorded for participants’ button press ‘yes’ responses to word stimuli in the lexical decision task. The peaks of negative RTs represent observations that are impossible.\n\nHow do we deal with the impossible RT values?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe density plot shows us that the raw ML lexical decision RT variable includes negative RT values corresponding to incorrect response. These have to be removed.\n\nWe can do this quite efficiently by creating a subset of the original “raw” data, defined according to the RT variable using the {dpyr} library filter() function.\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all.correct &lt;- filter(ML.all, RT &gt;= 200)\n\nThe filter code is written to subset the data by rows using a condition on the values of the RT variable.\nML.all.correct &lt;- filter(ML.all, RT &gt;= 200) works as follows.\n\nML.all.correct &lt;- filter(ML.all ...) creates a new data-set with a new name ML.all.correct from the old data-set ML.all using the filter() function.\nfilter(... RT &gt;= 200) specifies an argument for the filter() function.\n\nIn effect, we are asking R to check every value in the RT column.\n\nR will do a check through the ML.all data-set, row by row.\nIf a row includes an RT that is greater than or equal to 200 then that row will be included in the new data-set ML.all.correct. This is what I mean by using a condition.\nBut if a row includes an RT that is less than 200, then that row will not be included. We express this condition as RT &gt;= 200.\n\n\n\n\n\n\n\n\nHow can we check that the filter operation worked in the way we should expect it to?\n\nAfter we have removed negative (error) RTs, we can check that the size of the data-set – here, the number of rows – matches our expectations.\n\nHow do we do that?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe length() function will count the elements in whatever object is specified as an argument in the function call.\n\nThis means that if you put a variable name into the function as in length(data$variable) it will count how long that variable is – how many rows there are in the column.\nIf that variable happens to be, as here, part of a data-set, the same calculation will tell you how many rows there are in the data-set as a whole.\nIf you just enter length(data), naming some data-set, then the function will return a count of the number of columns in the data-set.\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlength(ML.all$RT)\n\n[1] 5440\n\nlength(ML.all.correct$RT)\n\n[1] 5257\n\n\nIf you run the length() function calls then you should see that the length or number of observations or rows in the ML.all.correct data-set should be smaller than the number of observations in the ML.all data-set.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIt is wise to check that the operations you perform to tidy, process or wrangle data actually do do what you mean them to do. Checks can be performed, for each processing stage, by:\n\nForming expectations or predictions about what the operation is supposed to do e.g. filter out some rows by some number;\nCheck what you get against these predictions e.g. count the number of rows before versus after filtering.\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all.correct %&gt;%\n  ggplot(aes(x = RT)) +\n  geom_density(size=1.5) + \n  geom_rug(alpha = .2) +\n  ggtitle(\"Correct RTs\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 2: Density plot showing word recognition reaction time, correct responses only\n\n\n\n\n\n\n\n\n\n\n\n\nChange the threshold for including RTs from RT &gt;= 200 to something else: you can change the number, or you can change the operator from &gt;= to a different comparison (try =, &lt;, &lt;=, &gt;.\nCan you assess what impact the change has?\n\n\n\n\nFigure 2 shows that we have successfully removed all errors (negative RTs) but now we see just how skewed the RT distribution is. Note the long tail of longer RTs.\nGenerally, we assume that departures from a model’s predictions about our observations (the linear model residuals) are normally distributed, and we often assume that the relationship between outcome and predictor variables is linear (Cohen et al., 2003). We can ensure that our data are compliant with both assumptions by transforming the RT distribution.\n\nWhat transformation should we use?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nPsychology researchers often take the log (often the log base 10) of RT values before performing an analysis. Transforming RTs to the log base 10 of RT values has the effect of correcting the skew – bringing the larger RTs ‘closer’ (e.g., \\(1000 = 3\\) in log10) to those near the middle which do not change as much (e.g. \\(500 = 2.7\\) in log10).\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all.correct$logrt &lt;- log10(ML.all.correct$RT)            \n\nThe log10() function works as follows:-\n\nML.all.correct$logrt &lt;- log10(...) creates a a new variable logrt, adding it to the ML.all.correct data-set. The variable is created using the transformation function log10().\nlog10(ML.all.correct$RT) creates a the new variable by transforming (to log10) the values of the old variable, RT.\n\n\n\n\n\n\n\nWe can see the effect of the transformation if we plot the log10 transformed RTs (see Figure 3).\n\nDo we arrive at a distribution that more closely approximates the normal distribution?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all.correct %&gt;%\n  ggplot(aes(x = logrt)) +\n  geom_density(size = 1.5) + \n  geom_rug(alpha = .2) +\n  ggtitle(\"Correct log10 RTs\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 3: Density plot showing log10 transformed reaction time, correct responses only\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThere are other log transformation functions and we often see researchers using the natural log instead of the log base 10 as discussed here\n\n\n\n\n\nML asked all participants in a sample of people to read a selection of words, a sample of words from the language.\nFor each participant, we will have multiple observations and these observations will not be independent. One participant will tend to be slower or less accurate compared to another.\n\nWhat does this look like?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all.correct %&gt;%\n  group_by(subjectID) %&gt;%\n  mutate(mean_logrt = mean(logrt, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(subjectID = fct_reorder(subjectID, mean_logrt)) %&gt;%\n  ggplot(aes(x = logrt)) +\n  geom_density(size = 1.25) +\n  facet_wrap(~ subjectID) +\n  geom_vline(xintercept = 2.778807, colour = \"red\", linetype = 2) +\n  scale_x_continuous(breaks = c(2.5,3)) +\n  ggtitle(\"Plot showing distribution of logRT for each participant; red line shows mean log10 RT\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 4: Density plot showing log10 transformed reaction time, correct responses, separately for each participant\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor each stimulus word, there are multiple observations and these observations will not be independent. One stimulus may prove to be more challenging to all participants compared to another, eliciting slower or less accurate responses on average.\n\nWhat does this look like?\n\n\n\n\n\n\n\nWe model the effects of interest, using all the data (hence, complete pooling) but ignoring the differences between participants. This means we can see something of the ‘true’ picture of our data through the linear model results though we should be aware that the linear model will miss important information, which the mixed-effects model will include, that would improve its performance.\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe linear model is fit in R using the lm() function.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all.correct.lm  &lt;- lm(logrt ~\n                             \n                             LgSUBTLCD,     \n                           \n                           data = ML.all.correct)\n\nsummary(ML.all.correct.lm)\n\n\nCall:\nlm(formula = logrt ~ LgSUBTLCD, data = ML.all.correct)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.41677 -0.07083 -0.01163  0.05489  0.53411 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.885383   0.007117  405.41   &lt;2e-16 ***\nLgSUBTLCD   -0.033850   0.002209  -15.32   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1095 on 5255 degrees of freedom\nMultiple R-squared:  0.04277,   Adjusted R-squared:  0.04259 \nF-statistic: 234.8 on 1 and 5255 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nVary the linear model using different outcomes or predictors.\n\nThe ML study data, like the CP study data, are rich with possibility. It would be useful to experiment with data like this because this will prepare you for similar analysis targets in future.\nChange the predictor from frequency to something else: what do you see when you visualize the relationship between outcome and predictor variables using scatterplots?\n\n\n\n\n\n\n\nTip\n\n\n\nSpecify linear models with different predictors: do the relationships you see in plots match the coefficients you see in the model estimates?\n\nYou should both estimate the effects of variables and visualize the relationships between variables using scatterplots.\nIf you combine reflection on the model estimates with evaluation of what the plots show you then you will be able to see how reading model results and reading plots can reveal the correspondences between the two ways of looking at your data.\n\n\n\n\n\n\n\n\n\nYou should specify, here, a model of the frequency effect that takes into account:\n\nthe random effect of participants on intercepts;\nthe random effect of participants on the slopes of the frequency (LgSUBTLCD) effect;\nas well as the random effect of items on intercepts.\n\nYou can specify the random effects of participants allowing for covariance between the random intercepts and the random slopes.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all.correct.lmer  &lt;- lmer(logrt ~\n\n                           LgSUBTLCD +\n\n                           (LgSUBTLCD + 1|subjectID) +\n\n                           (1|item_name),\n\n                         data = ML.all.correct)\n\nsummary(ML.all.correct.lmer)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: logrt ~ LgSUBTLCD + (LgSUBTLCD + 1 | subjectID) + (1 | item_name)\n   Data: ML.all.correct\n\nREML criterion at convergence: -9868.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.6307 -0.6324 -0.1483  0.4340  5.6132 \n\nRandom effects:\n Groups    Name        Variance  Std.Dev. Corr \n item_name (Intercept) 0.0003268 0.01808       \n subjectID (Intercept) 0.0054212 0.07363       \n           LgSUBTLCD   0.0002005 0.01416  -0.63\n Residual              0.0084333 0.09183       \nNumber of obs: 5257, groups:  item_name, 160; subjectID, 34\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)  2.887997   0.015479 186.577\nLgSUBTLCD   -0.034471   0.003693  -9.333\n\nCorrelation of Fixed Effects:\n          (Intr)\nLgSUBTLCD -0.764\n\n\nAs will now be getting familiar, the code works as follows:\n\nML.all.correct.lmer  &lt;- lmer(...) creates a linear mixed-effects model object using the lmer() function.\nlogrt ~ LgSUBTLCD the fixed effect in the model is expressed as a formula in which the outcome or dependent variable logrt is predicted ~ by the independent or predictor variable LgSUBTLCD word frequency.\n\nThe random effects part of the model is then specified as follows.\n\nWe first have the random effects associated with random differences between participants:\n\n\n(...|subjectID) adds random effects corresponding to random differences between sample groups (participants subjects) coded by the subjectID variable.\n(...1 |subjectID) including random differences between sample groups (subjectID) in intercepts coded 1.\n(LgSUBTLCD... |subjectID) and random differences between sample groups (subjectID) in the slopes of the frequency effect coded by using theLgSUBTLCD variable name.\n\n\nThen, we have the random effects associated with random differences between stimuli:\n\n\n(1|item_name) adds a random effect to account for random differences between sample groups (item_name) in intercepts coded 1.\n\n\n...(..., data = ML.all.correct) specifies the data-set in which you can find the variables named in the model fitting code.\nLastly, we can then specify summary(ML.all.correct.lmer) to get a summary of the fitted model.\n\n\n\n\n\nCan you explain the results shown in the model summary?\n\nYou can be guided by the in-depth explanation of how. to read the analysis results in the chapter on developing linear mixed-effects models.\n\n\n\n\n\n\nYou can specify models where the fixed effects part is the same (e.g., you assume $RT $ word frequency) but, for various reasons, you may suppose different random effects structures.\nIn this scenario — the model fixed effects part is the same but the random effects are different — you may be required to evaluate which model structure fits the data better, to the extent that that can be assessed, given your sample data.\n\nTo begin, fit a model with just the fixed effects of intercept and frequency, and the random effects of participants or items on intercepts only.\n\n\nWe exclude the (LgSUBTLCD + ...|subjectID) specification for the random effect of participants on the slope of the frequency LgSUBTLCD effect.\n\nWe use REML fitting, if we want to compare (as we will) models with the same fixed effects but different random effects.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all.correct.lmer.REML.si  &lt;- lmer(logrt ~ LgSUBTLCD + \n                                    \n                                          (1|subjectID) + (1|item_name),\n\n       data = ML.all.correct, REML = TRUE)\n\nsummary(ML.all.correct.lmer.REML.si)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: logrt ~ LgSUBTLCD + (1 | subjectID) + (1 | item_name)\n   Data: ML.all.correct\n\nREML criterion at convergence: -9845.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.5339 -0.6375 -0.1567  0.4364  5.5851 \n\nRandom effects:\n Groups    Name        Variance  Std.Dev.\n item_name (Intercept) 0.0003204 0.01790 \n subjectID (Intercept) 0.0032650 0.05714 \n Residual              0.0085285 0.09235 \nNumber of obs: 5257, groups:  item_name, 160; subjectID, 34\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)  2.887697   0.013253   217.9\nLgSUBTLCD   -0.034390   0.002774   -12.4\n\nCorrelation of Fixed Effects:\n          (Intr)\nLgSUBTLCD -0.658\n\n\nIf you look at the code chunk, you can see that:\n\nREML = TRUE is the only change to the code: it specifies the change in model fitting method;\nalso, I changed the model name to ML.all.correct.lmer.REML.si to be able to distinguish the maximum likelihood from the restricted maximum likelihood model.\n\n\n\n\nFollowing Baayen et al. (2008a), we can then run a series of models with just one random effect.\n\nNext, fit a model with just the random effect of items on intercepts.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all.correct.lmer.REML.i  &lt;- lmer(logrt ~\n\n       LgSUBTLCD + (1|item_name),\n\n       data = ML.all.correct, REML = TRUE)\n\nsummary(ML.all.correct.lmer.REML.i)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: logrt ~ LgSUBTLCD + (1 | item_name)\n   Data: ML.all.correct\n\nREML criterion at convergence: -8337\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.7324 -0.6455 -0.1053  0.4944  4.8970 \n\nRandom effects:\n Groups    Name        Variance  Std.Dev.\n item_name (Intercept) 0.0002364 0.01537 \n Residual              0.0117640 0.10846 \nNumber of obs: 5257, groups:  item_name, 160\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)  2.886765   0.009047  319.07\nLgSUBTLCD   -0.034206   0.002811  -12.17\n\nCorrelation of Fixed Effects:\n          (Intr)\nLgSUBTLCD -0.977\n\n\n\n\n\n\nNext, fit a model with just the random effect of participants on intercepts.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all.correct.lmer.REML.s  &lt;- lmer(logrt ~\n\n       LgSUBTLCD + (1|subjectID),\n\n       data = ML.all.correct, REML = TRUE)\n\nsummary(ML.all.correct.lmer.REML.s)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: logrt ~ LgSUBTLCD + (1 | subjectID)\n   Data: ML.all.correct\n\nREML criterion at convergence: -9786.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.5843 -0.6443 -0.1589  0.4434  5.5266 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n subjectID (Intercept) 0.003275 0.05723 \n Residual              0.008837 0.09401 \nNumber of obs: 5257, groups:  subjectID, 34\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)  2.885751   0.011561  249.60\nLgSUBTLCD   -0.033888   0.001897  -17.87\n\nCorrelation of Fixed Effects:\n          (Intr)\nLgSUBTLCD -0.517\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNotice how each different model has a distinct name.\n\nWe will use these different names to compare the models, next.\n\n\n\n\n\n\nIf we now run Likelihood Ratio Test comparisons of these models, we are effectively examining if one of the random effects can be dispensed with: if its inclusion makes no difference to the likelihood of the model then it is not needed.\n\nIs the random effect of subjects on intercepts justified?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nCompare models, first, with ML.all.correct.lmer.REML.si versus without ML.all.correct.lmer.REML.i the random effect of subjects on intercepts.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nanova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.i, refit = FALSE)\n\nData: ML.all.correct\nModels:\nML.all.correct.lmer.REML.i: logrt ~ LgSUBTLCD + (1 | item_name)\nML.all.correct.lmer.REML.si: logrt ~ LgSUBTLCD + (1 | subjectID) + (1 | item_name)\n                            npar     AIC     BIC logLik -2*log(L)  Chisq Df\nML.all.correct.lmer.REML.i     4 -8329.0 -8302.7 4168.5   -8337.0          \nML.all.correct.lmer.REML.si    5 -9835.1 -9802.3 4922.6   -9845.1 1508.1  1\n                            Pr(&gt;Chisq)    \nML.all.correct.lmer.REML.i                \nML.all.correct.lmer.REML.si  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\nIs the random effect of items on intercepts justified?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nCompare models with ML.all.correct.lmer.REML.si versus without ML.all.correct.lmer.REML.s the random effect of items on intercepts.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nanova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.s, refit = FALSE)\n\nData: ML.all.correct\nModels:\nML.all.correct.lmer.REML.s: logrt ~ LgSUBTLCD + (1 | subjectID)\nML.all.correct.lmer.REML.si: logrt ~ LgSUBTLCD + (1 | subjectID) + (1 | item_name)\n                            npar     AIC     BIC logLik -2*log(L)  Chisq Df\nML.all.correct.lmer.REML.s     4 -9778.3 -9752.0 4893.2   -9786.3          \nML.all.correct.lmer.REML.si    5 -9835.1 -9802.3 4922.6   -9845.1 58.825  1\n                            Pr(&gt;Chisq)    \nML.all.correct.lmer.REML.s                \nML.all.correct.lmer.REML.si  1.723e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nWe compare models using the anova() function.\n\nanova() does the model comparison, for the models named in the list in brackets.\n\nYou can do this for the foregoing series of models but notice that in the code we specify:\n\nrefit = FALSE\n\nWhat happens if we do not add that bit?\nWhat you will see if you run the anova() function call, without therefit = FALSE argument – try it – is that you will then get the warning refitting model(s) with ML (instead of REML).\nWhy? The immediate reason for this warning is that we have to specify refit = FALSE because otherwise R will compare ML fitted models. The refitting occurs by default.\nWhat is the reason for the imposition of this default?\nPinheiro & Bates (2000) advise that if one is fitting models with random effects the estimates are more accurate if the models are fitted using Restricted Maximum Likelihood (REML). That is achieved in the lmer() function call by adding the argument REML=TRUE. Pinheiro & Bates (2000) further recommend (see, e.g., pp.82-) that if you compare models:\n\nwith the same fixed effects\nbut with varying random effects\nthen the models should be fitted using Restricted Maximum Likelihood.\n\nRead a discussion of the issues, with references, in chapter on developing linear mixed-effects models. See that chapter, also, for an in-depth account of what the ’anova()` results tell you.\n\n\n\nIs it justified to add a random effect of participants on the slopes of the frequency effect?\n\nHow should you address this question?\n\nFirst, you will need to fit a model incorporating the random effect of participants on the slopes of the frequency effect.\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou will want to make sure that this random slopes model is comparable to the next-most complex model: ML.all.correct.lmer.REML.si.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all.correct.lmer.REML.slopes  &lt;- lmer(logrt ~ LgSUBTLCD + \n                                           \n                                          (LgSUBTLCD + 1|subjectID) +\n                                          (1|item_name),\n\n       data = ML.all.correct, REML = TRUE)\n\nLooking at the code:\n\nWith (LgSUBTLCD + 1 |subjectID) we specify a random effect of subjects on intercepts and on the slope of the frequency effects.\nWe do not specify – it happens by default – the estimation of the covariance of random differences among subjects in intercepts and random differences among subjects in the slope of the frequency effect.\n\n\n\n\nThen you will want to compare two models that are the same except for the incorporation of the the random effect of participants on the slopes of the frequency effect.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nanova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.slopes, \n      refit = FALSE)\n\nData: ML.all.correct\nModels:\nML.all.correct.lmer.REML.si: logrt ~ LgSUBTLCD + (1 | subjectID) + (1 | item_name)\nML.all.correct.lmer.REML.slopes: logrt ~ LgSUBTLCD + (LgSUBTLCD + 1 | subjectID) + (1 | item_name)\n                                npar     AIC     BIC logLik -2*log(L)  Chisq Df\nML.all.correct.lmer.REML.si        5 -9835.1 -9802.3 4922.6   -9845.1          \nML.all.correct.lmer.REML.slopes    7 -9854.1 -9808.1 4934.0   -9868.1 22.934  2\n                                Pr(&gt;Chisq)    \nML.all.correct.lmer.REML.si                   \nML.all.correct.lmer.REML.slopes  1.047e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\n\nIf you look at the fixed effects summary, you can see that we do not get p-values by default. To calculate p-values, we need to count residual degrees of freedom. The authors of the {lme4} library that furnishes the lmer() function do not (as e.g. Baayen et al., 2008b discuss) think that it is sensible to estimate the residual degrees of freedom for a model in terms of the number of observations. This is because the number of observations concerns one level of a multilevel data-set that might be structured with respect to some number of subjects, some number of items. This means that one cannot then accurately calculate p-values to go with the t-tests on the coefficients estimates; therefore they do not.\n\n\nWe can run mixed-effects models with p-values from significance tests on the estimates of the fixed effects coefficients using the library(lmerTest).\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlibrary(lmerTest)\n\nML.all.correct.lmer.REML.slopes  &lt;- lmer(logrt ~ LgSUBTLCD + \n                                           \n                                            (LgSUBTLCD + 1|subjectID) +\n                                            (1|item_name),\n\n                                          data = ML.all.correct, REML = TRUE)\n\nsummary(ML.all.correct.lmer.REML.slopes)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: logrt ~ LgSUBTLCD + (LgSUBTLCD + 1 | subjectID) + (1 | item_name)\n   Data: ML.all.correct\n\nREML criterion at convergence: -9868.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.6307 -0.6324 -0.1483  0.4340  5.6132 \n\nRandom effects:\n Groups    Name        Variance  Std.Dev. Corr \n item_name (Intercept) 0.0003268 0.01808       \n subjectID (Intercept) 0.0054212 0.07363       \n           LgSUBTLCD   0.0002005 0.01416  -0.63\n Residual              0.0084333 0.09183       \nNumber of obs: 5257, groups:  item_name, 160; subjectID, 34\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)  2.887997   0.015479 47.782839 186.577  &lt; 2e-16 ***\nLgSUBTLCD   -0.034471   0.003693 60.338786  -9.333 2.59e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n          (Intr)\nLgSUBTLCD -0.764\n\n\n\n\n\nBasically, the call to access the lmerTest library ensures that when we run the lmer() function we get a calculation of an approximation to the denominator degrees of freedom that enables the calculation of the p-value for the t-test for the fixed effects coefficient.\n\n\n\n\n\n\nTip\n\n\n\nAn alternative approach to doing significance tests of hypothesized effects, is to compare models with versus without the effect of interest.\n\n\n\n\n\n\n\n\nIt will be useful for you to examine model comparisons with a different set of models for the same data.\nYou could try to run a series of models in which the fixed effects variable is something different, for example, the effect of word Length or the effect of orthographic neighbourhood size Ortho_N.\n\nRun models with these variables or pick your own set of predictor variables to get estimates of fixed effects.\n\nIn this series of models, specify a set of models where:\n\nEach model has the same fixed effect e.g. logrt ~ Length;\nBut you vary the random effects components so that, at the end, you can assess what random effects are justified by better model fit to data.\n\nIn the following, I set out another series of models as an example of what you can do.\n\nRandom effects of participants on intercepts and slopes, random effect of items on intercepts\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all.correct.lmer.REML.slopes  &lt;- lmer(logrt ~ Length + \n                                           \n                                           (Length + 1|subjectID) +\n                                           (1|item_name),\n                                         \n                                         data = ML.all.correct, REML = TRUE)\n\nsummary(ML.all.correct.lmer.REML.slopes)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: logrt ~ Length + (Length + 1 | subjectID) + (1 | item_name)\n   Data: ML.all.correct\n\nREML criterion at convergence: -9749.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.4052 -0.6300 -0.1563  0.4364  5.5167 \n\nRandom effects:\n Groups    Name        Variance  Std.Dev. Corr \n item_name (Intercept) 9.043e-04 0.030072      \n subjectID (Intercept) 3.055e-03 0.055272      \n           Length      9.039e-05 0.009507 -0.33\n Residual              8.474e-03 0.092056      \nNumber of obs: 5257, groups:  item_name, 160; subjectID, 34\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept) 2.764e+00  1.870e-02 1.110e+02 147.782   &lt;2e-16 ***\nLength      3.740e-03  4.038e-03 1.200e+02   0.926    0.356    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n       (Intr)\nLength -0.844\n\n\n\n\n\n\nRandom effect of participants on intercepts, random effect of items on intercepts\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all.correct.lmer.REML.si  &lt;- lmer(logrt ~ Length + \n                                       \n                                       (1|subjectID) + \n                                       (1|item_name),\n                                     \n                                     data = ML.all.correct, REML = TRUE)\n\nsummary(ML.all.correct.lmer.REML.si)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: logrt ~ Length + (1 | subjectID) + (1 | item_name)\n   Data: ML.all.correct\n\nREML criterion at convergence: -9738.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.5428 -0.6289 -0.1535  0.4357  5.5149 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n item_name (Intercept) 0.000901 0.03002 \n subjectID (Intercept) 0.003259 0.05709 \n Residual              0.008523 0.09232 \nNumber of obs: 5257, groups:  item_name, 160; subjectID, 34\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept) 2.764e+00  1.885e-02 1.725e+02 146.625   &lt;2e-16 ***\nLength      3.723e-03  3.691e-03 1.513e+02   1.009    0.315    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n       (Intr)\nLength -0.843\n\n\n\n\n\n\nRandom effect of items on intercepts\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all.correct.lmer.REML.i  &lt;- lmer(logrt ~ Length + \n                                       \n                                       (1|item_name),\n                                     \n                                     data = ML.all.correct, REML = TRUE)\n\nsummary(ML.all.correct.lmer.REML.i)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: logrt ~ Length + (1 | item_name)\n   Data: ML.all.correct\n\nREML criterion at convergence: -8233.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.4940 -0.6411 -0.1042  0.4930  4.8136 \n\nRandom effects:\n Groups    Name        Variance  Std.Dev.\n item_name (Intercept) 0.0008171 0.02858 \n Residual              0.0117545 0.10842 \nNumber of obs: 5257, groups:  item_name, 160\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept) 2.765e+00  1.622e-02 1.489e+02 170.471   &lt;2e-16 ***\nLength      3.487e-03  3.716e-03 1.487e+02   0.938     0.35    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n       (Intr)\nLength -0.986\n\n\n\n\n\n\nRandom effect of participants on intercepts\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all.correct.lmer.REML.s  &lt;- lmer(logrt ~ Length + \n                                      \n                                      (1|subjectID),\n                                    \n                                    data = ML.all.correct, REML = TRUE)\n\nsummary(ML.all.correct.lmer.REML.s)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: logrt ~ Length + (1 | subjectID)\n   Data: ML.all.correct\n\nREML criterion at convergence: -9481.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.6945 -0.6475 -0.1608  0.4572  5.2837 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n subjectID (Intercept) 0.003272 0.05720 \n Residual              0.009367 0.09678 \nNumber of obs: 5257, groups:  subjectID, 34\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept) 2.760e+00  1.266e-02 8.792e+01 218.088   &lt;2e-16 ***\nLength      4.346e-03  1.832e-03 5.222e+03   2.372   0.0177 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n       (Intr)\nLength -0.623\n\n\n\n\n\n\nModel comparisons\n\nI would compare the models in the sequence, shown in the foregoing, one pair of models at a time, to keep it simple.\n\nWhen you look at the model comparison, ask: is the difference between the models a piece of complexity (an effect) whose inclusion in the more complex model is justified or warranted by improved model fit to data?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nIf we run model comparisons, we can see that random effects of subjects or of items on intercepts, as well as random effect of subjects on slopes, are justified by significantly improved model fit to data.\n\nanova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.s, refit=FALSE)\n\nData: ML.all.correct\nModels:\nML.all.correct.lmer.REML.s: logrt ~ Length + (1 | subjectID)\nML.all.correct.lmer.REML.si: logrt ~ Length + (1 | subjectID) + (1 | item_name)\n                            npar     AIC     BIC logLik -2*log(L)  Chisq Df\nML.all.correct.lmer.REML.s     4 -9473.9 -9447.7 4741.0   -9481.9          \nML.all.correct.lmer.REML.si    5 -9728.9 -9696.1 4869.4   -9738.9 256.97  1\n                            Pr(&gt;Chisq)    \nML.all.correct.lmer.REML.s                \nML.all.correct.lmer.REML.si  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.i, refit=FALSE)\n\nData: ML.all.correct\nModels:\nML.all.correct.lmer.REML.i: logrt ~ Length + (1 | item_name)\nML.all.correct.lmer.REML.si: logrt ~ Length + (1 | subjectID) + (1 | item_name)\n                            npar     AIC     BIC logLik -2*log(L)  Chisq Df\nML.all.correct.lmer.REML.i     4 -8225.5 -8199.2 4116.8   -8233.5          \nML.all.correct.lmer.REML.si    5 -9728.9 -9696.1 4869.4   -9738.9 1505.4  1\n                            Pr(&gt;Chisq)    \nML.all.correct.lmer.REML.i                \nML.all.correct.lmer.REML.si  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.slopes, refit=FALSE)\n\nData: ML.all.correct\nModels:\nML.all.correct.lmer.REML.si: logrt ~ Length + (1 | subjectID) + (1 | item_name)\nML.all.correct.lmer.REML.slopes: logrt ~ Length + (Length + 1 | subjectID) + (1 | item_name)\n                                npar     AIC     BIC logLik -2*log(L) Chisq Df\nML.all.correct.lmer.REML.si        5 -9728.9 -9696.1 4869.4   -9738.9         \nML.all.correct.lmer.REML.slopes    7 -9735.5 -9689.5 4874.8   -9749.5 10.61  2\n                                Pr(&gt;Chisq)   \nML.all.correct.lmer.REML.si                  \nML.all.correct.lmer.REML.slopes   0.004965 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\n\nReproduce the plots in the chapter and slides\nThis part is optional for you to view and work with.\n\nRun the code, and consider the code steps only if you are interested in how the materials for the book chapter were created.\n\n\n\nIn the chapter on developing linear mixed-effects models I talk about pooling, no pooling and partial pooling, and the impact on estimates.\nTo produce the plots I discuss there, I adapted Tristan Mahr’s code, here:\n\nhttps://www.tjmahr.com/plotting-partial-pooling-in-mixed-effects-models/\n\nWe work through a sequence of fitting models, getting estimates, and producing plots.\n\nGet no pooling estimates by running lm() for each participant.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\ndf_no_pooling &lt;- lmList(logrt ~\n                          \n                          LgSUBTLCD | subjectID, ML.all.correct) %&gt;% \n  coef() %&gt;% \n  # Subject IDs are stored as row-names. Make them an explicit column\n  rownames_to_column(\"subjectID\") %&gt;% \n  rename(Intercept = `(Intercept)`, Slope_frequency = LgSUBTLCD) %&gt;% \n  add_column(Model = \"No pooling\")\n\n# -- you can uncomment and inspect the tibble produced by these lines of code\n# summary(df_no_pooling)\n\n\n\n\n\nMake the subjectID and Model label vectors a factor.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\ndf_no_pooling$subjectID &lt;- as.factor(df_no_pooling$subjectID)\ndf_no_pooling$Model &lt;- as.factor(df_no_pooling$Model)\n\n\n\n\n\nIn contrast, we might consider a complete pooling model where all the information from the participants is combined together.\n\n\nFit a single mdoel for all the data pooled together.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nm_pooled &lt;- lm(logrt ~ LgSUBTLCD, ML.all.correct) \n\n\n\n\n\nRepeat the intercept and slope terms for each participant.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\ndf_pooled &lt;- data_frame(\n  Model = \"Complete pooling\",\n  subjectID = unique(ML.all.correct$subjectID),\n  Intercept = coef(m_pooled)[1], \n  Slope_frequency = coef(m_pooled)[2])\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nℹ Please use `tibble()` instead.\n\n# -- inspect\n# summary(df_pooled)\n\n\n\n\n\nMake the model label vector a factor.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\ndf_pooled$Model &lt;- as.factor(df_pooled$Model)\n\n\n\n\n\nWe can compare these two approaches: instead of calculating the regression lines with stat_smooth(), we can use geom_abline() to draw the lines from our dataframe of intercept and slope parameters.\n\nJoin the raw data so we can use plot the points and the lines.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\ndf_models &lt;- bind_rows(df_pooled, df_no_pooling) %&gt;% \n  left_join(ML.all.correct, by = \"subjectID\")\n\nWarning in left_join(., ML.all.correct, by = \"subjectID\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 3 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n\n\n\n\nProduce the plot:\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\np_model_comparison &lt;- ggplot(df_models) + \n  aes(x = LgSUBTLCD, y = logrt) + \n  # Set the color mapping in this layer so the points don't get a color\n  geom_point(alpha = .05) +\n  geom_abline(aes(intercept = Intercept, \n                  slope = Slope_frequency, \n                  color = Model),\n              size = .75) +\n  facet_wrap(~ subjectID) +\n  scale_x_continuous(breaks = 1.5:4 * 1) + \n  theme_bw() + \n  theme(legend.position = \"top\")\n\np_model_comparison\n\n\n\n\n\n\n\n\n\n\n\n\nCompare no vs. complete vs. partial pooling estimates\n\n\nMixed-effects models represent partial pooling of data.\n\nWe fit a separate line for each cluster of data, one for each participant.\n\nthe lmList() function in {lme4} automates this process.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\ndf_no_pooling &lt;- lmList(logrt ~\n                          \n                          LgSUBTLCD | subjectID, ML.all.correct) %&gt;% \n  coef() %&gt;% \n  # Subject IDs are stored as row-names. Make them an explicit column\n  rownames_to_column(\"subjectID\") %&gt;% \n  rename(Intercept = `(Intercept)`, Slope_frequency = LgSUBTLCD) %&gt;% \n  add_column(Model = \"No pooling\")\n\n# -- inspect the tibble produced by these lines of code\n# summary(df_no_pooling)\n\n# -- make the subjectID and model label vectors a factor\ndf_no_pooling$subjectID &lt;- as.factor(df_no_pooling$subjectID)\ndf_no_pooling$Model &lt;- as.factor(df_no_pooling$Model)\n\n\n\n\nIn contrast, we might consider a complete pooling model where all the information from the participants is combined together.\n\nWe fit a single model on all the data pooled together.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nm_pooled &lt;- lm(logrt ~ LgSUBTLCD, ML.all.correct) \n\n# -- repeat the intercept and slope terms for each participant\ndf_pooled &lt;- data_frame(\n  Model = \"Complete pooling\",\n  subjectID = unique(ML.all.correct$subjectID),\n  Intercept = coef(m_pooled)[1], \n  Slope_frequency = coef(m_pooled)[2])\n\n# -- inspect\n# summary(df_pooled)\n\n# -- make the model label vector a factor\ndf_pooled$Model &lt;- as.factor(df_pooled$Model)\n\n\n\n\nWe can fit a mixed-effects model including the fixed effects of the intercept and the frequency effect\n\nas well as the random effects due to differences between participants (subject ID) in intercepts or in the slope of the frequency effect, as well as differences between items (item_name) in intercepts.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nML.all.correct.lmer  &lt;- lmer(logrt ~\n                               \n                               LgSUBTLCD + (LgSUBTLCD + 1|subjectID) + (1|item_name),     \n                             \n                             data = ML.all.correct, REML = FALSE)\n# summary(ML.all.correct.lmer)\n\n\n\n\nTo visualize these estimates, we extract each participant’s intercept and slope using coef().\n\n\n\n\n\n\nCode\n\n\n\n\n\n\n# -- make a dataframe with the fitted effects\ndf_partial_pooling &lt;- coef(ML.all.correct.lmer)[[\"subjectID\"]] %&gt;% \n  rownames_to_column(\"subjectID\") %&gt;% \n  as_tibble() %&gt;% \n  rename(Intercept = `(Intercept)`, Slope_frequency = LgSUBTLCD) %&gt;% \n  add_column(Model = \"Partial pooling\")\n\n# -- inspect the tibble produced by these lines of code\n# summary(df_partial_pooling)\n\n# -- make the subjectID and model label vectors both factors\ndf_partial_pooling$subjectID &lt;- as.factor(df_partial_pooling$subjectID)\ndf_partial_pooling$Model &lt;- as.factor(df_partial_pooling$Model)\n\n\n\n\nVisualize the comparison by producing a plot using a data-set consisting of of all three sets of model estimates.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\ndf_models &lt;- rbind(df_pooled, df_no_pooling, df_partial_pooling) %&gt;%\n  left_join(ML.all.correct, by = \"subjectID\")\n\nWarning in left_join(., ML.all.correct, by = \"subjectID\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 3 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n# -- inspect the differences between the data-sets\n# summary(df_pooled)\n# summary(df_no_pooling)\n# summary(df_partial_pooling)\n# summary(ML.all.correct)\n# summary(df_models)\n\n# -- we  produce a plot showing the no-pooling, complete-pooling and partial-pooling (mixed effects)\n# estimates\n\np_model_comparison &lt;- ggplot(df_models) +\n  aes(x = LgSUBTLCD, y = logrt) +\n  # Set the color mapping in this layer so the points don't get a color\n  geom_point(alpha = .05) +\n  geom_abline(aes(intercept = Intercept,\n                  slope = Slope_frequency,\n                  color = Model),\n              size = .75) +\n  facet_wrap(~ subjectID) +\n  scale_x_continuous(breaks = 1.5:4 * 1) +\n  theme_bw() +\n  theme(legend.position = \"top\")\n\np_model_comparison\n\n\n\n\n\n\n\n\n\n\n\n\nZoom in on a few participants:\n\n\nuse filter() to select a small number of participants.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\ndf_zoom &lt;- df_models %&gt;% \n  filter(subjectID %in% c(\"EB5\", \"JP3\", \"JL3\", \"AA1\"))\n\n# -- re-generate the plot with this zoomed-in selection data-set\n\np_model_comparison &lt;- ggplot(df_zoom) + \n  aes(x = LgSUBTLCD, y = logrt) + \n  # Set the color mapping in this layer so the points don't get a color\n  geom_point(alpha = .05) +\n  geom_abline(aes(intercept = Intercept, \n                  slope = Slope_frequency, \n                  color = Model),\n              size = .75) +\n  facet_wrap(~ subjectID) +\n  scale_x_continuous(breaks = 1.5:4 * 1) + \n  theme_bw() + \n  theme(legend.position = \"top\")\n\np_model_comparison\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe partial pooling model pulls (or shrinks) more extreme estimates towards an overall average. We can visualize this shrinkage effect by plotting a scatterplot of intercept and slope parameters from each model and connecting estimates for the same participant.\n\nWe use arrows to connect the different estimates for each participant, different estimates from no-pooling (per-participant) compared to partial-pooling (mixed-effects) models.\nThe plot shows how more extreme estimates are shrunk towards the global average estimate.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\ndf_fixef &lt;- data_frame(\n  Model = \"Partial pooling (average)\",\n  Intercept = fixef(ML.all.correct.lmer)[1],\n  Slope_frequency = fixef(ML.all.correct.lmer)[2])\n\n# Complete pooling / fixed effects are center of gravity in the plot\ndf_gravity &lt;- df_pooled %&gt;% \n  distinct(Model, Intercept, Slope_frequency) %&gt;% \n  rbind(df_fixef)\n# df_gravity\n#&gt; # A tibble: 2 x 3\n#&gt;                       Model Intercept Slope_Days\n#&gt;                       &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1          Complete pooling  252.3207   10.32766\n#&gt; 2 Partial pooling (average)  252.5426   10.45212\n\ndf_pulled &lt;- rbind(df_no_pooling, df_partial_pooling)\n\nggplot(df_pulled) + \n  aes(x = Intercept, y = Slope_frequency, color = Model) + \n  geom_point(size = 2) + \n  geom_point(data = df_gravity, size = 5) + \n  # Draw an arrow connecting the observations between models\n  geom_path(aes(group = subjectID, color = NULL), \n            arrow = arrow(length = unit(.02, \"npc\"))) + \n  # Use ggrepel to jitter the labels away from the points\n  ggrepel::geom_text_repel(\n    aes(label = subjectID, color = NULL),\n    data = df_no_pooling) +\n  # Don't forget 373\n  # ggrepel::geom_text_repel(\n  #   aes(label = subjectID, color = NULL), \n  #   data = filter(df_partial_pooling, Subject == \"373\")) + \n  theme(legend.position = \"right\") + \n  # ggtitle(\"Pooling of regression parameters\") + \n  xlab(\"Intercept estimate\") + \n  ylab(\"Slope estimate\") + \n  scale_color_brewer(palette = \"Dark2\") +\n  theme_bw()\n\nWarning: ggrepel: 6 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfter the practical class, we will reveal the answers that are currently hidden.\nThe answers version of the webpage will present my answers for questions, and some extra information where that is helpful.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 18. Developing linear mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/04-glmm-workbook.html",
    "href": "PSYC412/part2/04-glmm-workbook.html",
    "title": "Week 19. Workbook introduction to Generalized Linear Mixed-effects Models",
    "section": "",
    "text": "Welcome to your overview of the work we will do together in Week 19.\nWe extend our understanding and skills by moving to examine data where the outcome variable is categorical: this is a context that requires the use of Generalized Linear Mixed-effects Models (GLMMs).\n\n\n\n\n\n\nImportant\n\n\n\nCategorical outcomes cannot be analyzed using linear models, in whatever form, without having to make some important compromises.\n\n\nYou need to do something about the categorical nature of the outcome.\n\n\nHere, we look at Generalized Linear Mixed-effects Models (GLMMs): we can use these models to analyze outcome variables of different kinds, including outcome variables like response accuracy that are coded using discrete categories (e.g. correct vs. incorrect).\nIn this workbook, and in our conceptual introduction, our aims are to:\n\nRecognize the limitations of alternative (traditional) methods for analyzing such outcomes.\nUnderstand the practical reasons for using GLMMs when we analyze discrete outcome variables.\nPractice running GLMMs with varying random effects structures.\nPractice reporting the results of GLMMs, including through the use of model plots.\n\n\n\n\nYou will see, next, the lectures we share to explain the concepts you will learn about, and the practical data analysis skills you will develop. Then you will see information about the practical materials you can use to build and practise your skills.\nEvery week, you will learn best if you first watch the lectures then do the practical exercises.\n\n\n\n\n\n\nLinked resources\n\n\n\n\nWe learned about multilevel structured data in the conceptual introduction to multilevel data and the workbook introduction to multilevel data.\nWe then deepened our understanding by looking at the analysis of data from studies with repeated-measures designs in the conceptual introduction to linear mixed-effects models and the workbook introduction to mixed-effects models.\nWe further extended our understanding and practice skills in the chapter on developing linear mixed-effects models and the corresponding workbook.\n\nThis workbook introduction on Generalized Linear Mixed-effects Models (GLMMs) is linked to the corresponding chapter where the explanation of ideas or of practical analysis steps is set out more extensively or in more depth.\n\n\n\n\nThe lecture materials for this week are presented in three short parts.\nClick on a link and your browser should open a tab showing the Panopto video for the lecture part.\n\nPart 1 (20 minutes): Understand the reasons for using Generalized Linear Mixed-effects Models (GLMMs); categorical outcomes like response accuracy and the bad compromises involved in (traditional methods) not using GLMMs to analyze them; understanding what the generalized part of this involves.\n\n\n\nPart 2 (16 minutes): The questions, design and methods of the working data example study; category coding practicalities; specifying a GLMM appropriate to the design; the GLMM results summary, reading the results, visualizing the results.\n\n\n\nPart 3 (19 minutes): What random effects should we include; recognizing when a model has got into trouble, and what to do about it; general approaches to model specification; reporting model results.\n\n\n\n\n\n\n\n\n\n\n\nDownload the lecture slides\n\n\n\nYou can download the lecture slides in two different versions:\n\n402-week-20-GLMM.pdf: exactly as delivered [700 KB];\n402-week-20-GLMM_6pp.pdf: printable version, six-slides-per-page [850 KB].\n\nThe GLMM.pdf version is the version delivered for the lecture recordings. To make the slides easier to download, I produced a six-slide-per-page version, GLMM_6pp.pdf. This should be easier to download and print out if that is what you want to do.\n\n\n\n\n\nWe will be working with data collected for a study investigating word learning in children, reported by Ricketts et al. (2021). You will see that the study design has both a repeated measures aspect because each child is asked to respond to multiple stimuli, and a longitudinal aspect because responses are recorded at two time points. Because responses were observed to multiple stimuli for each child, and because responses were recorded at multiple time points, the data have a multilevel structure. These features require the use of mixed-effects models for analysis.\nWe will see, also, that the study involves the factorial manipulation of learning conditions. This means that, when you see the description of the study design, you will see embedded in it a 2 x 2 factorial design. You will be able to generalize from our work here to many other research contexts where psychologists conduct experiments in which conditions are manipulated according to a factorial design.\nHowever, our focus now is on the fact that the outcome for analysis is the accuracy of the responses made by children to word targets in a spelling task. The categorical nature of accuracy as an outcome is the reason why we now turn to use Generalized Linear Mixed-effects Models.\nYou can read more about these data in the conceptual introduction chapter on GLMMs.\nWe addressed three research questions and tested predictions in relation to each question.\n\nDoes the presence of orthography promote greater word learning?\n\n\nWe predicted that children would demonstrate greater orthographic learning for words that they had seen (orthography present condition) versus not seen (orthography absent condition).\n\n\nWill orthographic facilitation be greater when the presence of orthography is emphasized explicitly during teaching?\n\n\nWe expected to observe an interaction between instructions and orthography, with the highest levels of learning when the orthography present condition was combined with explicit instructions.\n\n\nDoes word consistency moderate the orthographic facilitation effect?\n\n\nFor orthographic learning, we expected that the presence of orthography might be particularly beneficial for words with higher spelling-sound consistency, with learning highest when children saw and heard the word, and these codes provided overlapping information.\n\n\n\n\n\n\n\nImportant\n\n\n\nGet the data: get the data file and the .R script you can use to do the exercises that will support your learning.\n\nYou can download the files folder for this chapter by clicking on the link 04-glmm.zip.\n\n\n\nThe practical materials folder includes data files and an .R script:\nIn this chapter, we will be working with the data about the orthographic post-test outcome for the Ricketts word learning study:\n\nlong.orth_2020-08-11.csv\n\nThe data file is collected together with the .R script:\n\n04-glmm-workbook.R the workbook you will need to do the practical exercises.\n\nThe data come from the Ricketts et al. (2021) study, and you can access the analysis code and data for that study, in full, at the OSF repository here\n\n\n\n\n\n\nImportant\n\n\n\nYou can access the sign-in page for R-Studio Server here\n\n\n\n\nThe aims of the practical work are to:\n\nUnderstand the reasons for using Generalized Linear Mixed-effects models (GLMMs) when we analyze discrete outcome variables.\nRecognize the limitations of alternative methods for analyzing such outcomes.\nPractice running GLMMs with varying random effects structures.\nPractice reporting the results of GLMMs, including through the use of model plots.\n\nMy recommendations for learning are that you should aim to:\n\nrun GLMMs of demonstration data;\nrun GLMMs of alternate data sets;\nplay with the .R code used to create examples for the lecture;\nand edit example code to create alternate visualizations.\n\n\n\n\nNow you will progress through a series of tasks, and challenges, to aid your learning.\n\n\n\n\n\n\nWarning\n\n\n\nWe will work with the data file:\n\nlong.orth_2020-08-11.csv\n\n\n\nWe again split the steps into into parts, tasks and questions.\nWe are going to work through the following workflow steps: each step is labelled as a practical part.\n\nSet-up\nLoad the data\nTidy data\nAnalyze the data: random intercepts\nAnalyze the data: model comparisons\n\nIn the following, we will guide you through the tasks and questions step by step.\n\n\n\n\n\n\nImportant\n\n\n\nAn answers version of the workbook will be provided after the practical class.\n\n\n\n\n\nTo begin, we set up our environment in R.\n\n\nUse the library() function to make the functions we need available to you.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlibrary(broom)\nlibrary(effects)\n\nLoading required package: carData\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\nlibrary(gridExtra)\nlibrary(here)\n\nhere() starts at /Users/padraic/MSc_411_412_2025-26\n\nlibrary(lattice)\nlibrary(knitr)\nlibrary(lme4)\n\nLoading required package: Matrix\n\nlibrary(MuMIn)\n\nRegistered S3 methods overwritten by 'MuMIn':\n  method        from \n  nobs.multinom broom\n  nobs.fitdistr broom\n\nlibrary(sjPlot)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::combine()     masks gridExtra::combine()\n✖ tidyr::expand()      masks Matrix::expand()\n✖ dplyr::filter()      masks stats::filter()\n✖ dplyr::lag()         masks stats::lag()\n✖ tidyr::pack()        masks Matrix::pack()\n✖ ggplot2::set_theme() masks sjPlot::set_theme()\n✖ tidyr::unpack()      masks Matrix::unpack()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\n\n\n\n\nRead the data file into R:\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlong.orth &lt;- read_csv(\"long.orth_2020-08-11.csv\", \n                      col_types = cols(\n                        Participant = col_factor(),\n                        Time = col_factor(),\n                        Study = col_factor(),\n                        Instructions = col_factor(),\n                        Version = col_factor(),\n                        Word = col_factor(),\n                        Orthography = col_factor(),\n                        Measure = col_factor(),\n                        Spelling.transcription = col_factor()\n                      )\n                    )\n\n\n\n\nYou can see, here, that within the read_csv() function call, I specify col_types, instructing R how to treat a number of different variables.\n\nYou can read more about this convenient way to control the read-in process here.\n\n\n\n\n\nThe data are already tidy: each column in long.orth_2020-08-11.csv corresponds to a variable and each row corresponds to an observation. However, we need to do a bit of work, before we can run any analyses, to fix the coding of the categorical predictor (or independent) variables: the factors Orthography, Instructions, and Time.\n\n\nIt is always a good to inspect what you have got when you read a data file in to R.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nsummary(long.orth)\n\n\n\n\nYou will see that some of the variables included in the .csv file are listed, following, with information about value coding or calculation.\n\nParticipant – Participant identity codes were used to anonymize participation.\nTime – Test time was coded 1 (time 1) or 2 (time 2). For the Study 1 longitudinal data, it can be seen that each participant identity code is associated with observations taken at test times 1 and 2.\nInstructions – Variable coding for whether participants undertook training in the explicit} or incidental} conditions.\nWord – Letter string values showing the words presented as stimuli to the children.\nOrthography – Variable coding for whether participants had seen a word in training in the orthography absent or present conditions.\nConsistency-H – Calculated orthography-to-phonology consistency value for each word. -zConsistency-H – Standardized Consistency H scores\nScore – Outcome variable – for the orthographic post-test, responses were scored as 1 (correct, if the target spelling was produced in full) or 0 (incorrect, if the target spelling was not produced).\n\nThe summary will show you that we have a number of other variables available, including measures of individual differences in reading or reading-related abilities or knowledge, but we do not need to pay attention to them, for our exercises.\n\nPract.Q.1. Look at the summaries for the variables Time, Instructions and Orthography. Assuming that the read_csv() action did what was required, you will see how R presents factor summaries by default. What do the variable summaries tell you about the factor level coding, and the number of observations in each level?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nFor a factor like Orthography, the column values code for whether the observations in a data row are associated with the condition absent or the condition present.\nIt is simpler to talk about absent or present as being levels of Orthography condition.\n\nYou can ask R what the levels of a factor are using: levels(long.orth$Orthography)\n\n\n\n\n\nPract.A.1. The summary shows:\n\nTime (1, 655; 2, 608); Instructions (explicit, 592; incidental, 671); and Orthography (absent, 631; present, 632).\n\nPract.Q.2. Are there any surprises in the summary of factors?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe would hope to see equal numbers of observations at different levels for a factor.\n\n\n\n\nPract.A.2. It should be surprising that we do not have equal numbers of observations.\n\n\n\n\nFit a simple Generalized Linear Model with outcome (accuracy of response) Score, and Instructions as the predictor.\n\nNote that you are ignoring random effects, for this task.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nsummary(glm(Score ~ Instructions, family = \"binomial\", data = long.orth))\n\n\n\n\n\nPract.Q.3. What is the estimated effect of Instructions on Score?\n\n\nPract.A.3. The estimate is: Instructionsincidental -0.05932\n\n\nPract.Q.4. Can you briefly explain in words what the estimate says about how log odds Score (response correct vs. incorrect) changes in association with different Instructions conditions?\n\n\nPract.A.4. We can say, simply, that the log odds that a response would be correct were lower in the incidental compared to the explicit Instructions condition.\n\nThis task and these questions are designed to alert you to the challenges involved in estimating the effect of categorical variables, and of interpreting the effects estimates.\n\n\n\n\n\n\nTip\n\n\n\nThis model (and default coding) gives us an estimate of how the log odds of a child getting a response correct changes if we compare the responses in the explicit condition (here, treated as the baseline or reference level) with responses in the incidental condition.\nR tells us about the estimate by adding the name of the factor level that is not the reference level, here, incidental to the name of the variable Instructions whose effect is being estimated.\n\n\n\n\n\nHowever, it is better not to use R’s default dummy coding scheme if we are analyzing data, where the data come from a study involving two or more factors, and we want to estimate not just the main effects of the factors but also the effect of the interaction between the factors.\nIn our analyses, we want the coding that allows us to get estimates of the main effects of factors, and of the interaction effects, somewhat like what we would get from an ANOVA. This requires us to use effect coding.\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe can code whether a response was recorded in the absent or present condition using numbers. In dummy coding, for any observation, we would use a column of zeroes or ones to code condition: i.e., absent (0) or present (1). In effect coding, for any observation, we would use a column of ones or minus ones to code condition: i.e., absent (-1) or present (1). (With a factor with more than two levels, we would use more than one column to do the coding: the number of columns we would use would equal the number of factor condition levels minus one.) In effect coding, observations coded -1 are in the reference level.\nWith effect coding, the constant (i.e., the intercept for our model) is equal to the grand mean of all the observed responses. And the coefficient of each of the effect variables is equal to the difference between the mean of the group coded 1 and the grand mean.\n\nYou can read more about effect coding here or here.\n\n\n\n\nWe follow recommendations to use sum contrast coding for the experimental factors. Further, to make interpretation easier, we want the coding to work so that for both Orthography and Instructions conditions, doing something is the “high” level in the factor – hence:\n\nOrthography, absent (-1) vs. present (+1)\nInstructions, incidental (-1) vs. explicit (+1)\nTime, test time 1 (-1) vs. time 2 (+1)\n\nWe use a modified version of the contr.sum() function (provided in the {memisc} library) that allows us to define the base or reference level for the factor manually (see documentation).\nWe need to start by loading the {memisc} library.\n\nlibrary(memisc)\n\nIn some circumstances, this can create warnings.\n\nYou can see information about the potential warnings here.\n\n\nPract.Q.5. Can you work out how to use the contr.sum() function to change the coding of the categorical variables (factors) in the example data-set from dummy to effects coding?\n\n\nIt would be sensible to examine how R sees the coding of the different levels for each factor, before and then after you use contr.sum() to use that coding.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nIn the following sequence, I first check how R codes the levels of each factor by default, I then change the coding, and check that the change gets me what I want.\nWe want effects coding for the orthography condition factor, with orthography condition coded as -1, +1. Check the coding.\n\ncontrasts(long.orth$Orthography)\n\n        present\nabsent        0\npresent       1\n\n\nYou can see that Orthography condition is initially coded, by default, using dummy coding: absent (0); present (1). We want to change the coding, then check that we have got what we want.\n\ncontrasts(long.orth$Orthography) &lt;- contr.sum(2, base = 1)\ncontrasts(long.orth$Orthography)\n\n         2\nabsent  -1\npresent  1\n\n\nWe want effects coding for the Instructions condition factor, with Instructions condition coded as -1, +1. Check the coding.\n\ncontrasts(long.orth$Instructions)\n\n           incidental\nexplicit            0\nincidental          1\n\n\nChange it.\n\ncontrasts(long.orth$Instructions) &lt;- contr.sum(2, base = 2)\ncontrasts(long.orth$Instructions)\n\n            1\nexplicit    1\nincidental -1\n\n\nWe want effects coding for the Time factor, with Time coded as -1, +1 Check the coding.\n\ncontrasts(long.orth$Time)\n\n  2\n1 0\n2 1\n\n\nChange it.\n\ncontrasts(long.orth$Time) &lt;- contr.sum(2, base = 1)\ncontrasts(long.orth$Time)\n\n   2\n1 -1\n2  1\n\n\nIn these chunks of code, I use contr.sum(a, base = b) to do the coding, where a is the number of levels in a factor (replace a with the right number), and b tells R which level to use as the baseline or reference level (replace b with the right number). I usually need to check the coding before and after I specify it.\n\n\n\n\nPract.Q.6. Experiment: what happens if you change the first number for one of the factors?\n\nRun the following code example, and reflect on what you see:\n\ncontrasts(long.orth$Time) &lt;- contr.sum(3, base = 1)\ncontrasts(long.orth$Time)\n\n\nPract.A.6. Changing the first number from 2 will result in warnings, showing that the number must match the number of factor levels for the factor.\n\n\nPract.Q.7. Experiment: what happens if you change the base number for one of the factors?\n\nRun the following code example, and reflect on what you see:\n\ncontrasts(long.orth$Time) &lt;- contr.sum(2, base = 1)\ncontrasts(long.orth$Time)\n\n\nPract.A.7. Changing the base number changes which level gets coded as -1 vs. 1.\n\n\n\n\n\n\n\nSpecify a model with:\n\nThe correct function\nOutcome = Score\nPredictors including:\n\n\nTime + Orthography + Instructions + zConsistency_H +\nOrthography:Instructions +\nOrthography:zConsistency_H +\n\n\nPlus random effects:\n\n\nRandom effect of participants (Participant) on intercepts\nRandom effect of items (Word) on intercepts\n\n\nUsing the \"bobyqa\" optimizer\n\nRun the code to fit the model and get a summary of results.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlong.orth.min.glmer &lt;- glmer(Score ~ \n                               \n                          Time + Orthography + Instructions + zConsistency_H + \n                               \n                          Orthography:Instructions +\n                               \n                          Orthography:zConsistency_H +\n                               \n                          (1 | Participant) + \n                               \n                          (1 |Word),\n                             \n                          family = \"binomial\", \n                          glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)),\n                             \n                          data = long.orth)\n\nsummary(long.orth.min.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (1 |  \n    Participant) + (1 | Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1040.4    1086.7    -511.2    1022.4      1254 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.0994 -0.4083 -0.2018  0.2019  7.4940 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n Participant (Intercept) 1.840    1.357   \n Word        (Intercept) 2.224    1.491   \nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.878464   0.443942  -4.231 2.32e-05 ***\nTime2                        0.050136   0.083325   0.602    0.547    \nOrthography2                 0.455009   0.086813   5.241 1.59e-07 ***\nInstructions1                0.042290   0.230335   0.184    0.854    \nzConsistency_H              -0.618092   0.384002  -1.610    0.107    \nOrthography2:Instructions1   0.005786   0.083187   0.070    0.945    \nOrthography2:zConsistency_H  0.014611   0.083105   0.176    0.860    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.008                                   \nOrthogrphy2 -0.059  0.008                            \nInstructns1  0.014  0.025  0.001                     \nzCnsstncy_H  0.016 -0.002 -0.029 -0.001              \nOrthgrp2:I1 -0.002 -0.001  0.049 -0.045  0.000       \nOrthgr2:C_H -0.027  0.001  0.179  0.000 -0.035 -0.007\n\n\n\n\n\n\nPract.Q.8. Experiment: replace the fixed effects with another variable e.g. mean_z_read (an aggregate measure of reading skill) to get a feel for how the code works.\n\n\nWhat is the estimate for the mean_z_read effect?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlong.orth.min.glmer.expt &lt;- glmer(Score ~ mean_z_read +\n\n                               (1 | Participant) +\n\n                               (1 |Word),\n\n                             family = \"binomial\",\n                             glmerControl(optimizer=\"bobyqa\", \n                                          optCtrl=list(maxfun=2e5)),\n\n                             data = long.orth)\n\nsummary(long.orth.min.glmer.expt)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ mean_z_read + (1 | Participant) + (1 | Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1000.4    1021.0    -496.2     992.4      1259 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-6.1454 -0.4180 -0.2111  0.2543  7.3457 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n Participant (Intercept) 0.1363   0.3692  \n Word        (Intercept) 2.3481   1.5324  \nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.7967     0.4027  -4.461 8.14e-06 ***\nmean_z_read   1.5090     0.1522   9.913  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nmean_z_read -0.121\n\n\n\n\n\n\nPract.A.8. The estimate is: mean_z_read   1.5090\n\n\nPract.Q.9. Can you briefly indicate in words what the estimate says about how log odds Score (response correct vs. incorrect) changes in association with mean_z_read score?\n\n\nPract.A.9. We can say, simply, that log odds that a response would be correct were higher for increasing values of mean_z_read.\n\n\n\n\nFit the first model you were asked to fit.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlong.orth.min.glmer &lt;- glmer(Score ~ \n                               \n                          Time + Orthography + Instructions + zConsistency_H + \n                               \n                          Orthography:Instructions +\n                               \n                          Orthography:zConsistency_H +\n                               \n                          (1 | Participant) + \n                               \n                          (1 |Word),\n                             \n                          family = \"binomial\", \n                          glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)),\n                             \n                          data = long.orth)\n\nsummary(long.orth.min.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (1 |  \n    Participant) + (1 | Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1040.4    1086.7    -511.2    1022.4      1254 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.0994 -0.4083 -0.2018  0.2019  7.4940 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n Participant (Intercept) 1.840    1.357   \n Word        (Intercept) 2.224    1.491   \nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.878464   0.443942  -4.231 2.32e-05 ***\nTime2                        0.050136   0.083325   0.602    0.547    \nOrthography2                 0.455009   0.086813   5.241 1.59e-07 ***\nInstructions1                0.042290   0.230335   0.184    0.854    \nzConsistency_H              -0.618092   0.384002  -1.610    0.107    \nOrthography2:Instructions1   0.005786   0.083187   0.070    0.945    \nOrthography2:zConsistency_H  0.014611   0.083105   0.176    0.860    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.008                                   \nOrthogrphy2 -0.059  0.008                            \nInstructns1  0.014  0.025  0.001                     \nzCnsstncy_H  0.016 -0.002 -0.029 -0.001              \nOrthgrp2:I1 -0.002 -0.001  0.049 -0.045  0.000       \nOrthgr2:C_H -0.027  0.001  0.179  0.000 -0.035 -0.007\n\n\n\n\n\nThen produce plots to show the predicted change in outcome, given the model effects estimates.\n\nCan you work out how to do this?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThere are different ways to complete this task. Two convenient methods involve:\n\nusing the {sjPlot} plot_model() function;\nusing the {effects} library effect() function\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nFirst using the {sjPlot} plot_model() function:\n\nporth &lt;- plot_model(long.orth.min.glmer,\n           type=\"pred\",\n           terms = \"Orthography\") +\n         theme_bw() +\n         ggtitle(\"Predicted probability\") +\n         ylim(0,1)\n\nYou are calculating adjusted predictions on the population-level (i.e.\n  `type = \"fixed\"`) for a *generalized* linear mixed model.\n  This may produce biased estimates due to Jensen's inequality. Consider\n  setting `bias_correction = TRUE` to correct for this bias.\n  See also the documentation of the `bias_correction` argument.\n\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n\npzconsH &lt;- plot_model(long.orth.min.glmer,\n           type=\"pred\",\n           terms = \"zConsistency_H\") +\n         theme_bw() +\n         ggtitle(\"Predicted probability\") +\n         ylim(0,1)\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n\ngrid.arrange(porth, pzconsH,\n            ncol=2)\n\n\n\n\n\n\n\n\nSecond using the {effects} library effect() function”\n\nporth &lt;- plot(effect(\"Orthography\", mod = long.orth.min.glmer))\n\nNOTE: Orthography is not a high-order term in the model\n\npzconsH &lt;- plot(effect(\"zConsistency_H\", mod = long.orth.min.glmer))\n\nNOTE: zConsistency_H is not a high-order term in the model\n\ngrid.arrange(porth, pzconsH,\n            ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nNotice that in the preceding code chunks, I assign the plot objects to names and then use `grid.arrange() to present the named plots in grids.\n\nTo produce and show a plot, don’t do that, just adapt and use the plot functions, as shown in the next code example.\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nplot_model(long.orth.min.glmer,\n           type=\"pred\",\n           terms = \"Instructions\") +\n  theme_bw() +\n  ggtitle(\"Predicted probability\") +\n  ylim(0,1)\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nTake a look at the documentation:\n\nhttps://strengejacke.github.io/sjPlot/articles/plot_interactions.html\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nWe can get a plot showing the predicted impact of the Instructions x Orthography interaction effect as follows.\n\nplot_model(long.orth.min.glmer,\n           type=\"pred\",\n           terms = c(\"Instructions\", \"Orthography\")) +\n  theme_bw() +\n  ggtitle(\"Predicted probability\") +\n  ylim(0,1)\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\nIgnoring unknown labels:\n• linetype : \"Orthography\"\n• shape : \"Orthography\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the models all have the same fixed effects.\nNote also that while we will be comparing models varying in random effects we are not going to use REML=TRUE\n\nSee here for why:\n\nhttps://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#reml-for-glmms\nFirst fit the minimum random intercepts model.\nSpecify a model with:\n\nThe correct function\nOutcome = Score\nPredictors =\n\n\nTime + Orthography + Instructions + zConsistency_H +\nOrthography:Instructions +\nOrthography:zConsistency_H +\n\n\nPlus random effects:\n\n\nRandom effect of participants (Participant) on intercepts\nRandom effect of items (Word) on intercepts\n\n\nUsing the \"bobyqa\" optimizer\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nRun the code to fit the model and get a summary of results, as shown in the chapter\n\nlong.orth.min.glmer &lt;- glmer(Score ~ \n                               \n                            Time + Orthography + Instructions + zConsistency_H + \n                               \n                            Orthography:Instructions +\n                               \n                            Orthography:zConsistency_H +\n                               \n                               (1 | Participant) + \n                               \n                               (1 |Word),\n                             \n                            family = \"binomial\", \n                            glmerControl(optimizer=\"bobyqa\", \n                                         optCtrl=list(maxfun=2e5)),\n                             \n                             data = long.orth)\n\nsummary(long.orth.min.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (1 |  \n    Participant) + (1 | Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1040.4    1086.7    -511.2    1022.4      1254 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.0994 -0.4083 -0.2018  0.2019  7.4940 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n Participant (Intercept) 1.840    1.357   \n Word        (Intercept) 2.224    1.491   \nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.878464   0.443942  -4.231 2.32e-05 ***\nTime2                        0.050136   0.083325   0.602    0.547    \nOrthography2                 0.455009   0.086813   5.241 1.59e-07 ***\nInstructions1                0.042290   0.230335   0.184    0.854    \nzConsistency_H              -0.618092   0.384002  -1.610    0.107    \nOrthography2:Instructions1   0.005786   0.083187   0.070    0.945    \nOrthography2:zConsistency_H  0.014611   0.083105   0.176    0.860    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.008                                   \nOrthogrphy2 -0.059  0.008                            \nInstructns1  0.014  0.025  0.001                     \nzCnsstncy_H  0.016 -0.002 -0.029 -0.001              \nOrthgrp2:I1 -0.002 -0.001  0.049 -0.045  0.000       \nOrthgr2:C_H -0.027  0.001  0.179  0.000 -0.035 -0.007\n\n\n\n\n\nSecond the maximum model: this will take several seconds to run.\nSpecify a model with:\n\nThe correct function\nOutcome = Score\nPredictors =\n\n\nTime + Orthography + Instructions + zConsistency_H +\nOrthography:Instructions +\nOrthography:zConsistency_H +\n\n\nPlus random effects:\n\n\nRandom effect of participants (Participant) on intercepts\nRandom effect of items (Word) on intercepts\n\n\nPlus:\n\n\nRandom effects of participants on the slopes of within-participant effects\nRandom effects of items on the slopes of within-item effects\n\n\nPlus:\n\n\nAllow for covariance between random intercepts and random slopes\n\n\nUsing the \"bobyqa\" optimizer\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nRun the code to fit the model and get a summary of results, as shown in the chapter\n\nlong.orth.max.glmer &lt;- glmer(Score ~ \n                           \n                      Time + Orthography + Instructions + zConsistency_H + \n                           \n                      Orthography:Instructions +\n                           \n                      Orthography:zConsistency_H +\n                           \n                      (Time + Orthography + zConsistency_H + 1 | Participant) + \n                           \n                      (Time + Orthography + Instructions + 1 |Word),\n                         \n                      family = \"binomial\",\n                      glmerControl(optimizer=\"bobyqa\", \n                                   optCtrl=list(maxfun=2e5)),\n                         \n                         data = long.orth)\n\nboundary (singular) fit: see help('isSingular')\n\nsummary(long.orth.max.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (Time +  \n    Orthography + zConsistency_H + 1 | Participant) + (Time +  \n    Orthography + Instructions + 1 | Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1053.6    1192.4    -499.8     999.6      1236 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.1014 -0.4027 -0.1723  0.2037  7.0331 \n\nRandom effects:\n Groups      Name           Variance Std.Dev. Corr             \n Participant (Intercept)    2.043127 1.42938                   \n             Time2          0.005675 0.07533   0.62            \n             Orthography2   0.079980 0.28281   0.78 -0.01      \n             zConsistency_H 0.065576 0.25608   0.49  0.99 -0.16\n Word        (Intercept)    2.793448 1.67136                   \n             Time2          0.046736 0.21618   0.14            \n             Orthography2   0.093740 0.30617  -0.68 -0.81      \n             Instructions1  0.212706 0.46120  -0.74 -0.05  0.38\nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -2.10099    0.49589  -4.237 2.27e-05 ***\nTime2                        0.02077    0.12285   0.169 0.865740    \nOrthography2                 0.52480    0.15496   3.387 0.000708 ***\nInstructions1                0.24467    0.27281   0.897 0.369805    \nzConsistency_H              -0.67818    0.36311  -1.868 0.061803 .  \nOrthography2:Instructions1  -0.05133    0.10004  -0.513 0.607907    \nOrthography2:zConsistency_H  0.05850    0.11634   0.503 0.615064    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.097                                   \nOrthogrphy2 -0.280 -0.187                            \nInstructns1 -0.278  0.023  0.109                     \nzCnsstncy_H  0.062  0.005 -0.064  0.120              \nOrthgrp2:I1  0.024  0.005 -0.071  0.212 -0.065       \nOrthgr2:C_H -0.062  0.049  0.246 -0.031 -0.440  0.001\noptimizer (bobyqa) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\n\n\n\nThen fit models, building on the first minimal (random intercepts) model: adding one extra random effect at a time.\nAdd code to include a random effect of participants on the slopes of the effect of Orthography.\n\nAnd add code to include a random effect of items on the slopes of the effect of Orthography.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nRun the code to fit the model and get a summary of results, as shown in the chapter\n\nlong.orth.2.glmer &lt;- glmer(Score ~ \n                             Time + Orthography + Instructions + zConsistency_H + \n                             \n                             Orthography:Instructions +\n                             \n                             Orthography:zConsistency_H +\n                             \n                             (dummy(Orthography) + 1 || Participant) + \n                             \n                             (dummy(Orthography) + 1 || Word),\n                           \n                           family = \"binomial\", \n                           glmerControl(optimizer=\"bobyqa\", \n                                        optCtrl=list(maxfun=2e5)),\n                           \n                           data = long.orth)\n\nsummary(long.orth.2.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) +  \n    1 || Participant) + (dummy(Orthography) + 1 || Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1041.0    1097.6    -509.5    1019.0      1252 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9373 -0.4132 -0.1952  0.1826  6.9614 \n\nRandom effects:\n Groups        Name               Variance Std.Dev.\n Participant   (Intercept)        1.57092  1.2534  \n Participant.1 dummy(Orthography) 0.57624  0.7591  \n Word          (Intercept)        2.36284  1.5372  \n Word.1        dummy(Orthography) 0.02101  0.1450  \nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.9025027  0.4512443  -4.216 2.49e-05 ***\nTime2                        0.0501974  0.0837186   0.600 0.548775    \nOrthography2                 0.4135727  0.1120792   3.690 0.000224 ***\nInstructions1                0.0455920  0.2234615   0.204 0.838333    \nzConsistency_H              -0.6254414  0.3958971  -1.580 0.114151    \nOrthography2:Instructions1   0.0019343  0.1038694   0.019 0.985142    \nOrthography2:zConsistency_H -0.0007112  0.0877740  -0.008 0.993535    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.007                                   \nOrthogrphy2  0.034  0.007                            \nInstructns1  0.013  0.025  0.007                     \nzCnsstncy_H  0.017 -0.002 -0.024 -0.002              \nOrthgrp2:I1  0.003  0.001  0.043  0.126  0.000       \nOrthgr2:C_H -0.029  0.001  0.182  0.001 -0.024 -0.011\n\n\n\n\n\nAdd code to include a random effect of items on the slopes of the effect of Instructions.\n\n\n\n\n\n\nCode\n\n\n\n\n\nRun the code to fit the model and get a summary of results, as shown in the chapter\n\nlong.orth.3.glmer &lt;- glmer(Score ~ \n                             Time + Orthography + Instructions + zConsistency_H + \n                             \n                             Orthography:Instructions +\n                             \n                             Orthography:zConsistency_H +\n                             \n                          (dummy(Orthography) + 1 || Participant) + \n                             \n                          (dummy(Orthography) + dummy(Instructions) + 1 || Word),\n                           \n                           family = \"binomial\", \n                           glmerControl(optimizer=\"bobyqa\", \n                                        optCtrl=list(maxfun=2e5)),\n                           \n                           data = long.orth)\n\nsummary(long.orth.3.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) +  \n    1 || Participant) + (dummy(Orthography) + dummy(Instructions) +  \n    1 || Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1036.5    1098.2    -506.2    1012.5      1251 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9951 -0.4068 -0.1920  0.1838  5.9308 \n\nRandom effects:\n Groups        Name                Variance Std.Dev.\n Participant   (Intercept)         1.64393  1.2822  \n Participant.1 dummy(Orthography)  0.55604  0.7457  \n Word          (Intercept)         1.94313  1.3940  \n Word.1        dummy(Orthography)  0.01607  0.1268  \n Word.2        dummy(Instructions) 0.86694  0.9311  \nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.9621261  0.4400649  -4.459 8.25e-06 ***\nTime2                        0.0507347  0.0843237   0.602  0.54740    \nOrthography2                 0.4263703  0.1114244   3.827  0.00013 ***\nInstructions1                0.1907423  0.2632605   0.725  0.46874    \nzConsistency_H              -0.6270900  0.3669904  -1.709  0.08750 .  \nOrthography2:Instructions1  -0.0265729  0.1048392  -0.253  0.79991    \nOrthography2:zConsistency_H -0.0006305  0.0878823  -0.007  0.99428    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.007                                   \nOrthogrphy2  0.028  0.008                            \nInstructns1 -0.127  0.022  0.019                     \nzCnsstncy_H  0.017 -0.002 -0.026  0.011              \nOrthgrp2:I1  0.013  0.001  0.003  0.068 -0.009       \nOrthgr2:C_H -0.030  0.002  0.182  0.002 -0.026 -0.018\n\n\n\n\n\nAdd code to include a random effect of participants on the slopes of the effect of consistency (zConsistency_H).\n\n\n\n\n\n\nCode\n\n\n\n\n\nRun the code to fit the model and get a summary of results, as shown in the chapter\n\nlong.orth.4.a.glmer &lt;- glmer(Score ~ \n                             Time + Orthography + Instructions + zConsistency_H + \n                             \n                             Orthography:Instructions +\n                             \n                             Orthography:zConsistency_H +\n                             \n                      (dummy(Orthography) + zConsistency_H + 1 || Participant) + \n                             \n                      (dummy(Orthography) + dummy(Instructions) + 1 || Word),\n                           \n                           family = \"binomial\", \n                           glmerControl(optimizer=\"bobyqa\", \n                                        optCtrl=list(maxfun=2e5)),\n                           \n                           data = long.orth)\n\nboundary (singular) fit: see help('isSingular')\n\nsummary(long.orth.4.a.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) +  \n    zConsistency_H + 1 || Participant) + (dummy(Orthography) +  \n    dummy(Instructions) + 1 || Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1038.5    1105.3    -506.2    1012.5      1250 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9952 -0.4068 -0.1920  0.1838  5.9309 \n\nRandom effects:\n Groups        Name                Variance  Std.Dev. \n Participant   (Intercept)         1.644e+00 1.282e+00\n Participant.1 dummy(Orthography)  5.560e-01 7.456e-01\n Participant.2 zConsistency_H      2.090e-10 1.446e-05\n Word          (Intercept)         1.943e+00 1.394e+00\n Word.1        dummy(Orthography)  1.604e-02 1.266e-01\n Word.2        dummy(Instructions) 8.669e-01 9.311e-01\nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.9621306  0.4400477  -4.459 8.24e-06 ***\nTime2                        0.0507346  0.0843236   0.602  0.54740    \nOrthography2                 0.4263730  0.1114188   3.827  0.00013 ***\nInstructions1                0.1907330  0.2632583   0.725  0.46875    \nzConsistency_H              -0.6270898  0.3669803  -1.709  0.08749 .  \nOrthography2:Instructions1  -0.0265706  0.1048370  -0.253  0.79992    \nOrthography2:zConsistency_H -0.0006352  0.0878782  -0.007  0.99423    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.007                                   \nOrthogrphy2  0.028  0.008                            \nInstructns1 -0.127  0.022  0.019                     \nzCnsstncy_H  0.017 -0.002 -0.026  0.011              \nOrthgrp2:I1  0.013  0.001  0.003  0.068 -0.009       \nOrthgr2:C_H -0.030  0.002  0.182  0.002 -0.026 -0.018\noptimizer (bobyqa) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\n\n\n\nAdd code to include a random effect of participants on the slopes of the effect of Time.\n\nAnd add code to include a random effect of items on the slopes of the effect of Time.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nRun the code to fit the model and get a summary of results, as shown in the chapter\n\nlong.orth.4.b.glmer &lt;- glmer(Score ~ \n                             Time + Orthography + Instructions + zConsistency_H + \n                             \n                             Orthography:Instructions +\n                             \n                             Orthography:zConsistency_H +\n                             \n          (dummy(Orthography) + dummy(Time) + 1 || Participant) + \n                             \n          (dummy(Orthography) + dummy(Instructions) + dummy(Time) + 1 || Word),\n                           \n                           family = \"binomial\", \n                           glmerControl(optimizer=\"bobyqa\", \n                                        optCtrl=list(maxfun=2e5)),\n                           \n                           data = long.orth)\n\nboundary (singular) fit: see help('isSingular')\n\nsummary(long.orth.4.b.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) +  \n    dummy(Time) + 1 || Participant) + (dummy(Orthography) + dummy(Instructions) +  \n    dummy(Time) + 1 || Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1040.5    1112.5    -506.2    1012.5      1249 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9952 -0.4068 -0.1920  0.1838  5.9309 \n\nRandom effects:\n Groups        Name                Variance  Std.Dev. \n Participant   (Intercept)         1.644e+00 1.2821917\n Participant.1 dummy(Orthography)  5.560e-01 0.7456314\n Participant.2 dummy(Time)         0.000e+00 0.0000000\n Word          (Intercept)         1.943e+00 1.3939636\n Word.1        dummy(Orthography)  1.604e-02 0.1266475\n Word.2        dummy(Instructions) 8.669e-01 0.9310619\n Word.3        dummy(Time)         1.992e-08 0.0001411\nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.9621304  0.4400607  -4.459 8.24e-06 ***\nTime2                        0.0507347  0.0843238   0.602  0.54740    \nOrthography2                 0.4263730  0.1114191   3.827  0.00013 ***\nInstructions1                0.1907329  0.2632608   0.725  0.46876    \nzConsistency_H              -0.6270910  0.3669875  -1.709  0.08750 .  \nOrthography2:Instructions1  -0.0265706  0.1048370  -0.253  0.79992    \nOrthography2:zConsistency_H -0.0006352  0.0878785  -0.007  0.99423    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.007                                   \nOrthogrphy2  0.028  0.008                            \nInstructns1 -0.127  0.022  0.019                     \nzCnsstncy_H  0.017 -0.002 -0.026  0.011              \nOrthgrp2:I1  0.013  0.001  0.003  0.068 -0.009       \nOrthgr2:C_H -0.030  0.002  0.182  0.002 -0.026 -0.018\noptimizer (bobyqa) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\n\n\n\n\nPract.Q.10. Which models converge and which models do not converge?\n\n\nPract.A.10. Models 4.a. (consistency) and 4.b. (time) should not converge.\n\n\nPract.Q.11. How can you tell?\n\n\nPract.A.11. Convergence warnings are presented for models that do not converge:\n\noptimizer (bobyqa) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nanova(long.orth.min.glmer, long.orth.2.glmer)\n\nData: long.orth\nModels:\nlong.orth.min.glmer: Score ~ Time + Orthography + Instructions + zConsistency_H + Orthography:Instructions + Orthography:zConsistency_H + (1 | Participant) + (1 | Word)\nlong.orth.2.glmer: Score ~ Time + Orthography + Instructions + zConsistency_H + Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) + 1 || Participant) + (dummy(Orthography) + 1 || Word)\n                    npar    AIC    BIC  logLik -2*log(L)  Chisq Df Pr(&gt;Chisq)\nlong.orth.min.glmer    9 1040.4 1086.7 -511.20    1022.4                     \nlong.orth.2.glmer     11 1041.0 1097.6 -509.51    1019.0 3.3909  2     0.1835\n\nanova(long.orth.min.glmer, long.orth.3.glmer)\n\nData: long.orth\nModels:\nlong.orth.min.glmer: Score ~ Time + Orthography + Instructions + zConsistency_H + Orthography:Instructions + Orthography:zConsistency_H + (1 | Participant) + (1 | Word)\nlong.orth.3.glmer: Score ~ Time + Orthography + Instructions + zConsistency_H + Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) + 1 || Participant) + (dummy(Orthography) + dummy(Instructions) + 1 || Word)\n                    npar    AIC    BIC  logLik -2*log(L)  Chisq Df Pr(&gt;Chisq)  \nlong.orth.min.glmer    9 1040.4 1086.7 -511.20    1022.4                       \nlong.orth.3.glmer     12 1036.5 1098.2 -506.24    1012.5 9.9115  3    0.01933 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\n\nPract.Q.12. What does using bobyqa do? Delete glmerControl() and report what impact does this have?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nRun the code to fit the model and get a summary of results\n\nlong.orth.3.glmer.check &lt;- glmer(Score ~\n                            \n                            Time + Orthography + Instructions + zConsistency_H +\n\n                            Orthography:Instructions +\n\n                            Orthography:zConsistency_H +\n\n                            (dummy(Orthography) + 1 || Participant) +\n\n                            (dummy(Orthography) + dummy(Instructions) + 1 || Word),\n\n                           family = \"binomial\",\n\n                           data = long.orth)\n\nsummary(long.orth.3.glmer.check)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) +  \n    1 || Participant) + (dummy(Orthography) + dummy(Instructions) +  \n    1 || Word)\n   Data: long.orth\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1036.5    1098.2    -506.2    1012.5      1251 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9949 -0.4068 -0.1920  0.1837  5.9304 \n\nRandom effects:\n Groups        Name                Variance Std.Dev.\n Participant   (Intercept)         1.64374  1.2821  \n Participant.1 dummy(Orthography)  0.55646  0.7460  \n Word          (Intercept)         1.94423  1.3944  \n Word.1        dummy(Orthography)  0.01615  0.1271  \n Word.2        dummy(Instructions) 0.86769  0.9315  \nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.9617535  0.4401423  -4.457 8.31e-06 ***\nTime2                        0.0507351  0.0843245   0.602  0.54740    \nOrthography2                 0.4263842  0.1114435   3.826  0.00013 ***\nInstructions1                0.1906959  0.2632724   0.724  0.46886    \nzConsistency_H              -0.6275205  0.3670921  -1.709  0.08737 .  \nOrthography2:Instructions1  -0.0265347  0.1048529  -0.253  0.80022    \nOrthography2:zConsistency_H -0.0006278  0.0878892  -0.007  0.99430    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.007                                   \nOrthogrphy2  0.028  0.008                            \nInstructns1 -0.127  0.022  0.019                     \nzCnsstncy_H  0.017 -0.002 -0.026  0.011              \nOrthgrp2:I1  0.013  0.001  0.003  0.068 -0.009       \nOrthgr2:C_H -0.030  0.002  0.182  0.002 -0.026 -0.018\n\n\n\n\n\n\nPract.A.12. Deleting the bobyqa requirement seems to have no impact on models that converge.\n\n\nPract.Q.13. What does using dummy() in the random effects coding do?\n\n\nExperiment: check out models 2 or 3, removing dummy() from the random effects coding to see what happens.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nRun the code to fit the model and get a summary of results\n\nlong.orth.3.glmer.check &lt;- glmer(Score ~\n                             Time + Orthography + Instructions + zConsistency_H +\n\n                             Orthography:Instructions +\n\n                             Orthography:zConsistency_H +\n\n                             (Orthography + 1 || Participant) +\n\n                             (Orthography + Instructions + 1 || Word),\n\n                           family = \"binomial\",\n                           glmerControl(optimizer=\"bobyqa\", \n                                        optCtrl=list(maxfun=2e5)),\n\n                           data = long.orth)\n\nboundary (singular) fit: see help('isSingular')\n\nsummary(long.orth.3.glmer.check)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (Orthography +  \n    1 || Participant) + (Orthography + Instructions + 1 || Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1039.1    1131.6    -501.5    1003.1      1245 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.3164 -0.4102 -0.1850  0.2049  6.3675 \n\nRandom effects:\n Groups        Name                   Variance  Std.Dev.  Corr\n Participant   (Intercept)            1.277e-14 1.130e-07     \n Participant.1 Orthographyabsent      1.235e+00 1.111e+00     \n               Orthographypresent     2.637e+00 1.624e+00 1.00\n Word          (Intercept)            7.695e-10 2.774e-05     \n Word.1        Orthographyabsent      1.637e+00 1.279e+00     \n               Orthographypresent     8.779e-01 9.370e-01 1.00\n Word.2        Instructionsexplicit   4.259e-01 6.526e-01     \n               Instructionsincidental 2.373e+00 1.540e+00 1.00\nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.98581    0.46206  -4.298 1.73e-05 ***\nTime2                        0.04821    0.08442   0.571 0.567946    \nOrthography2                 0.44338    0.12723   3.485 0.000492 ***\nInstructions1                0.22950    0.26577   0.864 0.387847    \nzConsistency_H              -0.61318    0.33663  -1.822 0.068524 .  \nOrthography2:Instructions1  -0.03827    0.09513  -0.402 0.687475    \nOrthography2:zConsistency_H  0.01581    0.09969   0.159 0.874019    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.007                                   \nOrthogrphy2 -0.127  0.014                            \nInstructns1 -0.259  0.022  0.007                     \nzCnsstncy_H  0.020 -0.003 -0.059  0.041              \nOrthgrp2:I1  0.020  0.004 -0.021  0.277 -0.031       \nOrthgr2:C_H -0.041  0.003  0.256 -0.002 -0.410 -0.016\noptimizer (bobyqa) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\n\n\n\n\nPract.A.13. If we refit model 3 but with the random effects:\n\n(Orthography + 1 || Participant) +\nOrthography + Instructions + 1 || Word),\nWe get:\n\na convergence warning\nrandom effects variances and covariances that we did not ask for: including some bad signs\n\n\n\n\n\n\nAfter the practical class, we will reveal the answers that are currently hidden.\nThe answers version of the webpage will present my answers for questions, and some extra information where that is helpful.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 19. Workbook introduction to Generalized Linear Mixed-effects Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/04-glmm-workbook.html#sec-glmm-workbook-targets",
    "href": "PSYC412/part2/04-glmm-workbook.html#sec-glmm-workbook-targets",
    "title": "Week 19. Workbook introduction to Generalized Linear Mixed-effects Models",
    "section": "",
    "text": "Here, we look at Generalized Linear Mixed-effects Models (GLMMs): we can use these models to analyze outcome variables of different kinds, including outcome variables like response accuracy that are coded using discrete categories (e.g. correct vs. incorrect).\nIn this workbook, and in our conceptual introduction, our aims are to:\n\nRecognize the limitations of alternative (traditional) methods for analyzing such outcomes.\nUnderstand the practical reasons for using GLMMs when we analyze discrete outcome variables.\nPractice running GLMMs with varying random effects structures.\nPractice reporting the results of GLMMs, including through the use of model plots.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 19. Workbook introduction to Generalized Linear Mixed-effects Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/04-glmm-workbook.html#sec-glmm-workbook-resources",
    "href": "PSYC412/part2/04-glmm-workbook.html#sec-glmm-workbook-resources",
    "title": "Week 19. Workbook introduction to Generalized Linear Mixed-effects Models",
    "section": "",
    "text": "You will see, next, the lectures we share to explain the concepts you will learn about, and the practical data analysis skills you will develop. Then you will see information about the practical materials you can use to build and practise your skills.\nEvery week, you will learn best if you first watch the lectures then do the practical exercises.\n\n\n\n\n\n\nLinked resources\n\n\n\n\nWe learned about multilevel structured data in the conceptual introduction to multilevel data and the workbook introduction to multilevel data.\nWe then deepened our understanding by looking at the analysis of data from studies with repeated-measures designs in the conceptual introduction to linear mixed-effects models and the workbook introduction to mixed-effects models.\nWe further extended our understanding and practice skills in the chapter on developing linear mixed-effects models and the corresponding workbook.\n\nThis workbook introduction on Generalized Linear Mixed-effects Models (GLMMs) is linked to the corresponding chapter where the explanation of ideas or of practical analysis steps is set out more extensively or in more depth.\n\n\n\n\nThe lecture materials for this week are presented in three short parts.\nClick on a link and your browser should open a tab showing the Panopto video for the lecture part.\n\nPart 1 (20 minutes): Understand the reasons for using Generalized Linear Mixed-effects Models (GLMMs); categorical outcomes like response accuracy and the bad compromises involved in (traditional methods) not using GLMMs to analyze them; understanding what the generalized part of this involves.\n\n\n\nPart 2 (16 minutes): The questions, design and methods of the working data example study; category coding practicalities; specifying a GLMM appropriate to the design; the GLMM results summary, reading the results, visualizing the results.\n\n\n\nPart 3 (19 minutes): What random effects should we include; recognizing when a model has got into trouble, and what to do about it; general approaches to model specification; reporting model results.\n\n\n\n\n\n\n\n\n\n\n\nDownload the lecture slides\n\n\n\nYou can download the lecture slides in two different versions:\n\n402-week-20-GLMM.pdf: exactly as delivered [700 KB];\n402-week-20-GLMM_6pp.pdf: printable version, six-slides-per-page [850 KB].\n\nThe GLMM.pdf version is the version delivered for the lecture recordings. To make the slides easier to download, I produced a six-slide-per-page version, GLMM_6pp.pdf. This should be easier to download and print out if that is what you want to do.\n\n\n\n\n\nWe will be working with data collected for a study investigating word learning in children, reported by Ricketts et al. (2021). You will see that the study design has both a repeated measures aspect because each child is asked to respond to multiple stimuli, and a longitudinal aspect because responses are recorded at two time points. Because responses were observed to multiple stimuli for each child, and because responses were recorded at multiple time points, the data have a multilevel structure. These features require the use of mixed-effects models for analysis.\nWe will see, also, that the study involves the factorial manipulation of learning conditions. This means that, when you see the description of the study design, you will see embedded in it a 2 x 2 factorial design. You will be able to generalize from our work here to many other research contexts where psychologists conduct experiments in which conditions are manipulated according to a factorial design.\nHowever, our focus now is on the fact that the outcome for analysis is the accuracy of the responses made by children to word targets in a spelling task. The categorical nature of accuracy as an outcome is the reason why we now turn to use Generalized Linear Mixed-effects Models.\nYou can read more about these data in the conceptual introduction chapter on GLMMs.\nWe addressed three research questions and tested predictions in relation to each question.\n\nDoes the presence of orthography promote greater word learning?\n\n\nWe predicted that children would demonstrate greater orthographic learning for words that they had seen (orthography present condition) versus not seen (orthography absent condition).\n\n\nWill orthographic facilitation be greater when the presence of orthography is emphasized explicitly during teaching?\n\n\nWe expected to observe an interaction between instructions and orthography, with the highest levels of learning when the orthography present condition was combined with explicit instructions.\n\n\nDoes word consistency moderate the orthographic facilitation effect?\n\n\nFor orthographic learning, we expected that the presence of orthography might be particularly beneficial for words with higher spelling-sound consistency, with learning highest when children saw and heard the word, and these codes provided overlapping information.\n\n\n\n\n\n\n\nImportant\n\n\n\nGet the data: get the data file and the .R script you can use to do the exercises that will support your learning.\n\nYou can download the files folder for this chapter by clicking on the link 04-glmm.zip.\n\n\n\nThe practical materials folder includes data files and an .R script:\nIn this chapter, we will be working with the data about the orthographic post-test outcome for the Ricketts word learning study:\n\nlong.orth_2020-08-11.csv\n\nThe data file is collected together with the .R script:\n\n04-glmm-workbook.R the workbook you will need to do the practical exercises.\n\nThe data come from the Ricketts et al. (2021) study, and you can access the analysis code and data for that study, in full, at the OSF repository here\n\n\n\n\n\n\nImportant\n\n\n\nYou can access the sign-in page for R-Studio Server here\n\n\n\n\nThe aims of the practical work are to:\n\nUnderstand the reasons for using Generalized Linear Mixed-effects models (GLMMs) when we analyze discrete outcome variables.\nRecognize the limitations of alternative methods for analyzing such outcomes.\nPractice running GLMMs with varying random effects structures.\nPractice reporting the results of GLMMs, including through the use of model plots.\n\nMy recommendations for learning are that you should aim to:\n\nrun GLMMs of demonstration data;\nrun GLMMs of alternate data sets;\nplay with the .R code used to create examples for the lecture;\nand edit example code to create alternate visualizations.\n\n\n\n\nNow you will progress through a series of tasks, and challenges, to aid your learning.\n\n\n\n\n\n\nWarning\n\n\n\nWe will work with the data file:\n\nlong.orth_2020-08-11.csv\n\n\n\nWe again split the steps into into parts, tasks and questions.\nWe are going to work through the following workflow steps: each step is labelled as a practical part.\n\nSet-up\nLoad the data\nTidy data\nAnalyze the data: random intercepts\nAnalyze the data: model comparisons\n\nIn the following, we will guide you through the tasks and questions step by step.\n\n\n\n\n\n\nImportant\n\n\n\nAn answers version of the workbook will be provided after the practical class.\n\n\n\n\n\nTo begin, we set up our environment in R.\n\n\nUse the library() function to make the functions we need available to you.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlibrary(broom)\nlibrary(effects)\n\nLoading required package: carData\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\nlibrary(gridExtra)\nlibrary(here)\n\nhere() starts at /Users/padraic/MSc_411_412_2025-26\n\nlibrary(lattice)\nlibrary(knitr)\nlibrary(lme4)\n\nLoading required package: Matrix\n\nlibrary(MuMIn)\n\nRegistered S3 methods overwritten by 'MuMIn':\n  method        from \n  nobs.multinom broom\n  nobs.fitdistr broom\n\nlibrary(sjPlot)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::combine()     masks gridExtra::combine()\n✖ tidyr::expand()      masks Matrix::expand()\n✖ dplyr::filter()      masks stats::filter()\n✖ dplyr::lag()         masks stats::lag()\n✖ tidyr::pack()        masks Matrix::pack()\n✖ ggplot2::set_theme() masks sjPlot::set_theme()\n✖ tidyr::unpack()      masks Matrix::unpack()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\n\n\n\n\nRead the data file into R:\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlong.orth &lt;- read_csv(\"long.orth_2020-08-11.csv\", \n                      col_types = cols(\n                        Participant = col_factor(),\n                        Time = col_factor(),\n                        Study = col_factor(),\n                        Instructions = col_factor(),\n                        Version = col_factor(),\n                        Word = col_factor(),\n                        Orthography = col_factor(),\n                        Measure = col_factor(),\n                        Spelling.transcription = col_factor()\n                      )\n                    )\n\n\n\n\nYou can see, here, that within the read_csv() function call, I specify col_types, instructing R how to treat a number of different variables.\n\nYou can read more about this convenient way to control the read-in process here.\n\n\n\n\n\nThe data are already tidy: each column in long.orth_2020-08-11.csv corresponds to a variable and each row corresponds to an observation. However, we need to do a bit of work, before we can run any analyses, to fix the coding of the categorical predictor (or independent) variables: the factors Orthography, Instructions, and Time.\n\n\nIt is always a good to inspect what you have got when you read a data file in to R.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nsummary(long.orth)\n\n\n\n\nYou will see that some of the variables included in the .csv file are listed, following, with information about value coding or calculation.\n\nParticipant – Participant identity codes were used to anonymize participation.\nTime – Test time was coded 1 (time 1) or 2 (time 2). For the Study 1 longitudinal data, it can be seen that each participant identity code is associated with observations taken at test times 1 and 2.\nInstructions – Variable coding for whether participants undertook training in the explicit} or incidental} conditions.\nWord – Letter string values showing the words presented as stimuli to the children.\nOrthography – Variable coding for whether participants had seen a word in training in the orthography absent or present conditions.\nConsistency-H – Calculated orthography-to-phonology consistency value for each word. -zConsistency-H – Standardized Consistency H scores\nScore – Outcome variable – for the orthographic post-test, responses were scored as 1 (correct, if the target spelling was produced in full) or 0 (incorrect, if the target spelling was not produced).\n\nThe summary will show you that we have a number of other variables available, including measures of individual differences in reading or reading-related abilities or knowledge, but we do not need to pay attention to them, for our exercises.\n\nPract.Q.1. Look at the summaries for the variables Time, Instructions and Orthography. Assuming that the read_csv() action did what was required, you will see how R presents factor summaries by default. What do the variable summaries tell you about the factor level coding, and the number of observations in each level?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nFor a factor like Orthography, the column values code for whether the observations in a data row are associated with the condition absent or the condition present.\nIt is simpler to talk about absent or present as being levels of Orthography condition.\n\nYou can ask R what the levels of a factor are using: levels(long.orth$Orthography)\n\n\n\n\n\nPract.A.1. The summary shows:\n\nTime (1, 655; 2, 608); Instructions (explicit, 592; incidental, 671); and Orthography (absent, 631; present, 632).\n\nPract.Q.2. Are there any surprises in the summary of factors?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe would hope to see equal numbers of observations at different levels for a factor.\n\n\n\n\nPract.A.2. It should be surprising that we do not have equal numbers of observations.\n\n\n\n\nFit a simple Generalized Linear Model with outcome (accuracy of response) Score, and Instructions as the predictor.\n\nNote that you are ignoring random effects, for this task.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nsummary(glm(Score ~ Instructions, family = \"binomial\", data = long.orth))\n\n\n\n\n\nPract.Q.3. What is the estimated effect of Instructions on Score?\n\n\nPract.A.3. The estimate is: Instructionsincidental -0.05932\n\n\nPract.Q.4. Can you briefly explain in words what the estimate says about how log odds Score (response correct vs. incorrect) changes in association with different Instructions conditions?\n\n\nPract.A.4. We can say, simply, that the log odds that a response would be correct were lower in the incidental compared to the explicit Instructions condition.\n\nThis task and these questions are designed to alert you to the challenges involved in estimating the effect of categorical variables, and of interpreting the effects estimates.\n\n\n\n\n\n\nTip\n\n\n\nThis model (and default coding) gives us an estimate of how the log odds of a child getting a response correct changes if we compare the responses in the explicit condition (here, treated as the baseline or reference level) with responses in the incidental condition.\nR tells us about the estimate by adding the name of the factor level that is not the reference level, here, incidental to the name of the variable Instructions whose effect is being estimated.\n\n\n\n\n\nHowever, it is better not to use R’s default dummy coding scheme if we are analyzing data, where the data come from a study involving two or more factors, and we want to estimate not just the main effects of the factors but also the effect of the interaction between the factors.\nIn our analyses, we want the coding that allows us to get estimates of the main effects of factors, and of the interaction effects, somewhat like what we would get from an ANOVA. This requires us to use effect coding.\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe can code whether a response was recorded in the absent or present condition using numbers. In dummy coding, for any observation, we would use a column of zeroes or ones to code condition: i.e., absent (0) or present (1). In effect coding, for any observation, we would use a column of ones or minus ones to code condition: i.e., absent (-1) or present (1). (With a factor with more than two levels, we would use more than one column to do the coding: the number of columns we would use would equal the number of factor condition levels minus one.) In effect coding, observations coded -1 are in the reference level.\nWith effect coding, the constant (i.e., the intercept for our model) is equal to the grand mean of all the observed responses. And the coefficient of each of the effect variables is equal to the difference between the mean of the group coded 1 and the grand mean.\n\nYou can read more about effect coding here or here.\n\n\n\n\nWe follow recommendations to use sum contrast coding for the experimental factors. Further, to make interpretation easier, we want the coding to work so that for both Orthography and Instructions conditions, doing something is the “high” level in the factor – hence:\n\nOrthography, absent (-1) vs. present (+1)\nInstructions, incidental (-1) vs. explicit (+1)\nTime, test time 1 (-1) vs. time 2 (+1)\n\nWe use a modified version of the contr.sum() function (provided in the {memisc} library) that allows us to define the base or reference level for the factor manually (see documentation).\nWe need to start by loading the {memisc} library.\n\nlibrary(memisc)\n\nIn some circumstances, this can create warnings.\n\nYou can see information about the potential warnings here.\n\n\nPract.Q.5. Can you work out how to use the contr.sum() function to change the coding of the categorical variables (factors) in the example data-set from dummy to effects coding?\n\n\nIt would be sensible to examine how R sees the coding of the different levels for each factor, before and then after you use contr.sum() to use that coding.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nIn the following sequence, I first check how R codes the levels of each factor by default, I then change the coding, and check that the change gets me what I want.\nWe want effects coding for the orthography condition factor, with orthography condition coded as -1, +1. Check the coding.\n\ncontrasts(long.orth$Orthography)\n\n        present\nabsent        0\npresent       1\n\n\nYou can see that Orthography condition is initially coded, by default, using dummy coding: absent (0); present (1). We want to change the coding, then check that we have got what we want.\n\ncontrasts(long.orth$Orthography) &lt;- contr.sum(2, base = 1)\ncontrasts(long.orth$Orthography)\n\n         2\nabsent  -1\npresent  1\n\n\nWe want effects coding for the Instructions condition factor, with Instructions condition coded as -1, +1. Check the coding.\n\ncontrasts(long.orth$Instructions)\n\n           incidental\nexplicit            0\nincidental          1\n\n\nChange it.\n\ncontrasts(long.orth$Instructions) &lt;- contr.sum(2, base = 2)\ncontrasts(long.orth$Instructions)\n\n            1\nexplicit    1\nincidental -1\n\n\nWe want effects coding for the Time factor, with Time coded as -1, +1 Check the coding.\n\ncontrasts(long.orth$Time)\n\n  2\n1 0\n2 1\n\n\nChange it.\n\ncontrasts(long.orth$Time) &lt;- contr.sum(2, base = 1)\ncontrasts(long.orth$Time)\n\n   2\n1 -1\n2  1\n\n\nIn these chunks of code, I use contr.sum(a, base = b) to do the coding, where a is the number of levels in a factor (replace a with the right number), and b tells R which level to use as the baseline or reference level (replace b with the right number). I usually need to check the coding before and after I specify it.\n\n\n\n\nPract.Q.6. Experiment: what happens if you change the first number for one of the factors?\n\nRun the following code example, and reflect on what you see:\n\ncontrasts(long.orth$Time) &lt;- contr.sum(3, base = 1)\ncontrasts(long.orth$Time)\n\n\nPract.A.6. Changing the first number from 2 will result in warnings, showing that the number must match the number of factor levels for the factor.\n\n\nPract.Q.7. Experiment: what happens if you change the base number for one of the factors?\n\nRun the following code example, and reflect on what you see:\n\ncontrasts(long.orth$Time) &lt;- contr.sum(2, base = 1)\ncontrasts(long.orth$Time)\n\n\nPract.A.7. Changing the base number changes which level gets coded as -1 vs. 1.\n\n\n\n\n\n\n\nSpecify a model with:\n\nThe correct function\nOutcome = Score\nPredictors including:\n\n\nTime + Orthography + Instructions + zConsistency_H +\nOrthography:Instructions +\nOrthography:zConsistency_H +\n\n\nPlus random effects:\n\n\nRandom effect of participants (Participant) on intercepts\nRandom effect of items (Word) on intercepts\n\n\nUsing the \"bobyqa\" optimizer\n\nRun the code to fit the model and get a summary of results.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlong.orth.min.glmer &lt;- glmer(Score ~ \n                               \n                          Time + Orthography + Instructions + zConsistency_H + \n                               \n                          Orthography:Instructions +\n                               \n                          Orthography:zConsistency_H +\n                               \n                          (1 | Participant) + \n                               \n                          (1 |Word),\n                             \n                          family = \"binomial\", \n                          glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)),\n                             \n                          data = long.orth)\n\nsummary(long.orth.min.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (1 |  \n    Participant) + (1 | Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1040.4    1086.7    -511.2    1022.4      1254 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.0994 -0.4083 -0.2018  0.2019  7.4940 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n Participant (Intercept) 1.840    1.357   \n Word        (Intercept) 2.224    1.491   \nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.878464   0.443942  -4.231 2.32e-05 ***\nTime2                        0.050136   0.083325   0.602    0.547    \nOrthography2                 0.455009   0.086813   5.241 1.59e-07 ***\nInstructions1                0.042290   0.230335   0.184    0.854    \nzConsistency_H              -0.618092   0.384002  -1.610    0.107    \nOrthography2:Instructions1   0.005786   0.083187   0.070    0.945    \nOrthography2:zConsistency_H  0.014611   0.083105   0.176    0.860    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.008                                   \nOrthogrphy2 -0.059  0.008                            \nInstructns1  0.014  0.025  0.001                     \nzCnsstncy_H  0.016 -0.002 -0.029 -0.001              \nOrthgrp2:I1 -0.002 -0.001  0.049 -0.045  0.000       \nOrthgr2:C_H -0.027  0.001  0.179  0.000 -0.035 -0.007\n\n\n\n\n\n\nPract.Q.8. Experiment: replace the fixed effects with another variable e.g. mean_z_read (an aggregate measure of reading skill) to get a feel for how the code works.\n\n\nWhat is the estimate for the mean_z_read effect?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlong.orth.min.glmer.expt &lt;- glmer(Score ~ mean_z_read +\n\n                               (1 | Participant) +\n\n                               (1 |Word),\n\n                             family = \"binomial\",\n                             glmerControl(optimizer=\"bobyqa\", \n                                          optCtrl=list(maxfun=2e5)),\n\n                             data = long.orth)\n\nsummary(long.orth.min.glmer.expt)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ mean_z_read + (1 | Participant) + (1 | Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1000.4    1021.0    -496.2     992.4      1259 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-6.1454 -0.4180 -0.2111  0.2543  7.3457 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n Participant (Intercept) 0.1363   0.3692  \n Word        (Intercept) 2.3481   1.5324  \nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.7967     0.4027  -4.461 8.14e-06 ***\nmean_z_read   1.5090     0.1522   9.913  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nmean_z_read -0.121\n\n\n\n\n\n\nPract.A.8. The estimate is: mean_z_read   1.5090\n\n\nPract.Q.9. Can you briefly indicate in words what the estimate says about how log odds Score (response correct vs. incorrect) changes in association with mean_z_read score?\n\n\nPract.A.9. We can say, simply, that log odds that a response would be correct were higher for increasing values of mean_z_read.\n\n\n\n\nFit the first model you were asked to fit.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlong.orth.min.glmer &lt;- glmer(Score ~ \n                               \n                          Time + Orthography + Instructions + zConsistency_H + \n                               \n                          Orthography:Instructions +\n                               \n                          Orthography:zConsistency_H +\n                               \n                          (1 | Participant) + \n                               \n                          (1 |Word),\n                             \n                          family = \"binomial\", \n                          glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)),\n                             \n                          data = long.orth)\n\nsummary(long.orth.min.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (1 |  \n    Participant) + (1 | Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1040.4    1086.7    -511.2    1022.4      1254 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.0994 -0.4083 -0.2018  0.2019  7.4940 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n Participant (Intercept) 1.840    1.357   \n Word        (Intercept) 2.224    1.491   \nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.878464   0.443942  -4.231 2.32e-05 ***\nTime2                        0.050136   0.083325   0.602    0.547    \nOrthography2                 0.455009   0.086813   5.241 1.59e-07 ***\nInstructions1                0.042290   0.230335   0.184    0.854    \nzConsistency_H              -0.618092   0.384002  -1.610    0.107    \nOrthography2:Instructions1   0.005786   0.083187   0.070    0.945    \nOrthography2:zConsistency_H  0.014611   0.083105   0.176    0.860    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.008                                   \nOrthogrphy2 -0.059  0.008                            \nInstructns1  0.014  0.025  0.001                     \nzCnsstncy_H  0.016 -0.002 -0.029 -0.001              \nOrthgrp2:I1 -0.002 -0.001  0.049 -0.045  0.000       \nOrthgr2:C_H -0.027  0.001  0.179  0.000 -0.035 -0.007\n\n\n\n\n\nThen produce plots to show the predicted change in outcome, given the model effects estimates.\n\nCan you work out how to do this?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThere are different ways to complete this task. Two convenient methods involve:\n\nusing the {sjPlot} plot_model() function;\nusing the {effects} library effect() function\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nFirst using the {sjPlot} plot_model() function:\n\nporth &lt;- plot_model(long.orth.min.glmer,\n           type=\"pred\",\n           terms = \"Orthography\") +\n         theme_bw() +\n         ggtitle(\"Predicted probability\") +\n         ylim(0,1)\n\nYou are calculating adjusted predictions on the population-level (i.e.\n  `type = \"fixed\"`) for a *generalized* linear mixed model.\n  This may produce biased estimates due to Jensen's inequality. Consider\n  setting `bias_correction = TRUE` to correct for this bias.\n  See also the documentation of the `bias_correction` argument.\n\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n\npzconsH &lt;- plot_model(long.orth.min.glmer,\n           type=\"pred\",\n           terms = \"zConsistency_H\") +\n         theme_bw() +\n         ggtitle(\"Predicted probability\") +\n         ylim(0,1)\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n\ngrid.arrange(porth, pzconsH,\n            ncol=2)\n\n\n\n\n\n\n\n\nSecond using the {effects} library effect() function”\n\nporth &lt;- plot(effect(\"Orthography\", mod = long.orth.min.glmer))\n\nNOTE: Orthography is not a high-order term in the model\n\npzconsH &lt;- plot(effect(\"zConsistency_H\", mod = long.orth.min.glmer))\n\nNOTE: zConsistency_H is not a high-order term in the model\n\ngrid.arrange(porth, pzconsH,\n            ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nNotice that in the preceding code chunks, I assign the plot objects to names and then use `grid.arrange() to present the named plots in grids.\n\nTo produce and show a plot, don’t do that, just adapt and use the plot functions, as shown in the next code example.\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nplot_model(long.orth.min.glmer,\n           type=\"pred\",\n           terms = \"Instructions\") +\n  theme_bw() +\n  ggtitle(\"Predicted probability\") +\n  ylim(0,1)\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nTake a look at the documentation:\n\nhttps://strengejacke.github.io/sjPlot/articles/plot_interactions.html\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nWe can get a plot showing the predicted impact of the Instructions x Orthography interaction effect as follows.\n\nplot_model(long.orth.min.glmer,\n           type=\"pred\",\n           terms = c(\"Instructions\", \"Orthography\")) +\n  theme_bw() +\n  ggtitle(\"Predicted probability\") +\n  ylim(0,1)\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\nIgnoring unknown labels:\n• linetype : \"Orthography\"\n• shape : \"Orthography\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the models all have the same fixed effects.\nNote also that while we will be comparing models varying in random effects we are not going to use REML=TRUE\n\nSee here for why:\n\nhttps://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#reml-for-glmms\nFirst fit the minimum random intercepts model.\nSpecify a model with:\n\nThe correct function\nOutcome = Score\nPredictors =\n\n\nTime + Orthography + Instructions + zConsistency_H +\nOrthography:Instructions +\nOrthography:zConsistency_H +\n\n\nPlus random effects:\n\n\nRandom effect of participants (Participant) on intercepts\nRandom effect of items (Word) on intercepts\n\n\nUsing the \"bobyqa\" optimizer\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nRun the code to fit the model and get a summary of results, as shown in the chapter\n\nlong.orth.min.glmer &lt;- glmer(Score ~ \n                               \n                            Time + Orthography + Instructions + zConsistency_H + \n                               \n                            Orthography:Instructions +\n                               \n                            Orthography:zConsistency_H +\n                               \n                               (1 | Participant) + \n                               \n                               (1 |Word),\n                             \n                            family = \"binomial\", \n                            glmerControl(optimizer=\"bobyqa\", \n                                         optCtrl=list(maxfun=2e5)),\n                             \n                             data = long.orth)\n\nsummary(long.orth.min.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (1 |  \n    Participant) + (1 | Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1040.4    1086.7    -511.2    1022.4      1254 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.0994 -0.4083 -0.2018  0.2019  7.4940 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n Participant (Intercept) 1.840    1.357   \n Word        (Intercept) 2.224    1.491   \nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.878464   0.443942  -4.231 2.32e-05 ***\nTime2                        0.050136   0.083325   0.602    0.547    \nOrthography2                 0.455009   0.086813   5.241 1.59e-07 ***\nInstructions1                0.042290   0.230335   0.184    0.854    \nzConsistency_H              -0.618092   0.384002  -1.610    0.107    \nOrthography2:Instructions1   0.005786   0.083187   0.070    0.945    \nOrthography2:zConsistency_H  0.014611   0.083105   0.176    0.860    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.008                                   \nOrthogrphy2 -0.059  0.008                            \nInstructns1  0.014  0.025  0.001                     \nzCnsstncy_H  0.016 -0.002 -0.029 -0.001              \nOrthgrp2:I1 -0.002 -0.001  0.049 -0.045  0.000       \nOrthgr2:C_H -0.027  0.001  0.179  0.000 -0.035 -0.007\n\n\n\n\n\nSecond the maximum model: this will take several seconds to run.\nSpecify a model with:\n\nThe correct function\nOutcome = Score\nPredictors =\n\n\nTime + Orthography + Instructions + zConsistency_H +\nOrthography:Instructions +\nOrthography:zConsistency_H +\n\n\nPlus random effects:\n\n\nRandom effect of participants (Participant) on intercepts\nRandom effect of items (Word) on intercepts\n\n\nPlus:\n\n\nRandom effects of participants on the slopes of within-participant effects\nRandom effects of items on the slopes of within-item effects\n\n\nPlus:\n\n\nAllow for covariance between random intercepts and random slopes\n\n\nUsing the \"bobyqa\" optimizer\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nRun the code to fit the model and get a summary of results, as shown in the chapter\n\nlong.orth.max.glmer &lt;- glmer(Score ~ \n                           \n                      Time + Orthography + Instructions + zConsistency_H + \n                           \n                      Orthography:Instructions +\n                           \n                      Orthography:zConsistency_H +\n                           \n                      (Time + Orthography + zConsistency_H + 1 | Participant) + \n                           \n                      (Time + Orthography + Instructions + 1 |Word),\n                         \n                      family = \"binomial\",\n                      glmerControl(optimizer=\"bobyqa\", \n                                   optCtrl=list(maxfun=2e5)),\n                         \n                         data = long.orth)\n\nboundary (singular) fit: see help('isSingular')\n\nsummary(long.orth.max.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (Time +  \n    Orthography + zConsistency_H + 1 | Participant) + (Time +  \n    Orthography + Instructions + 1 | Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1053.6    1192.4    -499.8     999.6      1236 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.1014 -0.4027 -0.1723  0.2037  7.0331 \n\nRandom effects:\n Groups      Name           Variance Std.Dev. Corr             \n Participant (Intercept)    2.043127 1.42938                   \n             Time2          0.005675 0.07533   0.62            \n             Orthography2   0.079980 0.28281   0.78 -0.01      \n             zConsistency_H 0.065576 0.25608   0.49  0.99 -0.16\n Word        (Intercept)    2.793448 1.67136                   \n             Time2          0.046736 0.21618   0.14            \n             Orthography2   0.093740 0.30617  -0.68 -0.81      \n             Instructions1  0.212706 0.46120  -0.74 -0.05  0.38\nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -2.10099    0.49589  -4.237 2.27e-05 ***\nTime2                        0.02077    0.12285   0.169 0.865740    \nOrthography2                 0.52480    0.15496   3.387 0.000708 ***\nInstructions1                0.24467    0.27281   0.897 0.369805    \nzConsistency_H              -0.67818    0.36311  -1.868 0.061803 .  \nOrthography2:Instructions1  -0.05133    0.10004  -0.513 0.607907    \nOrthography2:zConsistency_H  0.05850    0.11634   0.503 0.615064    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.097                                   \nOrthogrphy2 -0.280 -0.187                            \nInstructns1 -0.278  0.023  0.109                     \nzCnsstncy_H  0.062  0.005 -0.064  0.120              \nOrthgrp2:I1  0.024  0.005 -0.071  0.212 -0.065       \nOrthgr2:C_H -0.062  0.049  0.246 -0.031 -0.440  0.001\noptimizer (bobyqa) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\n\n\n\nThen fit models, building on the first minimal (random intercepts) model: adding one extra random effect at a time.\nAdd code to include a random effect of participants on the slopes of the effect of Orthography.\n\nAnd add code to include a random effect of items on the slopes of the effect of Orthography.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nRun the code to fit the model and get a summary of results, as shown in the chapter\n\nlong.orth.2.glmer &lt;- glmer(Score ~ \n                             Time + Orthography + Instructions + zConsistency_H + \n                             \n                             Orthography:Instructions +\n                             \n                             Orthography:zConsistency_H +\n                             \n                             (dummy(Orthography) + 1 || Participant) + \n                             \n                             (dummy(Orthography) + 1 || Word),\n                           \n                           family = \"binomial\", \n                           glmerControl(optimizer=\"bobyqa\", \n                                        optCtrl=list(maxfun=2e5)),\n                           \n                           data = long.orth)\n\nsummary(long.orth.2.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) +  \n    1 || Participant) + (dummy(Orthography) + 1 || Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1041.0    1097.6    -509.5    1019.0      1252 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9373 -0.4132 -0.1952  0.1826  6.9614 \n\nRandom effects:\n Groups        Name               Variance Std.Dev.\n Participant   (Intercept)        1.57092  1.2534  \n Participant.1 dummy(Orthography) 0.57624  0.7591  \n Word          (Intercept)        2.36284  1.5372  \n Word.1        dummy(Orthography) 0.02101  0.1450  \nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.9025027  0.4512443  -4.216 2.49e-05 ***\nTime2                        0.0501974  0.0837186   0.600 0.548775    \nOrthography2                 0.4135727  0.1120792   3.690 0.000224 ***\nInstructions1                0.0455920  0.2234615   0.204 0.838333    \nzConsistency_H              -0.6254414  0.3958971  -1.580 0.114151    \nOrthography2:Instructions1   0.0019343  0.1038694   0.019 0.985142    \nOrthography2:zConsistency_H -0.0007112  0.0877740  -0.008 0.993535    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.007                                   \nOrthogrphy2  0.034  0.007                            \nInstructns1  0.013  0.025  0.007                     \nzCnsstncy_H  0.017 -0.002 -0.024 -0.002              \nOrthgrp2:I1  0.003  0.001  0.043  0.126  0.000       \nOrthgr2:C_H -0.029  0.001  0.182  0.001 -0.024 -0.011\n\n\n\n\n\nAdd code to include a random effect of items on the slopes of the effect of Instructions.\n\n\n\n\n\n\nCode\n\n\n\n\n\nRun the code to fit the model and get a summary of results, as shown in the chapter\n\nlong.orth.3.glmer &lt;- glmer(Score ~ \n                             Time + Orthography + Instructions + zConsistency_H + \n                             \n                             Orthography:Instructions +\n                             \n                             Orthography:zConsistency_H +\n                             \n                          (dummy(Orthography) + 1 || Participant) + \n                             \n                          (dummy(Orthography) + dummy(Instructions) + 1 || Word),\n                           \n                           family = \"binomial\", \n                           glmerControl(optimizer=\"bobyqa\", \n                                        optCtrl=list(maxfun=2e5)),\n                           \n                           data = long.orth)\n\nsummary(long.orth.3.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) +  \n    1 || Participant) + (dummy(Orthography) + dummy(Instructions) +  \n    1 || Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1036.5    1098.2    -506.2    1012.5      1251 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9951 -0.4068 -0.1920  0.1838  5.9308 \n\nRandom effects:\n Groups        Name                Variance Std.Dev.\n Participant   (Intercept)         1.64393  1.2822  \n Participant.1 dummy(Orthography)  0.55604  0.7457  \n Word          (Intercept)         1.94313  1.3940  \n Word.1        dummy(Orthography)  0.01607  0.1268  \n Word.2        dummy(Instructions) 0.86694  0.9311  \nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.9621261  0.4400649  -4.459 8.25e-06 ***\nTime2                        0.0507347  0.0843237   0.602  0.54740    \nOrthography2                 0.4263703  0.1114244   3.827  0.00013 ***\nInstructions1                0.1907423  0.2632605   0.725  0.46874    \nzConsistency_H              -0.6270900  0.3669904  -1.709  0.08750 .  \nOrthography2:Instructions1  -0.0265729  0.1048392  -0.253  0.79991    \nOrthography2:zConsistency_H -0.0006305  0.0878823  -0.007  0.99428    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.007                                   \nOrthogrphy2  0.028  0.008                            \nInstructns1 -0.127  0.022  0.019                     \nzCnsstncy_H  0.017 -0.002 -0.026  0.011              \nOrthgrp2:I1  0.013  0.001  0.003  0.068 -0.009       \nOrthgr2:C_H -0.030  0.002  0.182  0.002 -0.026 -0.018\n\n\n\n\n\nAdd code to include a random effect of participants on the slopes of the effect of consistency (zConsistency_H).\n\n\n\n\n\n\nCode\n\n\n\n\n\nRun the code to fit the model and get a summary of results, as shown in the chapter\n\nlong.orth.4.a.glmer &lt;- glmer(Score ~ \n                             Time + Orthography + Instructions + zConsistency_H + \n                             \n                             Orthography:Instructions +\n                             \n                             Orthography:zConsistency_H +\n                             \n                      (dummy(Orthography) + zConsistency_H + 1 || Participant) + \n                             \n                      (dummy(Orthography) + dummy(Instructions) + 1 || Word),\n                           \n                           family = \"binomial\", \n                           glmerControl(optimizer=\"bobyqa\", \n                                        optCtrl=list(maxfun=2e5)),\n                           \n                           data = long.orth)\n\nboundary (singular) fit: see help('isSingular')\n\nsummary(long.orth.4.a.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) +  \n    zConsistency_H + 1 || Participant) + (dummy(Orthography) +  \n    dummy(Instructions) + 1 || Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1038.5    1105.3    -506.2    1012.5      1250 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9952 -0.4068 -0.1920  0.1838  5.9309 \n\nRandom effects:\n Groups        Name                Variance  Std.Dev. \n Participant   (Intercept)         1.644e+00 1.282e+00\n Participant.1 dummy(Orthography)  5.560e-01 7.456e-01\n Participant.2 zConsistency_H      2.090e-10 1.446e-05\n Word          (Intercept)         1.943e+00 1.394e+00\n Word.1        dummy(Orthography)  1.604e-02 1.266e-01\n Word.2        dummy(Instructions) 8.669e-01 9.311e-01\nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.9621306  0.4400477  -4.459 8.24e-06 ***\nTime2                        0.0507346  0.0843236   0.602  0.54740    \nOrthography2                 0.4263730  0.1114188   3.827  0.00013 ***\nInstructions1                0.1907330  0.2632583   0.725  0.46875    \nzConsistency_H              -0.6270898  0.3669803  -1.709  0.08749 .  \nOrthography2:Instructions1  -0.0265706  0.1048370  -0.253  0.79992    \nOrthography2:zConsistency_H -0.0006352  0.0878782  -0.007  0.99423    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.007                                   \nOrthogrphy2  0.028  0.008                            \nInstructns1 -0.127  0.022  0.019                     \nzCnsstncy_H  0.017 -0.002 -0.026  0.011              \nOrthgrp2:I1  0.013  0.001  0.003  0.068 -0.009       \nOrthgr2:C_H -0.030  0.002  0.182  0.002 -0.026 -0.018\noptimizer (bobyqa) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\n\n\n\nAdd code to include a random effect of participants on the slopes of the effect of Time.\n\nAnd add code to include a random effect of items on the slopes of the effect of Time.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nRun the code to fit the model and get a summary of results, as shown in the chapter\n\nlong.orth.4.b.glmer &lt;- glmer(Score ~ \n                             Time + Orthography + Instructions + zConsistency_H + \n                             \n                             Orthography:Instructions +\n                             \n                             Orthography:zConsistency_H +\n                             \n          (dummy(Orthography) + dummy(Time) + 1 || Participant) + \n                             \n          (dummy(Orthography) + dummy(Instructions) + dummy(Time) + 1 || Word),\n                           \n                           family = \"binomial\", \n                           glmerControl(optimizer=\"bobyqa\", \n                                        optCtrl=list(maxfun=2e5)),\n                           \n                           data = long.orth)\n\nboundary (singular) fit: see help('isSingular')\n\nsummary(long.orth.4.b.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) +  \n    dummy(Time) + 1 || Participant) + (dummy(Orthography) + dummy(Instructions) +  \n    dummy(Time) + 1 || Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1040.5    1112.5    -506.2    1012.5      1249 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9952 -0.4068 -0.1920  0.1838  5.9309 \n\nRandom effects:\n Groups        Name                Variance  Std.Dev. \n Participant   (Intercept)         1.644e+00 1.2821917\n Participant.1 dummy(Orthography)  5.560e-01 0.7456314\n Participant.2 dummy(Time)         0.000e+00 0.0000000\n Word          (Intercept)         1.943e+00 1.3939636\n Word.1        dummy(Orthography)  1.604e-02 0.1266475\n Word.2        dummy(Instructions) 8.669e-01 0.9310619\n Word.3        dummy(Time)         1.992e-08 0.0001411\nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.9621304  0.4400607  -4.459 8.24e-06 ***\nTime2                        0.0507347  0.0843238   0.602  0.54740    \nOrthography2                 0.4263730  0.1114191   3.827  0.00013 ***\nInstructions1                0.1907329  0.2632608   0.725  0.46876    \nzConsistency_H              -0.6270910  0.3669875  -1.709  0.08750 .  \nOrthography2:Instructions1  -0.0265706  0.1048370  -0.253  0.79992    \nOrthography2:zConsistency_H -0.0006352  0.0878785  -0.007  0.99423    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.007                                   \nOrthogrphy2  0.028  0.008                            \nInstructns1 -0.127  0.022  0.019                     \nzCnsstncy_H  0.017 -0.002 -0.026  0.011              \nOrthgrp2:I1  0.013  0.001  0.003  0.068 -0.009       \nOrthgr2:C_H -0.030  0.002  0.182  0.002 -0.026 -0.018\noptimizer (bobyqa) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\n\n\n\n\nPract.Q.10. Which models converge and which models do not converge?\n\n\nPract.A.10. Models 4.a. (consistency) and 4.b. (time) should not converge.\n\n\nPract.Q.11. How can you tell?\n\n\nPract.A.11. Convergence warnings are presented for models that do not converge:\n\noptimizer (bobyqa) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nanova(long.orth.min.glmer, long.orth.2.glmer)\n\nData: long.orth\nModels:\nlong.orth.min.glmer: Score ~ Time + Orthography + Instructions + zConsistency_H + Orthography:Instructions + Orthography:zConsistency_H + (1 | Participant) + (1 | Word)\nlong.orth.2.glmer: Score ~ Time + Orthography + Instructions + zConsistency_H + Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) + 1 || Participant) + (dummy(Orthography) + 1 || Word)\n                    npar    AIC    BIC  logLik -2*log(L)  Chisq Df Pr(&gt;Chisq)\nlong.orth.min.glmer    9 1040.4 1086.7 -511.20    1022.4                     \nlong.orth.2.glmer     11 1041.0 1097.6 -509.51    1019.0 3.3909  2     0.1835\n\nanova(long.orth.min.glmer, long.orth.3.glmer)\n\nData: long.orth\nModels:\nlong.orth.min.glmer: Score ~ Time + Orthography + Instructions + zConsistency_H + Orthography:Instructions + Orthography:zConsistency_H + (1 | Participant) + (1 | Word)\nlong.orth.3.glmer: Score ~ Time + Orthography + Instructions + zConsistency_H + Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) + 1 || Participant) + (dummy(Orthography) + dummy(Instructions) + 1 || Word)\n                    npar    AIC    BIC  logLik -2*log(L)  Chisq Df Pr(&gt;Chisq)  \nlong.orth.min.glmer    9 1040.4 1086.7 -511.20    1022.4                       \nlong.orth.3.glmer     12 1036.5 1098.2 -506.24    1012.5 9.9115  3    0.01933 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\n\nPract.Q.12. What does using bobyqa do? Delete glmerControl() and report what impact does this have?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nRun the code to fit the model and get a summary of results\n\nlong.orth.3.glmer.check &lt;- glmer(Score ~\n                            \n                            Time + Orthography + Instructions + zConsistency_H +\n\n                            Orthography:Instructions +\n\n                            Orthography:zConsistency_H +\n\n                            (dummy(Orthography) + 1 || Participant) +\n\n                            (dummy(Orthography) + dummy(Instructions) + 1 || Word),\n\n                           family = \"binomial\",\n\n                           data = long.orth)\n\nsummary(long.orth.3.glmer.check)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) +  \n    1 || Participant) + (dummy(Orthography) + dummy(Instructions) +  \n    1 || Word)\n   Data: long.orth\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1036.5    1098.2    -506.2    1012.5      1251 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9949 -0.4068 -0.1920  0.1837  5.9304 \n\nRandom effects:\n Groups        Name                Variance Std.Dev.\n Participant   (Intercept)         1.64374  1.2821  \n Participant.1 dummy(Orthography)  0.55646  0.7460  \n Word          (Intercept)         1.94423  1.3944  \n Word.1        dummy(Orthography)  0.01615  0.1271  \n Word.2        dummy(Instructions) 0.86769  0.9315  \nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.9617535  0.4401423  -4.457 8.31e-06 ***\nTime2                        0.0507351  0.0843245   0.602  0.54740    \nOrthography2                 0.4263842  0.1114435   3.826  0.00013 ***\nInstructions1                0.1906959  0.2632724   0.724  0.46886    \nzConsistency_H              -0.6275205  0.3670921  -1.709  0.08737 .  \nOrthography2:Instructions1  -0.0265347  0.1048529  -0.253  0.80022    \nOrthography2:zConsistency_H -0.0006278  0.0878892  -0.007  0.99430    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.007                                   \nOrthogrphy2  0.028  0.008                            \nInstructns1 -0.127  0.022  0.019                     \nzCnsstncy_H  0.017 -0.002 -0.026  0.011              \nOrthgrp2:I1  0.013  0.001  0.003  0.068 -0.009       \nOrthgr2:C_H -0.030  0.002  0.182  0.002 -0.026 -0.018\n\n\n\n\n\n\nPract.A.12. Deleting the bobyqa requirement seems to have no impact on models that converge.\n\n\nPract.Q.13. What does using dummy() in the random effects coding do?\n\n\nExperiment: check out models 2 or 3, removing dummy() from the random effects coding to see what happens.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nRun the code to fit the model and get a summary of results\n\nlong.orth.3.glmer.check &lt;- glmer(Score ~\n                             Time + Orthography + Instructions + zConsistency_H +\n\n                             Orthography:Instructions +\n\n                             Orthography:zConsistency_H +\n\n                             (Orthography + 1 || Participant) +\n\n                             (Orthography + Instructions + 1 || Word),\n\n                           family = \"binomial\",\n                           glmerControl(optimizer=\"bobyqa\", \n                                        optCtrl=list(maxfun=2e5)),\n\n                           data = long.orth)\n\nboundary (singular) fit: see help('isSingular')\n\nsummary(long.orth.3.glmer.check)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (Orthography +  \n    1 || Participant) + (Orthography + Instructions + 1 || Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1039.1    1131.6    -501.5    1003.1      1245 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.3164 -0.4102 -0.1850  0.2049  6.3675 \n\nRandom effects:\n Groups        Name                   Variance  Std.Dev.  Corr\n Participant   (Intercept)            1.277e-14 1.130e-07     \n Participant.1 Orthographyabsent      1.235e+00 1.111e+00     \n               Orthographypresent     2.637e+00 1.624e+00 1.00\n Word          (Intercept)            7.695e-10 2.774e-05     \n Word.1        Orthographyabsent      1.637e+00 1.279e+00     \n               Orthographypresent     8.779e-01 9.370e-01 1.00\n Word.2        Instructionsexplicit   4.259e-01 6.526e-01     \n               Instructionsincidental 2.373e+00 1.540e+00 1.00\nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.98581    0.46206  -4.298 1.73e-05 ***\nTime2                        0.04821    0.08442   0.571 0.567946    \nOrthography2                 0.44338    0.12723   3.485 0.000492 ***\nInstructions1                0.22950    0.26577   0.864 0.387847    \nzConsistency_H              -0.61318    0.33663  -1.822 0.068524 .  \nOrthography2:Instructions1  -0.03827    0.09513  -0.402 0.687475    \nOrthography2:zConsistency_H  0.01581    0.09969   0.159 0.874019    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.007                                   \nOrthogrphy2 -0.127  0.014                            \nInstructns1 -0.259  0.022  0.007                     \nzCnsstncy_H  0.020 -0.003 -0.059  0.041              \nOrthgrp2:I1  0.020  0.004 -0.021  0.277 -0.031       \nOrthgr2:C_H -0.041  0.003  0.256 -0.002 -0.410 -0.016\noptimizer (bobyqa) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\n\n\n\n\nPract.A.13. If we refit model 3 but with the random effects:\n\n(Orthography + 1 || Participant) +\nOrthography + Instructions + 1 || Word),\nWe get:\n\na convergence warning\nrandom effects variances and covariances that we did not ask for: including some bad signs\n\n\n\n\n\n\nAfter the practical class, we will reveal the answers that are currently hidden.\nThe answers version of the webpage will present my answers for questions, and some extra information where that is helpful.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 19. Workbook introduction to Generalized Linear Mixed-effects Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/LICENSE.html",
    "href": "PSYC412/part2/LICENSE.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "GNU GENERAL PUBLIC LICENSE\n                   Version 3, 29 June 2007\nCopyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/ Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\n                        Preamble\nThe GNU General Public License is a free, copyleft license for software and other kinds of works.\nThe licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change all versions of a program–to make sure it remains free software for all its users. We, the Free Software Foundation, use the GNU General Public License for most of our software; it applies also to any other work released this way by its authors. You can apply it to your programs, too.\nWhen we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.\nTo protect your rights, we need to prevent others from denying you these rights or asking you to surrender the rights. Therefore, you have certain responsibilities if you distribute copies of the software, or if you modify it: responsibilities to respect the freedom of others.\nFor example, if you distribute copies of such a program, whether gratis or for a fee, you must pass on to the recipients the same freedoms that you received. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights.\nDevelopers that use the GNU GPL protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License giving you legal permission to copy, distribute and/or modify it.\nFor the developers’ and authors’ protection, the GPL clearly explains that there is no warranty for this free software. For both users’ and authors’ sake, the GPL requires that modified versions be marked as changed, so that their problems will not be attributed erroneously to authors of previous versions.\nSome devices are designed to deny users access to install or run modified versions of the software inside them, although the manufacturer can do so. This is fundamentally incompatible with the aim of protecting users’ freedom to change the software. The systematic pattern of such abuse occurs in the area of products for individuals to use, which is precisely where it is most unacceptable. Therefore, we have designed this version of the GPL to prohibit the practice for those products. If such problems arise substantially in other domains, we stand ready to extend this provision to those domains in future versions of the GPL, as needed to protect the freedom of users.\nFinally, every program is threatened constantly by software patents. States should not allow patents to restrict development and use of software on general-purpose computers, but in those that do, we wish to avoid the special danger that patents applied to a free program could make it effectively proprietary. To prevent this, the GPL assures that patents cannot be used to render the program non-free.\nThe precise terms and conditions for copying, distribution and modification follow.\n                   TERMS AND CONDITIONS\n\nDefinitions.\n\n“This License” refers to version 3 of the GNU General Public License.\n“Copyright” also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.\n“The Program” refers to any copyrightable work licensed under this License. Each licensee is addressed as “you”. “Licensees” and “recipients” may be individuals or organizations.\nTo “modify” a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a “modified version” of the earlier work or a work “based on” the earlier work.\nA “covered work” means either the unmodified Program or a work based on the Program.\nTo “propagate” a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.\nTo “convey” a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.\nAn interactive user interface displays “Appropriate Legal Notices” to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.\n\nSource Code.\n\nThe “source code” for a work means the preferred form of the work for making modifications to it. “Object code” means any non-source form of a work.\nA “Standard Interface” means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.\nThe “System Libraries” of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A “Major Component”, in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.\nThe “Corresponding Source” for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work’s System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.\nThe Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.\nThe Corresponding Source for a work in source code form is that same work.\n\nBasic Permissions.\n\nAll rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.\nYou may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.\nConveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.\n\nProtecting Users’ Legal Rights From Anti-Circumvention Law.\n\nNo covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.\nWhen you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work’s users, your or third parties’ legal rights to forbid circumvention of technological measures.\n\nConveying Verbatim Copies.\n\nYou may convey verbatim copies of the Program’s source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.\nYou may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.\n\nConveying Modified Source Versions.\n\nYou may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:\na) The work must carry prominent notices stating that you modified\nit, and giving a relevant date.\n\nb) The work must carry prominent notices stating that it is\nreleased under this License and any conditions added under section\n7.  This requirement modifies the requirement in section 4 to\n\"keep intact all notices\".\n\nc) You must license the entire work, as a whole, under this\nLicense to anyone who comes into possession of a copy.  This\nLicense will therefore apply, along with any applicable section 7\nadditional terms, to the whole of the work, and all its parts,\nregardless of how they are packaged.  This License gives no\npermission to license the work in any other way, but it does not\ninvalidate such permission if you have separately received it.\n\nd) If the work has interactive user interfaces, each must display\nAppropriate Legal Notices; however, if the Program has interactive\ninterfaces that do not display Appropriate Legal Notices, your\nwork need not make them do so.\nA compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an “aggregate” if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation’s users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.\n\nConveying Non-Source Forms.\n\nYou may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:\na) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by the\nCorresponding Source fixed on a durable physical medium\ncustomarily used for software interchange.\n\nb) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by a\nwritten offer, valid for at least three years and valid for as\nlong as you offer spare parts or customer support for that product\nmodel, to give anyone who possesses the object code either (1) a\ncopy of the Corresponding Source for all the software in the\nproduct that is covered by this License, on a durable physical\nmedium customarily used for software interchange, for a price no\nmore than your reasonable cost of physically performing this\nconveying of source, or (2) access to copy the\nCorresponding Source from a network server at no charge.\n\nc) Convey individual copies of the object code with a copy of the\nwritten offer to provide the Corresponding Source.  This\nalternative is allowed only occasionally and noncommercially, and\nonly if you received the object code with such an offer, in accord\nwith subsection 6b.\n\nd) Convey the object code by offering access from a designated\nplace (gratis or for a charge), and offer equivalent access to the\nCorresponding Source in the same way through the same place at no\nfurther charge.  You need not require recipients to copy the\nCorresponding Source along with the object code.  If the place to\ncopy the object code is a network server, the Corresponding Source\nmay be on a different server (operated by you or a third party)\nthat supports equivalent copying facilities, provided you maintain\nclear directions next to the object code saying where to find the\nCorresponding Source.  Regardless of what server hosts the\nCorresponding Source, you remain obligated to ensure that it is\navailable for as long as needed to satisfy these requirements.\n\ne) Convey the object code using peer-to-peer transmission, provided\nyou inform other peers where the object code and Corresponding\nSource of the work are being offered to the general public at no\ncharge under subsection 6d.\nA separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.\nA “User Product” is either (1) a “consumer product”, which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, “normally used” refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.\n“Installation Information” for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.\nIf you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).\nThe requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.\nCorresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.\n\nAdditional Terms.\n\n“Additional permissions” are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.\nWhen you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.\nNotwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:\na) Disclaiming warranty or limiting liability differently from the\nterms of sections 15 and 16 of this License; or\n\nb) Requiring preservation of specified reasonable legal notices or\nauthor attributions in that material or in the Appropriate Legal\nNotices displayed by works containing it; or\n\nc) Prohibiting misrepresentation of the origin of that material, or\nrequiring that modified versions of such material be marked in\nreasonable ways as different from the original version; or\n\nd) Limiting the use for publicity purposes of names of licensors or\nauthors of the material; or\n\ne) Declining to grant rights under trademark law for use of some\ntrade names, trademarks, or service marks; or\n\nf) Requiring indemnification of licensors and authors of that\nmaterial by anyone who conveys the material (or modified versions of\nit) with contractual assumptions of liability to the recipient, for\nany liability that these contractual assumptions directly impose on\nthose licensors and authors.\nAll other non-permissive additional terms are considered “further restrictions” within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.\nIf you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.\nAdditional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.\n\nTermination.\n\nYou may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).\nHowever, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.\nMoreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.\nTermination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.\n\nAcceptance Not Required for Having Copies.\n\nYou are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.\n\nAutomatic Licensing of Downstream Recipients.\n\nEach time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.\nAn “entity transaction” is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party’s predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.\nYou may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.\n\nPatents.\n\nA “contributor” is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor’s “contributor version”.\nA contributor’s “essential patent claims” are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, “control” includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.\nEach contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor’s essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.\nIn the following three paragraphs, a “patent license” is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To “grant” such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.\nIf you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. “Knowingly relying” means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient’s use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.\nIf, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.\nA patent license is “discriminatory” if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.\nNothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.\n\nNo Surrender of Others’ Freedom.\n\nIf conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.\n\nUse with the GNU Affero General Public License.\n\nNotwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU Affero General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the special requirements of the GNU Affero General Public License, section 13, concerning interaction through a network will apply to the combination as such.\n\nRevised Versions of this License.\n\nThe Free Software Foundation may publish revised and/or new versions of the GNU General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.\nEach version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU General Public License “or any later version” applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU General Public License, you may choose any version ever published by the Free Software Foundation.\nIf the Program specifies that a proxy can decide which future versions of the GNU General Public License can be used, that proxy’s public statement of acceptance of a version permanently authorizes you to choose that version for the Program.\nLater license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.\n\nDisclaimer of Warranty.\n\nTHERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM “AS IS” WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n\nLimitation of Liability.\n\nIN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\nInterpretation of Sections 15 and 16.\n\nIf the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.\n                 END OF TERMS AND CONDITIONS\n\n        How to Apply These Terms to Your New Programs\nIf you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.\nTo do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the “copyright” line and a pointer to where the full notice is found.\n&lt;one line to give the program's name and a brief idea of what it does.&gt;\nCopyright (C) &lt;year&gt;  &lt;name of author&gt;\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\nAlso add information on how to contact you by electronic and paper mail.\nIf the program does terminal interaction, make it output a short notice like this when it starts in an interactive mode:\n&lt;program&gt;  Copyright (C) &lt;year&gt;  &lt;name of author&gt;\nThis program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.\nThis is free software, and you are welcome to redistribute it\nunder certain conditions; type `show c' for details.\nThe hypothetical commands show w' andshow c’ should show the appropriate parts of the General Public License. Of course, your program’s commands might be different; for a GUI interface, you would use an “about box”.\nYou should also get your employer (if you work as a programmer) or school, if any, to sign a “copyright disclaimer” for the program, if necessary. For more information on this, and how to apply and follow the GNU GPL, see https://www.gnu.org/licenses/.\nThe GNU General Public License does not permit incorporating your program into proprietary programs. If your program is a subroutine library, you may consider it more useful to permit linking proprietary applications with the library. If this is what you want to do, use the GNU Lesser General Public License instead of this License. But first, please read https://www.gnu.org/licenses/why-not-lgpl.html.\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "PSYC412",
      "LICENSE"
    ]
  },
  {
    "objectID": "PSYC412/part2/03-mixed.html",
    "href": "PSYC412/part2/03-mixed.html",
    "title": "Week 18. Conceptual introduction to developing linear mixed-effects models",
    "section": "",
    "text": "Linear mixed-effects models are important, interesting, and sometimes challenging.\nWe have worked through a series of chapters in which we have aimed to learn:\n\nTo recognize the situations where we shall see multilevel structured data and therefore where we will need to apply multilevel or mixed-effects models.\nTo understand the nature and the advantages of these models: what they are, and why they work better than other kinds of models, given multilevel data.\nTo practice how we code for mixed-effects models, and how we read or write about the results.\n\nWe now need to develop our understanding and skills further. And we now need to examine some of the complexities that we may face when we work with mixed-effects models.\nOur approach will continue to depend on verbal explanation, visualization and a practical code-based approach to the modeling.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nShrinkage or regularization means that models of data should be excited by the data but not too excited.\n\n\nThis means our models work better if they are informed by all the data, and take into account random differences but also if they are not too strongly influenced by individual (participant or item) data.\n\n\n\nWe are probably now at a stage, in the development of our skills and understanding, where we can be more specific about our targets for learning: what capacities or abilities we want to have by the time we complete the course. I have held back specifying the targets in this way because, first, we had to learn the basic vocabulary. Now that we have done that, we can lay out the targets against which we can assess the progression of our learning.\n\n\n\n\n\n\nImportant\n\n\n\nWe have three capacities we seek to develop. These include the capacity:\n\nto understand mixed-effects models;\nto work with these models practically or efficiently in R;\nand to communicate their results effectively (to ourselves and others).\n\n\n\nWe should be aware that the development of skills and understanding in relation to each of these capacities will travel at different speeds for different people, and within any person at different speeds for different capacities.\nWe should also be aware that our internal evaluation of our understanding will not exactly match the evaluation that comes from external assessment. In other words, we might not be satisfied with our understanding but, still, our understanding might be satisfactory. It might be that we can learn to say in words what mixed-effects models are or involve, or what their results mean, very effectively even if we remain unsure about our understanding.\nFor these reasons, I specify what we are aiming to develop in terms of what we can do. You can test your development against this checklist of targets for learning.\n\nWe want to develop the capacity to understand mixed-effects models, the capacity to:\n\n\nrecognize where data have a multilevel structure;\nrecognize where multilevel or mixed-effects models are required;\ndistinguish the elements of a mixed-effects model, including fixed effects and random effects;\nexplain how random effects can be understood in terms of random differences (or deviations) between groups or classes or individuals, in intercepts or slopes;\nexplain how random effects can be understood in terms of variances, as a means to account for random differences between groups or classes or individuals in intercepts or slopes;\nexplain how mixed-effects models work better than linear models, for multilevel structured data;\nexplain how mixed-effects models work better because they allow partial-pooling of estimates.\n\n\nWe want to develop the capacity to work practically with mixed-effects models in R, the capacity to:\n\n\nspecify a mixed-effects model in lmer() code;\nidentify how the mixed-effects model code varies, depending on the kinds of random effects that are assumed;\nidentify the elements of the output or results that come from an lmer() mixed-effects analysis;\ninterpret the fixed-effects estimates;\ninterpret the random effects estimates, including both the variance and covariance estimates.\n\n\nWe want to develop the capacity to communicate the results of mixed-effects models effectively, to ourselves and to others, the capacity to:\n\n\ndescribe in words and summary tables the results of a mixed-effects model;\nvisualize the effects estimates or predictions from a mixed-effects model.\n\n\n\n\nI have provided a collection of materials you can use.\nPreviously:\n\nWe learned about multilevel structured data in the conceptual introduction to multilevel data and the workbook introduction to multilevel data.\nWe then deepened our understanding by looking at the analysis of data from studies with repeated-measures designs in the conceptual introduction to linear mixed-effects models and the workbook introduction to mixed-effects models.\nThis week, you can practical exercies in the chapter on developing linear mixed-effects models.\n\nHere, I explain what they are and how I suggest you use them.\n1. Video recordings of lectures\n1.1. I have recorded a lecture in three parts. The lectures should be accessible by anyone who has the link.\n\nPart 1 – about 13 minutes\nPart 2 – about 13 minutes\nPart 3 – about 24 minutes\n\n1.2. I suggest you watch the recordings then read the rest of this chapter.\n\nThe lectures provide a summary of the main points.\n\n1.3. You can download the lecture slides in three different versions:\n\n402-week-19-LME-3.pdf: high resolution .pdf, exactly as delivered [1.3 MB];\n402-week-19-LME-3_1pp.pdf: printable version, one-slide-per-page [1.2 MB];\n402-week-19-LME-3_6pp.pdf: printable version, six-slides-per-page [1.3 MB].\n\nThe high resolution version is the version delivered for the lecture recordings. To make the slides easier to download, I produced lower resolution versions: 1pp and 6pp. These should be easier to download and print out if that is what you want to do.\n2. Chapter: 03-mixed\n2.1. I have written this chapter to discuss the main ideas and set out the practical steps you can follow to start to develop the skills required to analyse multilevel structured data with crossed random effects.\n2.2. The practical elements include data tidying, visualization and analysis steps.\n2.3. You can read the chapter, run the code, and do the exercises.\n\nRead in the example ML word recognition study data-set.\nEdit example code to create alternate visualizations of variable distributions and of the relationships between critical variables.\nExperiment with the .R code used to work with the example data.\nRun linear mixed-effects models of demonstration data.\nRun linear mixed-effects models of alternate data sets.\nReview the recommended readings (Section 1.16).\n\n3. Practical materials\n3.1 In the following sections, I describe the practical steps, and associated practical materials (exercise workbooks and data), you can use for your learning.\n\n\n\nIn this chapter, we will be working with the ML word recognition study data-set. ML examined visual word recognition in younger and older adults using the lexical decision task.\nIn lexical decision, participants are presented with a stimulus: a string of letters that is either a real word (e.g., ‘car’) or a made-up or non-word (e.g., ‘cas’). Participants are required to respond to the stimulus by pressing a button to indicate either that they think the stimulus is a word or that they think it is a non-word. Each complete sequence of events, in which a stimulus is presented and a response is recorded, is known as a trial. In the lexical decision task implemented by ML, all study participants were presented with a mix of 160 word stimuli and 160 non-word stimuli, in random order, in a total of 320 trials.\nEach stimulus was presented one at a time on the computer screen. The critical outcome measure was the reaction time (RT) or latency for each response. Observed RT represents the interval of time from the moment the stimulus was first presented (the stimulus onset) to the moment the response was made (the response onset).\nLexical decision is a very popular technique for examining word recognition, especially in adults. While not every graduate student will be interested in word recognition, or reading, everyone should understand that tasks like lexical decision are similar to a range of other tasks used in experimental psychological science.\nThe critical feature of the study, here, is that we have an outcome – a decision response – observed multiple times (for each stimulus) for each participant. We shall be analyzing the speed of response, reaction time (RT), measured in milliseconds (ms).\nIn our analyses, the focus of our interest will be on the ways in which participant attributes (like age) or word properties (like frequency) influence the speed of response in a task designed measure the ability to recognize visually presented English words. In analyzing the effects of participant attributes on recognition response RTs, we will use data – about those attributes – that were recorded using a mix of survey questions (about age, etc.) and standardized ability tests that were administered to study participants alongside the lexical decision task.\nThe total number of participants for this study was 39, including a group of younger adults and a group of older adults. Information was collected about the participants’ age, education and gender. In addition, participants were asked to complete ability measures (TOWRE sight word and phonemic tests, Torgesen et al. (1999)) and a measure of reading experience (Author Recognition Test, ART, Masterson & Hayes (2007)).\n\n\nInstead of posing a simple and general research question, we shall orient our work around a set of quite specific predictions. ML hypothesized:\n\nEffects of stimulus attributes\n\n\nPredicting that words that are shorter, that look like more other words, and that appear frequently in the language will be easier to recognize.\n\n\nEffects of participant attributes\n\n\nPredicting that older readers would be faster and more accurate than younger readers in word recognition.\n\n\nEffects of interactions between the effects of word attributes and person attributes.\n\n\nPredicting that better (older) readers will show smaller effects of word attributes.\n\nIn this chapter, we can focus on one specific prediction as we work through the practical steps of conducting an analysis using linear mixed-effects models.\n\n\n\n\n\n\nNote\n\n\n\n\nResearch hypothesis: Words that are shorter, that look like more other words, and that appear frequently in the language will be easier to recognize.\n\n\n\n\n\n\nIn summary, ML collected data on lexical decision task response reaction times (RTs) and accuracy and information on participants, including age, reading ability and reading experience. In addition, she collected information on the properties of the lexical decision stimulus items, including variables like the length or frequency of words (values taken from the English Lexicon Project, Balota et al. (2007)).\nThe ML study data includes the following variables that we will work with (as well as some you can ignore):\n\nIdentifying variables\n\n\nsubjectID – identifying code for participants\nitem_name – words presented as stimuli\nitem_number – identifying code for words presented\n\n\nResponse variables\n\n\nRT – response reaction time (ms), for responses to words\n\n\nParticipant attribute variables\n\n\nAge – in years\nGender – coded M (male), F (female)\nTOWRE_wordacc – word reading skill, words read correctly (out of 104)\nTOWRE_nonwordacc – nonword reading skill, nonwords (made up words) read correctly (out of 63)\nART_HRminusFR – reading experience score\n\n\nStimulus property variables\n\n\nLength – word length, in letters\nOrtho_N – orthographic neighbourhood size, how many other words in English a stimulus word looks like\nOLD – orthographic Levenshtein distance, how many letter edits (addition, deletion or substitution) it would take to make a stimulus word look like another English word (a measure of orthographic neighbourhood) (Yarkoni et al., 2008)\nBG_Sum, BG_Mean, BG_Freq_By_Pos – measures of how common are pairs of letters that compose stimulus words\nSUBTLWF, LgSUBTLWF, SUBTLCD, LgSUBTLCD – measures of how common stimulus words are, taken from the SUBTLEX corpus analysis of word frequency (Brysbaert & New, 2009)\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nGet the data: get the data file and the .R script you can use to do the exercises that will support your learning.\n\nYou can download the files folder for this chapter by clicking on the link 03-mixed.zip.\n\n\n\nThe practical materials folder includes data files and an .R script:\n\nsubjects.behaviour.words-310114.csv which holds information about the (word) stimuli, participants, and the responses recorded in the ML study.\n\nThe .csv file is a comma separated values file and can be opened in Excel.\nThe data file is collected together with the .R script:\n\n402-03-mixed-workbook.R the workbook you will need to do the practical exercises.\n\nDuring practical sessions, each week, you can use the workbook to prompt your code construction and your thinking, with support.\n\n\nAfter practical sessions, you will be able to download an answers version of the workbook .R so check back here after the session.\n\n\n\n\n\n\nImportant\n\n\n\nGet the answers: get the data file and the .R script with answers.\n\nYou can download the files folder for this chapter by clicking on the link 03-mixed-answers.zip.\n\n\n\nThe link to this folder will not work until after a session has finished.\nWhen the link is live, you will be able to download a folder including:\n\n402-03-mixed-workbook-with-answers.R with answers to questions and example code for doing the exercises.\n\n\n\n\n\n\nIn the Introduction to linear mixed-effects models chapter section on data tidying, we saw how we may need to tidy the data we collect in experimental studies: combining data about responses with data about participant attributes or stimulus properties, and restructuring the data so that they are in a tidy format. For this class, many steps in the process of data tidying were completed previously. Thus, we only need to perform steps 1, 3 and 4 of the usual data tidying process:\n\nImport the data or read the data into R, see Section 1.6.1;\nRestructure the data;\nSelect or transform variables, see Section 1.6.4;\nFilter observations, see Section 1.6.3.\n\nWe are going to first filter the observations, then transform the outcome variable. We will explain why we have to do this as we proceed.\nWe will use tidyverse library functions to do this work, as usual.\n\nlibrary(tidyverse)\n\n\n\nI am going to assume you have downloaded the data file, and that you know where it is. We use read_csv to read one file into R.\n\nML.all &lt;- read_csv(\"subjects.behaviour.words-310114.csv\", na = \"-999\")\n\nThe data file subjects.behaviour.words-310114.csv holds all the data about everything (behaviour, participants, stimuli) we need for our analysis work.\n\n\n\n\n\n\nTip\n\n\n\nIt is always a good idea to first inspect what you have got when you read a data file into R before you do anything more demanding.\n\nYou cannot assume that the data are what you think they are\nor that the data are structured or coded in the ways that you think (or have been told) they should be structured or coded.\n\n\n\nYou can inspect the first few rows of the data-set using head().\n\n\n\n\n\nitem_number\nsubjectID\nTest\nAge\nYears_in_education\nGender\nTOWRE_wordacc\nTOWRE_nonwordacc\nART_HRminusFR\nRT\nCOT\nSubject\nTrial.order\nitem_name\nLength\nOrtho_N\nBG_Sum\nBG_Mean\nBG_Freq_By_Pos\nitem_type\nSUBTLWF\nLgSUBTLWF\nSUBTLCD\nLgSUBTLCD\nOLD\n\n\n\n\n1\nGB9\nALT\n21\n11\nF\n78\n41\n18\n368.66\n134057.8\nGB9\n54\nwent\n4\n15\n249\n198\n29\nword\n411.51\n4.322\n79.6\n3.825\n1.2\n\n\n1\nNH1\nTAL\n52\n18\nM\n78\n56\n33\n724.83\n742737.4\nNH1\n148\nwent\n4\n15\n249\n198\n29\nword\n411.51\n4.322\n79.6\n3.825\n1.2\n\n\n1\nA15\nLTA\n21\n16\nF\n95\n57\n9\n483.71\n861801.0\nA15\n278\nwent\n4\n15\n249\n198\n29\nword\n411.51\n4.322\n79.6\n3.825\n1.2\n\n\n1\nB18\nTLA\n69\n11\nM\n85\n54\n10\n517.62\n1024583.4\nb18\n318\nwent\n4\n15\n249\n198\n29\nword\n411.51\n4.322\n79.6\n3.825\n1.2\n\n\n\n\n\n\n\nYou can examine all the variables using summary().\n\nsummary(ML.all)\n\n  item_number      subjectID             Test                Age       \n Min.   :  1.00   Length:5440        Length:5440        Min.   :16.00  \n 1st Qu.: 40.75   Class :character   Class :character   1st Qu.:21.00  \n Median : 80.50   Mode  :character   Mode  :character   Median :21.00  \n Mean   : 80.50                                         Mean   :36.94  \n 3rd Qu.:120.25                                         3rd Qu.:53.00  \n Max.   :160.00                                         Max.   :73.00  \n Years_in_education    Gender          TOWRE_wordacc    TOWRE_nonwordacc\n Min.   :11.00      Length:5440        Min.   : 68.00   Min.   :16.00   \n 1st Qu.:13.00      Class :character   1st Qu.: 84.00   1st Qu.:50.00   \n Median :16.00      Mode  :character   Median : 93.00   Median :55.50   \n Mean   :14.94                         Mean   : 91.24   Mean   :52.41   \n 3rd Qu.:16.00                         3rd Qu.: 98.00   3rd Qu.:57.00   \n Max.   :19.00                         Max.   :104.00   Max.   :63.00   \n ART_HRminusFR         RT               COT            Subject         \n Min.   : 1.00   Min.   :-2000.0   Min.   :  50094   Length:5440       \n 1st Qu.: 7.00   1st Qu.:  498.1   1st Qu.: 297205   Class :character  \n Median :11.00   Median :  577.6   Median : 552854   Mode  :character  \n Mean   :15.15   Mean   :  565.3   Mean   : 575780                     \n 3rd Qu.:21.00   3rd Qu.:  677.4   3rd Qu.: 810108                     \n Max.   :43.00   Max.   : 1978.4   Max.   :1583651                     \n  Trial.order     item_name             Length       Ortho_N      \n Min.   : 21.0   Length:5440        Min.   :3.0   Min.   : 0.000  \n 1st Qu.:100.8   Class :character   1st Qu.:4.0   1st Qu.: 3.000  \n Median :180.5   Mode  :character   Median :4.0   Median : 6.000  \n Mean   :180.5                      Mean   :4.3   Mean   : 7.069  \n 3rd Qu.:260.2                      3rd Qu.:5.0   3rd Qu.:11.000  \n Max.   :340.0                      Max.   :6.0   Max.   :24.000  \n     BG_Sum          BG_Mean       BG_Freq_By_Pos   item_type        \n Min.   :  3.00   Min.   :  1.00   Min.   :  1.0   Length:5440       \n 1st Qu.: 81.75   1st Qu.: 67.75   1st Qu.: 74.5   Class :character  \n Median :151.50   Median :153.50   Median :158.0   Mode  :character  \n Mean   :155.89   Mean   :153.82   Mean   :149.6                     \n 3rd Qu.:234.75   3rd Qu.:239.25   3rd Qu.:227.0                     \n Max.   :314.00   Max.   :316.00   Max.   :295.0                     \n    SUBTLWF          LgSUBTLWF        SUBTLCD        LgSUBTLCD    \n Min.   :   0.57   Min.   :1.477   Min.   : 0.32   Min.   :1.447  \n 1st Qu.:  17.36   1st Qu.:2.947   1st Qu.: 6.67   1st Qu.:2.748  \n Median :  69.30   Median :3.549   Median :23.64   Median :3.298  \n Mean   : 442.01   Mean   :3.521   Mean   :36.52   Mean   :3.137  \n 3rd Qu.: 290.70   3rd Qu.:4.171   3rd Qu.:65.24   3rd Qu.:3.739  \n Max.   :6161.41   Max.   :5.497   Max.   :99.70   Max.   :3.922  \n      OLD       \n Min.   :1.000  \n 1st Qu.:1.288  \n Median :1.550  \n Mean   :1.512  \n 3rd Qu.:1.750  \n Max.   :2.050  \n\n\nThe summary shows some features of the data-set, or of how R interprets the data-set, that are of immediate interest to us, though we do not necessarily have to do anything about them.\n\nWe can see statistical summaries – showing the mean, median, minimum and maximum, etc. – of numeric variables like the outcome variable RT.\nWe can see statistical summaries, also, of variables that comprise number values but which we do not want to be treated as numbers, e.g., the word stimulus coding variable item_number.\nWe can see that some variables are simply listed as Class: character. That tells us that one or more values in the columns in the datasheet that correspond to these variables are words or strings of letters or alphanumeric characters.\nThere is no sign of the presence of missing values in this data-set, no counts of NAs.\n\nWe do not really want R to treat a coding variable like item_number as numeric: it functions as a categorical or nominal variable, a factor. And we want R to treat coding variables like subjectID as factors. In the Introduction to multilevel data chapter section on coercion, we saw how we can require R to handle variables exactly as we require it to using coercion. In the Introduction to linear mixed-effects models chapter section on data loading, we saw how we can determine how R treats variables at the read-in stage, using col_types() specification. We are going to do neither here because we do not have to do this work; not doing it will have no impact on our analyses at this point.\nWhat we do need to do is deal with a problem that is already apparent in the summary statistics – did you spot it? If we look at the summary, we can see that RT includes values as low as -2000. That cannot be right.\n\n\n\nWe should examine the distribution of the outcome variable, lexical decision response reaction time (RT in ms). Observations about variable value distributions are a part of Exploratory Data Analysis and serve to catch errors in the data-set (e.g. incorrectly recorded scores) but also to inform the researcher’s understanding of their own data.\nWe shall examine the distribution of the outcome variable, lexical decision response reaction time (RT in ms), using density plots. An alternative method would be to use histograms. I choose to use density plots because they allow the easy comparison of the distributions of values of a continuous numeric variable like reaction time. A density plot shows a curve. You can say that the density corresponds to the height of the curve for a given value of the variable being depicted, and that it is related to the probability of observing values of the variable within some range of values (Howell, 2016).\nGetting a density plot of RTs of responses is easy using ggplot() code.\n\nML.all %&gt;%\n  ggplot(aes(x = RT)) +\n  geom_density(size=1.5) +\n  geom_rug(alpha = .2) +\n  ggtitle(\"Raw RT\") +\n  theme_bw()  \n\n\n\n\n\n\n\nFigure 1: Density plot showing word recognition reaction time, correct and incorrect responses\n\n\n\n\n\nThe code delivers a plot (Figure 1) showing three peaks in the distribution of RT values. You can see that there is a peak of RT observations around 500-1000ms, another smaller peak around -500ms, and a third smaller peak around -2000ms.\nThe density plot shows the reaction times recorded for participants’ button press ‘yes’ responses to word stimuli in the lexical decision task. The peaks of negative RTs represent observations that are impossible.\nRemember that reaction time, in a task like lexical decision, represents the interval in time between the onset of a task stimulus (in lexical decision, a word or a nonword) and the onset of the response (the button press to indicate the lexical decision). We cannot have negative time intervals. The explanation is that ML collected her data using the DMDX experimental software application (Forster & Forster, 2003). DMDX records the reaction times for incorrect responses as negative RTs.\nThe code to produce Figure 1 works in a series of steps.\n\nML.all %&gt;% takes the data-set, from the ML study, that we have read in to the R workspace and pipes it to the visualization code, next.\nggplot(aes(x = RT)) + creates a plot object in which the x-axis variable is specified as RT. The values of this variable will be mapped to geometric objects, i.e. plot features, that you can see, next.\ngeom_density(size=1.5) + first displays the distribution of values in the variable RT as a density curve. The argument size=1.5 tells R to make the line \\(1.5 \\times\\) the thickness of the line used by default to show variation in density.\n\nSome further information is added to the plot, next.\n\ngeom_rug(alpha = .2) + with a command that tells R to add a rug plot below the density curve.\nggtitle(\"Raw RT\") makes a plot title.\n\nNotice that beneath the curve of the density plot, you can see a series of vertical lines. Each line represents the x-axis location of an RT observation in the ML study data set. This rug plot represents the distribution of RT observations in one dimension.\n\ngeom_rug() draws a vertical line at each location on the x-axis that we observe a value of the variable, RT, named in aes(x = RT).\ngeom_rug(alpha = .2) reduces the opacity of each line, using alpha, to ensure the reader can see how the RT observations are denser in some places than others.\n\nYou can see that we have many more observations of RTs from around 250ms to 1250ms, where the rug of lines is thickest, under the peak of the density plot. This indicates what the two kinds of plots are doing.\n\n\nYou should try out alternative visualisation methods to reveal the patterns in the distribution of variables in the ML data-set (or in your own data).\n\nTake a look at the geoms documented in the {ggplot2} library reference section here.\nExperiment with code to answer the following questions:\n\n\nWould a histogram or a frequency polygon provide a more informative view? Take a look here for advice.\nWhat about a dotplot? Take a look here for advice\n\n\n\n\n\nThe density plot shows us that the raw ML lexical decision RT variable includes negative RT values corresponding to incorrect response. These have to be removed. We can do this quite efficiently by creating a subset of the original “raw” data, defined according to the RT variable using the {dpyr} library filter() function.\n\nML.all.correct &lt;- filter(ML.all, RT &gt;= 200)\n\nThe filter code is written to subset the data by rows using a condition on the values of the RT variable.\nML.all.correct &lt;- filter(ML.all, RT &gt;= 200) works as follows.\n\nML.all.correct &lt;- filter(ML.all ...) creates a new data-set with a new name ML.all.correct from the old data-set ML.all using the filter() function.\nfilter(... RT &gt;= 200) specifies an argument for the filter() function.\n\nIn effect, we are asking R to check every value in the RT column.\n\nR will do a check through the ML.all data-set, row by row.\nIf a row includes an RT that is greater than or equal to 200 then that row will be included in the new data-set ML.all.correct. This is what I mean by using a condition.\nBut if a row includes an RT that is less than 200, then that row will not be included. We express this condition as RT &gt;= 200.\n\nAfter we have removed negative (error) RTs, we check that the size of the data-set – here, the number of rows – matches our expectations. We do this to make sure that we did the filter operation correctly.\n\nlength(ML.all$RT)\n\n[1] 5440\n\nlength(ML.all.correct$RT)\n\n[1] 5257\n\n\nIf you run the length() function calls then you should see that the length or number of observations or rows in the ML.all.correct data-set should be smaller than the number of observations in the ML.all data-set.\n\n\n\n\n\n\nTip\n\n\n\nIt is wise to check that the operations you perform to tidy, process or wrangle data actually do do what you mean them to do. Checks can be performed, for each processing stage, by:\n\nForming expectations or predictions about what the operation is supposed to do e.g. filter out some rows by some number;\nCheck what you get against these predictions e.g. count the number of rows before versus after filtering.\n\n\n\nThe length() function will count the elements in whatever object is specified as an argument in the function call.\n\nThis means that if you put a variable name into the function as in length(data$variable) it will count how long that variable is – how many rows there are in the column.\nIf that variable happens to be, as here, part of a data-set, the same calculation will tell you how many rows there are in the data-set as a whole.\nIf you just enter length(data), naming some data-set, then the function will return a count of the number of columns in the data-set.\n\nHaving obtained a new data frame with data on just those trials where responses were correct, we can plot the distribution of RTs for just the correct responses (Figure 2).\n\nML.all.correct %&gt;%\n  ggplot(aes(x = RT)) +\n  geom_density(size=1.5) + \n  geom_rug(alpha = .2) +\n  ggtitle(\"Correct RTs\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 2: Density plot showing word recognition reaction time, correct responses only\n\n\n\n\n\n\n\nVary the filter conditions in different ways.\n\nChange the threshold for including RTs from RT &gt;= 200 to something else: you can change the number, or you can change the operator from &gt;= to a different comparison (try =, &lt;, &lt;=, &gt;.\nCan you assess what impact the change has?\n\nNote that you can count the number of observations (rows) in a data-set using e.g. length().\n\n\n\nI choose to filter out or exclude not only error responses (where \\(RT &lt; 0ms\\)) but also short reaction times (where \\(RT &lt; 200ms\\)). I think that any response in the lexical decision task that is recorded as less than 200ms cannot possibly represent a real word recognition response. Participants who complete experimental psychological tasks can and do press the button before they have time to engage the psychological processes (like word recognition) that the tasks we administer are designed to probe (like lexical decision).\nThere is some relevant literature that concerns the speed at which neural word recognition processes operate. However, I think you should note that the threshold I am setting for exclusion, here, is essentially arbitrary. If you think about it, I could have set the threshold at any number from \\(100-300ms\\) or some other range.\n\n\n\n\n\n\nWarning\n\n\n\nWhat is guiding me in setting the filter threshold is experience. But other researchers will have different experiences and set different thresholds.\n\nThis is why using exclusion criteria to remove data is problematic.\n\n\n\nFiltering or re-coding observations is an important element of the research workflow in psychological science. How we do or do not remove observations from original data may have an impact on our results (as explored by Steegen et al. (2016)). It is important, therefore, that we learn how to do this reproducibly using, for example, R scripts that we can share with our research reports.\nI would argue that, at minimum, a researcher should report their research including:\n\nWhat exclusion criteria they use to remove data, explaining why.\nReport analyses with and without exclusions, to indicate if their results are sensitive to their decisions.\n\nYou can read further information about the practicalities of using R to do filtering here.\nYou can read a brief discussion of the impacts of researcher choices in data-set construction in Steegen et al. (2016).\n\n\n\n\nFigure 2 shows that we have successfully removed all errors (negative RTs) but now we see just how skewed the RT distribution is. Note the long tail of longer RTs.\nMost researchers assume that participants – healthy young adults – take about 500-1000ms to perform the task and that values outside that range correspond to either fast guesses (RTs that are too short) or to distracted or tired or bored responses (RTs that are too long). In theory, the lexical decision task should be probing automatic cognitive processes, measuring the steps from perception to visual word recognition in the time interval between the moment the stimulus is first shown and the moment the button is pressed by the participant to indicate a response. Thus, it might seem natural to exclude extreme RT values which might correspond not to automatic cognitive processes but to unknowable distraction events or boredom and inattention. However, we shall complete no further data exclusions.\nFor now, we can look at a commonly used method to deal with the skew that we typically see when we examine reaction time distributions. RT distributions are usually skewed with a long tail of longer RTs. You can always take longer to press the button but there is a limit to how much faster you can make your response.\nGenerally, we assume that departures from a model’s predictions about our observations (the linear model residuals) are normally distributed, and we often assume that the relationship between outcome and predictor variables is linear (Cohen et al., 2003). We can ensure that our data are compliant with both assumptions by transforming the RT distribution.\nIt is not cheating to transform variables. Transformations of data variables can be helpful for a variety of reasons in the analysis of psychological data (Cohen et al., 2003; Gelman & Hill, 2007). I do recommend, however, that you are careful to report what transformations you use, and why you do them.\nPsychology researchers often take the log (often the log base 10) of RT values before performing an analysis. Transforming RTs to the log base 10 of RT values has the effect of correcting the skew – bringing the larger RTs ‘closer’ (e.g., \\(1000 = 3\\) in log10) to those near the middle which do not change as much (e.g. \\(500 = 2.7\\) in log10).\n\nML.all.correct$logrt &lt;- log10(ML.all.correct$RT)            \n\nThe log10() function works as follows:-\n\nML.all.correct$logrt &lt;- log10(...) creates a a new variable logrt, adding it to the ML.all.correct data-set. The variable is created using the transformation function log10().\nlog10(ML.all.correct$RT) creates a the new variable by transforming (to log10) the values of the old variable, RT.\n\nWe can see the effect of the transformation if we plot the log10 transformed RTs (see Figure 3). We arrive at a distribution that more closely approximates the normal distribution.\n\nML.all.correct %&gt;%\n  ggplot(aes(x = logrt)) +\n  geom_density(size = 1.5) + \n  geom_rug(alpha = .2) +\n  ggtitle(\"Correct log10 RTs\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 3: Density plot showing log10 transformed reaction time, correct responses only\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThere are other log transformation functions and we often see researchers using the natural log instead of the log base 10 as discussed here\n\n\n\n\n\nEven when data have been structured appropriately, we will still, often, need to do some tidying before we can do an analysis. Most research work involving quantitative evidence requires a big chunk of data tidying or other processing before you get to the statistics.\nOur data are now ready for analysis.\n\n\n\n\nAs we saw in the Introduction to linear mixed-effects models chapter, many Psychologists conduct studies where it is not sensible to think of observations as being nested (Baayen et al., 2008a). In this chapter, we turn to the ML word recognition study data-set, which has a structure similar to the CP study data that we worked with previously. Again, the core concern is that the data come from a study with a repeated-measures design where the experimenter presented multiple stimuli for response to each participant, for several participants, so that we have multiple observations for each participant and multiple observations for each stimulus. Getting practice with this kind of data will help you to easily recognize what you have got when you see it in your own work.\nML asked all participants in a sample of people to read a selection of words, a sample of words from the language.\nFor each participant, we will have multiple observations and these observations will not be independent. One participant will tend to be slower or less accurate compared to another. Her responses may be more or less susceptible to the effects of the experimental variables. The lowest trial-level observations can be grouped with respect to participants. However, the data can also be grouped by stimuli.\nFor each stimulus word, there are multiple observations and these observations will not be independent. One stimulus may prove to be more challenging to all participants compared to another, eliciting slower or less accurate responses on average. In addition, if there are within-items effects, we may ask if the impact of those within-items effects is more prominent, stronger, among responses to some items compared to others.\nGiven this common repeated-measures design, we can analyse the outcome variable in relation to:\n\nfixed effects: the impact of independent variables like participant reading skill or word frequency;\nrandom effects: the impact of random or unexplained differences between participants and also between stimuli.\n\n\n\n\nWe are going to respond to the multilevel (or crossed random effects) structure in the data by using linear mixed-effects models to analyze the data. This week, we are going to look at what mixed-effects models do from a new perspective.\nOur concern will be with different ways of thinking about why mixed-effects models are superior to linear models where data have a multilevel structure. Mixed-effects models tend to be more accurate in this (very common) situation because of what is called partial pooling and shrinkage or regularization. We use our practical example to explore these ideas.\n\n\nTo get started, we can examine – for each individual separately – the distribution of log RT observations, in Figure 4.\n\nML.all.correct %&gt;%\n  group_by(subjectID) %&gt;%\n  mutate(mean_logrt = mean(logrt, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(subjectID = fct_reorder(subjectID, mean_logrt)) %&gt;%\n  ggplot(aes(x = logrt)) +\n  geom_density(size = 1.25) +\n  facet_wrap(~ subjectID) +\n  geom_vline(xintercept = 2.778807, colour = \"red\", linetype = 2) +\n  scale_x_continuous(breaks = c(2.5,3)) +\n  ggtitle(\"Plot showing distribution of logRT for each participant; red line shows mean log10 RT\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 4: Density plot showing log10 transformed reaction time, correct responses, separately for each participant\n\n\n\n\n\nFigure 4 shows that RT distributions vary considerably between people. The plot imposes a dashed red line to indicate where the mean log10 RT is, calculated over all observations in the data-set. The plot shows the distribution of log RT for each participant, as a density drawn separately for each person. The individual plots are ordered by the mean log RT calculated per person, so plots appear in order from the fastest to the slowest.\nThe grid of plots illustrates some interesting features about the data in the ML study sample. You can see how the distribution of log RT varies between individuals: some people show widely spread reaction times; some people show quite tight or narrow distributions. You can see how the shapes of the distributions varies: some people show skew; others do not. I do not see that the variation in the shapes of the distributions is related to the average speed of the person’s responses.\nI think the key message of the plot is that some distributions are wider (RTs are more spread out) than others. We might be concerned that people who present more variable reaction times (wider distributions) may be associated with less reliable estimates of their average response speed, or of the impact of word attributes (like word frequency) on their response speed.\n\n\nThe plotting code I used to produce Figure 4 progresses through a series of steps. This example demonstrates how you can combine data tidying and plotting steps in a single sequence, using tidyverse functions and the %&gt;% pipe, so I will take the time to explain what is going on.\nMy aim is to create a grid of individual plots, showing the distribution of log RTs for each participant, so that the plots are presented in order, from the fastest participant to the slowest. Take a look at the plotting code. We can explain how it works, step by step.\n\nML.all.correct %&gt;%\n  group_by(subjectID) %&gt;%\n  mutate(mean_logrt = mean(logrt, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(subjectID = fct_reorder(subjectID, mean_logrt)) %&gt;%\n  ggplot(aes(x = logrt)) +\n  geom_density(size = 1.25) +\n  facet_wrap(~ subjectID) +\n  geom_vline(xintercept = 2.778807, colour = \"red\", linetype = 2) +\n  scale_x_continuous(breaks = c(2.5,3)) +\n  ggtitle(\"Plot showing distribution of logRT for each participant; red line shows mean log10 RT\") +\n  theme_bw()\n\nYou will see that we present the distribution of RTs using geom_density() and that we present a separate plot for each person’s data using facet_wrap(). To these elements, we add some pre-processing steps to calculate the average response speed of each individual, and to reorder the data-set by those averages.\nIt will make it easier to understand what is going on if we consider the code in chunks.\nFirst, we pre-process the data before we feed it into the plotting code.\n\nML.all.correct %&gt;%\n  group_by(subjectID) %&gt;%\n  mutate(mean_logrt = mean(logrt, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(subjectID = fct_reorder(subjectID, mean_logrt)) %&gt;%\n  ...\n\n\nML.all.correct %&gt;% takes the selected filtered data-set ML.all.correct and pipes it %&gt;% to the next step.\ngroup_by(subjectID) %&gt;% tells R to group the data by subject ID. We have a set of multiple log RT observations for each subjectID because each participant was asked to respond to multiple word stimuli.\nmutate(mean_logrt = mean(logrt, na.rm = TRUE)) next calculates and stores the mean log RT for each person. We create a new variable mean_logrt. We calculate the average of the set of log RTs recorded for each subjectID and construct the new variable mean_logrt from these averages.\n\nWe do not need to treat the data in groups so we remove the grouping, next.\n\nUsing ungroup() %&gt;% means that, having grouped the data to calculate the mean log RTs, we ungroup the data-set so that R can look at all observations in the next step.\nmutate(subjectID = fct_reorder(subjectID, mean_logrt)) %&gt;% asks R to look at all log RT observations in the data-set, and change the top-to-bottom order of the rows.\n\nWe ask R to order observations – using subjectID in fct_reorder() – so that each person’s data are listed by their average speed, mean_logrt from the fastest to the slowest. We then pipe these ordered data to the plotting code, next.\nIf you delete or comment out these first lines, you will see that R uses just a default ordering, drawing the plot for each person in the alphabetical order of their subjectID codes.\nTry it. Don’t forget to start with ML.all.correct %&gt;%.\nSecond, we draw the plots, using the data we have pre-processed.\n\nML.all.correct %&gt;%\n...\n  ggplot(aes(x = logrt)) +\n  geom_density(size = 1.25) +\n  facet_wrap(~ subjectID) +\n...\n\nThe key functions that create a grid of density plots are the following.\n\nggplot(aes(x = logrt)) tells R to work with logrt as the x-axis variable. We shall be plotting the distribution of logrt.\ngeom_density(...) draws a density plot to show the distribution of log RT, using a thicker line size = 1.25\nfacet_wrap(~ subjectID) creates a different plot for each level of the subjectID factor: we want to see a separate plot for each participant.\n\n\nfacet_wrap(~ subjectID) works to split the data-set up by participant, with observations corresponding to each participant identified by their subjectID, and to then split the plotting to show the distribution of log RT separately for each participant.\n\nI wanted to present the plots in order of the average speed of response of participants. If you look at Figure 4, you can see that the position of the peak of the log RT distribution for each participant moves, from the fastest plots where the peak is around \\(log RT = 2.5\\) (shown from the top left of the grid), to the slowest plots where the peak is around \\(log RT = 2.75\\) (shown towards the bottom right of the grid)\nWe can then use further ggplot functions to edit the appearance of the plot, to make it more useful.\n\n...\n  geom_vline(xintercept = 2.778807, colour = \"red\", linetype = 2) +\n  scale_x_continuous(breaks = c(2.5,3)) +\n  ggtitle(\"Plot showing distribution of logRT for each participant; red line shows mean log10 RT\") +\n  theme_bw()\n\n\ngeom_vline(xintercept = 2.778807, colour = \"red\", linetype = 2) draws a vertical red dashed line at the location of the mean log RT, the average of all log RTs over all participants in the data-set.\nscale_x_continuous(breaks = c(2.5,3)) adjusts the x-axis labeling. The ggplot default might draw too many x-axis labels i.e. showing possible log RT values as tick marks on the bottom line of the plot. I want to avoid this as sometimes all the labels can be crowded together, making them harder to read.\n\n\nDrawing a vertical line at the mean calculated overall is designed to help the reader (you) calibrate their comparison of the data from different people.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIt is often useful to experiment with example code to figure out how it works.\n\n\nOne way you can do this is by commenting out one line of code, at a time by putting the # at the start of the line.\n\nIf you do this, you can see what the line of code does by, effectively, asking R to ignore it.\n\nAnother way you can experiment with code is by seeing what you can change and what effect the changes have.\n\nCan you work out how to adapt the plotting code to show a grid of histograms instead of density plots?\nCan you work out how to adapt the code to show a grid of plots indicating the distribution of log RT by different words instead of participants?\n\n\n\n\n\nAs we have discussed in previous chapters, a good way to approach a mixed-effects analysis is by first estimating the effects of the experimental variables (here, frequency) using linear models, ignoring the hierarchical structure in the data.\n\n\n\n\n\n\nNote\n\n\n\nA linear model of multilevel structured data can be regarded as an approximation to the better analysis.\n\n\nWe model the effects of interest, using all the data (hence, complete pooling) but ignoring the differences between participants. This means we can see something of the ‘true’ picture of our data through the linear model results but the linear model misses important information, which the mixed-effects model will include, that would improve its performance.\nAs we saw, in a similar analysis in the Introduction to linear mixed-effects models chapter, we can estimate the relationship between reading reaction times (here, lexical decision RTs) and word frequency using a linear model:\n\\[\nY_{ij} = \\beta_0 + \\beta_1X_j + e_{ij}\n\\]\nWhere:\n\n\\(Y_{ij}\\) is the value of the observed outcome variable, the log RT of the response made by the \\(i\\) participant to the \\(j\\) item;\n\\(\\beta_1X_j\\) refers to the fixed effect of the explanatory variable (here, word frequency), where the frequency value \\(X_j\\) is different for different words \\(j\\), and \\(\\beta_1\\) is the estimated coefficient of the effect due to the relationship between response speed and word frequency;\n\\(e_{ij}\\) is the residual error term, representing the differences between observed \\(Y_{ij}\\) and predicted values (given the model) for each response made by the \\(i\\) participant to the \\(j\\) item.\n\nThe linear model is fit in R using the lm() function.\n\nML.all.correct.lm  &lt;- lm(logrt ~\n                             \n                             LgSUBTLCD,     \n                           \n                           data = ML.all.correct)\n\nsummary(ML.all.correct.lm)\n\n\nCall:\nlm(formula = logrt ~ LgSUBTLCD, data = ML.all.correct)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.41677 -0.07083 -0.01163  0.05489  0.53411 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.885383   0.007117  405.41   &lt;2e-16 ***\nLgSUBTLCD   -0.033850   0.002209  -15.32   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1095 on 5255 degrees of freedom\nMultiple R-squared:  0.04277,   Adjusted R-squared:  0.04259 \nF-statistic: 234.8 on 1 and 5255 DF,  p-value: &lt; 2.2e-16\n\n\nIn the estimates from this linear model, we see an approximate first answer to our prediction.\n\n\n\n\n\n\nNote\n\n\n\n\nResearch hypothesis: Words that are shorter, that look like more other words, and that appear frequently in the language will be easier to recognize.\nResult: We can see that, in this first analysis, the estimated effect of word frequency is \\(\\beta = -0.033850\\).\n\n\n\nI know this looks like a very small number but you should realize that the estimates for the coefficients of fixed effects like the frequency effect are scaled according to the outcome. Here, the outcome is log10 RT, where a log10 RT of 3 equals 1000ms, and, as we can calculate in R\n\nlog10(0.925)\n\n[1] -0.03385827\n\n\nAlso, remember that frequency is scaled in logs too, so the estimate of the coefficient tells us how log10 RT changes for unit change in log frequency. The coefficient represents the estimated change in log10 RT for unit change in log frequency LgSUBTLCD.\nThe estimate indicates that as word log frequency increases, responses logRT decreases by \\(-0.033850\\).\nIn this model, all the information from all participants is analyzed. In discussions of mixed-effects analyses, we say that this is a complete pooling model. This is because all the data have been pooled together, that is, we use all observations in the sample to estimate the effect of frequency.\nIn this model, the observations are assumed to be independent. However, we suppose that the assumption of independence is questionable given the expectation that participants will differ in their overall speed, and in the extent to which their response speed is affected by factors like word frequency.\n\n\n\nVary the linear model using different outcomes or predictors.\n\n\nThe ML study data, like the CP study data, are rich with possibility. It would be useful to experiment with it.\n\n\nChange the predictor from frequency to something else: what do you see when you visualize the relationship between outcome and predictor variables using scatterplots?\nSpecify linear models with different predictors: do the relationships you see in plots match the coefficients you see in the model estimates?\n\nI would recommend that you both estimate the effects of variables and visualize the relationships between variables using scatterplots. If you combine reflection on the model estimates with evaluation of what the plots show you then you will be able to see how reading model results and reading plots can reveal the correspondences between the two ways of looking at your data.\n\n\n\n\nWe can examine variation between participants by analyzing the data for each participant’s responses separately, fitting a different linear model of the effect of word frequency on lexical decision RTs for each participant separately. Figure 5 presents a grid or trellis of plots, one plot per person. In each plot, you can see points corresponding to the log RT of the responses made by each participant to the stimulus words.\n\n\n\n\n\n\nTip\n\n\n\nIn working with R, we often benefit from the vast R knowledge ecosystem.\n\nI was able to produce the sequence of plots Figure 5, Figure 6, Figure 7 and Figure 8 thanks to this very helpful blog post by TJ Mahr\n\n\n\nIn all plots, the pink or red line represents the complete pooling model estimate of the effect of frequency on response RTs. The line is the same for each participant because there is only one estimated effect, based on all data for all participants.\nIn addition, in each plot, you can see a green line. You can see that the line varies between participants. This represents the effect of frequency estimated using just the data for each participant, analyzed separately. These are the no pooling estimates. We call them the no pooling estimates because each is based just on the data from one participant.\n\n\n\n\n\n\n\n\nFigure 5: Plot showing the relationship between logRT and log frequency (LgSUBTLCD) separately for each participant; red-pink line shows the complete pooling estimate, blue-green line shows the no-pooling estimate\n\n\n\n\n\nFigure 5 reveals substantial differences between participants in both average response speed and the frequency effect.\nWe may further predict variation in standard errors between participants given, also, the differences between participants in the spread of log RT, illustrated by Figure 4. Basically, where the distribution of log RT is more widely spread out, for any one participant, there it will be harder for us to estimate with certainty the mean or the sources of variance for the participant’s response speed.\nYou will notice that the no pooling and complete pooling estimates tend to be quite similar. But for some participants – more than for others – there is variation between the estimates.\nYou can reflect that the complete pooling is unsatisfactory because it ignores the variation between the participants: some people are slower than others; some people do show a larger frequency effect than others. You can also reflect that the no pooling is unsatisfactory because it ignores the similarities between the participants.\nWhile there is variation between participants there is also similarity across the group so that the effect of frequency is similar between participants.\n\n\n\n\n\n\nImportant\n\n\n\nWhat we need is an analytic method that is capable of both estimating the overall average population-level effect (here, of word frequency) and taking into account the differences between sampling units (here, participants).\nThat method is linear mixed-effects modeling.\n\n\n\n\n\n\n\n\nAs you have seen before, we can account for the variation – the differences between participants in intercepts and slopes.\nFirst, we model the intercept as two terms:\n\\[\n\\beta_{0i} = \\gamma_0 + U_{0i}\n\\]\nWhere:\n\n\\(\\gamma_{0}\\) is the average intercept, and\n\\(u_{0i}\\) is the difference for each participant between their intercept and the average intercept.\n\nSecond, we can model the frequency effect as two terms:\n\\[\n\\beta_{1i} = \\gamma_1 + U_{1i}\n\\]\nWhere:\n\n\\(\\gamma_{10}\\) is the average slope, and:\n\\(U_{1i}\\) represents the difference for each participant between the slope of their frequency effect and the average slope.\n\nWe can then incorporate in a single model the fixed effects due to the average intercept and the average frequency effect, as well as the random effects – the error variance due to unexplained differences between participants in intercepts and in frequency effects:\n\\[\nY_{ij} = \\gamma_0 + \\gamma_1X_j + U_{0i}+ U_{1i}X_j + e_{ij}\n\\]\nWhere the outcome \\(Y_{ij}\\) is related to:\n\nthe average intercept \\(\\gamma_0\\) and differences between \\(i\\) participants in the intercept \\(U_{0i}\\);\nthe average effect of the explanatory variable frequency \\(\\gamma_1X_j\\) and differences between \\(i\\) participants in the slope \\(U_{1i}X_j\\);\nin addition to residual error variance \\(e_{ij}\\).\n\n\n\n\nAs we first saw in the Introduction to linear mixed-effects models chapter, in conducting mixed-effects analyses, we do not aim to examine the specific deviation (here, for each participant) from the average intercept or the average effect or slope. We estimate just the spread of deviations by-participants.\nA mixed-effects model like our final model actually includes fixed effects corresponding to the intercept and the slope of the word frequency effect plus the variances:\n\n\\(var(U_{0i})\\) variance of deviations by-participants from the average intercept;\n\\(var(U_{1i}X_j)\\) variance of deviations by-participants from the average slope of the frequency effect;\n\\(var(e_{ij})\\) residuals, at the response level, after taking into account all other terms.\n\nWe may expect the random effects of participants or items to covary: for example, participants who are slow to respond may also be more susceptible to the frequency effect. Thus our specification of the random effects of the model can incorporate terms corresponding to the covariance of random effects:\n\n\\(covar(U_{0i}, U_{1i}X_j)\\)\n\n\n\n\nAs we know, some words elicit slower and some elicit faster responses on average. As we discussed in the last chapter, in the Introduction to linear mixed-effects models chapter section on the impact of stimulus variation, if we did not take such variation into account, we might spuriously identify an experimental effect actually due just to unexplained between-items differences in intercepts (Clark, 1973; Raaijmakers et al., 1999) committing an error: the language as fixed effect fallacy.\nWe can model the random effect of items on intercepts by modeling the intercept as two terms:\n\\[\n\\beta_{0j} = \\gamma_0 + W_{0j}\n\\]\nWhere:\n\n\\(\\gamma_{0}\\) is the average intercept, and\n\\(W_{0j}\\) represents the deviation, for each word, between the average intercept and the per-word intercept.\n\nNote that I ignore the possibility, for now, of differences between items in the slopes of fixed effects but I do come back to this.\nThe term the language as fixed effect fallacy (Clark, 1973; Raaijmakers et al., 1999) implies that thinking about the random effects of stimulus differences applies only when we are looking at experiments about language. But you should remember that we need to think about the impact of random differences between stimuli whenever we present samples of stimuli to participants, and we collect observations about multiple responses for each stimulus. This is true whatever the nature of the stimuli (see e.g. Judd et al., 2012).\n\n\n\nOur model can now incorporate the random effects of participants as well as items:\n\\[\nY_{ij} = \\gamma_0 + \\gamma_1X_j + U_{0i}+ U_{1i}X_j + W_{0j} + e_{ij}\n\\]\nIn this model, the outcome \\(Y_{ij}\\) is related to:\n\nthe average intercept \\(\\gamma_0\\) and the word frequency effect \\(\\gamma_1X_j\\);\nplus random effects due to unexplained differences between participants in intercepts \\(U_{0i}\\) and in the slope of the frequency effect \\(U_{1i}X_j\\);\nas well as random differences between items in intercepts \\(W_{0j}\\);\nin addition to the residual term \\(e_{ij}\\).\n\n\n\n\nWe fit a mixed-effects model of the \\(logrt \\sim \\text{frequency}\\) relationship using the lmer() function, taking into account:\n\nthe fact that the study data have a hierarchical structure – with observations sensibly grouped by participant;\nthe fact that both the frequency effect, and average speed, may vary between participants;\nand the fact that the average speed of response can vary between responses to different stimuli.\n\nThe model syntax corresponds to the statistical formula and the code is written as:\n\nML.all.correct.lmer  &lt;- lmer(logrt ~\n\n                           LgSUBTLCD +\n\n                           (LgSUBTLCD + 1|subjectID) +\n\n                           (1|item_name),\n\n                         data = ML.all.correct)\n\nsummary(ML.all.correct.lmer)\n\nAs will now be getting familiar, the code works as follows:\n\nML.all.correct.lmer  &lt;- lmer(...) creates a linear mixed-effects model object using the lmer() function.\nlogrt ~ LgSUBTLCD the fixed effect in the model is expressed as a formula in which the outcome or dependent variable logrt is predicted ~ by the independent or predictor variable LgSUBTLCD word frequency.\n\n\n\n\n\n\n\nTip\n\n\n\nIf there were more terms in the model, the terms would be added in series separated by +\n\n\nThe random effects part of the model is then specified as follows.\n\nWe first have the random effects associated with random differences between participants:\n\n\n(...|subjectID) adds random effects corresponding to random differences between sample groups (participants subjects) coded by the subjectID variable.\n(...1 |subjectID) including random differences between sample groups (subjectID) in intercepts coded 1.\n(LgSUBTLCD... |subjectID) and random differences between sample groups (subjectID) in the slopes of the frequency effect coded by using theLgSUBTLCD variable name.\n\n\nThen, we have the random effects associated with random differences between stimuli:\n\n\n(1|item_name) adds a random effect to account for random differences between sample groups (item_name) in intercepts coded 1.\n\n\n...(..., data = ML.all.correct) specifies the data-set in which you can find the variables named in the model fitting code.\nLastly, we can then specify summary(ML.all.correct.lmer) to get a summary of the fitted model.\n\n\n\n\nIf you run the model code as written then you would see the following results.\n\nML.all.correct.lmer  &lt;- lmer(logrt ~\n\n                           LgSUBTLCD +\n\n                           (LgSUBTLCD + 1|subjectID) +\n\n                           (1|item_name),\n\n                         data = ML.all.correct)\n\nsummary(ML.all.correct.lmer)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: logrt ~ LgSUBTLCD + (LgSUBTLCD + 1 | subjectID) + (1 | item_name)\n   Data: ML.all.correct\n\nREML criterion at convergence: -9868.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.6307 -0.6324 -0.1483  0.4340  5.6132 \n\nRandom effects:\n Groups    Name        Variance  Std.Dev. Corr \n item_name (Intercept) 0.0003268 0.01808       \n subjectID (Intercept) 0.0054212 0.07363       \n           LgSUBTLCD   0.0002005 0.01416  -0.63\n Residual              0.0084333 0.09183       \nNumber of obs: 5257, groups:  item_name, 160; subjectID, 34\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)  2.887997   0.015479 186.577\nLgSUBTLCD   -0.034471   0.003693  -9.333\n\nCorrelation of Fixed Effects:\n          (Intr)\nLgSUBTLCD -0.764\n\n\nIn these results, we see:\n\nFirst, information about the function used to fit the model, and the model object created by the lmer() function call.\nThen, we see the model formula logrt ~ LgSUBTLCD + (LgSUBTLCD + 1|subjectID) + (1|item_name).\nThen, we see REML criterion at convergence about the model fitting process, which we can usually ignore.\nThen, we see information about the distribution of the model residuals.\nThen, we see theRandom Effects.\n\nNotice that the statistics are Variance Std.Dev. Corr., that is, the variance, the corresponding standard deviation, and the correlation estimates associated with the random effects.\n\nWe see Residual error variance, just like in a linear model, corresponding to a distribution or spread of deviations between the model prediction and the observed RT for each response made by a participant to a stimulus.\nWe see Variance terms corresponding to what can be understood as group-level residuals. Here, the variance is estimated for the spread in random differences between the average intercept (over all data) and the intercept for each participant, and the variance due to random differences between the average slope of the frequency effect and the slope for each participant.\nWe also see the variance estimated for the spread in random differences between the average intercept (over all data) and the intercept for responses to each word stimulus.\nAnd we see the Corr estimate, telling us about the covariance between random deviations (between participants) in the intercepts and in the slopes of the frequency effect.\n\n\nLast, just as for linear models, we see estimates of the coefficients (of the slopes) of the fixed effects, the intercept and the slope of the logrts ~ LgSUBTLCD relationship.\n\nWe can compare this estimate with our previous lm() estimate for the effect of frequency.\n\n\n\n\n\n\nNote\n\n\n\n\nResearch hypothesis: Words that are shorter, that look like more other words, and that appear frequently in the language will be easier to recognize.\nResult: We can see that, in this mixed-effects analysis, the estimated effect of word frequency is now \\(\\beta = -0.034471\\).\n\n\n\nThe estimate is different, a bit smaller. While the change in the estimate is also small, we may remember that we are looking at slope estimates for predicted change in log RT, in an experimental research area in which effects are often of the order of 10s of milliseconds. The estimates, and changes in the estimates, will tend to be quite small.\n\n\n\n\n\n\nWarning\n\n\n\nNote that we see coefficient estimates, as in a linear model summary but no p-values.\n\nWe will come back to this, see Section 1.13.4.\nHowever, note that if \\(t &gt;= 2\\) we can suppose that (for a large data-set) an effect is significant at the \\(.05\\) significance level.\n\n\n\n\n\n\n\nWhat is the impact of the incorporation of random effects – the variance and covariance terms – in mixed-effects models? Mixed-effects models can be understood, in general, as a method to compromise between ignoring the differences between groups (here, participants constitute groups of data) as in complete pooling or focusing entirely on each group (participant) as in no pooling (Gelman & Hill, 2007). In this discussion, I am going to refer to the differences between participants but you can assume that the lesson applies generally to any situation in which you have different units in a multilevel structured data-set in which the units correspond to groups or clusters of data.\n\n\nThe problem with ignoring the differences between groups (participants), as in the complete pooling model (here, the linear model), has been obvious when we examined the differences between participants (or between classes) in slopes and intercepts in previous weeks. The problem with focusing entirely on each participant, as in the no pooling model, has not been made apparent in our discussion yet.\nIf we analyze each participant separately then we will get, for each participant, for our model of the frequency effect, the per-participant estimate of the intercept and the per-participant estimate of the slope of the frequency effect. These no-pooling estimates will tend to exaggerate or overstate the differences between participants (Gelman & Hill, 2007). By basing the estimates on just the data for a person, in each per-participant analysis, the no-pooling approach overfits the data.\nYou could say that the no-pooling approach gives us estimates that depend too much on the sample of data we have got, and are unlikely to be similar to the estimates we would see in other samples in future studies.\n\n\n\n\n\n\nTip\n\n\n\nThe no-pooling estimates are too strongly influenced by the data we are currently analyzing.\n\n\n\n\n\nIf we look closely at Figure 5, we can see that there are similarities as well as differences between participants. Our analysis must take both into account.\nWhat happens in mixed-effects models is that we pool information, calculating the estimates for each participant, in part based on the information we have for the whole sample (all participants, in complete pooling), in part based on the information we have about the specific participant (one participant, in no pooling). Thus, for example, the estimated intercept for a participant in a mixed-effects model is given by the weighted average (Snijders & Bosker, 2004) of:\n\nthe intercept estimate given by an analysis of just that participant’s data (no pooling estimate;\nand the intercept estimate given by analysis of all participants’ data (complete pooling estimate).\n\nThe weighted average will reflect our relative level of information about the participant’s responses compared to how much information we have about all participants’ responses.\nFor some participants, we will have less information – maybe they made many errors, so we have fewer correct responses for an analysis. For these people, because we have less information, the intercept estimate will get pulled (shrunk) towards the overall (complete pooling, all data) estimate.\nFor other participants, we have more information – maybe they made all correct responses. For these people, because we have more information, the intercept estimate will be based more on the data for each participant.\nTo make sense of what this means, think about the differences between participants in how much reliable information we can have, given our sample, about their average level of response speed or about how they are affected by experimental variables. Think back to my comments about Figure 4, about the differences between participants in how spread out the distributions of their log RT values are. Recall that I said that where participants’ responses are more spread out – just as where we have less observations for some participants than for others – we shall inevitably have less certainty about our estimates for the effects that influence their performance if we base our account on just their data. Mixed-effects models perform better – as prediction models – than no pooling approaches because they are not relying, for any participant, on just their sometimes unreliable data.\nWe can look again at a plot showing the data for each participant. Figure 6 presents a grid or trellis of plots, one plot per person. In each plot, you can see points corresponding to the RT of each response made by a participant to a stimulus word. In all plots, the pink line represents the complete pooling data model estimate of the effect of frequency on response RTs. In each plot, the green line represents the effect of frequency estimated using just the data for each participant, the no pooling estimates. Now, we also see blue lines that represent the mixed-effects model partial pooling estimates.\n\n\n\n\n\n\n\n\nFigure 6: Plot showing the relationship between logRT and log frequency (LgSUBTLCD) separately for each participant; pink line shows the complete pooling estimate green line shows the no-pooling estimate; and blue line shows the linear mixed-effects model partial pooling estimate\n\n\n\n\n\nIt is quite difficult to identify, in this sample, where the partial pooling and no pooling estimates differ. We can focus on a few clear examples. Figure 7) presents a grid of plots for just four participants. I have picked some extreme examples but the plot illustrates how: (1.) for some participants e.g. AA1 all estimates are practically identical; (2.) for some participants EB5 JL3 JP3 the no-pooling and complete-pooling estimates are really quite different and (3.) for some participants JL3 JP3 the no-pooling and partial-pooling estimates are quite different.\n\n\n\n\n\n\n\n\nFigure 7: Plot showing the relationship between logRT and log frequency (LgSUBTLCD) separately for each participant – for participants AA1, EB5, JL3 and JP3; pink line shows the complete pooling estimate green line shows the no-pooling estimate; and blue line shows the linear mixed-effects model partial pooling estimate\n\n\n\n\n\nIn general, partial pooling will apply both to estimates of intercepts and to estimates of the slopes of fixed effects like the influence of word frequency in reaction time. Likewise, if we consider this idea in general, we can see how it should work whether we are talking about groups or clusters of data grouped by participant or by stimulus or by school, class or clinic, etc.\nFormally, whether an estimate for a participant (in our example) is pulled more or less towards the overall estimate will depend not just on the number of data-points we have for that person. The optimal combined estimate for a participant is termed the Empirical Bayes estimate and the weighting – the extent to which the per-participant ‘estimate’ depends on the participant’s data or the overall data – depends on the reliability of the estimate (of the intercept or the frequency effect) given by analyzing that participant’s data (Snijders & Bosker, 2004). If you think about it, smaller samples – e.g. where a participant completed less correct responses – will give you less reliable estimates (and so will samples that show more variation).\nWhat we are looking at, here, is a form of regularization in which we use all the sources of information we can to ensure we take into account the variability in the data while not getting over-excited by extreme differences (McElreath, 2020). We want to see estimates pulled towards an overall average where we have little data or unreliable estimates. We can see how strongly estimates can be shrunk in a plot like Figure 8.\nFigure 8 illustrates the shrinkage effect. I plotted a scatterplot of intercept and slope parameters from each model (models with different kinds of pooling), and connect estimates for the same participant. The plot uses arrows to connect the different estimates for each participant, different estimates from no-pooling (per-participant) compared to partial-pooling (mixed-effects) models. The plot shows how more extreme estimates are shrunk towards the global average estimate.\n\n\n\n\n\n\n\n\nFigure 8: Plot illustrating shrinkage: big green and pink points show the complete pooling and partial pooling (average) estimates for the slope and intercept; orange and purple points show the no pooling (orange) and partial pooling (purple) estimates for each person; estimates for a person are connected by arrows to show the direction towards which no pooling estimates are pulled or shrunk\n\n\n\n\n\nWe can see how estimates are pulled towards the average intercept and frequency effect estimates. The shrinkage effect is stronger for more extreme estimates like JL3 JP3. It is weaker for estimates more (realistically) like the overall group estimates like AA1.\n\n\n\n\nBefore we move on, we can think briefly about how the mixed-effects models are estimated (Snijders & Bosker, 2004). Where do the numbers come from? I am happy to stick to a fairly non-technical intuitive explanation of the computation of LMEs but others, wishing to understand things more deeply, can find computational details in Pinheiro & Bates (2000), among other places. Mixed-effects models are estimated iteratively:\n\n\nIf we knew the random effects, we could find the fixed effects estimates by minimizing differences – like linear modeling.\nIf we knew the fixed effects – the regression coefficients – we could work out the residuals and the random effects.\n\nAt the start, we know neither, but we can move between partial estimation of fixed and random effect in an iterative approach.\n\nUsing provisional values for the fixed effects to estimate the random effects.\nUsing provisional values for the random effects to estimate the fixed effects again.\nTo converge on the maximum likelihood estimates of effects – when the estimates stop changing.\n\nIn mixed-effects models, the things that are estimated are the fixed effects (the intercept, the slope of the frequency effect, in our example), along with the variance and correlation terms associated with the random effects. Previously, I referred to the partial-pooling mixed-effects ‘estimates’ of the intercept or the frequency effect for each person, using the quotation marks because, strictly, these estimates are actually predictions, Best Unbiased Linear Predictions (BLUPs), based on the estimates of the fixed and random effects.\n\n\nMostly, our main concern, in working with mixed-effects models, is over what effects we should include, what model we should specify. But we should prepare for the fact sometimes happens that models fail to converge, which is to say, the model fitting algorithm fails to settle on some set of parameter estimates but has reached the limit in the number of iterations over which it has attempted to find a satisfactory set of estimates.\nIn my experience, convergence problems do arise, typically, if one is analyzing categorical outcome data (e.g accuracy) where there may be not enough observations to distinguish satisfactory estimates given a quite complex hypothesized model. In other words, you might run into convergence problems but it will not happen often and only where you are already dealing with quite a complex situation. We take a look at this concern in more depth, in the next class.\n\n\n\n\nUp to this point, we have discussed the empirical or conceptual reasons we should expect to take into account, in our model, the effects on the outcome due to systematic differences in the experimental variables, e.g., in stimulus word frequency frequency, or to random differences between participants or between stimuli. We can now think about how we should statistically evaluate the relative usefulness of these different fixed effects or random effects, where usefulness is judged in relation to our capacity to explain outcome variance, or to improve model fit to sample data. We shall take an approach that follows the approach set out by Baayen, Bates and others (Baayen et al., 2008b; Bates et al., 2015; Matuschek et al., 2017).\nIn this approach, we shall look at the choices that psychology researchers have to make. Researchers using statistical models are always faced with choices. As we have seen, these choices begin even before we start to do analyses, as when we make decisions about data-set construction (Steegen et al., 2016). The need to make choices is always present for all the kinds of models we work with. This may not always be obvious because, for example, in using some data analysis software, researchers may rely on defaults with limited indication that that is what they are doing.\n\n\n\n\n\n\nImportant\n\n\n\nJust because we are making choices does not mean we are operating subjectively in a non-scientific fashion. Rather, provided we work in an appropriate mode of transparency or reflexiveness, we can work with an awareness of our options and the context for the data analysis (see the very helpful discussion in Gelman & Hennig, 2017).\n\n\n\n\nIt is very common to see researchers using a process of model comparison to try to identify an account for their data in terms of estimates of fixed and random effects. A few key concepts are relevant to taking this approach effectively.\nWe will focus on building a series of models up to the most complex model supported by the data. What does model complexity mean here? I am talking about something like the difference between a model including just main effects (simpler) and a model including both main effects and the interaction between the effects (more complex), or, I am talking about a model included just fixed effects (simpler) versus a model including fixed effects as well as random effects (more complex).\nResearchers may engage in comparing models to examine if one or more random effects should be included in their linear mixed-effects model. They may not be sure if they should include all random effects, that is, all random effects that could be included, given a range of grouping variables, like participant, class or stimulus, and given a range of possible effects, such as whether slopes or intercepts might vary.\nResearchers may do model comparison to check if adding the effect of an experimental variable is justified. Maybe they are conducting an exploratory study in which they want to investigate if using some measurement variable helps to explain variation in the outcome. Perhaps they are conducting an experimental study in which they want to test if the experimental manipulation, or the difference between conditions, has an impact on the outcome.\n\n\n\n\n\n\nTip\n\n\n\nAcross these scenarios, we can test if an effect should be included or if its inclusion in a model is justified by comparing models with versus without the term that corresponds to the effect.\n\n\nIn some studies, researchers conduct model comparisons like this in order to obtain null hypothesis significance tests for the effects of the experimental variables.\nTypically, the model comparisons are focused on whether some measurement of model fit is or is not different when we do versus do not include the effect in question in the model.\n\n\nAs our discussion progresses, I think it would be helpful to reflect on some of the questions that you may be asking yourself.\n1. What about multiple comparisons?\nYou might well ask yourself:\n\nIf we engage in a bunch of comparisons to check if we should or should not include a variable, isn’t this just exploiting researcher degrees of freedom?\n\nOr, you might ask:\n\nIf we are conducting multiple tests on the same data, aren’t we running the risk of raising the Type I error (false positive) rate because we are doing multiple comparisons?\n\nI think these are good questions but, here, my task is to explain what people do, why they do it, and how it helps in your data analysis.\n2. Is any model the best?\n\nSo you are looking at models with varying fixed effects (fitted using ML) or models with varying random effects (fitted using REML). How do you decide which model is better?\n\nSome researchers argue that trying to decide which model is better or best is inappropriate (see e.g. a. Gelman, 2015). As the famous saying by George Box has it (Box, 1976): “All models are wrong.” 1 We may say, nevertheless, that some models are useful. Some models are more useful than others, perhaps, because they explain or predict outcomes better, depending on your criteria, and the cost-benefit analysis.\nHere, I will explain the model comparison process while acknowledging this point. This is because researchers often model comparison techniques to evaluate the relative usefulness of different alternate models.\n\n\n\n\nYou will often encounter, in the psychological research literature, Information Criteria statistics like BIC: they are understood within an approach: Information-theoretic methods. They are grounded in the insight that you have reality and then you have approximating models. The distance between a model and reality corresponds to the information lost when we use a model to approximate reality. Information criteria – AIC or BIC – are estimates of information loss. The process of model selection aims to minimize information loss.\nI will not discuss information criteria methods of model evaluation in detail, here, because psychologists frequently use the Likelihood Ratio Test method (Meteyard & Davies, 2020), see following. (Take a look at, e.g., Burnham (2004) for a readable discussion, if you are interested.) However, you should have some idea of what information criteria statistics (like AIC and BIC) mean because you will see these statistics in the outputs from model comparisons using the anova() function, which we shall review a bit later (Section 1.12.3).\nIn summary, Akaike showed you could estimate information loss in terms of the likelihood of the model given the data – Akaike Information Criteria, AIC:\n\n\n\\[\nAIC = -2ln(l) + 2k\n\\]\nWhere:\n\n\\(-2ln(l)\\) is -2 times the log of the likelihood of the model given the data,\nwhere \\((l)\\) the likelihood\nis proportional to the probability of observed data conditional on some hypothesis being true.\n\nYou want a more likely model – less information loss, closer to reality – you want more negative or lower AIC. You can identify models that are more likely – closer to reality – with models with less wide errors, i.e. smaller residuals.\nYou could better approximate reality by including lots of predictors, specifying a more complex model. Models with more parameters may fit the data better but some of those effects may be spurious. Adding \\(+ 2k\\) penalizes complexity, speaking crudely, and so helps us to focus on the more parsimonious less complex model that best fits the data.\nSchwartz proposed an alternative estimate – Bayesian Information Criteria: BIC:\n\n\n\\[\nBIC = -2ln(l) + kln(N)\n\\]\nWhere:\n\n\\(-2ln(l)\\) is -2 times the log of the likelihood of the model given the data.\n\\(+ kln(N)\\) is the number of parameters in the model times the log of the sample size.\n\nThus the penalty for greater complexity is heavier in BIC.\nWe see that AIC and BIC differ in the second term. A deeper difference is that AIC estimates information loss when the true model may not be among the models being considered while BIC assumes that the true model is within the set of models being considered.\nAt this point we just need to think about Model selection and judgment using AIC and BIC.\n\nCompare a simpler model: model 1, just main effects; model 2, main effects plus interactions.\n\nIf the more complex model better approximates reality then it will be more likely given the data.\nBIC or AIC will be closer to negative infinity: \\(-2ln(l)\\) will be larger e.g. 10 is better than 1000, -1000 better than -10.\n\nAIC and BIC should move in the same direction. They usually will.\nAIC will tend to allow more complex models and that may be necessary when the researcher is engaged in a more exploratory study or wants more accurate predictions (that would be better supported by maximising the information going into the model). Using the BIC will tend to favour simpler models and that may be necessary when the researcher seeks models that replicate over the long run. Maybe a simpler model will less likely include predictors estimated because they are needed to fit noise or random outcome variation.\n\n\n\nPinheiro & Bates (2000; see also Barr et al., 2013; Matuschek et al., 2017) recommend that models of varying predictor sets can be compared using Likelihood Ratio Test comparison (LRTs) where the simple model is nested inside the more complex model.\nThe “nested”, here, means that the predictors in the simpler model are a subset of the predictors in the more complex model. For example, you might have just main effects in the simpler model but both main and interaction effects in the more complex model. Or, in another example, you might have just random effects of subjects or items on intercepts in the simpler model but both random effects on intercepts and random effects on slopes of fixed effects in the more complex model.\n\n\n\n\n\n\nWarning\n\n\n\nWhen you compare models using the Likelihood ratio test, LRT, you are comparing alternate models of the same data.\n\n\nBarr et al. (2013) note that we can compare models varying in the fixed effects (but constant in the random effects) or models varying in the random effects (but constant in the fixed effects) using LRTs. I have frequently reported model comparisons using the Likelihood ratio test, LRT. In part, this is for analytic reasons: I can compare simple and complex models getting multiple information criteria statistics for the models being compared in one function call, anova([model1], [model2]. In part, it is for social pragmatic reasons: the LRT comparison yields a significance p-value so that I can say, using the comparison, something like “The more complex model provided a significantly better fit to observation (LRT comparison, … p \\(=\\) …”\nIn a Likelihood Ratio Test, the test statistic is the comparison of the likelihood of the simpler model with the more complex model. Fortunately for us, we can R to calculate the model likelihood and do the model comparison (Section 1.13.2).\nThe comparison of models works by division: we divide the likelihood of the more complex model by the likelihood of the simpler model, calculating a likelihood ratio.\n\\[\n\\chi^2 = 2log\\frac{likelihood-complex}{likelihood-simple}\n\\]\nThe likelihood ratio value is then compared to the \\(\\chi^2\\) distribution for a significance test. In this significance test, we assume the null hypothesis that the simpler model is adequate as an account of the outcome variance. We calculate the p-value for the significance test using a number for the degrees of freedom equal to the difference in the number of parameters of the models being compared.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nHow should you proceed when you decide to use mixed-effects models?\n\nI think the answer to that question depends on whether you are doing a study that is confirmatory or exploratory.\n\n\n\nIn short, if you have pre-registered the design of your study and, as part of that registration, you recorded the hypotheses you plan to test, as well as the analysis method you plan to use to test your hypotheses, then the answer is simple: fit the model you said you were going to use.\nThese days, if you have not pre-registered your analysis plans, you are practically-speaking engaged in exploratory work. If you are doing an exploratory study, then you will need to make some choices, in part, depending on the nature of the sample you are working with, and other aspects of the research context, but it will help to keep things simple.\nIn an exploratory study, I would keep things simple by comparing a series of models, fitted with different sets of predictor variables (fixed effects).\n\n\n\n\n\n\nWarning\n\n\n\nNote: if you are running mixed-effects models in R you cannot run lmer() models with just fixed effects.\n\n\nWhat I do is this: for a data-set like the ML study data, where the data were collected using a repeated-measures design:\n\nso that all participants saw all stimuli,\nand both participants and stimuli were sampled (from the wider populations of readers or words),\nthen I would run a series of models\nso that the different models have varying sets of fixed effects\nbut all models in the series have the same random effects: the random effects of subjects and items on intercepts.\n\nIn my experience, the estimates and associated significance levels associated with fixed effects can vary quite a bit depending on what other variables are included in the model. This has led me to take an approach where I am not varying too much how predictors are included in the model.\nAs noted, this will not really apply if you are doing an confirmatory study in which you are obliged to include the manipulated variables. However, if you are doing something a bit more exploratory then you might have to think about the kinds of predictors you include in your model, and how or when you include them.\n\n\n\n\n\n\nTip\n\n\n\nIn what order should you examine the usefulness of different sets of fixed effects?\n\nThis is a difficult question to answer and the difficulty is one reason why I think we need to be cautious when we engage in model comparison to try to get to a model of our data.\n\n\n\nMy advice would be to plan out in advance a sequence of model comparisons.\n\nYou should begin with simpler models with fewer effects.\nYou should begin with those effects whose impacts are well established and well understood by you.\nIf there is a whole set of well established effects typically included in an analysis in the field in which you are working, it might be sensible to include all the effects in a single step.\nThen, I would use subsequent incremental steps to increase model complexity by adding effects that are theoretically justified, i.e., hypothesized, but which may be new, or may depend on the experimental manipulation you are testing out.\n\nHaving established a model with some set of sensible fixed effects (guided by information criteria or LRT statistics), I would then turn my attention to the random effects component of the model. As noted, we may expect to see random differences between subjects (and possibly between items) in both the level of average performance – random effects of subjects or items on intercepts – and in the slopes of fixed effects – random effects of subjects or items on slopes.\nWhat I do is this:\n\nFor a data-set like ML’s, I examine firstly if both random effects of subjects and items on intercepts are required.\nI then check if random effects of subjects or items on slopes are additionally required in the model.\n\nThe distinction between exploratory and confirmatory studies breaks down, in my experience, when we start thinking about what random effects should be included in a model. It will be useful to review, here, Barr et al. (2013) and Matuschek et al. (2017) for an interesting discussion, and contrasting approaches.\n\n\nBefore we go any further, we need to briefly discuss one key choice that we face in working with mixed-effects models. This concerns the difference between Restricted Maximum Likelihood (REML) and Maximum Likelihood (ML) estimation methods. Both methods are iterative.\nThe lmer() function has defaults, like any analysis function, so we often do not need to make the choice explicit. We do need to when we compare models that vary in fixed effects, or in random effects.\n\nRestricted maximum likelihood\n\nIn R: REML=TRUE is stated in the lmer() function call.\n\nREML estimates the variance components while taking into account the loss of degrees of freedom resulting from the estimation of the fixed effects: REML estimates vary if the fixed effects vary.\nTherefore it is not recommended to compare the likelihood of models varying in fixed effects and fitted using REML (Pinheiro & Bates, 2000).\nThe REML method is recommended for comparing the likelihood of models with the same fixed effects but different random effects.\nREML is more accurate for random effects estimation.\n\n\nMaximum likelihood\n\nIn R: REML=FALSE is stated in thelmer() function call.\n\nML estimation methods can be used to fit models with varying fixed effects but the same random effects.\nML estimation: a good place to start when building-up model complexity – adding parameters to an empty model.\nPinheiro & Bates (2000) advise that the approach is anti-conservative (it will sometimes indicate effects where there are none there) but Barr et al. (2013) argue that their analyses suggest that that is not so.\n\n\n\n\n\n\n\nAs noted, it is recommended (Pinheiro & Bates, 2000) that we compare models of varying random effects using Restricted Maximum Likelihood (REML) fitting. We might be comparing different models with different sets of random effects if we are in the process of working out whether our model should include both random intercepts and random slopes. I think it is sensible to build up model complexity in the random component so that we are working through a series of model comparisons, comparing more simple with more complex models where the more complex model includes the same terms as the simpler model but adds some more.\nIn analyzing the effect of frequency on log RT for the ML study data, we can examine whether the random effects of subjects or of items on intercepts are necessary. Then we can examine if we should take into account random effects of subjects on the slope of the fixed effect of frequency, in addition to the random effects on intercepts.\nTo begin with, we look at a simpler model. We can fit a model with just the fixed effects of intercept and frequency, and the random effects of participants or items on intercepts only. We exclude the (LgSUBTLCD + ...|subjectID) specification for the random effect of participants on the slope of the frequency LgSUBTLCD effect.\nWe use REML fitting, as follows:\n\nML.all.correct.lmer.REML.si  &lt;- lmer(logrt ~ LgSUBTLCD + \n                                    \n                                          (1|subjectID) + (1|item_name),\n\n       data = ML.all.correct, REML = TRUE)\n\nsummary(ML.all.correct.lmer.REML.si)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: logrt ~ LgSUBTLCD + (1 | subjectID) + (1 | item_name)\n   Data: ML.all.correct\n\nREML criterion at convergence: -9845.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.5339 -0.6375 -0.1567  0.4364  5.5851 \n\nRandom effects:\n Groups    Name        Variance  Std.Dev.\n item_name (Intercept) 0.0003204 0.01790 \n subjectID (Intercept) 0.0032650 0.05714 \n Residual              0.0085285 0.09235 \nNumber of obs: 5257, groups:  item_name, 160; subjectID, 34\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)  2.887697   0.013253   217.9\nLgSUBTLCD   -0.034390   0.002774   -12.4\n\nCorrelation of Fixed Effects:\n          (Intr)\nLgSUBTLCD -0.658\n\n\nIf you look at the code chunk, you can see that:\n\nREML = TRUE is the only change to the code: it specifies the change in model fitting method;\nalso, I changed the model name to ML.all.correct.lmer.REML.si to be able to distinguish the maximum likelihood from the restricted maximum likelihood model.\n\nFollowing Baayen et al. (2008a), we can then run a series of models with just one random effect. Firstly, just the random effect of items on intercepts:\n\nML.all.correct.lmer.REML.i  &lt;- lmer(logrt ~\n\n       LgSUBTLCD + (1|item_name),\n\n       data = ML.all.correct, REML = TRUE)\n\nsummary(ML.all.correct.lmer.REML.i)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: logrt ~ LgSUBTLCD + (1 | item_name)\n   Data: ML.all.correct\n\nREML criterion at convergence: -8337\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.7324 -0.6455 -0.1053  0.4944  4.8970 \n\nRandom effects:\n Groups    Name        Variance  Std.Dev.\n item_name (Intercept) 0.0002364 0.01537 \n Residual              0.0117640 0.10846 \nNumber of obs: 5257, groups:  item_name, 160\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)  2.886765   0.009047  319.07\nLgSUBTLCD   -0.034206   0.002811  -12.17\n\nCorrelation of Fixed Effects:\n          (Intr)\nLgSUBTLCD -0.977\n\n\nSecondly, just the random effect of subjects on intercepts:\n\nML.all.correct.lmer.REML.s  &lt;- lmer(logrt ~\n\n       LgSUBTLCD + (1|subjectID),\n\n       data = ML.all.correct, REML = TRUE)\n\nsummary(ML.all.correct.lmer.REML.s)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: logrt ~ LgSUBTLCD + (1 | subjectID)\n   Data: ML.all.correct\n\nREML criterion at convergence: -9786.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.5843 -0.6443 -0.1589  0.4434  5.5266 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n subjectID (Intercept) 0.003275 0.05723 \n Residual              0.008837 0.09401 \nNumber of obs: 5257, groups:  subjectID, 34\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)  2.885751   0.011561  249.60\nLgSUBTLCD   -0.033888   0.001897  -17.87\n\nCorrelation of Fixed Effects:\n          (Intr)\nLgSUBTLCD -0.517\n\n\nIf we now run Likelihood Ratio Test comparisons of these models, we are effectively examining if one of the random effects can be dispensed with: if its inclusion makes no difference to the likelihood of the model then it is not needed. Is the random effect of subjects on intercepts justified?\n\nCompare models, first, with ML.all.correct.lmer.REML.si versus without ML.all.correct.lmer.REML.i the random effect of subjects on intercepts.\nThen compare models with ML.all.correct.lmer.REML.si versus without ML.all.correct.lmer.REML.s the random effect of items on intercepts.\n\n\nanova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.i, refit = FALSE)\nanova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.s, refit = FALSE)\n\n\n\nWe compare models using the anova() function.\n\nanova() does the model comparison, for the models named in the list in brackets.\n\nYou can do this for the foregoing series of models but notice that in the code we specify:\n\nrefit = FALSE`\n\nWhat happens if we do not add that bit? What you will see if you run the anova() function call, without therefit = FALSE argument – try it – is that you will then get the warning refitting model(s) with ML (instead of REML).\nWhy? The immediate reason for this warning is that we have to specify refit = FALSE because otherwise R will compare ML fitted models. The refitting occurs by default.\nWhat is the reason for the imposition of this default?\nYou will recall that Pinheiro & Bates (2000) advise that if one is fitting models with random effects the estimates are more accurate if the models are fitted using Restricted Maximum Likelihood (REML). That is achieved in the lmer() function call by adding the argument REML=TRUE. Pinheiro & Bates (2000) further recommend (see, e.g., pp.82-) that if you compare models:\n\nwith the same fixed effects\nbut with varying random effects\nthen the models should be fitted using Restricted Maximum Likelihood.\n\nR refits models, for the anova() comparison using ML even if we originally specified REML fitting. It does this to stop users from comparing REML-fitted models when those models are specified with different sets of fixed effects, as discussed here. The reason for this is explained by Ben Bolker (in this discussion): analyses of simulated data analyses suggest that it does not make much difference whether we use REML or ML when we are comparing models with the same fixed effects but varying random effects. It does matter very much, however, that we should fit models using ML when we are comparing models with the same random effects but differing fixed effects. You will remember that REML estimates vary if the fixed effects vary.\n\n\n\nWhen we run the anova() function call, it can be seen that the random effects of subjects on intercepts is required.\n\nanova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.i, refit = FALSE)\n\nData: ML.all.correct\nModels:\nML.all.correct.lmer.REML.i: logrt ~ LgSUBTLCD + (1 | item_name)\nML.all.correct.lmer.REML.si: logrt ~ LgSUBTLCD + (1 | subjectID) + (1 | item_name)\n                            npar     AIC     BIC logLik -2*log(L)  Chisq Df\nML.all.correct.lmer.REML.i     4 -8329.0 -8302.7 4168.5   -8337.0          \nML.all.correct.lmer.REML.si    5 -9835.1 -9802.3 4922.6   -9845.1 1508.1  1\n                            Pr(&gt;Chisq)    \nML.all.correct.lmer.REML.i                \nML.all.correct.lmer.REML.si  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIf you look at the results of the model comparison then you should notice:\n\nThe ML.all.correct.lmer.REML.si model is more complex than the ML.all.correct.lmer.REML.i model.\n\n\nML.all.correct.lmer.REML.si includes LgSUBTLCD + (1 | subjectID) + (1 | item_name)\nML.all.correct.lmer.REML.i includes LgSUBTLCD + (1 | item_name).\n\n\nThe more complex model ML.all.correct.lmer.REML.si has AIC (-9835.1) and BIC (-9802.3) numbers that are larger or more negative, and has a likelihood (4922.6) that is larger than the simpler model ML.all.correct.lmer.REML.i which has AIC (-8329.0), BIC (-8302.7) and likelihood (4168.5).\n\n\nThe \\(\\chi^2 = 1508.1\\) statistic, on 1 Df has a p-value of Pr(&gt;Chisq) &lt;2.2e-16.\n\nYou can say that the comparison of the model ML.all.correct.lmer.REML.si (with the random effect of participants on intercepts) versus the model ML.all.correct.lmer.REML.i (without the random effect of participants on intercepts) shows that the inclusion of the random effect of participants on intercepts is warranted by a significant difference in model fit. (I highlight here the language you can use in your reporting.)\nThe second model comparison shows that the random effects of items on intercepts is also justified.\n\nanova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.s, refit = FALSE)\n\nData: ML.all.correct\nModels:\nML.all.correct.lmer.REML.s: logrt ~ LgSUBTLCD + (1 | subjectID)\nML.all.correct.lmer.REML.si: logrt ~ LgSUBTLCD + (1 | subjectID) + (1 | item_name)\n                            npar     AIC     BIC logLik -2*log(L)  Chisq Df\nML.all.correct.lmer.REML.s     4 -9778.3 -9752.0 4893.2   -9786.3          \nML.all.correct.lmer.REML.si    5 -9835.1 -9802.3 4922.6   -9845.1 58.825  1\n                            Pr(&gt;Chisq)    \nML.all.correct.lmer.REML.s                \nML.all.correct.lmer.REML.si  1.723e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIf you look at the results of the model comparison then you should notice:\n\nThe ML.all.correct.lmer.REML.si model is more complex than the ML.all.correct.lmer.REML.s model.\n\n\nML.all.correct.lmer.REML.si includes LgSUBTLCD + (1 | subjectID) + (1 | item_name).\nML.all.correct.lmer.REML.s includes LgSUBTLCD + (1 | subjectID)\n\n\nThe more complex model ML.all.correct.lmer.REML.si has AIC (-9835.1) and BIC (-9802.3) numbers that are larger or more negative, and has a likelihood (4922.6) that is larger than the simpler model ML.all.correct.lmer.REML.s which has AIC (-9778.3) and BIC (-9752.0) and likelihood (4893.2).\n\n\nThe \\(\\chi^2 = 58.825\\) statistic, on 1 Df has a p-value of Pr(&gt;Chisq) 1.723e-14.\n\nYou can say that the comparison of a model ML.all.correct.lmer.REML.si with versus a model ML.all.correct.lmer.REML.s without the random effect of items on intercepts shows that the inclusion of the random effect of items on intercepts is warranted by a significant difference in model fit.\nI would conclude that both random effects of subjects and items on intercepts are required.\nWe can draw this conclusion because the difference between the model including just the random effect of items on intercepts anova-ML-all-correct-lmer-REML-i, or the model including just the random effect of subjects on intercepts anova-ML-all-correct-lmer-REML-s, compared to the model including both the random effect of items on intercepts and of subjects on intercepts anova-ML-all-correct-lmer-REML is significant. This tells us that the absence of the term accounting for the random effect of subjects on intercepts is associated with a significant decrease in model fit to data, in model likelihood.\n\n\n\n\nWe should next consider whether it is justified or warranted to include in our model a term capturing the random effect of participants in the slope of the frequency effect. We may hold or we may make theoretical assumptions that justify including this random effect. Some researchers might ask: does the inclusion of the random effect seem warranted by improved model fit to data?\nI should acknowledge, here, that there is an on-going discussion over what random effects should be included in mixed-effects models (see Meteyard & Davies (2020) for an overview). The discussion can be seen from a number of different perspectives. Key articles include those published by Baayen et al. (2008b), Bates et al. (2015), Barr et al. (2013) and Matuschek et al. (2017).\nYou could be advised that a mixed-effects model should include all random effects that make sense a priori, so, here, we are talking about the random effects of participants on intercepts and on the slopes of all fixed effects that are in your model (variances and covariances) as well as all the random effects of items on intercepts and on slopes. This is characterized as the keep it maximal approach, associated with Barr et al. (2013), though the discussion in that article is more nuanced than this sounds.\nOr, you could be advised that a mixed-effects model should only include those random effects that appear to be justified or warranted by their usefulness in accounting for the data. In practice, this may mean, you should include only those random effects that appear justified by improved model fit to data, as indicated by a model comparison (see e.g. Bates et al., 2015; Matuschek et al., 2017).\n\n\n\n\n\n\nTip\n\n\n\nI think, in practice, that maximal models can run into convergence problems. This means that many researchers adopt an approach which you could call: Maximum justifiable.\n\nThis involves fitting a model, including all the random effects that make sense,\nthat are justified by improved model fit to data (given a significance test, model comparison)\nfor a model that actually converges.\n\n\n\nAt present, viewpoints in discussions around the specification of random effects are associated with arguments that, as Barr et al. (2013) discuss, more comprehensive models appear to control the Type I (false positive) error rate better, or that, as Matuschek et al. (2017) argue, control over the risk of false positives may come at the cost of increasing the Type II (false negative) error rate).\nI think that it would seem to be axiomatic that a researcher should seek to account for all the potential sources of variance – fixed effects or random effects – that may influence observed outcomes. In practice, however, you may have insufficient data or inadequate measures to enable you to fit a model that converges with all the random effects, or to enable you to fit a model that converges that can estimate what may, in fact, be very small random effects variances or covariances. This is why some researchers are moving to adopt Bayesian mixed-effects modeling methods, as discussed by the developmental Psychologist, Michael Frank, for example, here. And as exemplified by my work here.\n\n\n\n\n\n\nWarning\n\n\n\nThis discussion raises a question.\n\nRandom slopes of what?\n\n\n\nIn general, and simplifying things a bit, if an effect is manipulated within grouping units then we should specify random effects terms that allow us to take into account random differences between groups (e.g, between participants, stimulus words, or classes) in intercepts or in the slopes of the effects of theoretical interest, the fixed effects. The language of within-subjects or between-subjects effects is common in statistical education in psychological science and, I guess, it is a legacy of the focus of that education on ANOVA. A nice explanation of the difference between within-subjects or between-subjects effects can be found in Barr et al. (2013).\nIn short, if a participant provides response data under multiple levels of an experimental condition, or in response to multiple levels of a predictor variable (e.g. a person responds to multiple words, with differing frequency levels) then we are going to estimate or test the effect of that condition or that variable as if the condition is manipulated within-subjects or as if the responses to the variable vary within-subjects. If and only if we are in this situation, we can draw a plot of the kind you see in Figure 6: where we may be able to see the way that the slope of the effect of the variable differs between participants. (In contrast, for example, outside of longitudinal studies, we would identify age as a between-subjects rather than a within-subjects variable and, if you think about it, we could not draw a grid of plots like Figure 6 to examine how the slope of the age effect might differ between participants.) In this situation, we can and should (as Barr et al. (2013) argue) specify random effects terms to account for between-participant differences in the slopes of the fixed effect.\nWe can examine the utility of random effects by comparing models with the same fixed effects but with varying random effects. We can specify a fixed effect term inside the random effects part of the mixed-effects model code, as we saw in Section 1.9.5.\n\nML.all.correct.lmer.REML.slopes  &lt;- lmer(logrt ~ LgSUBTLCD + \n                                           \n                                                (LgSUBTLCD + 1|subjectID) + (1|item_name),\n\n       data = ML.all.correct, REML = TRUE)\n\nLooking at the code:\n\nWith (LgSUBTLCD + 1 |subjectID) we specify a random effect of subjects on intercepts and on the slope of the frequency effects.\nWe do not specify – it happens by default – the estimation of the covariance of random differences among subjects in intercepts and random differences among subjects in the slope of the frequency effect.\n\nAnd as before, we can use anova() to check whether the increase in model complexity associated with the addition of random slopes terms is justified by an increase in model fit to data.\n\nanova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.slopes, refit = FALSE)\n\nData: ML.all.correct\nModels:\nML.all.correct.lmer.REML.si: logrt ~ LgSUBTLCD + (1 | subjectID) + (1 | item_name)\nML.all.correct.lmer.REML.slopes: logrt ~ LgSUBTLCD + (LgSUBTLCD + 1 | subjectID) + (1 | item_name)\n                                npar     AIC     BIC logLik -2*log(L)  Chisq Df\nML.all.correct.lmer.REML.si        5 -9835.1 -9802.3 4922.6   -9845.1          \nML.all.correct.lmer.REML.slopes    7 -9854.1 -9808.1 4934.0   -9868.1 22.934  2\n                                Pr(&gt;Chisq)    \nML.all.correct.lmer.REML.si                   \nML.all.correct.lmer.REML.slopes  1.047e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInspection of the results shows us that, here, adjusting the model to include random effects of subjects in the slopes of the fixed effect of word frequency does improve model fit to data. In this situationn, we can report that:\n\nThe inclusion of the random effect is warranted by improved model fit to data (\\(\\chi^2 (1 df) = 22.9, p &lt; .001\\))}\n\n\n\n\nIf you look at the fixed effects summary, you can see that we do not get p-values by default. To calculate p-values, we need to count residual degrees of freedom. The authors of the {lme4} library that furnishes the lmer() function do not (as e.g. Baayen et al., 2008b discuss) think that it is sensible to estimate the residual degrees of freedom for a model in terms of the number of observations. This is because the number of observations concerns one level of a multilevel data-set that might be structured with respect to some number of subjects, some number of items. This means that one cannot then accurately calculate p-values to go with the t-tests on the coefficients estimates; therefore they do not.\nWhile this makes sense to me (see comments earlier on Bayesian methods), Psychologists will often need p-values. This is now relatively easy.\nWe can run mixed-effects models with p-values from significance tests on the estimates of the fixed effects coefficients using the library(lmerTest).\n\nlibrary(lmerTest)\n\nML.all.correct.lmer.REML.slopes  &lt;- lmer(logrt ~ LgSUBTLCD + \n                                           \n                                                (LgSUBTLCD + 1|subjectID) + (1|item_name),\n\n       data = ML.all.correct, REML = TRUE)\n\nsummary(ML.all.correct.lmer.REML.slopes)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: logrt ~ LgSUBTLCD + (LgSUBTLCD + 1 | subjectID) + (1 | item_name)\n   Data: ML.all.correct\n\nREML criterion at convergence: -9868.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.6307 -0.6324 -0.1483  0.4340  5.6132 \n\nRandom effects:\n Groups    Name        Variance  Std.Dev. Corr \n item_name (Intercept) 0.0003268 0.01808       \n subjectID (Intercept) 0.0054212 0.07363       \n           LgSUBTLCD   0.0002005 0.01416  -0.63\n Residual              0.0084333 0.09183       \nNumber of obs: 5257, groups:  item_name, 160; subjectID, 34\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)  2.887997   0.015479 47.782839 186.577  &lt; 2e-16 ***\nLgSUBTLCD   -0.034471   0.003693 60.338786  -9.333 2.59e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n          (Intr)\nLgSUBTLCD -0.764\n\n\nBasically, the call to access the lmerTest library ensures that when we run the lmer() function we get a calculation of an approximation to the denominator degrees of freedom that enables the calculation of the p-value for the t-test for the fixed effects coefficient. An alternative, as I have noted (Section 1.12.3) is to compare models with versus without the effect of interest.\n\n\nIt will be useful for you to examine model comparisons with a different set of models for the same data.\nYou could try to run a series of models in which the fixed effects variable is something different, for example, the effect of word Length: or the effect of orthographic neighbourhood sizeOrtho_N:.\nI would consider the model comparisons in the sequence shown in the foregoing, one pair of models at a time, to keep it simple. When you look at the model comparison, ask: is the difference between the models a piece of complexity (an effect) whose inclusion in the more complex model is justified or warranted by improved model fit to data?\n\n\n\n\n\n\n\nHow do we report the analyses and their results?\nI think it may help if I analyze the structure and content of a results report that I did, in Davies et al. (2013). The report is published together with data and analysis code.\nIf you look at the report, you can identify the kinds of information that I think you should communicate.\n\nBecause it was an exploratory study, I started by reporting the comparison of models varying in fixed effects.\nI explain what predictors are included in each model.\nI explain how I make decisions about which model to select.\nI then go on to discuss the comparison of models varying in random effects.\n\nI think it is important to be as clear as possible about what model comparison process (if any) you may undertake.\n\nWe stepped through a series of models. Firstly, assuming the same random effects of subjects and items on intercepts, we compared models differing in fixed effects: a model (model 1) with just initialstress factors; a model (model 2) with initialstress factors plus linear effects due to the orthographic.form, frequency, semantic, and bigram.frequency factors; and lastly a model (model 3) with the same factors as model 2 but adding restricted cubic splines for the frequency and orthographic.form factors to examine the evidence for the presence of curvilinear effects of frequency and length (the orthographic.form factor loads heavily on length).\n\nNotice also that I try to standardize the language and structure of the paragraphs – that kind of repetition or rhythm helps the reader, I think, by making what is not repeated – the model specifications – more apparent. Your style may differ, however, and that’s alright.\n\nWe evaluated whether the inclusion of random effects was necessary in the final model (model 3) using LRT comparisons between models with the same fixed effects structure but differing random effects. Here, following Pinheiro & Bates (2000; see, also, Baayen, 2008), models were fitted using the REML=TRUE setting in lmer. We compared models that included: (i.) both random effects of subjects and items, as specified for model 3; (ii.) just the random effect of subjects; (iii.) just the random effect of items.\n\n\n\n\n\n\n\nTip\n\n\n\nI want you to notice something more, concerning the predictors included in each different model:\n\nI do not include predictors one at a time, I include predictors in sets.\n\n\n\nFor the Davies et al. (2013) data-set, I include first the set of phonetic coding variables then the set of psycholinguistic variables (see the paper for details). I include linear effects then additional terms allowing the effects to be curvilinear.\nFinally, you can see that I report model comparisons in terms of Likelihood Ratio Tests. I do this, firstly, in order to report comparisons conducted to examine the basis for selecting one model out of a set of possible models varying in fixed effects:\n\nComparing models 1 and 2, models with initialstress factors but differing in whether they did or did not include key psycholinguistic factors like orthographic.form, the LRT statistic was significant (\\(\\chi^2 = 1,007, 4 df, p = 2 * 10^-16\\)). Comparing models 2 and 3, i.e. models with initialstress and key psycholinguistic components but differing in whether they did or did not use restricted cubic splines to fit the orthographic.form and frequency effects, the LRT statistic was significant (\\(\\chi^2 = 23, 2 df, p = 1 * 10^-5\\)).\n\nThen I report the selection of models varying in random effects:\n\nWe compared models that included: (i.) both random effects of subjects and items, as specified for model 3; (ii.) just the random effect of subjects; (iii.) just the random effect of items. The difference between models (i.) and (ii.) was significant (\\(\\chi^2 = 185, 1 df, p = 2 * 10^-16\\)) indicating inclusion of an item effect was justified. The difference between models (i.) and (iii.) was significant (\\(\\chi^2 = 17,388, 1 df, p = 2 * 10^-16\\)) indicating inclusion of a subject effect was justified.\n\nIf you look at the report, you will see, also, that present a summary table showing the estimates for the fixed effects, and a series of plots indicating the predicted change in outcome (reading RT) given variation in the values of the predictor variables.\nIn summary, I think we can and should report both an outline of the process of development of the model or models we use to estimate the effects of interest, and the estimates we derive through the modeling.\n\n\n\n\n\n\nTip\n\n\n\nI would advise you to report:\n\nA summary of fixed effects – just like in linear models, with coefficient estimates, standard errors, t and p (if you use it);\nRandom effects variance and covariance (as applicable);\nModel building processes or model comparisons (if used).\n\nI recommend presenting the final model summary in a table that is structured like a multiple regression model summary table showing both random and the fixed effects.\n\n\nI also think it helps the reader to know what model is the basis for any estimates presented (see Meteyard & Davies, 2020 for further advice).\n\n\n\n\nWe examined another example of data from a repeated measures design study, this time, from a study involving adults responding to the lexical decision task, the ML study data-set.\nWe explored in more depth why linear mixed-effects models are more effective than other kinds of models when we are analyzing data with multilevel or crossed random effects structure. We discussed the critical ideas: pooling, and shrinkage. And we looked at how mixed-effects models employ partial-pooling so as to be more effective than alternative approaches dependent on complete pooling or no pooling estimates.\nMixed-effects models work better because they use both information from the whole data-set and information about each group (item or participant). This ensures that model estimates take into account random differences but are regularized so that they are not dominated by less reliable group-level information.\nWe considered, briefly, how mixed-effects models are estimated.\nThen we examined, in depth, how mixed-effects models are fitted, compared and evaluated. The model comparison approach was set out, and we looked at both practical steps and at some of the tricky questions that, in practice, psychologists are learning to deal with.\nWe discussed how to compare models with varying random or fixed effects. We focused, especially, on the comparison of models with varying random effects. Methods for model comparison, including the use of information criteria and the Likelihood Ratio Test, were considered.\nWe discussed p-values, questions about calculating them, and a simple method for getting them when we need to report significance tests.\nWe discussed how mixed-effects models should be reported.\n\n\nWe used two functions to fit and evaluate mixed-effects models.\n\nlmer() to fit mixed-effects models\nanova() to compare two or more models using AIC, BIC and the Likelihood Ratio Test.\nWe used the lmerTest library to furnish significance tests for coefficient estimates of fixed effects.\n\n\n\n\n\nThe most influential papers, at present, for the practice of mixed-effects modeling in psychological science are those by Baayen et al. (2008b), Bates et al. (2015), Barr et al. (2013) and Matuschek et al. (2017). Each of these papers makes critical points and, in my view, each is clearly written with a good use of examples grounded in the scenarios psychologists often encounter.\nBroader concerns about how are approach modeling, and what we look for as scientists, are discussed in Burnham (2004), Gelman & Hill (2007) and Gelman & Hennig (2017).\nA very useful FAQ on the practicalities of working with mixed-effects models can be found here.\n\n\n\n\n\n\nBaayen, R. H., Davidson, D. J., & Bates, D. M. (2008b). Mixed-effects modeling with crossed random effects for subjects and items. Journal of Memory and Language, 59(4), 390–412. https://doi.org/10.1016/j.jml.2007.12.005\n\n\nBaayen, R. H., Davidson, D. J., & Bates, D. M. (2008a). Mixed-effects modeling with crossed random effects for subjects and items. Journal of Memory and Language, 59(4), 390–412. https://doi.org/10.1016/j.jml.2007.12.005\n\n\nBalota, D. a., Yap, M. J., Cortese, M. J., Hutchison, K. a., Kessler, B., Loftis, B., Neely, J. H., Nelson, D. L., Simpson, G. B., & Treiman, R. (2007). The english lexicon project. Behavior Research Methods, 39(3), 445–459. https://doi.org/10.3758/BF03193014\n\n\nBarr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of Memory and Language, 68, 255–278.\n\n\nBates, D., Kliegl, R., Vasishth, S., & Baayen, H. (2015). Parsimonious mixed models. arXiv Preprint arXiv:1506.04967.\n\n\nBox, G. E. P. (1976). Science and statistics. Journal of the American Statistical Association, 71(356), 791–799. https://doi.org/10.2307/2286841\n\n\nBrysbaert, M., & New, B. (2009). Moving beyond kucera and francis: A critical evaluation of current word frequency norms and the introduction of a new and improved word frequency measure for american english. Behavior Research Methods, 41(4), 977–990. https://doi.org/10.3758/BRM.41.4.977\n\n\nBurnham, K. P. (2004). Multimodel inference: Understanding AIC and BIC in model selection. Sociological Methods & Research, 33(2), 261–304. https://doi.org/10.1177/0049124104268644\n\n\nClark, H. (Stanford. U. (1973). Clark_1973_LanguageAsAFixedEffectFallacy.pdf.\n\n\nCohen, J., Cohen, P., West, S. G., & Aiken, L. S. (2003). Applied multiple regression/correlation analysis for the behavioural sciences (3rd. edition). Lawrence Erlbaum Associates.\n\n\nDavies, R., Barbon, A., & Cuetos, F. (2013). Lexical and semantic age-of-acquisition effects on word naming in spanish. Memory and Cognition, 41(2), 297–311.\n\n\nForster, K. I., & Forster, J. C. (2003). DMDX: A windows display program with millisecond accuracy. Behavior Research Methods, Instruments, & Computers, 35, 116–124.\n\n\nGelman, a. (2015). The connection between varying treatment effects and the crisis of unreplicable research: A bayesian perspective. Journal of Management, 41(2), 632–643. https://doi.org/10.1177/0149206314525208\n\n\nGelman, A., & Hennig, C. (2017). Beyond subjective and objective in statistics. Journal of the Royal Statistical Society: Series A (Statistics in Society), 180(4), 967–1033.\n\n\nGelman, A., & Hill, J. (2007). Data analysis using regression and multilevel/hierarchical models. Cambridge University Press.\n\n\nHowell, D. C. (2016). Fundamental statistics for the behavioral sciences. Cengage learning.\n\n\nJudd, C. M., Westfall, J., & Kenny, D. A. (2012). Treating stimuli as a random factor in social psychology : A new and comprehensive solution to a pervasive but largely ignored problem. 103(1), 54–69. https://doi.org/10.1037/a0028347\n\n\nMasterson, J., & Hayes, M. (2007). Development and data for UK versions of an author and title recognition test for adults. Journal of Research in Reading, 30, 212–219.\n\n\nMatuschek, H., Kliegl, R., Vasishth, S., Baayen, H., & Bates, D. (2017). Balancing type i error and power in linear mixed models. Journal of Memory and Language, 94, 305–315. https://doi.org/10.1016/j.jml.2017.01.001\n\n\nMcElreath, R. (2020). : A bayesian course with examples in r and STAN (2nd ed.). Chapman; Hall/CRC. https://doi.org/10.1201/9780429029608\n\n\nMeteyard, L., & Davies, R. A. I. (2020). Best practice guidance for linear mixed-effects models in psychological science. Journal of Memory and Language, 112, 104092. https://doi.org/10.1016/j.jml.2020.104092\n\n\nPinheiro, J. C., & Bates, D. M. (2000). Mixed-effects models in s and s-plus (statistics and computing). Springer.\n\n\nRaaijmakers, J. G. W., Schrijnemakers, J. M. C., & Gremmen, F. (1999). How to deal with \"the language-as-fixed-effect fallacy\": Common misconceptions and alternative solutions. Journal of Memory and Language, 41(3), 416–426. https://doi.org/10.1006/jmla.1999.2650\n\n\nSnijders, T. A. B., & Bosker, R. J. (2004). Multilevel analysis: An introduction to basic and advanced multilevel modeling. Sage Publications Ltd.\n\n\nSteegen, S., Tuerlinckx, F., Gelman, A., & Vanpaemel, W. (2016). Increasing transparency through a multiverse analysis. Perspectives on Psychological Science, 11(5), 702–712.\n\n\nTorgesen, J. K., Rashotte, C. A., & Wagner, R. K. (1999). TOWRE: Test of word reading efficiency. Pro-ed Austin, TX.\n\n\nYarkoni, T., Balota, D., & Yap, M. (2008). Moving beyond coltheart’s n: A new measure of orthographic similarity. Psychonomic Bulletin & Review, 15(5), 971–979. https://doi.org/10.3758/PBR.15.5.971",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 18. Conceptual introduction to developing linear mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/03-mixed.html#sec-dev-mixed-motivations",
    "href": "PSYC412/part2/03-mixed.html#sec-dev-mixed-motivations",
    "title": "Week 18. Conceptual introduction to developing linear mixed-effects models",
    "section": "",
    "text": "Linear mixed-effects models are important, interesting, and sometimes challenging.\nWe have worked through a series of chapters in which we have aimed to learn:\n\nTo recognize the situations where we shall see multilevel structured data and therefore where we will need to apply multilevel or mixed-effects models.\nTo understand the nature and the advantages of these models: what they are, and why they work better than other kinds of models, given multilevel data.\nTo practice how we code for mixed-effects models, and how we read or write about the results.\n\nWe now need to develop our understanding and skills further. And we now need to examine some of the complexities that we may face when we work with mixed-effects models.\nOur approach will continue to depend on verbal explanation, visualization and a practical code-based approach to the modeling.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 18. Conceptual introduction to developing linear mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/03-mixed.html#sec-dev-mixed-ideas",
    "href": "PSYC412/part2/03-mixed.html#sec-dev-mixed-ideas",
    "title": "Week 18. Conceptual introduction to developing linear mixed-effects models",
    "section": "",
    "text": "Important\n\n\n\nShrinkage or regularization means that models of data should be excited by the data but not too excited.\n\n\nThis means our models work better if they are informed by all the data, and take into account random differences but also if they are not too strongly influenced by individual (participant or item) data.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 18. Conceptual introduction to developing linear mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/03-mixed.html#sec-dev-mixed-targets",
    "href": "PSYC412/part2/03-mixed.html#sec-dev-mixed-targets",
    "title": "Week 18. Conceptual introduction to developing linear mixed-effects models",
    "section": "",
    "text": "We are probably now at a stage, in the development of our skills and understanding, where we can be more specific about our targets for learning: what capacities or abilities we want to have by the time we complete the course. I have held back specifying the targets in this way because, first, we had to learn the basic vocabulary. Now that we have done that, we can lay out the targets against which we can assess the progression of our learning.\n\n\n\n\n\n\nImportant\n\n\n\nWe have three capacities we seek to develop. These include the capacity:\n\nto understand mixed-effects models;\nto work with these models practically or efficiently in R;\nand to communicate their results effectively (to ourselves and others).\n\n\n\nWe should be aware that the development of skills and understanding in relation to each of these capacities will travel at different speeds for different people, and within any person at different speeds for different capacities.\nWe should also be aware that our internal evaluation of our understanding will not exactly match the evaluation that comes from external assessment. In other words, we might not be satisfied with our understanding but, still, our understanding might be satisfactory. It might be that we can learn to say in words what mixed-effects models are or involve, or what their results mean, very effectively even if we remain unsure about our understanding.\nFor these reasons, I specify what we are aiming to develop in terms of what we can do. You can test your development against this checklist of targets for learning.\n\nWe want to develop the capacity to understand mixed-effects models, the capacity to:\n\n\nrecognize where data have a multilevel structure;\nrecognize where multilevel or mixed-effects models are required;\ndistinguish the elements of a mixed-effects model, including fixed effects and random effects;\nexplain how random effects can be understood in terms of random differences (or deviations) between groups or classes or individuals, in intercepts or slopes;\nexplain how random effects can be understood in terms of variances, as a means to account for random differences between groups or classes or individuals in intercepts or slopes;\nexplain how mixed-effects models work better than linear models, for multilevel structured data;\nexplain how mixed-effects models work better because they allow partial-pooling of estimates.\n\n\nWe want to develop the capacity to work practically with mixed-effects models in R, the capacity to:\n\n\nspecify a mixed-effects model in lmer() code;\nidentify how the mixed-effects model code varies, depending on the kinds of random effects that are assumed;\nidentify the elements of the output or results that come from an lmer() mixed-effects analysis;\ninterpret the fixed-effects estimates;\ninterpret the random effects estimates, including both the variance and covariance estimates.\n\n\nWe want to develop the capacity to communicate the results of mixed-effects models effectively, to ourselves and to others, the capacity to:\n\n\ndescribe in words and summary tables the results of a mixed-effects model;\nvisualize the effects estimates or predictions from a mixed-effects model.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 18. Conceptual introduction to developing linear mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/03-mixed.html#sec-dev-mixed-guide",
    "href": "PSYC412/part2/03-mixed.html#sec-dev-mixed-guide",
    "title": "Week 18. Conceptual introduction to developing linear mixed-effects models",
    "section": "",
    "text": "I have provided a collection of materials you can use.\nPreviously:\n\nWe learned about multilevel structured data in the conceptual introduction to multilevel data and the workbook introduction to multilevel data.\nWe then deepened our understanding by looking at the analysis of data from studies with repeated-measures designs in the conceptual introduction to linear mixed-effects models and the workbook introduction to mixed-effects models.\nThis week, you can practical exercies in the chapter on developing linear mixed-effects models.\n\nHere, I explain what they are and how I suggest you use them.\n1. Video recordings of lectures\n1.1. I have recorded a lecture in three parts. The lectures should be accessible by anyone who has the link.\n\nPart 1 – about 13 minutes\nPart 2 – about 13 minutes\nPart 3 – about 24 minutes\n\n1.2. I suggest you watch the recordings then read the rest of this chapter.\n\nThe lectures provide a summary of the main points.\n\n1.3. You can download the lecture slides in three different versions:\n\n402-week-19-LME-3.pdf: high resolution .pdf, exactly as delivered [1.3 MB];\n402-week-19-LME-3_1pp.pdf: printable version, one-slide-per-page [1.2 MB];\n402-week-19-LME-3_6pp.pdf: printable version, six-slides-per-page [1.3 MB].\n\nThe high resolution version is the version delivered for the lecture recordings. To make the slides easier to download, I produced lower resolution versions: 1pp and 6pp. These should be easier to download and print out if that is what you want to do.\n2. Chapter: 03-mixed\n2.1. I have written this chapter to discuss the main ideas and set out the practical steps you can follow to start to develop the skills required to analyse multilevel structured data with crossed random effects.\n2.2. The practical elements include data tidying, visualization and analysis steps.\n2.3. You can read the chapter, run the code, and do the exercises.\n\nRead in the example ML word recognition study data-set.\nEdit example code to create alternate visualizations of variable distributions and of the relationships between critical variables.\nExperiment with the .R code used to work with the example data.\nRun linear mixed-effects models of demonstration data.\nRun linear mixed-effects models of alternate data sets.\nReview the recommended readings (Section 1.16).\n\n3. Practical materials\n3.1 In the following sections, I describe the practical steps, and associated practical materials (exercise workbooks and data), you can use for your learning.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 18. Conceptual introduction to developing linear mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/03-mixed.html#sec-dev-mixed-data",
    "href": "PSYC412/part2/03-mixed.html#sec-dev-mixed-data",
    "title": "Week 18. Conceptual introduction to developing linear mixed-effects models",
    "section": "",
    "text": "In this chapter, we will be working with the ML word recognition study data-set. ML examined visual word recognition in younger and older adults using the lexical decision task.\nIn lexical decision, participants are presented with a stimulus: a string of letters that is either a real word (e.g., ‘car’) or a made-up or non-word (e.g., ‘cas’). Participants are required to respond to the stimulus by pressing a button to indicate either that they think the stimulus is a word or that they think it is a non-word. Each complete sequence of events, in which a stimulus is presented and a response is recorded, is known as a trial. In the lexical decision task implemented by ML, all study participants were presented with a mix of 160 word stimuli and 160 non-word stimuli, in random order, in a total of 320 trials.\nEach stimulus was presented one at a time on the computer screen. The critical outcome measure was the reaction time (RT) or latency for each response. Observed RT represents the interval of time from the moment the stimulus was first presented (the stimulus onset) to the moment the response was made (the response onset).\nLexical decision is a very popular technique for examining word recognition, especially in adults. While not every graduate student will be interested in word recognition, or reading, everyone should understand that tasks like lexical decision are similar to a range of other tasks used in experimental psychological science.\nThe critical feature of the study, here, is that we have an outcome – a decision response – observed multiple times (for each stimulus) for each participant. We shall be analyzing the speed of response, reaction time (RT), measured in milliseconds (ms).\nIn our analyses, the focus of our interest will be on the ways in which participant attributes (like age) or word properties (like frequency) influence the speed of response in a task designed measure the ability to recognize visually presented English words. In analyzing the effects of participant attributes on recognition response RTs, we will use data – about those attributes – that were recorded using a mix of survey questions (about age, etc.) and standardized ability tests that were administered to study participants alongside the lexical decision task.\nThe total number of participants for this study was 39, including a group of younger adults and a group of older adults. Information was collected about the participants’ age, education and gender. In addition, participants were asked to complete ability measures (TOWRE sight word and phonemic tests, Torgesen et al. (1999)) and a measure of reading experience (Author Recognition Test, ART, Masterson & Hayes (2007)).\n\n\nInstead of posing a simple and general research question, we shall orient our work around a set of quite specific predictions. ML hypothesized:\n\nEffects of stimulus attributes\n\n\nPredicting that words that are shorter, that look like more other words, and that appear frequently in the language will be easier to recognize.\n\n\nEffects of participant attributes\n\n\nPredicting that older readers would be faster and more accurate than younger readers in word recognition.\n\n\nEffects of interactions between the effects of word attributes and person attributes.\n\n\nPredicting that better (older) readers will show smaller effects of word attributes.\n\nIn this chapter, we can focus on one specific prediction as we work through the practical steps of conducting an analysis using linear mixed-effects models.\n\n\n\n\n\n\nNote\n\n\n\n\nResearch hypothesis: Words that are shorter, that look like more other words, and that appear frequently in the language will be easier to recognize.\n\n\n\n\n\n\nIn summary, ML collected data on lexical decision task response reaction times (RTs) and accuracy and information on participants, including age, reading ability and reading experience. In addition, she collected information on the properties of the lexical decision stimulus items, including variables like the length or frequency of words (values taken from the English Lexicon Project, Balota et al. (2007)).\nThe ML study data includes the following variables that we will work with (as well as some you can ignore):\n\nIdentifying variables\n\n\nsubjectID – identifying code for participants\nitem_name – words presented as stimuli\nitem_number – identifying code for words presented\n\n\nResponse variables\n\n\nRT – response reaction time (ms), for responses to words\n\n\nParticipant attribute variables\n\n\nAge – in years\nGender – coded M (male), F (female)\nTOWRE_wordacc – word reading skill, words read correctly (out of 104)\nTOWRE_nonwordacc – nonword reading skill, nonwords (made up words) read correctly (out of 63)\nART_HRminusFR – reading experience score\n\n\nStimulus property variables\n\n\nLength – word length, in letters\nOrtho_N – orthographic neighbourhood size, how many other words in English a stimulus word looks like\nOLD – orthographic Levenshtein distance, how many letter edits (addition, deletion or substitution) it would take to make a stimulus word look like another English word (a measure of orthographic neighbourhood) (Yarkoni et al., 2008)\nBG_Sum, BG_Mean, BG_Freq_By_Pos – measures of how common are pairs of letters that compose stimulus words\nSUBTLWF, LgSUBTLWF, SUBTLCD, LgSUBTLCD – measures of how common stimulus words are, taken from the SUBTLEX corpus analysis of word frequency (Brysbaert & New, 2009)\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nGet the data: get the data file and the .R script you can use to do the exercises that will support your learning.\n\nYou can download the files folder for this chapter by clicking on the link 03-mixed.zip.\n\n\n\nThe practical materials folder includes data files and an .R script:\n\nsubjects.behaviour.words-310114.csv which holds information about the (word) stimuli, participants, and the responses recorded in the ML study.\n\nThe .csv file is a comma separated values file and can be opened in Excel.\nThe data file is collected together with the .R script:\n\n402-03-mixed-workbook.R the workbook you will need to do the practical exercises.\n\nDuring practical sessions, each week, you can use the workbook to prompt your code construction and your thinking, with support.\n\n\nAfter practical sessions, you will be able to download an answers version of the workbook .R so check back here after the session.\n\n\n\n\n\n\nImportant\n\n\n\nGet the answers: get the data file and the .R script with answers.\n\nYou can download the files folder for this chapter by clicking on the link 03-mixed-answers.zip.\n\n\n\nThe link to this folder will not work until after a session has finished.\nWhen the link is live, you will be able to download a folder including:\n\n402-03-mixed-workbook-with-answers.R with answers to questions and example code for doing the exercises.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 18. Conceptual introduction to developing linear mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/03-mixed.html#sec-dev-mixed-tidy",
    "href": "PSYC412/part2/03-mixed.html#sec-dev-mixed-tidy",
    "title": "Week 18. Conceptual introduction to developing linear mixed-effects models",
    "section": "",
    "text": "In the Introduction to linear mixed-effects models chapter section on data tidying, we saw how we may need to tidy the data we collect in experimental studies: combining data about responses with data about participant attributes or stimulus properties, and restructuring the data so that they are in a tidy format. For this class, many steps in the process of data tidying were completed previously. Thus, we only need to perform steps 1, 3 and 4 of the usual data tidying process:\n\nImport the data or read the data into R, see Section 1.6.1;\nRestructure the data;\nSelect or transform variables, see Section 1.6.4;\nFilter observations, see Section 1.6.3.\n\nWe are going to first filter the observations, then transform the outcome variable. We will explain why we have to do this as we proceed.\nWe will use tidyverse library functions to do this work, as usual.\n\nlibrary(tidyverse)\n\n\n\nI am going to assume you have downloaded the data file, and that you know where it is. We use read_csv to read one file into R.\n\nML.all &lt;- read_csv(\"subjects.behaviour.words-310114.csv\", na = \"-999\")\n\nThe data file subjects.behaviour.words-310114.csv holds all the data about everything (behaviour, participants, stimuli) we need for our analysis work.\n\n\n\n\n\n\nTip\n\n\n\nIt is always a good idea to first inspect what you have got when you read a data file into R before you do anything more demanding.\n\nYou cannot assume that the data are what you think they are\nor that the data are structured or coded in the ways that you think (or have been told) they should be structured or coded.\n\n\n\nYou can inspect the first few rows of the data-set using head().\n\n\n\n\n\nitem_number\nsubjectID\nTest\nAge\nYears_in_education\nGender\nTOWRE_wordacc\nTOWRE_nonwordacc\nART_HRminusFR\nRT\nCOT\nSubject\nTrial.order\nitem_name\nLength\nOrtho_N\nBG_Sum\nBG_Mean\nBG_Freq_By_Pos\nitem_type\nSUBTLWF\nLgSUBTLWF\nSUBTLCD\nLgSUBTLCD\nOLD\n\n\n\n\n1\nGB9\nALT\n21\n11\nF\n78\n41\n18\n368.66\n134057.8\nGB9\n54\nwent\n4\n15\n249\n198\n29\nword\n411.51\n4.322\n79.6\n3.825\n1.2\n\n\n1\nNH1\nTAL\n52\n18\nM\n78\n56\n33\n724.83\n742737.4\nNH1\n148\nwent\n4\n15\n249\n198\n29\nword\n411.51\n4.322\n79.6\n3.825\n1.2\n\n\n1\nA15\nLTA\n21\n16\nF\n95\n57\n9\n483.71\n861801.0\nA15\n278\nwent\n4\n15\n249\n198\n29\nword\n411.51\n4.322\n79.6\n3.825\n1.2\n\n\n1\nB18\nTLA\n69\n11\nM\n85\n54\n10\n517.62\n1024583.4\nb18\n318\nwent\n4\n15\n249\n198\n29\nword\n411.51\n4.322\n79.6\n3.825\n1.2\n\n\n\n\n\n\n\nYou can examine all the variables using summary().\n\nsummary(ML.all)\n\n  item_number      subjectID             Test                Age       \n Min.   :  1.00   Length:5440        Length:5440        Min.   :16.00  \n 1st Qu.: 40.75   Class :character   Class :character   1st Qu.:21.00  \n Median : 80.50   Mode  :character   Mode  :character   Median :21.00  \n Mean   : 80.50                                         Mean   :36.94  \n 3rd Qu.:120.25                                         3rd Qu.:53.00  \n Max.   :160.00                                         Max.   :73.00  \n Years_in_education    Gender          TOWRE_wordacc    TOWRE_nonwordacc\n Min.   :11.00      Length:5440        Min.   : 68.00   Min.   :16.00   \n 1st Qu.:13.00      Class :character   1st Qu.: 84.00   1st Qu.:50.00   \n Median :16.00      Mode  :character   Median : 93.00   Median :55.50   \n Mean   :14.94                         Mean   : 91.24   Mean   :52.41   \n 3rd Qu.:16.00                         3rd Qu.: 98.00   3rd Qu.:57.00   \n Max.   :19.00                         Max.   :104.00   Max.   :63.00   \n ART_HRminusFR         RT               COT            Subject         \n Min.   : 1.00   Min.   :-2000.0   Min.   :  50094   Length:5440       \n 1st Qu.: 7.00   1st Qu.:  498.1   1st Qu.: 297205   Class :character  \n Median :11.00   Median :  577.6   Median : 552854   Mode  :character  \n Mean   :15.15   Mean   :  565.3   Mean   : 575780                     \n 3rd Qu.:21.00   3rd Qu.:  677.4   3rd Qu.: 810108                     \n Max.   :43.00   Max.   : 1978.4   Max.   :1583651                     \n  Trial.order     item_name             Length       Ortho_N      \n Min.   : 21.0   Length:5440        Min.   :3.0   Min.   : 0.000  \n 1st Qu.:100.8   Class :character   1st Qu.:4.0   1st Qu.: 3.000  \n Median :180.5   Mode  :character   Median :4.0   Median : 6.000  \n Mean   :180.5                      Mean   :4.3   Mean   : 7.069  \n 3rd Qu.:260.2                      3rd Qu.:5.0   3rd Qu.:11.000  \n Max.   :340.0                      Max.   :6.0   Max.   :24.000  \n     BG_Sum          BG_Mean       BG_Freq_By_Pos   item_type        \n Min.   :  3.00   Min.   :  1.00   Min.   :  1.0   Length:5440       \n 1st Qu.: 81.75   1st Qu.: 67.75   1st Qu.: 74.5   Class :character  \n Median :151.50   Median :153.50   Median :158.0   Mode  :character  \n Mean   :155.89   Mean   :153.82   Mean   :149.6                     \n 3rd Qu.:234.75   3rd Qu.:239.25   3rd Qu.:227.0                     \n Max.   :314.00   Max.   :316.00   Max.   :295.0                     \n    SUBTLWF          LgSUBTLWF        SUBTLCD        LgSUBTLCD    \n Min.   :   0.57   Min.   :1.477   Min.   : 0.32   Min.   :1.447  \n 1st Qu.:  17.36   1st Qu.:2.947   1st Qu.: 6.67   1st Qu.:2.748  \n Median :  69.30   Median :3.549   Median :23.64   Median :3.298  \n Mean   : 442.01   Mean   :3.521   Mean   :36.52   Mean   :3.137  \n 3rd Qu.: 290.70   3rd Qu.:4.171   3rd Qu.:65.24   3rd Qu.:3.739  \n Max.   :6161.41   Max.   :5.497   Max.   :99.70   Max.   :3.922  \n      OLD       \n Min.   :1.000  \n 1st Qu.:1.288  \n Median :1.550  \n Mean   :1.512  \n 3rd Qu.:1.750  \n Max.   :2.050  \n\n\nThe summary shows some features of the data-set, or of how R interprets the data-set, that are of immediate interest to us, though we do not necessarily have to do anything about them.\n\nWe can see statistical summaries – showing the mean, median, minimum and maximum, etc. – of numeric variables like the outcome variable RT.\nWe can see statistical summaries, also, of variables that comprise number values but which we do not want to be treated as numbers, e.g., the word stimulus coding variable item_number.\nWe can see that some variables are simply listed as Class: character. That tells us that one or more values in the columns in the datasheet that correspond to these variables are words or strings of letters or alphanumeric characters.\nThere is no sign of the presence of missing values in this data-set, no counts of NAs.\n\nWe do not really want R to treat a coding variable like item_number as numeric: it functions as a categorical or nominal variable, a factor. And we want R to treat coding variables like subjectID as factors. In the Introduction to multilevel data chapter section on coercion, we saw how we can require R to handle variables exactly as we require it to using coercion. In the Introduction to linear mixed-effects models chapter section on data loading, we saw how we can determine how R treats variables at the read-in stage, using col_types() specification. We are going to do neither here because we do not have to do this work; not doing it will have no impact on our analyses at this point.\nWhat we do need to do is deal with a problem that is already apparent in the summary statistics – did you spot it? If we look at the summary, we can see that RT includes values as low as -2000. That cannot be right.\n\n\n\nWe should examine the distribution of the outcome variable, lexical decision response reaction time (RT in ms). Observations about variable value distributions are a part of Exploratory Data Analysis and serve to catch errors in the data-set (e.g. incorrectly recorded scores) but also to inform the researcher’s understanding of their own data.\nWe shall examine the distribution of the outcome variable, lexical decision response reaction time (RT in ms), using density plots. An alternative method would be to use histograms. I choose to use density plots because they allow the easy comparison of the distributions of values of a continuous numeric variable like reaction time. A density plot shows a curve. You can say that the density corresponds to the height of the curve for a given value of the variable being depicted, and that it is related to the probability of observing values of the variable within some range of values (Howell, 2016).\nGetting a density plot of RTs of responses is easy using ggplot() code.\n\nML.all %&gt;%\n  ggplot(aes(x = RT)) +\n  geom_density(size=1.5) +\n  geom_rug(alpha = .2) +\n  ggtitle(\"Raw RT\") +\n  theme_bw()  \n\n\n\n\n\n\n\nFigure 1: Density plot showing word recognition reaction time, correct and incorrect responses\n\n\n\n\n\nThe code delivers a plot (Figure 1) showing three peaks in the distribution of RT values. You can see that there is a peak of RT observations around 500-1000ms, another smaller peak around -500ms, and a third smaller peak around -2000ms.\nThe density plot shows the reaction times recorded for participants’ button press ‘yes’ responses to word stimuli in the lexical decision task. The peaks of negative RTs represent observations that are impossible.\nRemember that reaction time, in a task like lexical decision, represents the interval in time between the onset of a task stimulus (in lexical decision, a word or a nonword) and the onset of the response (the button press to indicate the lexical decision). We cannot have negative time intervals. The explanation is that ML collected her data using the DMDX experimental software application (Forster & Forster, 2003). DMDX records the reaction times for incorrect responses as negative RTs.\nThe code to produce Figure 1 works in a series of steps.\n\nML.all %&gt;% takes the data-set, from the ML study, that we have read in to the R workspace and pipes it to the visualization code, next.\nggplot(aes(x = RT)) + creates a plot object in which the x-axis variable is specified as RT. The values of this variable will be mapped to geometric objects, i.e. plot features, that you can see, next.\ngeom_density(size=1.5) + first displays the distribution of values in the variable RT as a density curve. The argument size=1.5 tells R to make the line \\(1.5 \\times\\) the thickness of the line used by default to show variation in density.\n\nSome further information is added to the plot, next.\n\ngeom_rug(alpha = .2) + with a command that tells R to add a rug plot below the density curve.\nggtitle(\"Raw RT\") makes a plot title.\n\nNotice that beneath the curve of the density plot, you can see a series of vertical lines. Each line represents the x-axis location of an RT observation in the ML study data set. This rug plot represents the distribution of RT observations in one dimension.\n\ngeom_rug() draws a vertical line at each location on the x-axis that we observe a value of the variable, RT, named in aes(x = RT).\ngeom_rug(alpha = .2) reduces the opacity of each line, using alpha, to ensure the reader can see how the RT observations are denser in some places than others.\n\nYou can see that we have many more observations of RTs from around 250ms to 1250ms, where the rug of lines is thickest, under the peak of the density plot. This indicates what the two kinds of plots are doing.\n\n\nYou should try out alternative visualisation methods to reveal the patterns in the distribution of variables in the ML data-set (or in your own data).\n\nTake a look at the geoms documented in the {ggplot2} library reference section here.\nExperiment with code to answer the following questions:\n\n\nWould a histogram or a frequency polygon provide a more informative view? Take a look here for advice.\nWhat about a dotplot? Take a look here for advice\n\n\n\n\n\nThe density plot shows us that the raw ML lexical decision RT variable includes negative RT values corresponding to incorrect response. These have to be removed. We can do this quite efficiently by creating a subset of the original “raw” data, defined according to the RT variable using the {dpyr} library filter() function.\n\nML.all.correct &lt;- filter(ML.all, RT &gt;= 200)\n\nThe filter code is written to subset the data by rows using a condition on the values of the RT variable.\nML.all.correct &lt;- filter(ML.all, RT &gt;= 200) works as follows.\n\nML.all.correct &lt;- filter(ML.all ...) creates a new data-set with a new name ML.all.correct from the old data-set ML.all using the filter() function.\nfilter(... RT &gt;= 200) specifies an argument for the filter() function.\n\nIn effect, we are asking R to check every value in the RT column.\n\nR will do a check through the ML.all data-set, row by row.\nIf a row includes an RT that is greater than or equal to 200 then that row will be included in the new data-set ML.all.correct. This is what I mean by using a condition.\nBut if a row includes an RT that is less than 200, then that row will not be included. We express this condition as RT &gt;= 200.\n\nAfter we have removed negative (error) RTs, we check that the size of the data-set – here, the number of rows – matches our expectations. We do this to make sure that we did the filter operation correctly.\n\nlength(ML.all$RT)\n\n[1] 5440\n\nlength(ML.all.correct$RT)\n\n[1] 5257\n\n\nIf you run the length() function calls then you should see that the length or number of observations or rows in the ML.all.correct data-set should be smaller than the number of observations in the ML.all data-set.\n\n\n\n\n\n\nTip\n\n\n\nIt is wise to check that the operations you perform to tidy, process or wrangle data actually do do what you mean them to do. Checks can be performed, for each processing stage, by:\n\nForming expectations or predictions about what the operation is supposed to do e.g. filter out some rows by some number;\nCheck what you get against these predictions e.g. count the number of rows before versus after filtering.\n\n\n\nThe length() function will count the elements in whatever object is specified as an argument in the function call.\n\nThis means that if you put a variable name into the function as in length(data$variable) it will count how long that variable is – how many rows there are in the column.\nIf that variable happens to be, as here, part of a data-set, the same calculation will tell you how many rows there are in the data-set as a whole.\nIf you just enter length(data), naming some data-set, then the function will return a count of the number of columns in the data-set.\n\nHaving obtained a new data frame with data on just those trials where responses were correct, we can plot the distribution of RTs for just the correct responses (Figure 2).\n\nML.all.correct %&gt;%\n  ggplot(aes(x = RT)) +\n  geom_density(size=1.5) + \n  geom_rug(alpha = .2) +\n  ggtitle(\"Correct RTs\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 2: Density plot showing word recognition reaction time, correct responses only\n\n\n\n\n\n\n\nVary the filter conditions in different ways.\n\nChange the threshold for including RTs from RT &gt;= 200 to something else: you can change the number, or you can change the operator from &gt;= to a different comparison (try =, &lt;, &lt;=, &gt;.\nCan you assess what impact the change has?\n\nNote that you can count the number of observations (rows) in a data-set using e.g. length().\n\n\n\nI choose to filter out or exclude not only error responses (where \\(RT &lt; 0ms\\)) but also short reaction times (where \\(RT &lt; 200ms\\)). I think that any response in the lexical decision task that is recorded as less than 200ms cannot possibly represent a real word recognition response. Participants who complete experimental psychological tasks can and do press the button before they have time to engage the psychological processes (like word recognition) that the tasks we administer are designed to probe (like lexical decision).\nThere is some relevant literature that concerns the speed at which neural word recognition processes operate. However, I think you should note that the threshold I am setting for exclusion, here, is essentially arbitrary. If you think about it, I could have set the threshold at any number from \\(100-300ms\\) or some other range.\n\n\n\n\n\n\nWarning\n\n\n\nWhat is guiding me in setting the filter threshold is experience. But other researchers will have different experiences and set different thresholds.\n\nThis is why using exclusion criteria to remove data is problematic.\n\n\n\nFiltering or re-coding observations is an important element of the research workflow in psychological science. How we do or do not remove observations from original data may have an impact on our results (as explored by Steegen et al. (2016)). It is important, therefore, that we learn how to do this reproducibly using, for example, R scripts that we can share with our research reports.\nI would argue that, at minimum, a researcher should report their research including:\n\nWhat exclusion criteria they use to remove data, explaining why.\nReport analyses with and without exclusions, to indicate if their results are sensitive to their decisions.\n\nYou can read further information about the practicalities of using R to do filtering here.\nYou can read a brief discussion of the impacts of researcher choices in data-set construction in Steegen et al. (2016).\n\n\n\n\nFigure 2 shows that we have successfully removed all errors (negative RTs) but now we see just how skewed the RT distribution is. Note the long tail of longer RTs.\nMost researchers assume that participants – healthy young adults – take about 500-1000ms to perform the task and that values outside that range correspond to either fast guesses (RTs that are too short) or to distracted or tired or bored responses (RTs that are too long). In theory, the lexical decision task should be probing automatic cognitive processes, measuring the steps from perception to visual word recognition in the time interval between the moment the stimulus is first shown and the moment the button is pressed by the participant to indicate a response. Thus, it might seem natural to exclude extreme RT values which might correspond not to automatic cognitive processes but to unknowable distraction events or boredom and inattention. However, we shall complete no further data exclusions.\nFor now, we can look at a commonly used method to deal with the skew that we typically see when we examine reaction time distributions. RT distributions are usually skewed with a long tail of longer RTs. You can always take longer to press the button but there is a limit to how much faster you can make your response.\nGenerally, we assume that departures from a model’s predictions about our observations (the linear model residuals) are normally distributed, and we often assume that the relationship between outcome and predictor variables is linear (Cohen et al., 2003). We can ensure that our data are compliant with both assumptions by transforming the RT distribution.\nIt is not cheating to transform variables. Transformations of data variables can be helpful for a variety of reasons in the analysis of psychological data (Cohen et al., 2003; Gelman & Hill, 2007). I do recommend, however, that you are careful to report what transformations you use, and why you do them.\nPsychology researchers often take the log (often the log base 10) of RT values before performing an analysis. Transforming RTs to the log base 10 of RT values has the effect of correcting the skew – bringing the larger RTs ‘closer’ (e.g., \\(1000 = 3\\) in log10) to those near the middle which do not change as much (e.g. \\(500 = 2.7\\) in log10).\n\nML.all.correct$logrt &lt;- log10(ML.all.correct$RT)            \n\nThe log10() function works as follows:-\n\nML.all.correct$logrt &lt;- log10(...) creates a a new variable logrt, adding it to the ML.all.correct data-set. The variable is created using the transformation function log10().\nlog10(ML.all.correct$RT) creates a the new variable by transforming (to log10) the values of the old variable, RT.\n\nWe can see the effect of the transformation if we plot the log10 transformed RTs (see Figure 3). We arrive at a distribution that more closely approximates the normal distribution.\n\nML.all.correct %&gt;%\n  ggplot(aes(x = logrt)) +\n  geom_density(size = 1.5) + \n  geom_rug(alpha = .2) +\n  ggtitle(\"Correct log10 RTs\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 3: Density plot showing log10 transformed reaction time, correct responses only\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThere are other log transformation functions and we often see researchers using the natural log instead of the log base 10 as discussed here\n\n\n\n\n\nEven when data have been structured appropriately, we will still, often, need to do some tidying before we can do an analysis. Most research work involving quantitative evidence requires a big chunk of data tidying or other processing before you get to the statistics.\nOur data are now ready for analysis.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 18. Conceptual introduction to developing linear mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/03-mixed.html#sec-dev-mixed-crossed-random",
    "href": "PSYC412/part2/03-mixed.html#sec-dev-mixed-crossed-random",
    "title": "Week 18. Conceptual introduction to developing linear mixed-effects models",
    "section": "",
    "text": "As we saw in the Introduction to linear mixed-effects models chapter, many Psychologists conduct studies where it is not sensible to think of observations as being nested (Baayen et al., 2008a). In this chapter, we turn to the ML word recognition study data-set, which has a structure similar to the CP study data that we worked with previously. Again, the core concern is that the data come from a study with a repeated-measures design where the experimenter presented multiple stimuli for response to each participant, for several participants, so that we have multiple observations for each participant and multiple observations for each stimulus. Getting practice with this kind of data will help you to easily recognize what you have got when you see it in your own work.\nML asked all participants in a sample of people to read a selection of words, a sample of words from the language.\nFor each participant, we will have multiple observations and these observations will not be independent. One participant will tend to be slower or less accurate compared to another. Her responses may be more or less susceptible to the effects of the experimental variables. The lowest trial-level observations can be grouped with respect to participants. However, the data can also be grouped by stimuli.\nFor each stimulus word, there are multiple observations and these observations will not be independent. One stimulus may prove to be more challenging to all participants compared to another, eliciting slower or less accurate responses on average. In addition, if there are within-items effects, we may ask if the impact of those within-items effects is more prominent, stronger, among responses to some items compared to others.\nGiven this common repeated-measures design, we can analyse the outcome variable in relation to:\n\nfixed effects: the impact of independent variables like participant reading skill or word frequency;\nrandom effects: the impact of random or unexplained differences between participants and also between stimuli.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 18. Conceptual introduction to developing linear mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/03-mixed.html#sec-dev-mixed-working-lme",
    "href": "PSYC412/part2/03-mixed.html#sec-dev-mixed-working-lme",
    "title": "Week 18. Conceptual introduction to developing linear mixed-effects models",
    "section": "",
    "text": "We are going to respond to the multilevel (or crossed random effects) structure in the data by using linear mixed-effects models to analyze the data. This week, we are going to look at what mixed-effects models do from a new perspective.\nOur concern will be with different ways of thinking about why mixed-effects models are superior to linear models where data have a multilevel structure. Mixed-effects models tend to be more accurate in this (very common) situation because of what is called partial pooling and shrinkage or regularization. We use our practical example to explore these ideas.\n\n\nTo get started, we can examine – for each individual separately – the distribution of log RT observations, in Figure 4.\n\nML.all.correct %&gt;%\n  group_by(subjectID) %&gt;%\n  mutate(mean_logrt = mean(logrt, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(subjectID = fct_reorder(subjectID, mean_logrt)) %&gt;%\n  ggplot(aes(x = logrt)) +\n  geom_density(size = 1.25) +\n  facet_wrap(~ subjectID) +\n  geom_vline(xintercept = 2.778807, colour = \"red\", linetype = 2) +\n  scale_x_continuous(breaks = c(2.5,3)) +\n  ggtitle(\"Plot showing distribution of logRT for each participant; red line shows mean log10 RT\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 4: Density plot showing log10 transformed reaction time, correct responses, separately for each participant\n\n\n\n\n\nFigure 4 shows that RT distributions vary considerably between people. The plot imposes a dashed red line to indicate where the mean log10 RT is, calculated over all observations in the data-set. The plot shows the distribution of log RT for each participant, as a density drawn separately for each person. The individual plots are ordered by the mean log RT calculated per person, so plots appear in order from the fastest to the slowest.\nThe grid of plots illustrates some interesting features about the data in the ML study sample. You can see how the distribution of log RT varies between individuals: some people show widely spread reaction times; some people show quite tight or narrow distributions. You can see how the shapes of the distributions varies: some people show skew; others do not. I do not see that the variation in the shapes of the distributions is related to the average speed of the person’s responses.\nI think the key message of the plot is that some distributions are wider (RTs are more spread out) than others. We might be concerned that people who present more variable reaction times (wider distributions) may be associated with less reliable estimates of their average response speed, or of the impact of word attributes (like word frequency) on their response speed.\n\n\nThe plotting code I used to produce Figure 4 progresses through a series of steps. This example demonstrates how you can combine data tidying and plotting steps in a single sequence, using tidyverse functions and the %&gt;% pipe, so I will take the time to explain what is going on.\nMy aim is to create a grid of individual plots, showing the distribution of log RTs for each participant, so that the plots are presented in order, from the fastest participant to the slowest. Take a look at the plotting code. We can explain how it works, step by step.\n\nML.all.correct %&gt;%\n  group_by(subjectID) %&gt;%\n  mutate(mean_logrt = mean(logrt, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(subjectID = fct_reorder(subjectID, mean_logrt)) %&gt;%\n  ggplot(aes(x = logrt)) +\n  geom_density(size = 1.25) +\n  facet_wrap(~ subjectID) +\n  geom_vline(xintercept = 2.778807, colour = \"red\", linetype = 2) +\n  scale_x_continuous(breaks = c(2.5,3)) +\n  ggtitle(\"Plot showing distribution of logRT for each participant; red line shows mean log10 RT\") +\n  theme_bw()\n\nYou will see that we present the distribution of RTs using geom_density() and that we present a separate plot for each person’s data using facet_wrap(). To these elements, we add some pre-processing steps to calculate the average response speed of each individual, and to reorder the data-set by those averages.\nIt will make it easier to understand what is going on if we consider the code in chunks.\nFirst, we pre-process the data before we feed it into the plotting code.\n\nML.all.correct %&gt;%\n  group_by(subjectID) %&gt;%\n  mutate(mean_logrt = mean(logrt, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(subjectID = fct_reorder(subjectID, mean_logrt)) %&gt;%\n  ...\n\n\nML.all.correct %&gt;% takes the selected filtered data-set ML.all.correct and pipes it %&gt;% to the next step.\ngroup_by(subjectID) %&gt;% tells R to group the data by subject ID. We have a set of multiple log RT observations for each subjectID because each participant was asked to respond to multiple word stimuli.\nmutate(mean_logrt = mean(logrt, na.rm = TRUE)) next calculates and stores the mean log RT for each person. We create a new variable mean_logrt. We calculate the average of the set of log RTs recorded for each subjectID and construct the new variable mean_logrt from these averages.\n\nWe do not need to treat the data in groups so we remove the grouping, next.\n\nUsing ungroup() %&gt;% means that, having grouped the data to calculate the mean log RTs, we ungroup the data-set so that R can look at all observations in the next step.\nmutate(subjectID = fct_reorder(subjectID, mean_logrt)) %&gt;% asks R to look at all log RT observations in the data-set, and change the top-to-bottom order of the rows.\n\nWe ask R to order observations – using subjectID in fct_reorder() – so that each person’s data are listed by their average speed, mean_logrt from the fastest to the slowest. We then pipe these ordered data to the plotting code, next.\nIf you delete or comment out these first lines, you will see that R uses just a default ordering, drawing the plot for each person in the alphabetical order of their subjectID codes.\nTry it. Don’t forget to start with ML.all.correct %&gt;%.\nSecond, we draw the plots, using the data we have pre-processed.\n\nML.all.correct %&gt;%\n...\n  ggplot(aes(x = logrt)) +\n  geom_density(size = 1.25) +\n  facet_wrap(~ subjectID) +\n...\n\nThe key functions that create a grid of density plots are the following.\n\nggplot(aes(x = logrt)) tells R to work with logrt as the x-axis variable. We shall be plotting the distribution of logrt.\ngeom_density(...) draws a density plot to show the distribution of log RT, using a thicker line size = 1.25\nfacet_wrap(~ subjectID) creates a different plot for each level of the subjectID factor: we want to see a separate plot for each participant.\n\n\nfacet_wrap(~ subjectID) works to split the data-set up by participant, with observations corresponding to each participant identified by their subjectID, and to then split the plotting to show the distribution of log RT separately for each participant.\n\nI wanted to present the plots in order of the average speed of response of participants. If you look at Figure 4, you can see that the position of the peak of the log RT distribution for each participant moves, from the fastest plots where the peak is around \\(log RT = 2.5\\) (shown from the top left of the grid), to the slowest plots where the peak is around \\(log RT = 2.75\\) (shown towards the bottom right of the grid)\nWe can then use further ggplot functions to edit the appearance of the plot, to make it more useful.\n\n...\n  geom_vline(xintercept = 2.778807, colour = \"red\", linetype = 2) +\n  scale_x_continuous(breaks = c(2.5,3)) +\n  ggtitle(\"Plot showing distribution of logRT for each participant; red line shows mean log10 RT\") +\n  theme_bw()\n\n\ngeom_vline(xintercept = 2.778807, colour = \"red\", linetype = 2) draws a vertical red dashed line at the location of the mean log RT, the average of all log RTs over all participants in the data-set.\nscale_x_continuous(breaks = c(2.5,3)) adjusts the x-axis labeling. The ggplot default might draw too many x-axis labels i.e. showing possible log RT values as tick marks on the bottom line of the plot. I want to avoid this as sometimes all the labels can be crowded together, making them harder to read.\n\n\nDrawing a vertical line at the mean calculated overall is designed to help the reader (you) calibrate their comparison of the data from different people.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIt is often useful to experiment with example code to figure out how it works.\n\n\nOne way you can do this is by commenting out one line of code, at a time by putting the # at the start of the line.\n\nIf you do this, you can see what the line of code does by, effectively, asking R to ignore it.\n\nAnother way you can experiment with code is by seeing what you can change and what effect the changes have.\n\nCan you work out how to adapt the plotting code to show a grid of histograms instead of density plots?\nCan you work out how to adapt the code to show a grid of plots indicating the distribution of log RT by different words instead of participants?\n\n\n\n\n\nAs we have discussed in previous chapters, a good way to approach a mixed-effects analysis is by first estimating the effects of the experimental variables (here, frequency) using linear models, ignoring the hierarchical structure in the data.\n\n\n\n\n\n\nNote\n\n\n\nA linear model of multilevel structured data can be regarded as an approximation to the better analysis.\n\n\nWe model the effects of interest, using all the data (hence, complete pooling) but ignoring the differences between participants. This means we can see something of the ‘true’ picture of our data through the linear model results but the linear model misses important information, which the mixed-effects model will include, that would improve its performance.\nAs we saw, in a similar analysis in the Introduction to linear mixed-effects models chapter, we can estimate the relationship between reading reaction times (here, lexical decision RTs) and word frequency using a linear model:\n\\[\nY_{ij} = \\beta_0 + \\beta_1X_j + e_{ij}\n\\]\nWhere:\n\n\\(Y_{ij}\\) is the value of the observed outcome variable, the log RT of the response made by the \\(i\\) participant to the \\(j\\) item;\n\\(\\beta_1X_j\\) refers to the fixed effect of the explanatory variable (here, word frequency), where the frequency value \\(X_j\\) is different for different words \\(j\\), and \\(\\beta_1\\) is the estimated coefficient of the effect due to the relationship between response speed and word frequency;\n\\(e_{ij}\\) is the residual error term, representing the differences between observed \\(Y_{ij}\\) and predicted values (given the model) for each response made by the \\(i\\) participant to the \\(j\\) item.\n\nThe linear model is fit in R using the lm() function.\n\nML.all.correct.lm  &lt;- lm(logrt ~\n                             \n                             LgSUBTLCD,     \n                           \n                           data = ML.all.correct)\n\nsummary(ML.all.correct.lm)\n\n\nCall:\nlm(formula = logrt ~ LgSUBTLCD, data = ML.all.correct)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.41677 -0.07083 -0.01163  0.05489  0.53411 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.885383   0.007117  405.41   &lt;2e-16 ***\nLgSUBTLCD   -0.033850   0.002209  -15.32   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1095 on 5255 degrees of freedom\nMultiple R-squared:  0.04277,   Adjusted R-squared:  0.04259 \nF-statistic: 234.8 on 1 and 5255 DF,  p-value: &lt; 2.2e-16\n\n\nIn the estimates from this linear model, we see an approximate first answer to our prediction.\n\n\n\n\n\n\nNote\n\n\n\n\nResearch hypothesis: Words that are shorter, that look like more other words, and that appear frequently in the language will be easier to recognize.\nResult: We can see that, in this first analysis, the estimated effect of word frequency is \\(\\beta = -0.033850\\).\n\n\n\nI know this looks like a very small number but you should realize that the estimates for the coefficients of fixed effects like the frequency effect are scaled according to the outcome. Here, the outcome is log10 RT, where a log10 RT of 3 equals 1000ms, and, as we can calculate in R\n\nlog10(0.925)\n\n[1] -0.03385827\n\n\nAlso, remember that frequency is scaled in logs too, so the estimate of the coefficient tells us how log10 RT changes for unit change in log frequency. The coefficient represents the estimated change in log10 RT for unit change in log frequency LgSUBTLCD.\nThe estimate indicates that as word log frequency increases, responses logRT decreases by \\(-0.033850\\).\nIn this model, all the information from all participants is analyzed. In discussions of mixed-effects analyses, we say that this is a complete pooling model. This is because all the data have been pooled together, that is, we use all observations in the sample to estimate the effect of frequency.\nIn this model, the observations are assumed to be independent. However, we suppose that the assumption of independence is questionable given the expectation that participants will differ in their overall speed, and in the extent to which their response speed is affected by factors like word frequency.\n\n\n\nVary the linear model using different outcomes or predictors.\n\n\nThe ML study data, like the CP study data, are rich with possibility. It would be useful to experiment with it.\n\n\nChange the predictor from frequency to something else: what do you see when you visualize the relationship between outcome and predictor variables using scatterplots?\nSpecify linear models with different predictors: do the relationships you see in plots match the coefficients you see in the model estimates?\n\nI would recommend that you both estimate the effects of variables and visualize the relationships between variables using scatterplots. If you combine reflection on the model estimates with evaluation of what the plots show you then you will be able to see how reading model results and reading plots can reveal the correspondences between the two ways of looking at your data.\n\n\n\n\nWe can examine variation between participants by analyzing the data for each participant’s responses separately, fitting a different linear model of the effect of word frequency on lexical decision RTs for each participant separately. Figure 5 presents a grid or trellis of plots, one plot per person. In each plot, you can see points corresponding to the log RT of the responses made by each participant to the stimulus words.\n\n\n\n\n\n\nTip\n\n\n\nIn working with R, we often benefit from the vast R knowledge ecosystem.\n\nI was able to produce the sequence of plots Figure 5, Figure 6, Figure 7 and Figure 8 thanks to this very helpful blog post by TJ Mahr\n\n\n\nIn all plots, the pink or red line represents the complete pooling model estimate of the effect of frequency on response RTs. The line is the same for each participant because there is only one estimated effect, based on all data for all participants.\nIn addition, in each plot, you can see a green line. You can see that the line varies between participants. This represents the effect of frequency estimated using just the data for each participant, analyzed separately. These are the no pooling estimates. We call them the no pooling estimates because each is based just on the data from one participant.\n\n\n\n\n\n\n\n\nFigure 5: Plot showing the relationship between logRT and log frequency (LgSUBTLCD) separately for each participant; red-pink line shows the complete pooling estimate, blue-green line shows the no-pooling estimate\n\n\n\n\n\nFigure 5 reveals substantial differences between participants in both average response speed and the frequency effect.\nWe may further predict variation in standard errors between participants given, also, the differences between participants in the spread of log RT, illustrated by Figure 4. Basically, where the distribution of log RT is more widely spread out, for any one participant, there it will be harder for us to estimate with certainty the mean or the sources of variance for the participant’s response speed.\nYou will notice that the no pooling and complete pooling estimates tend to be quite similar. But for some participants – more than for others – there is variation between the estimates.\nYou can reflect that the complete pooling is unsatisfactory because it ignores the variation between the participants: some people are slower than others; some people do show a larger frequency effect than others. You can also reflect that the no pooling is unsatisfactory because it ignores the similarities between the participants.\nWhile there is variation between participants there is also similarity across the group so that the effect of frequency is similar between participants.\n\n\n\n\n\n\nImportant\n\n\n\nWhat we need is an analytic method that is capable of both estimating the overall average population-level effect (here, of word frequency) and taking into account the differences between sampling units (here, participants).\nThat method is linear mixed-effects modeling.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 18. Conceptual introduction to developing linear mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/03-mixed.html#sec-dev-mixed-lme",
    "href": "PSYC412/part2/03-mixed.html#sec-dev-mixed-lme",
    "title": "Week 18. Conceptual introduction to developing linear mixed-effects models",
    "section": "",
    "text": "As you have seen before, we can account for the variation – the differences between participants in intercepts and slopes.\nFirst, we model the intercept as two terms:\n\\[\n\\beta_{0i} = \\gamma_0 + U_{0i}\n\\]\nWhere:\n\n\\(\\gamma_{0}\\) is the average intercept, and\n\\(u_{0i}\\) is the difference for each participant between their intercept and the average intercept.\n\nSecond, we can model the frequency effect as two terms:\n\\[\n\\beta_{1i} = \\gamma_1 + U_{1i}\n\\]\nWhere:\n\n\\(\\gamma_{10}\\) is the average slope, and:\n\\(U_{1i}\\) represents the difference for each participant between the slope of their frequency effect and the average slope.\n\nWe can then incorporate in a single model the fixed effects due to the average intercept and the average frequency effect, as well as the random effects – the error variance due to unexplained differences between participants in intercepts and in frequency effects:\n\\[\nY_{ij} = \\gamma_0 + \\gamma_1X_j + U_{0i}+ U_{1i}X_j + e_{ij}\n\\]\nWhere the outcome \\(Y_{ij}\\) is related to:\n\nthe average intercept \\(\\gamma_0\\) and differences between \\(i\\) participants in the intercept \\(U_{0i}\\);\nthe average effect of the explanatory variable frequency \\(\\gamma_1X_j\\) and differences between \\(i\\) participants in the slope \\(U_{1i}X_j\\);\nin addition to residual error variance \\(e_{ij}\\).\n\n\n\n\nAs we first saw in the Introduction to linear mixed-effects models chapter, in conducting mixed-effects analyses, we do not aim to examine the specific deviation (here, for each participant) from the average intercept or the average effect or slope. We estimate just the spread of deviations by-participants.\nA mixed-effects model like our final model actually includes fixed effects corresponding to the intercept and the slope of the word frequency effect plus the variances:\n\n\\(var(U_{0i})\\) variance of deviations by-participants from the average intercept;\n\\(var(U_{1i}X_j)\\) variance of deviations by-participants from the average slope of the frequency effect;\n\\(var(e_{ij})\\) residuals, at the response level, after taking into account all other terms.\n\nWe may expect the random effects of participants or items to covary: for example, participants who are slow to respond may also be more susceptible to the frequency effect. Thus our specification of the random effects of the model can incorporate terms corresponding to the covariance of random effects:\n\n\\(covar(U_{0i}, U_{1i}X_j)\\)\n\n\n\n\nAs we know, some words elicit slower and some elicit faster responses on average. As we discussed in the last chapter, in the Introduction to linear mixed-effects models chapter section on the impact of stimulus variation, if we did not take such variation into account, we might spuriously identify an experimental effect actually due just to unexplained between-items differences in intercepts (Clark, 1973; Raaijmakers et al., 1999) committing an error: the language as fixed effect fallacy.\nWe can model the random effect of items on intercepts by modeling the intercept as two terms:\n\\[\n\\beta_{0j} = \\gamma_0 + W_{0j}\n\\]\nWhere:\n\n\\(\\gamma_{0}\\) is the average intercept, and\n\\(W_{0j}\\) represents the deviation, for each word, between the average intercept and the per-word intercept.\n\nNote that I ignore the possibility, for now, of differences between items in the slopes of fixed effects but I do come back to this.\nThe term the language as fixed effect fallacy (Clark, 1973; Raaijmakers et al., 1999) implies that thinking about the random effects of stimulus differences applies only when we are looking at experiments about language. But you should remember that we need to think about the impact of random differences between stimuli whenever we present samples of stimuli to participants, and we collect observations about multiple responses for each stimulus. This is true whatever the nature of the stimuli (see e.g. Judd et al., 2012).\n\n\n\nOur model can now incorporate the random effects of participants as well as items:\n\\[\nY_{ij} = \\gamma_0 + \\gamma_1X_j + U_{0i}+ U_{1i}X_j + W_{0j} + e_{ij}\n\\]\nIn this model, the outcome \\(Y_{ij}\\) is related to:\n\nthe average intercept \\(\\gamma_0\\) and the word frequency effect \\(\\gamma_1X_j\\);\nplus random effects due to unexplained differences between participants in intercepts \\(U_{0i}\\) and in the slope of the frequency effect \\(U_{1i}X_j\\);\nas well as random differences between items in intercepts \\(W_{0j}\\);\nin addition to the residual term \\(e_{ij}\\).\n\n\n\n\nWe fit a mixed-effects model of the \\(logrt \\sim \\text{frequency}\\) relationship using the lmer() function, taking into account:\n\nthe fact that the study data have a hierarchical structure – with observations sensibly grouped by participant;\nthe fact that both the frequency effect, and average speed, may vary between participants;\nand the fact that the average speed of response can vary between responses to different stimuli.\n\nThe model syntax corresponds to the statistical formula and the code is written as:\n\nML.all.correct.lmer  &lt;- lmer(logrt ~\n\n                           LgSUBTLCD +\n\n                           (LgSUBTLCD + 1|subjectID) +\n\n                           (1|item_name),\n\n                         data = ML.all.correct)\n\nsummary(ML.all.correct.lmer)\n\nAs will now be getting familiar, the code works as follows:\n\nML.all.correct.lmer  &lt;- lmer(...) creates a linear mixed-effects model object using the lmer() function.\nlogrt ~ LgSUBTLCD the fixed effect in the model is expressed as a formula in which the outcome or dependent variable logrt is predicted ~ by the independent or predictor variable LgSUBTLCD word frequency.\n\n\n\n\n\n\n\nTip\n\n\n\nIf there were more terms in the model, the terms would be added in series separated by +\n\n\nThe random effects part of the model is then specified as follows.\n\nWe first have the random effects associated with random differences between participants:\n\n\n(...|subjectID) adds random effects corresponding to random differences between sample groups (participants subjects) coded by the subjectID variable.\n(...1 |subjectID) including random differences between sample groups (subjectID) in intercepts coded 1.\n(LgSUBTLCD... |subjectID) and random differences between sample groups (subjectID) in the slopes of the frequency effect coded by using theLgSUBTLCD variable name.\n\n\nThen, we have the random effects associated with random differences between stimuli:\n\n\n(1|item_name) adds a random effect to account for random differences between sample groups (item_name) in intercepts coded 1.\n\n\n...(..., data = ML.all.correct) specifies the data-set in which you can find the variables named in the model fitting code.\nLastly, we can then specify summary(ML.all.correct.lmer) to get a summary of the fitted model.\n\n\n\n\nIf you run the model code as written then you would see the following results.\n\nML.all.correct.lmer  &lt;- lmer(logrt ~\n\n                           LgSUBTLCD +\n\n                           (LgSUBTLCD + 1|subjectID) +\n\n                           (1|item_name),\n\n                         data = ML.all.correct)\n\nsummary(ML.all.correct.lmer)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: logrt ~ LgSUBTLCD + (LgSUBTLCD + 1 | subjectID) + (1 | item_name)\n   Data: ML.all.correct\n\nREML criterion at convergence: -9868.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.6307 -0.6324 -0.1483  0.4340  5.6132 \n\nRandom effects:\n Groups    Name        Variance  Std.Dev. Corr \n item_name (Intercept) 0.0003268 0.01808       \n subjectID (Intercept) 0.0054212 0.07363       \n           LgSUBTLCD   0.0002005 0.01416  -0.63\n Residual              0.0084333 0.09183       \nNumber of obs: 5257, groups:  item_name, 160; subjectID, 34\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)  2.887997   0.015479 186.577\nLgSUBTLCD   -0.034471   0.003693  -9.333\n\nCorrelation of Fixed Effects:\n          (Intr)\nLgSUBTLCD -0.764\n\n\nIn these results, we see:\n\nFirst, information about the function used to fit the model, and the model object created by the lmer() function call.\nThen, we see the model formula logrt ~ LgSUBTLCD + (LgSUBTLCD + 1|subjectID) + (1|item_name).\nThen, we see REML criterion at convergence about the model fitting process, which we can usually ignore.\nThen, we see information about the distribution of the model residuals.\nThen, we see theRandom Effects.\n\nNotice that the statistics are Variance Std.Dev. Corr., that is, the variance, the corresponding standard deviation, and the correlation estimates associated with the random effects.\n\nWe see Residual error variance, just like in a linear model, corresponding to a distribution or spread of deviations between the model prediction and the observed RT for each response made by a participant to a stimulus.\nWe see Variance terms corresponding to what can be understood as group-level residuals. Here, the variance is estimated for the spread in random differences between the average intercept (over all data) and the intercept for each participant, and the variance due to random differences between the average slope of the frequency effect and the slope for each participant.\nWe also see the variance estimated for the spread in random differences between the average intercept (over all data) and the intercept for responses to each word stimulus.\nAnd we see the Corr estimate, telling us about the covariance between random deviations (between participants) in the intercepts and in the slopes of the frequency effect.\n\n\nLast, just as for linear models, we see estimates of the coefficients (of the slopes) of the fixed effects, the intercept and the slope of the logrts ~ LgSUBTLCD relationship.\n\nWe can compare this estimate with our previous lm() estimate for the effect of frequency.\n\n\n\n\n\n\nNote\n\n\n\n\nResearch hypothesis: Words that are shorter, that look like more other words, and that appear frequently in the language will be easier to recognize.\nResult: We can see that, in this mixed-effects analysis, the estimated effect of word frequency is now \\(\\beta = -0.034471\\).\n\n\n\nThe estimate is different, a bit smaller. While the change in the estimate is also small, we may remember that we are looking at slope estimates for predicted change in log RT, in an experimental research area in which effects are often of the order of 10s of milliseconds. The estimates, and changes in the estimates, will tend to be quite small.\n\n\n\n\n\n\nWarning\n\n\n\nNote that we see coefficient estimates, as in a linear model summary but no p-values.\n\nWe will come back to this, see Section 1.13.4.\nHowever, note that if \\(t &gt;= 2\\) we can suppose that (for a large data-set) an effect is significant at the \\(.05\\) significance level.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 18. Conceptual introduction to developing linear mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/03-mixed.html#sec-dev-mixed-regularisation",
    "href": "PSYC412/part2/03-mixed.html#sec-dev-mixed-regularisation",
    "title": "Week 18. Conceptual introduction to developing linear mixed-effects models",
    "section": "",
    "text": "What is the impact of the incorporation of random effects – the variance and covariance terms – in mixed-effects models? Mixed-effects models can be understood, in general, as a method to compromise between ignoring the differences between groups (here, participants constitute groups of data) as in complete pooling or focusing entirely on each group (participant) as in no pooling (Gelman & Hill, 2007). In this discussion, I am going to refer to the differences between participants but you can assume that the lesson applies generally to any situation in which you have different units in a multilevel structured data-set in which the units correspond to groups or clusters of data.\n\n\nThe problem with ignoring the differences between groups (participants), as in the complete pooling model (here, the linear model), has been obvious when we examined the differences between participants (or between classes) in slopes and intercepts in previous weeks. The problem with focusing entirely on each participant, as in the no pooling model, has not been made apparent in our discussion yet.\nIf we analyze each participant separately then we will get, for each participant, for our model of the frequency effect, the per-participant estimate of the intercept and the per-participant estimate of the slope of the frequency effect. These no-pooling estimates will tend to exaggerate or overstate the differences between participants (Gelman & Hill, 2007). By basing the estimates on just the data for a person, in each per-participant analysis, the no-pooling approach overfits the data.\nYou could say that the no-pooling approach gives us estimates that depend too much on the sample of data we have got, and are unlikely to be similar to the estimates we would see in other samples in future studies.\n\n\n\n\n\n\nTip\n\n\n\nThe no-pooling estimates are too strongly influenced by the data we are currently analyzing.\n\n\n\n\n\nIf we look closely at Figure 5, we can see that there are similarities as well as differences between participants. Our analysis must take both into account.\nWhat happens in mixed-effects models is that we pool information, calculating the estimates for each participant, in part based on the information we have for the whole sample (all participants, in complete pooling), in part based on the information we have about the specific participant (one participant, in no pooling). Thus, for example, the estimated intercept for a participant in a mixed-effects model is given by the weighted average (Snijders & Bosker, 2004) of:\n\nthe intercept estimate given by an analysis of just that participant’s data (no pooling estimate;\nand the intercept estimate given by analysis of all participants’ data (complete pooling estimate).\n\nThe weighted average will reflect our relative level of information about the participant’s responses compared to how much information we have about all participants’ responses.\nFor some participants, we will have less information – maybe they made many errors, so we have fewer correct responses for an analysis. For these people, because we have less information, the intercept estimate will get pulled (shrunk) towards the overall (complete pooling, all data) estimate.\nFor other participants, we have more information – maybe they made all correct responses. For these people, because we have more information, the intercept estimate will be based more on the data for each participant.\nTo make sense of what this means, think about the differences between participants in how much reliable information we can have, given our sample, about their average level of response speed or about how they are affected by experimental variables. Think back to my comments about Figure 4, about the differences between participants in how spread out the distributions of their log RT values are. Recall that I said that where participants’ responses are more spread out – just as where we have less observations for some participants than for others – we shall inevitably have less certainty about our estimates for the effects that influence their performance if we base our account on just their data. Mixed-effects models perform better – as prediction models – than no pooling approaches because they are not relying, for any participant, on just their sometimes unreliable data.\nWe can look again at a plot showing the data for each participant. Figure 6 presents a grid or trellis of plots, one plot per person. In each plot, you can see points corresponding to the RT of each response made by a participant to a stimulus word. In all plots, the pink line represents the complete pooling data model estimate of the effect of frequency on response RTs. In each plot, the green line represents the effect of frequency estimated using just the data for each participant, the no pooling estimates. Now, we also see blue lines that represent the mixed-effects model partial pooling estimates.\n\n\n\n\n\n\n\n\nFigure 6: Plot showing the relationship between logRT and log frequency (LgSUBTLCD) separately for each participant; pink line shows the complete pooling estimate green line shows the no-pooling estimate; and blue line shows the linear mixed-effects model partial pooling estimate\n\n\n\n\n\nIt is quite difficult to identify, in this sample, where the partial pooling and no pooling estimates differ. We can focus on a few clear examples. Figure 7) presents a grid of plots for just four participants. I have picked some extreme examples but the plot illustrates how: (1.) for some participants e.g. AA1 all estimates are practically identical; (2.) for some participants EB5 JL3 JP3 the no-pooling and complete-pooling estimates are really quite different and (3.) for some participants JL3 JP3 the no-pooling and partial-pooling estimates are quite different.\n\n\n\n\n\n\n\n\nFigure 7: Plot showing the relationship between logRT and log frequency (LgSUBTLCD) separately for each participant – for participants AA1, EB5, JL3 and JP3; pink line shows the complete pooling estimate green line shows the no-pooling estimate; and blue line shows the linear mixed-effects model partial pooling estimate\n\n\n\n\n\nIn general, partial pooling will apply both to estimates of intercepts and to estimates of the slopes of fixed effects like the influence of word frequency in reaction time. Likewise, if we consider this idea in general, we can see how it should work whether we are talking about groups or clusters of data grouped by participant or by stimulus or by school, class or clinic, etc.\nFormally, whether an estimate for a participant (in our example) is pulled more or less towards the overall estimate will depend not just on the number of data-points we have for that person. The optimal combined estimate for a participant is termed the Empirical Bayes estimate and the weighting – the extent to which the per-participant ‘estimate’ depends on the participant’s data or the overall data – depends on the reliability of the estimate (of the intercept or the frequency effect) given by analyzing that participant’s data (Snijders & Bosker, 2004). If you think about it, smaller samples – e.g. where a participant completed less correct responses – will give you less reliable estimates (and so will samples that show more variation).\nWhat we are looking at, here, is a form of regularization in which we use all the sources of information we can to ensure we take into account the variability in the data while not getting over-excited by extreme differences (McElreath, 2020). We want to see estimates pulled towards an overall average where we have little data or unreliable estimates. We can see how strongly estimates can be shrunk in a plot like Figure 8.\nFigure 8 illustrates the shrinkage effect. I plotted a scatterplot of intercept and slope parameters from each model (models with different kinds of pooling), and connect estimates for the same participant. The plot uses arrows to connect the different estimates for each participant, different estimates from no-pooling (per-participant) compared to partial-pooling (mixed-effects) models. The plot shows how more extreme estimates are shrunk towards the global average estimate.\n\n\n\n\n\n\n\n\nFigure 8: Plot illustrating shrinkage: big green and pink points show the complete pooling and partial pooling (average) estimates for the slope and intercept; orange and purple points show the no pooling (orange) and partial pooling (purple) estimates for each person; estimates for a person are connected by arrows to show the direction towards which no pooling estimates are pulled or shrunk\n\n\n\n\n\nWe can see how estimates are pulled towards the average intercept and frequency effect estimates. The shrinkage effect is stronger for more extreme estimates like JL3 JP3. It is weaker for estimates more (realistically) like the overall group estimates like AA1.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 18. Conceptual introduction to developing linear mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/03-mixed.html#sec-dev-mixed-estimation",
    "href": "PSYC412/part2/03-mixed.html#sec-dev-mixed-estimation",
    "title": "Week 18. Conceptual introduction to developing linear mixed-effects models",
    "section": "",
    "text": "Before we move on, we can think briefly about how the mixed-effects models are estimated (Snijders & Bosker, 2004). Where do the numbers come from? I am happy to stick to a fairly non-technical intuitive explanation of the computation of LMEs but others, wishing to understand things more deeply, can find computational details in Pinheiro & Bates (2000), among other places. Mixed-effects models are estimated iteratively:\n\n\nIf we knew the random effects, we could find the fixed effects estimates by minimizing differences – like linear modeling.\nIf we knew the fixed effects – the regression coefficients – we could work out the residuals and the random effects.\n\nAt the start, we know neither, but we can move between partial estimation of fixed and random effect in an iterative approach.\n\nUsing provisional values for the fixed effects to estimate the random effects.\nUsing provisional values for the random effects to estimate the fixed effects again.\nTo converge on the maximum likelihood estimates of effects – when the estimates stop changing.\n\nIn mixed-effects models, the things that are estimated are the fixed effects (the intercept, the slope of the frequency effect, in our example), along with the variance and correlation terms associated with the random effects. Previously, I referred to the partial-pooling mixed-effects ‘estimates’ of the intercept or the frequency effect for each person, using the quotation marks because, strictly, these estimates are actually predictions, Best Unbiased Linear Predictions (BLUPs), based on the estimates of the fixed and random effects.\n\n\nMostly, our main concern, in working with mixed-effects models, is over what effects we should include, what model we should specify. But we should prepare for the fact sometimes happens that models fail to converge, which is to say, the model fitting algorithm fails to settle on some set of parameter estimates but has reached the limit in the number of iterations over which it has attempted to find a satisfactory set of estimates.\nIn my experience, convergence problems do arise, typically, if one is analyzing categorical outcome data (e.g accuracy) where there may be not enough observations to distinguish satisfactory estimates given a quite complex hypothesized model. In other words, you might run into convergence problems but it will not happen often and only where you are already dealing with quite a complex situation. We take a look at this concern in more depth, in the next class.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 18. Conceptual introduction to developing linear mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/03-mixed.html#sec-dev-mixed-evaluating",
    "href": "PSYC412/part2/03-mixed.html#sec-dev-mixed-evaluating",
    "title": "Week 18. Conceptual introduction to developing linear mixed-effects models",
    "section": "",
    "text": "Up to this point, we have discussed the empirical or conceptual reasons we should expect to take into account, in our model, the effects on the outcome due to systematic differences in the experimental variables, e.g., in stimulus word frequency frequency, or to random differences between participants or between stimuli. We can now think about how we should statistically evaluate the relative usefulness of these different fixed effects or random effects, where usefulness is judged in relation to our capacity to explain outcome variance, or to improve model fit to sample data. We shall take an approach that follows the approach set out by Baayen, Bates and others (Baayen et al., 2008b; Bates et al., 2015; Matuschek et al., 2017).\nIn this approach, we shall look at the choices that psychology researchers have to make. Researchers using statistical models are always faced with choices. As we have seen, these choices begin even before we start to do analyses, as when we make decisions about data-set construction (Steegen et al., 2016). The need to make choices is always present for all the kinds of models we work with. This may not always be obvious because, for example, in using some data analysis software, researchers may rely on defaults with limited indication that that is what they are doing.\n\n\n\n\n\n\nImportant\n\n\n\nJust because we are making choices does not mean we are operating subjectively in a non-scientific fashion. Rather, provided we work in an appropriate mode of transparency or reflexiveness, we can work with an awareness of our options and the context for the data analysis (see the very helpful discussion in Gelman & Hennig, 2017).\n\n\n\n\nIt is very common to see researchers using a process of model comparison to try to identify an account for their data in terms of estimates of fixed and random effects. A few key concepts are relevant to taking this approach effectively.\nWe will focus on building a series of models up to the most complex model supported by the data. What does model complexity mean here? I am talking about something like the difference between a model including just main effects (simpler) and a model including both main effects and the interaction between the effects (more complex), or, I am talking about a model included just fixed effects (simpler) versus a model including fixed effects as well as random effects (more complex).\nResearchers may engage in comparing models to examine if one or more random effects should be included in their linear mixed-effects model. They may not be sure if they should include all random effects, that is, all random effects that could be included, given a range of grouping variables, like participant, class or stimulus, and given a range of possible effects, such as whether slopes or intercepts might vary.\nResearchers may do model comparison to check if adding the effect of an experimental variable is justified. Maybe they are conducting an exploratory study in which they want to investigate if using some measurement variable helps to explain variation in the outcome. Perhaps they are conducting an experimental study in which they want to test if the experimental manipulation, or the difference between conditions, has an impact on the outcome.\n\n\n\n\n\n\nTip\n\n\n\nAcross these scenarios, we can test if an effect should be included or if its inclusion in a model is justified by comparing models with versus without the term that corresponds to the effect.\n\n\nIn some studies, researchers conduct model comparisons like this in order to obtain null hypothesis significance tests for the effects of the experimental variables.\nTypically, the model comparisons are focused on whether some measurement of model fit is or is not different when we do versus do not include the effect in question in the model.\n\n\nAs our discussion progresses, I think it would be helpful to reflect on some of the questions that you may be asking yourself.\n1. What about multiple comparisons?\nYou might well ask yourself:\n\nIf we engage in a bunch of comparisons to check if we should or should not include a variable, isn’t this just exploiting researcher degrees of freedom?\n\nOr, you might ask:\n\nIf we are conducting multiple tests on the same data, aren’t we running the risk of raising the Type I error (false positive) rate because we are doing multiple comparisons?\n\nI think these are good questions but, here, my task is to explain what people do, why they do it, and how it helps in your data analysis.\n2. Is any model the best?\n\nSo you are looking at models with varying fixed effects (fitted using ML) or models with varying random effects (fitted using REML). How do you decide which model is better?\n\nSome researchers argue that trying to decide which model is better or best is inappropriate (see e.g. a. Gelman, 2015). As the famous saying by George Box has it (Box, 1976): “All models are wrong.” 1 We may say, nevertheless, that some models are useful. Some models are more useful than others, perhaps, because they explain or predict outcomes better, depending on your criteria, and the cost-benefit analysis.\nHere, I will explain the model comparison process while acknowledging this point. This is because researchers often model comparison techniques to evaluate the relative usefulness of different alternate models.\n\n\n\n\nYou will often encounter, in the psychological research literature, Information Criteria statistics like BIC: they are understood within an approach: Information-theoretic methods. They are grounded in the insight that you have reality and then you have approximating models. The distance between a model and reality corresponds to the information lost when we use a model to approximate reality. Information criteria – AIC or BIC – are estimates of information loss. The process of model selection aims to minimize information loss.\nI will not discuss information criteria methods of model evaluation in detail, here, because psychologists frequently use the Likelihood Ratio Test method (Meteyard & Davies, 2020), see following. (Take a look at, e.g., Burnham (2004) for a readable discussion, if you are interested.) However, you should have some idea of what information criteria statistics (like AIC and BIC) mean because you will see these statistics in the outputs from model comparisons using the anova() function, which we shall review a bit later (Section 1.12.3).\nIn summary, Akaike showed you could estimate information loss in terms of the likelihood of the model given the data – Akaike Information Criteria, AIC:\n\n\n\\[\nAIC = -2ln(l) + 2k\n\\]\nWhere:\n\n\\(-2ln(l)\\) is -2 times the log of the likelihood of the model given the data,\nwhere \\((l)\\) the likelihood\nis proportional to the probability of observed data conditional on some hypothesis being true.\n\nYou want a more likely model – less information loss, closer to reality – you want more negative or lower AIC. You can identify models that are more likely – closer to reality – with models with less wide errors, i.e. smaller residuals.\nYou could better approximate reality by including lots of predictors, specifying a more complex model. Models with more parameters may fit the data better but some of those effects may be spurious. Adding \\(+ 2k\\) penalizes complexity, speaking crudely, and so helps us to focus on the more parsimonious less complex model that best fits the data.\nSchwartz proposed an alternative estimate – Bayesian Information Criteria: BIC:\n\n\n\\[\nBIC = -2ln(l) + kln(N)\n\\]\nWhere:\n\n\\(-2ln(l)\\) is -2 times the log of the likelihood of the model given the data.\n\\(+ kln(N)\\) is the number of parameters in the model times the log of the sample size.\n\nThus the penalty for greater complexity is heavier in BIC.\nWe see that AIC and BIC differ in the second term. A deeper difference is that AIC estimates information loss when the true model may not be among the models being considered while BIC assumes that the true model is within the set of models being considered.\nAt this point we just need to think about Model selection and judgment using AIC and BIC.\n\nCompare a simpler model: model 1, just main effects; model 2, main effects plus interactions.\n\nIf the more complex model better approximates reality then it will be more likely given the data.\nBIC or AIC will be closer to negative infinity: \\(-2ln(l)\\) will be larger e.g. 10 is better than 1000, -1000 better than -10.\n\nAIC and BIC should move in the same direction. They usually will.\nAIC will tend to allow more complex models and that may be necessary when the researcher is engaged in a more exploratory study or wants more accurate predictions (that would be better supported by maximising the information going into the model). Using the BIC will tend to favour simpler models and that may be necessary when the researcher seeks models that replicate over the long run. Maybe a simpler model will less likely include predictors estimated because they are needed to fit noise or random outcome variation.\n\n\n\nPinheiro & Bates (2000; see also Barr et al., 2013; Matuschek et al., 2017) recommend that models of varying predictor sets can be compared using Likelihood Ratio Test comparison (LRTs) where the simple model is nested inside the more complex model.\nThe “nested”, here, means that the predictors in the simpler model are a subset of the predictors in the more complex model. For example, you might have just main effects in the simpler model but both main and interaction effects in the more complex model. Or, in another example, you might have just random effects of subjects or items on intercepts in the simpler model but both random effects on intercepts and random effects on slopes of fixed effects in the more complex model.\n\n\n\n\n\n\nWarning\n\n\n\nWhen you compare models using the Likelihood ratio test, LRT, you are comparing alternate models of the same data.\n\n\nBarr et al. (2013) note that we can compare models varying in the fixed effects (but constant in the random effects) or models varying in the random effects (but constant in the fixed effects) using LRTs. I have frequently reported model comparisons using the Likelihood ratio test, LRT. In part, this is for analytic reasons: I can compare simple and complex models getting multiple information criteria statistics for the models being compared in one function call, anova([model1], [model2]. In part, it is for social pragmatic reasons: the LRT comparison yields a significance p-value so that I can say, using the comparison, something like “The more complex model provided a significantly better fit to observation (LRT comparison, … p \\(=\\) …”\nIn a Likelihood Ratio Test, the test statistic is the comparison of the likelihood of the simpler model with the more complex model. Fortunately for us, we can R to calculate the model likelihood and do the model comparison (Section 1.13.2).\nThe comparison of models works by division: we divide the likelihood of the more complex model by the likelihood of the simpler model, calculating a likelihood ratio.\n\\[\n\\chi^2 = 2log\\frac{likelihood-complex}{likelihood-simple}\n\\]\nThe likelihood ratio value is then compared to the \\(\\chi^2\\) distribution for a significance test. In this significance test, we assume the null hypothesis that the simpler model is adequate as an account of the outcome variance. We calculate the p-value for the significance test using a number for the degrees of freedom equal to the difference in the number of parameters of the models being compared.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 18. Conceptual introduction to developing linear mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/03-mixed.html#sec-dev-mixed-model-steps",
    "href": "PSYC412/part2/03-mixed.html#sec-dev-mixed-model-steps",
    "title": "Week 18. Conceptual introduction to developing linear mixed-effects models",
    "section": "",
    "text": "Important\n\n\n\nHow should you proceed when you decide to use mixed-effects models?\n\nI think the answer to that question depends on whether you are doing a study that is confirmatory or exploratory.\n\n\n\nIn short, if you have pre-registered the design of your study and, as part of that registration, you recorded the hypotheses you plan to test, as well as the analysis method you plan to use to test your hypotheses, then the answer is simple: fit the model you said you were going to use.\nThese days, if you have not pre-registered your analysis plans, you are practically-speaking engaged in exploratory work. If you are doing an exploratory study, then you will need to make some choices, in part, depending on the nature of the sample you are working with, and other aspects of the research context, but it will help to keep things simple.\nIn an exploratory study, I would keep things simple by comparing a series of models, fitted with different sets of predictor variables (fixed effects).\n\n\n\n\n\n\nWarning\n\n\n\nNote: if you are running mixed-effects models in R you cannot run lmer() models with just fixed effects.\n\n\nWhat I do is this: for a data-set like the ML study data, where the data were collected using a repeated-measures design:\n\nso that all participants saw all stimuli,\nand both participants and stimuli were sampled (from the wider populations of readers or words),\nthen I would run a series of models\nso that the different models have varying sets of fixed effects\nbut all models in the series have the same random effects: the random effects of subjects and items on intercepts.\n\nIn my experience, the estimates and associated significance levels associated with fixed effects can vary quite a bit depending on what other variables are included in the model. This has led me to take an approach where I am not varying too much how predictors are included in the model.\nAs noted, this will not really apply if you are doing an confirmatory study in which you are obliged to include the manipulated variables. However, if you are doing something a bit more exploratory then you might have to think about the kinds of predictors you include in your model, and how or when you include them.\n\n\n\n\n\n\nTip\n\n\n\nIn what order should you examine the usefulness of different sets of fixed effects?\n\nThis is a difficult question to answer and the difficulty is one reason why I think we need to be cautious when we engage in model comparison to try to get to a model of our data.\n\n\n\nMy advice would be to plan out in advance a sequence of model comparisons.\n\nYou should begin with simpler models with fewer effects.\nYou should begin with those effects whose impacts are well established and well understood by you.\nIf there is a whole set of well established effects typically included in an analysis in the field in which you are working, it might be sensible to include all the effects in a single step.\nThen, I would use subsequent incremental steps to increase model complexity by adding effects that are theoretically justified, i.e., hypothesized, but which may be new, or may depend on the experimental manipulation you are testing out.\n\nHaving established a model with some set of sensible fixed effects (guided by information criteria or LRT statistics), I would then turn my attention to the random effects component of the model. As noted, we may expect to see random differences between subjects (and possibly between items) in both the level of average performance – random effects of subjects or items on intercepts – and in the slopes of fixed effects – random effects of subjects or items on slopes.\nWhat I do is this:\n\nFor a data-set like ML’s, I examine firstly if both random effects of subjects and items on intercepts are required.\nI then check if random effects of subjects or items on slopes are additionally required in the model.\n\nThe distinction between exploratory and confirmatory studies breaks down, in my experience, when we start thinking about what random effects should be included in a model. It will be useful to review, here, Barr et al. (2013) and Matuschek et al. (2017) for an interesting discussion, and contrasting approaches.\n\n\nBefore we go any further, we need to briefly discuss one key choice that we face in working with mixed-effects models. This concerns the difference between Restricted Maximum Likelihood (REML) and Maximum Likelihood (ML) estimation methods. Both methods are iterative.\nThe lmer() function has defaults, like any analysis function, so we often do not need to make the choice explicit. We do need to when we compare models that vary in fixed effects, or in random effects.\n\nRestricted maximum likelihood\n\nIn R: REML=TRUE is stated in the lmer() function call.\n\nREML estimates the variance components while taking into account the loss of degrees of freedom resulting from the estimation of the fixed effects: REML estimates vary if the fixed effects vary.\nTherefore it is not recommended to compare the likelihood of models varying in fixed effects and fitted using REML (Pinheiro & Bates, 2000).\nThe REML method is recommended for comparing the likelihood of models with the same fixed effects but different random effects.\nREML is more accurate for random effects estimation.\n\n\nMaximum likelihood\n\nIn R: REML=FALSE is stated in thelmer() function call.\n\nML estimation methods can be used to fit models with varying fixed effects but the same random effects.\nML estimation: a good place to start when building-up model complexity – adding parameters to an empty model.\nPinheiro & Bates (2000) advise that the approach is anti-conservative (it will sometimes indicate effects where there are none there) but Barr et al. (2013) argue that their analyses suggest that that is not so.\n\n\n\n\n\n\n\nAs noted, it is recommended (Pinheiro & Bates, 2000) that we compare models of varying random effects using Restricted Maximum Likelihood (REML) fitting. We might be comparing different models with different sets of random effects if we are in the process of working out whether our model should include both random intercepts and random slopes. I think it is sensible to build up model complexity in the random component so that we are working through a series of model comparisons, comparing more simple with more complex models where the more complex model includes the same terms as the simpler model but adds some more.\nIn analyzing the effect of frequency on log RT for the ML study data, we can examine whether the random effects of subjects or of items on intercepts are necessary. Then we can examine if we should take into account random effects of subjects on the slope of the fixed effect of frequency, in addition to the random effects on intercepts.\nTo begin with, we look at a simpler model. We can fit a model with just the fixed effects of intercept and frequency, and the random effects of participants or items on intercepts only. We exclude the (LgSUBTLCD + ...|subjectID) specification for the random effect of participants on the slope of the frequency LgSUBTLCD effect.\nWe use REML fitting, as follows:\n\nML.all.correct.lmer.REML.si  &lt;- lmer(logrt ~ LgSUBTLCD + \n                                    \n                                          (1|subjectID) + (1|item_name),\n\n       data = ML.all.correct, REML = TRUE)\n\nsummary(ML.all.correct.lmer.REML.si)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: logrt ~ LgSUBTLCD + (1 | subjectID) + (1 | item_name)\n   Data: ML.all.correct\n\nREML criterion at convergence: -9845.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.5339 -0.6375 -0.1567  0.4364  5.5851 \n\nRandom effects:\n Groups    Name        Variance  Std.Dev.\n item_name (Intercept) 0.0003204 0.01790 \n subjectID (Intercept) 0.0032650 0.05714 \n Residual              0.0085285 0.09235 \nNumber of obs: 5257, groups:  item_name, 160; subjectID, 34\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)  2.887697   0.013253   217.9\nLgSUBTLCD   -0.034390   0.002774   -12.4\n\nCorrelation of Fixed Effects:\n          (Intr)\nLgSUBTLCD -0.658\n\n\nIf you look at the code chunk, you can see that:\n\nREML = TRUE is the only change to the code: it specifies the change in model fitting method;\nalso, I changed the model name to ML.all.correct.lmer.REML.si to be able to distinguish the maximum likelihood from the restricted maximum likelihood model.\n\nFollowing Baayen et al. (2008a), we can then run a series of models with just one random effect. Firstly, just the random effect of items on intercepts:\n\nML.all.correct.lmer.REML.i  &lt;- lmer(logrt ~\n\n       LgSUBTLCD + (1|item_name),\n\n       data = ML.all.correct, REML = TRUE)\n\nsummary(ML.all.correct.lmer.REML.i)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: logrt ~ LgSUBTLCD + (1 | item_name)\n   Data: ML.all.correct\n\nREML criterion at convergence: -8337\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.7324 -0.6455 -0.1053  0.4944  4.8970 \n\nRandom effects:\n Groups    Name        Variance  Std.Dev.\n item_name (Intercept) 0.0002364 0.01537 \n Residual              0.0117640 0.10846 \nNumber of obs: 5257, groups:  item_name, 160\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)  2.886765   0.009047  319.07\nLgSUBTLCD   -0.034206   0.002811  -12.17\n\nCorrelation of Fixed Effects:\n          (Intr)\nLgSUBTLCD -0.977\n\n\nSecondly, just the random effect of subjects on intercepts:\n\nML.all.correct.lmer.REML.s  &lt;- lmer(logrt ~\n\n       LgSUBTLCD + (1|subjectID),\n\n       data = ML.all.correct, REML = TRUE)\n\nsummary(ML.all.correct.lmer.REML.s)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: logrt ~ LgSUBTLCD + (1 | subjectID)\n   Data: ML.all.correct\n\nREML criterion at convergence: -9786.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.5843 -0.6443 -0.1589  0.4434  5.5266 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n subjectID (Intercept) 0.003275 0.05723 \n Residual              0.008837 0.09401 \nNumber of obs: 5257, groups:  subjectID, 34\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)  2.885751   0.011561  249.60\nLgSUBTLCD   -0.033888   0.001897  -17.87\n\nCorrelation of Fixed Effects:\n          (Intr)\nLgSUBTLCD -0.517\n\n\nIf we now run Likelihood Ratio Test comparisons of these models, we are effectively examining if one of the random effects can be dispensed with: if its inclusion makes no difference to the likelihood of the model then it is not needed. Is the random effect of subjects on intercepts justified?\n\nCompare models, first, with ML.all.correct.lmer.REML.si versus without ML.all.correct.lmer.REML.i the random effect of subjects on intercepts.\nThen compare models with ML.all.correct.lmer.REML.si versus without ML.all.correct.lmer.REML.s the random effect of items on intercepts.\n\n\nanova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.i, refit = FALSE)\nanova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.s, refit = FALSE)\n\n\n\nWe compare models using the anova() function.\n\nanova() does the model comparison, for the models named in the list in brackets.\n\nYou can do this for the foregoing series of models but notice that in the code we specify:\n\nrefit = FALSE`\n\nWhat happens if we do not add that bit? What you will see if you run the anova() function call, without therefit = FALSE argument – try it – is that you will then get the warning refitting model(s) with ML (instead of REML).\nWhy? The immediate reason for this warning is that we have to specify refit = FALSE because otherwise R will compare ML fitted models. The refitting occurs by default.\nWhat is the reason for the imposition of this default?\nYou will recall that Pinheiro & Bates (2000) advise that if one is fitting models with random effects the estimates are more accurate if the models are fitted using Restricted Maximum Likelihood (REML). That is achieved in the lmer() function call by adding the argument REML=TRUE. Pinheiro & Bates (2000) further recommend (see, e.g., pp.82-) that if you compare models:\n\nwith the same fixed effects\nbut with varying random effects\nthen the models should be fitted using Restricted Maximum Likelihood.\n\nR refits models, for the anova() comparison using ML even if we originally specified REML fitting. It does this to stop users from comparing REML-fitted models when those models are specified with different sets of fixed effects, as discussed here. The reason for this is explained by Ben Bolker (in this discussion): analyses of simulated data analyses suggest that it does not make much difference whether we use REML or ML when we are comparing models with the same fixed effects but varying random effects. It does matter very much, however, that we should fit models using ML when we are comparing models with the same random effects but differing fixed effects. You will remember that REML estimates vary if the fixed effects vary.\n\n\n\nWhen we run the anova() function call, it can be seen that the random effects of subjects on intercepts is required.\n\nanova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.i, refit = FALSE)\n\nData: ML.all.correct\nModels:\nML.all.correct.lmer.REML.i: logrt ~ LgSUBTLCD + (1 | item_name)\nML.all.correct.lmer.REML.si: logrt ~ LgSUBTLCD + (1 | subjectID) + (1 | item_name)\n                            npar     AIC     BIC logLik -2*log(L)  Chisq Df\nML.all.correct.lmer.REML.i     4 -8329.0 -8302.7 4168.5   -8337.0          \nML.all.correct.lmer.REML.si    5 -9835.1 -9802.3 4922.6   -9845.1 1508.1  1\n                            Pr(&gt;Chisq)    \nML.all.correct.lmer.REML.i                \nML.all.correct.lmer.REML.si  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIf you look at the results of the model comparison then you should notice:\n\nThe ML.all.correct.lmer.REML.si model is more complex than the ML.all.correct.lmer.REML.i model.\n\n\nML.all.correct.lmer.REML.si includes LgSUBTLCD + (1 | subjectID) + (1 | item_name)\nML.all.correct.lmer.REML.i includes LgSUBTLCD + (1 | item_name).\n\n\nThe more complex model ML.all.correct.lmer.REML.si has AIC (-9835.1) and BIC (-9802.3) numbers that are larger or more negative, and has a likelihood (4922.6) that is larger than the simpler model ML.all.correct.lmer.REML.i which has AIC (-8329.0), BIC (-8302.7) and likelihood (4168.5).\n\n\nThe \\(\\chi^2 = 1508.1\\) statistic, on 1 Df has a p-value of Pr(&gt;Chisq) &lt;2.2e-16.\n\nYou can say that the comparison of the model ML.all.correct.lmer.REML.si (with the random effect of participants on intercepts) versus the model ML.all.correct.lmer.REML.i (without the random effect of participants on intercepts) shows that the inclusion of the random effect of participants on intercepts is warranted by a significant difference in model fit. (I highlight here the language you can use in your reporting.)\nThe second model comparison shows that the random effects of items on intercepts is also justified.\n\nanova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.s, refit = FALSE)\n\nData: ML.all.correct\nModels:\nML.all.correct.lmer.REML.s: logrt ~ LgSUBTLCD + (1 | subjectID)\nML.all.correct.lmer.REML.si: logrt ~ LgSUBTLCD + (1 | subjectID) + (1 | item_name)\n                            npar     AIC     BIC logLik -2*log(L)  Chisq Df\nML.all.correct.lmer.REML.s     4 -9778.3 -9752.0 4893.2   -9786.3          \nML.all.correct.lmer.REML.si    5 -9835.1 -9802.3 4922.6   -9845.1 58.825  1\n                            Pr(&gt;Chisq)    \nML.all.correct.lmer.REML.s                \nML.all.correct.lmer.REML.si  1.723e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIf you look at the results of the model comparison then you should notice:\n\nThe ML.all.correct.lmer.REML.si model is more complex than the ML.all.correct.lmer.REML.s model.\n\n\nML.all.correct.lmer.REML.si includes LgSUBTLCD + (1 | subjectID) + (1 | item_name).\nML.all.correct.lmer.REML.s includes LgSUBTLCD + (1 | subjectID)\n\n\nThe more complex model ML.all.correct.lmer.REML.si has AIC (-9835.1) and BIC (-9802.3) numbers that are larger or more negative, and has a likelihood (4922.6) that is larger than the simpler model ML.all.correct.lmer.REML.s which has AIC (-9778.3) and BIC (-9752.0) and likelihood (4893.2).\n\n\nThe \\(\\chi^2 = 58.825\\) statistic, on 1 Df has a p-value of Pr(&gt;Chisq) 1.723e-14.\n\nYou can say that the comparison of a model ML.all.correct.lmer.REML.si with versus a model ML.all.correct.lmer.REML.s without the random effect of items on intercepts shows that the inclusion of the random effect of items on intercepts is warranted by a significant difference in model fit.\nI would conclude that both random effects of subjects and items on intercepts are required.\nWe can draw this conclusion because the difference between the model including just the random effect of items on intercepts anova-ML-all-correct-lmer-REML-i, or the model including just the random effect of subjects on intercepts anova-ML-all-correct-lmer-REML-s, compared to the model including both the random effect of items on intercepts and of subjects on intercepts anova-ML-all-correct-lmer-REML is significant. This tells us that the absence of the term accounting for the random effect of subjects on intercepts is associated with a significant decrease in model fit to data, in model likelihood.\n\n\n\n\nWe should next consider whether it is justified or warranted to include in our model a term capturing the random effect of participants in the slope of the frequency effect. We may hold or we may make theoretical assumptions that justify including this random effect. Some researchers might ask: does the inclusion of the random effect seem warranted by improved model fit to data?\nI should acknowledge, here, that there is an on-going discussion over what random effects should be included in mixed-effects models (see Meteyard & Davies (2020) for an overview). The discussion can be seen from a number of different perspectives. Key articles include those published by Baayen et al. (2008b), Bates et al. (2015), Barr et al. (2013) and Matuschek et al. (2017).\nYou could be advised that a mixed-effects model should include all random effects that make sense a priori, so, here, we are talking about the random effects of participants on intercepts and on the slopes of all fixed effects that are in your model (variances and covariances) as well as all the random effects of items on intercepts and on slopes. This is characterized as the keep it maximal approach, associated with Barr et al. (2013), though the discussion in that article is more nuanced than this sounds.\nOr, you could be advised that a mixed-effects model should only include those random effects that appear to be justified or warranted by their usefulness in accounting for the data. In practice, this may mean, you should include only those random effects that appear justified by improved model fit to data, as indicated by a model comparison (see e.g. Bates et al., 2015; Matuschek et al., 2017).\n\n\n\n\n\n\nTip\n\n\n\nI think, in practice, that maximal models can run into convergence problems. This means that many researchers adopt an approach which you could call: Maximum justifiable.\n\nThis involves fitting a model, including all the random effects that make sense,\nthat are justified by improved model fit to data (given a significance test, model comparison)\nfor a model that actually converges.\n\n\n\nAt present, viewpoints in discussions around the specification of random effects are associated with arguments that, as Barr et al. (2013) discuss, more comprehensive models appear to control the Type I (false positive) error rate better, or that, as Matuschek et al. (2017) argue, control over the risk of false positives may come at the cost of increasing the Type II (false negative) error rate).\nI think that it would seem to be axiomatic that a researcher should seek to account for all the potential sources of variance – fixed effects or random effects – that may influence observed outcomes. In practice, however, you may have insufficient data or inadequate measures to enable you to fit a model that converges with all the random effects, or to enable you to fit a model that converges that can estimate what may, in fact, be very small random effects variances or covariances. This is why some researchers are moving to adopt Bayesian mixed-effects modeling methods, as discussed by the developmental Psychologist, Michael Frank, for example, here. And as exemplified by my work here.\n\n\n\n\n\n\nWarning\n\n\n\nThis discussion raises a question.\n\nRandom slopes of what?\n\n\n\nIn general, and simplifying things a bit, if an effect is manipulated within grouping units then we should specify random effects terms that allow us to take into account random differences between groups (e.g, between participants, stimulus words, or classes) in intercepts or in the slopes of the effects of theoretical interest, the fixed effects. The language of within-subjects or between-subjects effects is common in statistical education in psychological science and, I guess, it is a legacy of the focus of that education on ANOVA. A nice explanation of the difference between within-subjects or between-subjects effects can be found in Barr et al. (2013).\nIn short, if a participant provides response data under multiple levels of an experimental condition, or in response to multiple levels of a predictor variable (e.g. a person responds to multiple words, with differing frequency levels) then we are going to estimate or test the effect of that condition or that variable as if the condition is manipulated within-subjects or as if the responses to the variable vary within-subjects. If and only if we are in this situation, we can draw a plot of the kind you see in Figure 6: where we may be able to see the way that the slope of the effect of the variable differs between participants. (In contrast, for example, outside of longitudinal studies, we would identify age as a between-subjects rather than a within-subjects variable and, if you think about it, we could not draw a grid of plots like Figure 6 to examine how the slope of the age effect might differ between participants.) In this situation, we can and should (as Barr et al. (2013) argue) specify random effects terms to account for between-participant differences in the slopes of the fixed effect.\nWe can examine the utility of random effects by comparing models with the same fixed effects but with varying random effects. We can specify a fixed effect term inside the random effects part of the mixed-effects model code, as we saw in Section 1.9.5.\n\nML.all.correct.lmer.REML.slopes  &lt;- lmer(logrt ~ LgSUBTLCD + \n                                           \n                                                (LgSUBTLCD + 1|subjectID) + (1|item_name),\n\n       data = ML.all.correct, REML = TRUE)\n\nLooking at the code:\n\nWith (LgSUBTLCD + 1 |subjectID) we specify a random effect of subjects on intercepts and on the slope of the frequency effects.\nWe do not specify – it happens by default – the estimation of the covariance of random differences among subjects in intercepts and random differences among subjects in the slope of the frequency effect.\n\nAnd as before, we can use anova() to check whether the increase in model complexity associated with the addition of random slopes terms is justified by an increase in model fit to data.\n\nanova(ML.all.correct.lmer.REML.si, ML.all.correct.lmer.REML.slopes, refit = FALSE)\n\nData: ML.all.correct\nModels:\nML.all.correct.lmer.REML.si: logrt ~ LgSUBTLCD + (1 | subjectID) + (1 | item_name)\nML.all.correct.lmer.REML.slopes: logrt ~ LgSUBTLCD + (LgSUBTLCD + 1 | subjectID) + (1 | item_name)\n                                npar     AIC     BIC logLik -2*log(L)  Chisq Df\nML.all.correct.lmer.REML.si        5 -9835.1 -9802.3 4922.6   -9845.1          \nML.all.correct.lmer.REML.slopes    7 -9854.1 -9808.1 4934.0   -9868.1 22.934  2\n                                Pr(&gt;Chisq)    \nML.all.correct.lmer.REML.si                   \nML.all.correct.lmer.REML.slopes  1.047e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInspection of the results shows us that, here, adjusting the model to include random effects of subjects in the slopes of the fixed effect of word frequency does improve model fit to data. In this situationn, we can report that:\n\nThe inclusion of the random effect is warranted by improved model fit to data (\\(\\chi^2 (1 df) = 22.9, p &lt; .001\\))}\n\n\n\n\nIf you look at the fixed effects summary, you can see that we do not get p-values by default. To calculate p-values, we need to count residual degrees of freedom. The authors of the {lme4} library that furnishes the lmer() function do not (as e.g. Baayen et al., 2008b discuss) think that it is sensible to estimate the residual degrees of freedom for a model in terms of the number of observations. This is because the number of observations concerns one level of a multilevel data-set that might be structured with respect to some number of subjects, some number of items. This means that one cannot then accurately calculate p-values to go with the t-tests on the coefficients estimates; therefore they do not.\nWhile this makes sense to me (see comments earlier on Bayesian methods), Psychologists will often need p-values. This is now relatively easy.\nWe can run mixed-effects models with p-values from significance tests on the estimates of the fixed effects coefficients using the library(lmerTest).\n\nlibrary(lmerTest)\n\nML.all.correct.lmer.REML.slopes  &lt;- lmer(logrt ~ LgSUBTLCD + \n                                           \n                                                (LgSUBTLCD + 1|subjectID) + (1|item_name),\n\n       data = ML.all.correct, REML = TRUE)\n\nsummary(ML.all.correct.lmer.REML.slopes)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: logrt ~ LgSUBTLCD + (LgSUBTLCD + 1 | subjectID) + (1 | item_name)\n   Data: ML.all.correct\n\nREML criterion at convergence: -9868.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.6307 -0.6324 -0.1483  0.4340  5.6132 \n\nRandom effects:\n Groups    Name        Variance  Std.Dev. Corr \n item_name (Intercept) 0.0003268 0.01808       \n subjectID (Intercept) 0.0054212 0.07363       \n           LgSUBTLCD   0.0002005 0.01416  -0.63\n Residual              0.0084333 0.09183       \nNumber of obs: 5257, groups:  item_name, 160; subjectID, 34\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)  2.887997   0.015479 47.782839 186.577  &lt; 2e-16 ***\nLgSUBTLCD   -0.034471   0.003693 60.338786  -9.333 2.59e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n          (Intr)\nLgSUBTLCD -0.764\n\n\nBasically, the call to access the lmerTest library ensures that when we run the lmer() function we get a calculation of an approximation to the denominator degrees of freedom that enables the calculation of the p-value for the t-test for the fixed effects coefficient. An alternative, as I have noted (Section 1.12.3) is to compare models with versus without the effect of interest.\n\n\nIt will be useful for you to examine model comparisons with a different set of models for the same data.\nYou could try to run a series of models in which the fixed effects variable is something different, for example, the effect of word Length: or the effect of orthographic neighbourhood sizeOrtho_N:.\nI would consider the model comparisons in the sequence shown in the foregoing, one pair of models at a time, to keep it simple. When you look at the model comparison, ask: is the difference between the models a piece of complexity (an effect) whose inclusion in the more complex model is justified or warranted by improved model fit to data?",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 18. Conceptual introduction to developing linear mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/03-mixed.html#sec-dev-mixed-reporting-results",
    "href": "PSYC412/part2/03-mixed.html#sec-dev-mixed-reporting-results",
    "title": "Week 18. Conceptual introduction to developing linear mixed-effects models",
    "section": "",
    "text": "How do we report the analyses and their results?\nI think it may help if I analyze the structure and content of a results report that I did, in Davies et al. (2013). The report is published together with data and analysis code.\nIf you look at the report, you can identify the kinds of information that I think you should communicate.\n\nBecause it was an exploratory study, I started by reporting the comparison of models varying in fixed effects.\nI explain what predictors are included in each model.\nI explain how I make decisions about which model to select.\nI then go on to discuss the comparison of models varying in random effects.\n\nI think it is important to be as clear as possible about what model comparison process (if any) you may undertake.\n\nWe stepped through a series of models. Firstly, assuming the same random effects of subjects and items on intercepts, we compared models differing in fixed effects: a model (model 1) with just initialstress factors; a model (model 2) with initialstress factors plus linear effects due to the orthographic.form, frequency, semantic, and bigram.frequency factors; and lastly a model (model 3) with the same factors as model 2 but adding restricted cubic splines for the frequency and orthographic.form factors to examine the evidence for the presence of curvilinear effects of frequency and length (the orthographic.form factor loads heavily on length).\n\nNotice also that I try to standardize the language and structure of the paragraphs – that kind of repetition or rhythm helps the reader, I think, by making what is not repeated – the model specifications – more apparent. Your style may differ, however, and that’s alright.\n\nWe evaluated whether the inclusion of random effects was necessary in the final model (model 3) using LRT comparisons between models with the same fixed effects structure but differing random effects. Here, following Pinheiro & Bates (2000; see, also, Baayen, 2008), models were fitted using the REML=TRUE setting in lmer. We compared models that included: (i.) both random effects of subjects and items, as specified for model 3; (ii.) just the random effect of subjects; (iii.) just the random effect of items.\n\n\n\n\n\n\n\nTip\n\n\n\nI want you to notice something more, concerning the predictors included in each different model:\n\nI do not include predictors one at a time, I include predictors in sets.\n\n\n\nFor the Davies et al. (2013) data-set, I include first the set of phonetic coding variables then the set of psycholinguistic variables (see the paper for details). I include linear effects then additional terms allowing the effects to be curvilinear.\nFinally, you can see that I report model comparisons in terms of Likelihood Ratio Tests. I do this, firstly, in order to report comparisons conducted to examine the basis for selecting one model out of a set of possible models varying in fixed effects:\n\nComparing models 1 and 2, models with initialstress factors but differing in whether they did or did not include key psycholinguistic factors like orthographic.form, the LRT statistic was significant (\\(\\chi^2 = 1,007, 4 df, p = 2 * 10^-16\\)). Comparing models 2 and 3, i.e. models with initialstress and key psycholinguistic components but differing in whether they did or did not use restricted cubic splines to fit the orthographic.form and frequency effects, the LRT statistic was significant (\\(\\chi^2 = 23, 2 df, p = 1 * 10^-5\\)).\n\nThen I report the selection of models varying in random effects:\n\nWe compared models that included: (i.) both random effects of subjects and items, as specified for model 3; (ii.) just the random effect of subjects; (iii.) just the random effect of items. The difference between models (i.) and (ii.) was significant (\\(\\chi^2 = 185, 1 df, p = 2 * 10^-16\\)) indicating inclusion of an item effect was justified. The difference between models (i.) and (iii.) was significant (\\(\\chi^2 = 17,388, 1 df, p = 2 * 10^-16\\)) indicating inclusion of a subject effect was justified.\n\nIf you look at the report, you will see, also, that present a summary table showing the estimates for the fixed effects, and a series of plots indicating the predicted change in outcome (reading RT) given variation in the values of the predictor variables.\nIn summary, I think we can and should report both an outline of the process of development of the model or models we use to estimate the effects of interest, and the estimates we derive through the modeling.\n\n\n\n\n\n\nTip\n\n\n\nI would advise you to report:\n\nA summary of fixed effects – just like in linear models, with coefficient estimates, standard errors, t and p (if you use it);\nRandom effects variance and covariance (as applicable);\nModel building processes or model comparisons (if used).\n\nI recommend presenting the final model summary in a table that is structured like a multiple regression model summary table showing both random and the fixed effects.\n\n\nI also think it helps the reader to know what model is the basis for any estimates presented (see Meteyard & Davies, 2020 for further advice).",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 18. Conceptual introduction to developing linear mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/03-mixed.html#sec-dev-mixed-summary",
    "href": "PSYC412/part2/03-mixed.html#sec-dev-mixed-summary",
    "title": "Week 18. Conceptual introduction to developing linear mixed-effects models",
    "section": "",
    "text": "We examined another example of data from a repeated measures design study, this time, from a study involving adults responding to the lexical decision task, the ML study data-set.\nWe explored in more depth why linear mixed-effects models are more effective than other kinds of models when we are analyzing data with multilevel or crossed random effects structure. We discussed the critical ideas: pooling, and shrinkage. And we looked at how mixed-effects models employ partial-pooling so as to be more effective than alternative approaches dependent on complete pooling or no pooling estimates.\nMixed-effects models work better because they use both information from the whole data-set and information about each group (item or participant). This ensures that model estimates take into account random differences but are regularized so that they are not dominated by less reliable group-level information.\nWe considered, briefly, how mixed-effects models are estimated.\nThen we examined, in depth, how mixed-effects models are fitted, compared and evaluated. The model comparison approach was set out, and we looked at both practical steps and at some of the tricky questions that, in practice, psychologists are learning to deal with.\nWe discussed how to compare models with varying random or fixed effects. We focused, especially, on the comparison of models with varying random effects. Methods for model comparison, including the use of information criteria and the Likelihood Ratio Test, were considered.\nWe discussed p-values, questions about calculating them, and a simple method for getting them when we need to report significance tests.\nWe discussed how mixed-effects models should be reported.\n\n\nWe used two functions to fit and evaluate mixed-effects models.\n\nlmer() to fit mixed-effects models\nanova() to compare two or more models using AIC, BIC and the Likelihood Ratio Test.\nWe used the lmerTest library to furnish significance tests for coefficient estimates of fixed effects.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 18. Conceptual introduction to developing linear mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/03-mixed.html#sec-dev-mixed-recommended-reading",
    "href": "PSYC412/part2/03-mixed.html#sec-dev-mixed-recommended-reading",
    "title": "Week 18. Conceptual introduction to developing linear mixed-effects models",
    "section": "",
    "text": "The most influential papers, at present, for the practice of mixed-effects modeling in psychological science are those by Baayen et al. (2008b), Bates et al. (2015), Barr et al. (2013) and Matuschek et al. (2017). Each of these papers makes critical points and, in my view, each is clearly written with a good use of examples grounded in the scenarios psychologists often encounter.\nBroader concerns about how are approach modeling, and what we look for as scientists, are discussed in Burnham (2004), Gelman & Hill (2007) and Gelman & Hennig (2017).\nA very useful FAQ on the practicalities of working with mixed-effects models can be found here.\n\n\n\n\n\n\nBaayen, R. H., Davidson, D. J., & Bates, D. M. (2008b). Mixed-effects modeling with crossed random effects for subjects and items. Journal of Memory and Language, 59(4), 390–412. https://doi.org/10.1016/j.jml.2007.12.005\n\n\nBaayen, R. H., Davidson, D. J., & Bates, D. M. (2008a). Mixed-effects modeling with crossed random effects for subjects and items. Journal of Memory and Language, 59(4), 390–412. https://doi.org/10.1016/j.jml.2007.12.005\n\n\nBalota, D. a., Yap, M. J., Cortese, M. J., Hutchison, K. a., Kessler, B., Loftis, B., Neely, J. H., Nelson, D. L., Simpson, G. B., & Treiman, R. (2007). The english lexicon project. Behavior Research Methods, 39(3), 445–459. https://doi.org/10.3758/BF03193014\n\n\nBarr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of Memory and Language, 68, 255–278.\n\n\nBates, D., Kliegl, R., Vasishth, S., & Baayen, H. (2015). Parsimonious mixed models. arXiv Preprint arXiv:1506.04967.\n\n\nBox, G. E. P. (1976). Science and statistics. Journal of the American Statistical Association, 71(356), 791–799. https://doi.org/10.2307/2286841\n\n\nBrysbaert, M., & New, B. (2009). Moving beyond kucera and francis: A critical evaluation of current word frequency norms and the introduction of a new and improved word frequency measure for american english. Behavior Research Methods, 41(4), 977–990. https://doi.org/10.3758/BRM.41.4.977\n\n\nBurnham, K. P. (2004). Multimodel inference: Understanding AIC and BIC in model selection. Sociological Methods & Research, 33(2), 261–304. https://doi.org/10.1177/0049124104268644\n\n\nClark, H. (Stanford. U. (1973). Clark_1973_LanguageAsAFixedEffectFallacy.pdf.\n\n\nCohen, J., Cohen, P., West, S. G., & Aiken, L. S. (2003). Applied multiple regression/correlation analysis for the behavioural sciences (3rd. edition). Lawrence Erlbaum Associates.\n\n\nDavies, R., Barbon, A., & Cuetos, F. (2013). Lexical and semantic age-of-acquisition effects on word naming in spanish. Memory and Cognition, 41(2), 297–311.\n\n\nForster, K. I., & Forster, J. C. (2003). DMDX: A windows display program with millisecond accuracy. Behavior Research Methods, Instruments, & Computers, 35, 116–124.\n\n\nGelman, a. (2015). The connection between varying treatment effects and the crisis of unreplicable research: A bayesian perspective. Journal of Management, 41(2), 632–643. https://doi.org/10.1177/0149206314525208\n\n\nGelman, A., & Hennig, C. (2017). Beyond subjective and objective in statistics. Journal of the Royal Statistical Society: Series A (Statistics in Society), 180(4), 967–1033.\n\n\nGelman, A., & Hill, J. (2007). Data analysis using regression and multilevel/hierarchical models. Cambridge University Press.\n\n\nHowell, D. C. (2016). Fundamental statistics for the behavioral sciences. Cengage learning.\n\n\nJudd, C. M., Westfall, J., & Kenny, D. A. (2012). Treating stimuli as a random factor in social psychology : A new and comprehensive solution to a pervasive but largely ignored problem. 103(1), 54–69. https://doi.org/10.1037/a0028347\n\n\nMasterson, J., & Hayes, M. (2007). Development and data for UK versions of an author and title recognition test for adults. Journal of Research in Reading, 30, 212–219.\n\n\nMatuschek, H., Kliegl, R., Vasishth, S., Baayen, H., & Bates, D. (2017). Balancing type i error and power in linear mixed models. Journal of Memory and Language, 94, 305–315. https://doi.org/10.1016/j.jml.2017.01.001\n\n\nMcElreath, R. (2020). : A bayesian course with examples in r and STAN (2nd ed.). Chapman; Hall/CRC. https://doi.org/10.1201/9780429029608\n\n\nMeteyard, L., & Davies, R. A. I. (2020). Best practice guidance for linear mixed-effects models in psychological science. Journal of Memory and Language, 112, 104092. https://doi.org/10.1016/j.jml.2020.104092\n\n\nPinheiro, J. C., & Bates, D. M. (2000). Mixed-effects models in s and s-plus (statistics and computing). Springer.\n\n\nRaaijmakers, J. G. W., Schrijnemakers, J. M. C., & Gremmen, F. (1999). How to deal with \"the language-as-fixed-effect fallacy\": Common misconceptions and alternative solutions. Journal of Memory and Language, 41(3), 416–426. https://doi.org/10.1006/jmla.1999.2650\n\n\nSnijders, T. A. B., & Bosker, R. J. (2004). Multilevel analysis: An introduction to basic and advanced multilevel modeling. Sage Publications Ltd.\n\n\nSteegen, S., Tuerlinckx, F., Gelman, A., & Vanpaemel, W. (2016). Increasing transparency through a multiverse analysis. Perspectives on Psychological Science, 11(5), 702–712.\n\n\nTorgesen, J. K., Rashotte, C. A., & Wagner, R. K. (1999). TOWRE: Test of word reading efficiency. Pro-ed Austin, TX.\n\n\nYarkoni, T., Balota, D., & Yap, M. (2008). Moving beyond coltheart’s n: A new measure of orthographic similarity. Psychonomic Bulletin & Review, 15(5), 971–979. https://doi.org/10.3758/PBR.15.5.971",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 18. Conceptual introduction to developing linear mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/03-mixed.html#footnotes",
    "href": "PSYC412/part2/03-mixed.html#footnotes",
    "title": "Week 18. Conceptual introduction to developing linear mixed-effects models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBox also says: “It is inappropriate to be concerned about mice when there are tigers about.” (Box, 1976) That is too wise a saying to leave out.↩︎",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 18. Conceptual introduction to developing linear mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/01-multilevel-workbook.html",
    "href": "PSYC412/part2/01-multilevel-workbook.html",
    "title": "Week 16. Workbook introduction to multilevel data",
    "section": "",
    "text": "Welcome to your overview of the work we will do together in Week 16.\nWe are going to learn about a method or approach that is essential in modern data analysis: multilevel modeling.\nWe are going to invest the next few weeks in working on this approach.\nWe are going to do this because multilevel modeling is a very powerful and very flexible technique, and because you will learn best if you take the time to build up and deepen your understanding step-by-step.\n\n\nOur learning objectives include the development of key concepts and skills.\n\nconcepts – how data can have multilevel structures and what this requires in models\nskills – where skills comprise the capacity to:\n\n\nuse visualization to examine observations within groups\nrun linear models over all data and within each class\nuse the lmer() function to fit models of multilevel data\n\nWe are just getting started this week. Our plan will be to build depth and breadth in understanding as we progress over the next few weeks.\n\n\n\nYou will see, next, the lectures we share to explain the concepts you will learn about, and the practical data analysis skills you will develop. Then you will see information about the practical materials you can use to build and practise your skills.\nEvery week, you will learn best if you first watch the lectures then do the practical exercises.\n\n\n\n\n\n\nLinked resources\n\n\n\nTo help your learning, you can read about the ideas and the practical coding required for analyses in the chapters I wrote for this course.\nThe chapter: Conceptual introduction to multilevel data\n\n\n\n\nThe lecture materials for this week are presented in three short parts.\nClick on a link and your browser should open a tab showing the Panopto video for the lecture part.\n\nPart 1 (17 minutes) Multilevel data: What do we mean when we talk about multilevel structured data and why it matters.\n\n\n\nPart 2 (13 minutes): Getting started in learning, working with an example from education, children clustered in classes, analyzing data using linear models, seeing the differences between different classes.\n\n\n\nPart 3 (13 minutes): Doing multilevel models in practice, coding models using lmer(), a first look at analysis results.\n\n\n\n\n\n\n\n\n\n\n\nDownload the lecture slides\n\n\n\nThe slides presented in the videos can be downloaded here:\n\n402-week-17-LME-1.pdf: high resolution .pdf, exactly as delivered [4 MB];\n402-week-17-LME-1_1pp.pdf: low resolution .pdf, printable version, one-slide-per-page [359 KB];\n402-week-17-LME-1_6pp.pdf: low resolution .pdf, printable version, six-slides-per-page [359 KB].\n\nThe high resolution version is the version delivered for the lecture recordings. Because the images are produced to be high resolution, the file size is quite big (4 MB) so, to make the slides easier to download, I produced low resolution versions: 1pp and 6pp. These should be easier to download and print out if that is what you want to do.\n\n\n\n\n\nWe will be working with data taken from a study on education outcomes in Brazilian children, reported by Golino & Gomes (2014).\nYou can read more about these data in Conceptual introduction to multilevel data.\n\n\n\n\n\n\nImportant\n\n\n\nGet the data: get the data file and the .R script you can use to do the exercises that will support your learning.\n\nYou can download the files folder for this chapter by clicking on the link 01-multilevel.zip.\n\n\n\nThe practical materials folder includes:\n\nBAFACALO_DATASET.RData\n402-01-multilevel-workbook.R the workbook you can use to do the practical exercises.\n\nThe data file is in the .RData format: .RData is R’s own file format so the code you use to load and access the data for analysis is a bit simpler than you are used to, as we will see.\n\n\n\n\n\n\nImportant\n\n\n\nYou can access the sign-in page for R-Studio Server here\n\n\n\n\nHere, our learning targets are:\n\nconcepts: multilevel data and multilevel modeling\nskills: visualization – examine overall and within-class trends\nskills: run linear models over all data – and within each class\nskills: use the lmer() function to fit models of multilevel data\n\nThe aims of the practical work are to:\n\nGet practice running the code so that you can reproduce the figures and results from the lecture and in the book chapter.\nExercise skills by varying code – changing variables, changing options – so that you can see how the code works.\nUse the opportunity to reflect on and evaluate results – so that we can support growth in development of understanding of main ideas.\n\n\n\n\nNow you will progress through a series of tasks, and challenges, to aid your learning.\n\n\n\n\n\n\nWarning\n\n\n\nWe will work with the data file:\n\nAFACALO_DATASET.RData\n\n\n\nWe again split the steps into into parts, tasks and questions.\nWe are going to work through the following workflow steps: each step is labelled as a practical part.\n\nSet-up\nLoad the data\nInspect the data\nVisualize relationships\nAnalyze relationships using linear models\nVisualize relationships for each class\nMixed-effects analysis\nOptional exercises: extensions\n\nIn the following, we will guide you through the tasks and questions step by step.\n\n\n\n\n\n\nImportant\n\n\n\nAn answers version of the workbook will be revealed after the practical class.\n\n\n\n\n\nTo begin, we set up our environment in R.\n\n\nUse the library() function to make the functions we need available to you.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlibrary(broom)\nlibrary(gridExtra)\nlibrary(lme4)\n\nLoading required package: Matrix\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::combine() masks gridExtra::combine()\n✖ tidyr::expand()  masks Matrix::expand()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ tidyr::pack()    masks Matrix::pack()\n✖ tidyr::unpack()  masks Matrix::unpack()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\n\n\n\n\nThe data file is called:\n\nBAFACALO_DATASET.RData\n\nUse the load() function to read the data files into R:\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nload(\"BAFACALO_DATASET.RData\")\n\n\n\n\nGet summary statistics to inspect what you have got.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nsummary(BAFACALO_DATASET)\n\n participant_id   scholarity_father scholarity_mother household_income\n Min.   :  1.00   Min.   :0.000     Min.   :0.000     Min.   :1.000   \n 1st Qu.: 73.75   1st Qu.:1.000     1st Qu.:1.000     1st Qu.:1.000   \n Median :146.50   Median :2.000     Median :2.000     Median :2.000   \n Mean   :146.50   Mean   :1.838     Mean   :1.879     Mean   :1.663   \n 3rd Qu.:219.25   3rd Qu.:2.000     3rd Qu.:3.000     3rd Qu.:2.000   \n Max.   :292.00   Max.   :3.000     Max.   :3.000     Max.   :3.000   \n                  NA's   :20        NA's   :19        NA's   :19      \n  class_number previous_school_type      sex              age       \n M11    : 21   Min.   :2            Min.   :0.0000   Min.   :14.00  \n M15    : 20   1st Qu.:2            1st Qu.:0.0000   1st Qu.:15.00  \n M14    : 19   Median :2            Median :1.0000   Median :16.00  \n M36    : 19   Mean   :2            Mean   :0.5348   Mean   :15.71  \n M18    : 18   3rd Qu.:2            3rd Qu.:1.0000   3rd Qu.:16.00  \n (Other):133   Max.   :2            Max.   :1.0000   Max.   :20.00  \n NA's   : 62   NA's   :62           NA's   :62       NA's   :83     \n     VZi01            VZi02            VZi03            VZi04       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.9308   Mean   :0.9062   Mean   :0.8252   Mean   :0.9158  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :3        NA's   :4        NA's   :6        NA's   :7       \n     VZi05            VZi06            VZi07            VZi08       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.8576   Mean   :0.9094   Mean   :0.5445   Mean   :0.8362  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :4        NA's   :5        NA's   :11       NA's   :5       \n     VZi09            VZi10            VZi11            VZi12       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.8403   Mean   :0.9303   Mean   :0.8433   Mean   :0.9041  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :4        NA's   :5        NA's   :24       NA's   :21      \n     VZi13            VZi14            VZi15            VZi16       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:1.0000  \n Median :0.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.4749   Mean   :0.5039   Mean   :0.8593   Mean   :0.9032  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :33       NA's   :38       NA's   :29       NA's   :44      \n     VZi17            VZi18           VZi19            VZi20       \n Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:1.000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.000   Median :1.0000   Median :1.0000  \n Mean   :0.7206   Mean   :0.902   Mean   :0.9421   Mean   :0.6911  \n 3rd Qu.:1.0000   3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.000   Max.   :1.0000   Max.   :1.0000  \n NA's   :45       NA's   :47      NA's   :50       NA's   :46      \n     VZi21            VZi22            VZi23            VZi24       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.7811   Mean   :0.9412   Mean   :0.7788   Mean   :0.9021  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :59       NA's   :54       NA's   :66       NA's   :57      \n     VZi25            VZi26            VZi27            VZi28       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.9788   Mean   :0.8426   Mean   :0.7944   Mean   :0.9673  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :56       NA's   :76       NA's   :78       NA's   :78      \n     VZi29            VZi30            CFi01            CFi02       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.8841   Mean   :0.8889   Mean   :0.8694   Mean   :0.9021  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :85       NA's   :76       NA's   :1        NA's   :6       \n     CFi03            CFi04            CFi05            CFi06      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:0.000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.000  \n Mean   :0.8905   Mean   :0.8298   Mean   :0.6629   Mean   :0.747  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.000  \n NA's   :9        NA's   :10       NA's   :28       NA's   :39     \n     CFi07            CFi08            CFi09            CFi10       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.7234   Mean   :0.7696   Mean   :0.9072   Mean   :0.8397  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :57       NA's   :62       NA's   :1        NA's   :5       \n     CFi11            CFi12            CFi13            CFi14       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.9537   Mean   :0.5933   Mean   :0.6877   Mean   :0.9733  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :11       NA's   :24       NA's   :23       NA's   :30      \n     CFi15            CFi16            CFi17            CFi18       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.7406   Mean   :0.8455   Mean   :0.6926   Mean   :0.9683  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :53       NA's   :59       NA's   :9        NA's   :8       \n     CFi19            CFi20            CFi21            CFi22       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.7782   Mean   :0.5927   Mean   :0.7546   Mean   :0.7683  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :17       NA's   :17       NA's   :23       NA's   :46      \n     CFi23            CFi24            CFi25            CFi26       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.7956   Mean   :0.6453   Mean   :0.8662   Mean   :0.8794  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :67       NA's   :58       NA's   :8        NA's   :10      \n     CFi27            CFi28            CFi29            CFi30      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:1.000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.000  \n Mean   :0.7319   Mean   :0.6444   Mean   :0.6279   Mean   :0.753  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.000  \n NA's   :16       NA's   :22       NA's   :34       NA's   :45     \n     CFi31            CFi32            V1i01          V1i02       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.00   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:0.75   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :1.00   Median :1.0000  \n Mean   :0.7902   Mean   :0.7061   Mean   :0.75   Mean   :0.6667  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.00   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.00   Max.   :1.0000  \n NA's   :68       NA's   :64                      NA's   :40      \n     V1i03            V1i04            V1i05            V1i06       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.6329   Mean   :0.5548   Mean   :0.9821   Mean   :0.6667  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :85       NA's   :146      NA's   :13       NA's   :175     \n     V1i07            V1i08            V1i09            V1i10      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.000  \n Median :1.0000   Median :0.0000   Median :0.0000   Median :1.000  \n Mean   :0.6371   Mean   :0.4663   Mean   :0.4351   Mean   :0.668  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.000  \n NA's   :168      NA's   :84       NA's   :138      NA's   :51     \n     V1i11            V1i12            V1i13            V1i14      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:1.000  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :1.000  \n Mean   :0.4675   Mean   :0.4074   Mean   :0.3582   Mean   :0.964  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.000  \n NA's   :138      NA's   :130      NA's   :225      NA's   :14     \n     V1i15            V1i16            V1i17           V1i18      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.000   Min.   :0.000  \n 1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:1.000   1st Qu.:0.000  \n Median :1.0000   Median :0.0000   Median :1.000   Median :1.000  \n Mean   :0.9819   Mean   :0.3636   Mean   :0.799   Mean   :0.659  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.000   3rd Qu.:1.000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.000   Max.   :1.000  \n NA's   :15       NA's   :215      NA's   :88      NA's   :75     \n     V1i19            V1i20            V1i21           V1i22       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000  \n Median :1.0000   Median :0.0000   Median :0.000   Median :0.0000  \n Mean   :0.8473   Mean   :0.4466   Mean   :0.181   Mean   :0.0885  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.000   Max.   :1.0000  \n NA's   :17       NA's   :189      NA's   :187     NA's   :66      \n     V1i23            V1i24           V2i01            V2i02       \n Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.000   Median :0.0000   Median :1.0000  \n Mean   :0.7011   Mean   :0.587   Mean   :0.4536   Mean   :0.7056  \n 3rd Qu.:1.0000   3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.000   Max.   :1.0000   Max.   :1.0000  \n NA's   :118      NA's   :200     NA's   :109      NA's   :95      \n     V2i03            V2i04            V2i05            V2i06      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.000  \n Median :1.0000   Median :0.0000   Median :0.0000   Median :0.000  \n Mean   :0.6078   Mean   :0.4796   Mean   :0.3387   Mean   :0.336  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.000  \n NA's   :60       NA's   :194      NA's   :230      NA's   :167    \n     V2i07            V2i08            V2i09            V2i10       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :0.0000   Median :1.0000   Median :1.0000   Median :0.0000  \n Mean   :0.3333   Mean   :0.7644   Mean   :0.8122   Mean   :0.3735  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :166      NA's   :101      NA's   :63       NA's   :209     \n     V2i11            V2i12            V2i13            V2i14       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :0.0000   Median :1.0000   Median :1.0000  \n Mean   :0.6455   Mean   :0.3617   Mean   :0.5645   Mean   :0.6035  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :103      NA's   :104      NA's   :168      NA's   :65      \n     V2i15            V2i16            V2i17            V2i18       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:1.0000  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :1.0000  \n Mean   :0.2807   Mean   :0.4789   Mean   :0.2206   Mean   :0.7716  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :178      NA's   :221      NA's   :224      NA's   :130     \n     V3i01            V3i02            V3i03            V3i04       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.7717   Mean   :0.8007   Mean   :0.6917   Mean   :0.8783  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :16       NA's   :6        NA's   :39       NA's   :29      \n     V3i05            V3i06            V3i07            V3i08       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :0.0000   Median :0.0000  \n Mean   :0.6637   Mean   :0.7143   Mean   :0.4083   Mean   :0.3417  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :69       NA's   :110      NA's   :172      NA's   :172     \n     V3i09            V3i10            V3i11            V3i12       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:1.0000  \n Median :0.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.1497   Mean   :0.5728   Mean   :0.6528   Mean   :0.8421  \n 3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :145      NA's   :189      NA's   :76       NA's   :64      \n     V3i13            V3i14            V3i15            V3i16       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.2333   Mean   :0.8537   Mean   :0.6475   Mean   :0.5361  \n 3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :172      NA's   :169      NA's   :153      NA's   :29      \n     V3i17            V3i18            MA1i01          MA1i02      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:1.000   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :1.000   Median :1.0000  \n Mean   :0.8112   Mean   :0.5152   Mean   :0.766   Mean   :0.7143  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.000   Max.   :1.0000  \n NA's   :43       NA's   :94       NA's   :104     NA's   :117     \n     MA1i03           MA1i04           MA1i05           MA1i06      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.7299   Mean   :0.7989   Mean   :0.7333   Mean   :0.6835  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :118      NA's   :103      NA's   :142      NA's   :134     \n     MA1i07           MA1i08           MA1i09         MA1i10      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.00   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.00   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :1.00   Median :1.0000  \n Mean   :0.8133   Mean   :0.8589   Mean   :0.85   Mean   :0.6835  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.00   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.00   Max.   :1.0000  \n NA's   :142      NA's   :51       NA's   :92     NA's   :153     \n     MA1i11          MA1i12           MA1i13           MA1i14      \n Min.   :0.000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :1.000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.698   Mean   :0.9123   Mean   :0.8664   Mean   :0.7321  \n 3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :143     NA's   :64       NA's   :60       NA's   :124     \n     MA1i15           MA2i01           MA2i02          MA2i03    \n Min.   :0.0000   Min.   :0.0000   Min.   :0.000   Min.   :0.00  \n 1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:1.000   1st Qu.:1.00  \n Median :1.0000   Median :1.0000   Median :1.000   Median :1.00  \n Mean   :0.7429   Mean   :0.8373   Mean   :0.934   Mean   :0.88  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.000   3rd Qu.:1.00  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.000   Max.   :1.00  \n NA's   :117      NA's   :126      NA's   :80      NA's   :167   \n     MA2i04           MA2i05           MA2i06           MA2i07      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.9118   Mean   :0.8705   Mean   :0.8919   Mean   :0.9264  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :122      NA's   :153      NA's   :144      NA's   :129     \n     MA2i08           MA2i09           MA2i10          MVi01       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :1.000   Median :1.0000  \n Mean   :0.9522   Mean   :0.8676   Mean   :0.872   Mean   :0.9084  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.000   Max.   :1.0000  \n NA's   :83       NA's   :141      NA's   :128     NA's   :19      \n     MViO2            MViO3            MVi02            MViO4      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.000  \n Mean   :0.8851   Mean   :0.9137   Mean   :0.8881   Mean   :0.876  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.000  \n NA's   :31       NA's   :14       NA's   :24       NA's   :34     \n     MViO5            MVi03            MViO6            MViO7      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.000  \n Mean   :0.9504   Mean   :0.8606   Mean   :0.9825   Mean   :0.875  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.000  \n NA's   :10       NA's   :41       NA's   :6        NA's   :20     \n     MVi04            MViO8            MViO9            RGi01       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.8705   Mean   :0.8476   Mean   :0.8835   Mean   :0.7845  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :14       NA's   :23       NA's   :26       NA's   :9       \n     RGi02            RGi03            RGi04            RGi05       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.8143   Mean   :0.8185   Mean   :0.6392   Mean   :0.6875  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :12       NA's   :11       NA's   :37       NA's   :68      \n     RGi06            RGi07           RGi08            RGi09       \n Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:0.000   1st Qu.:1.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.000   Median :1.0000   Median :1.0000  \n Mean   :0.7578   Mean   :0.628   Mean   :0.8389   Mean   :0.8544  \n 3rd Qu.:1.0000   3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.000   Max.   :1.0000   Max.   :1.0000  \n NA's   :69       NA's   :85      NA's   :81       NA's   :86      \n     RGi10           RGi11            RGi12            RGi13       \n Min.   :0.000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.500   1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.000   Median :1.0000   Median :1.0000   Median :0.0000  \n Mean   :0.748   Mean   :0.8261   Mean   :0.7027   Mean   :0.4324  \n 3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :169     NA's   :154      NA's   :181      NA's   :255     \n     RGi14            RGi15         RLi01            RLi02       \n Min.   :0.0000   Min.   :0.0   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0   1st Qu.:1.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0   Median :1.0000   Median :1.0000  \n Mean   :0.9474   Mean   :0.8   Mean   :0.8897   Mean   :0.8828  \n 3rd Qu.:1.0000   3rd Qu.:1.0   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0   Max.   :1.0000   Max.   :1.0000  \n NA's   :197      NA's   :222   NA's   :2        NA's   :2       \n     RLi03            RLi04            RLi05            RLi06       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :0.0000   Median :1.0000  \n Mean   :0.8625   Mean   :0.6862   Mean   :0.4688   Mean   :0.8815  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :1        NA's   :2        NA's   :4        NA's   :5       \n     RLi07            RLi08            RLi09            RLi10       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.7042   Mean   :0.7163   Mean   :0.8505   Mean   :0.6725  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :8        NA's   :10       NA's   :11       NA's   :8       \n     RLi11            RLi12            RLi13            RLi14       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :0.0000   Median :1.0000   Median :0.0000  \n Mean   :0.6559   Mean   :0.4562   Mean   :0.6277   Mean   :0.2321  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :13       NA's   :18       NA's   :10       NA's   :12      \n     RLi15            RLi16            RLi17            RLi18       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :0.0000   Median :1.0000  \n Mean   :0.6431   Mean   :0.9381   Mean   :0.2962   Mean   :0.8676  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :9        NA's   :1        NA's   :5        NA's   :5       \n     RLi19           RLi20           RLi21            RLi22       \n Min.   :0.000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.000   1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.000   Median :0.000   Median :1.0000   Median :0.0000  \n Mean   :0.617   Mean   :0.259   Mean   :0.7367   Mean   :0.4488  \n 3rd Qu.:1.000   3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.000   Max.   :1.000   Max.   :1.0000   Max.   :1.0000  \n NA's   :10      NA's   :14      NA's   :11       NA's   :9       \n     RLi23            RLi24            RLi25            RLi26       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :0.0000   Median :0.0000  \n Mean   :0.8269   Mean   :0.5632   Mean   :0.3885   Mean   :0.3971  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :9        NA's   :15       NA's   :14       NA's   :15      \n     RLi27            RLi28            RLi29            RLi30       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.8885   Mean   :0.7201   Mean   :0.7426   Mean   :0.6527  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :14       NA's   :24       NA's   :20       NA's   :30      \n      Ii01             Ii02             Ii03             Ii04       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.9618   Mean   :0.8635   Mean   :0.7658   Mean   :0.8561  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :4        NA's   :43       NA's   :23       NA's   :21      \n      Ii05             Ii06             Ii07             Ii08       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.7961   Mean   :0.8352   Mean   :0.9299   Mean   :0.7371  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :37       NA's   :31       NA's   :21       NA's   :41      \n      Ii09            Ii10             Ii11             Ii12       \n Min.   :0.000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.786   Mean   :0.6161   Mean   :0.6284   Mean   :0.5353  \n 3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :35      NA's   :68       NA's   :74       NA's   :122     \n      Ii13             Ii14             Ii15              N        \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :13.00  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:27.00  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :32.00  \n Mean   :0.2229   Mean   :0.4938   Mean   :0.4724   Mean   :31.59  \n 3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:36.00  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :58.00  \n NA's   :126      NA's   :130      NA's   :165      NA's   :62     \n       P1             P2              P3           FI1             FI2        \n Min.   : 6.0   Min.   : 3.00   48     : 14   Min.   : 0.00   Min.   : 1.000  \n 1st Qu.:22.0   1st Qu.:17.00   42     : 12   1st Qu.: 9.00   1st Qu.: 7.000  \n Median :26.5   Median :19.00   32     : 11   Median :12.00   Median : 9.000  \n Mean   :27.6   Mean   :19.36   40     : 10   Mean   :12.85   Mean   : 9.461  \n 3rd Qu.:33.0   3rd Qu.:22.00   39     :  9   3rd Qu.:15.00   3rd Qu.:12.000  \n Max.   :50.0   Max.   :32.00   (Other):174   Max.   :38.00   Max.   :22.000  \n NA's   :62     NA's   :62      NA's   : 62   NA's   :62      NA's   :62      \n       FF           portuguese     english         math        biology   \n Min.   : 2.000   60     : 10   100    : 17   60     : 13   80     : 15  \n 1st Qu.: 6.000   76     :  9   79     : 11   69     :  9   60     : 13  \n Median : 8.000   65     :  8   96     :  8   70     :  9   70     : 10  \n Mean   : 9.061   73     :  8   81     :  7   62     :  8   68.5   :  4  \n 3rd Qu.:11.750   74     :  7   89     :  7   71     :  7   75     :  4  \n Max.   :20.000   (Other):188   (Other):180   (Other):184   (Other):184  \n NA's   :62       NA's   : 62   NA's   : 62   NA's   : 62   NA's   : 62  \n    physics      chimestry     geography      history   \n 60     : 16   79.9   :  3   60     :  7   87     :  5  \n 60.1   :  5   84.1   :  3   63.5   :  5   88.5   :  4  \n 0      :  4   85     :  3   66     :  5   71     :  3  \n 61.8   :  4   85.6   :  3   0      :  3   75.5   :  3  \n 66     :  4   89     :  3   64     :  3   79.5   :  3  \n (Other):197   (Other):215   (Other): 98   (Other): 91  \n NA's   : 62   NA's   : 62   NA's   :171   NA's   :183  \n\n\n\n\n\n\n\n\n\n\n\nTidying the data involves a number of tasks, some essential and some things we do for our convenience.\nWe start by selecting the variables we want:\n\nclass_number, participant_id, portuguese, english, math, physics\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nWe use the {dplyr} function select() to pick just the variables needed.\nWe create a new data-set (call it what you like), from the original, to hold just the selection of variables we want to use in the analysis steps, following.\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nbrazil &lt;- BAFACALO_DATASET %&gt;%\n                              select(\n                              class_number, participant_id,\n                              portuguese, english, math, physics\n                              )\n\n\n\n\n\nPract.Q.1. Do you know what you are doing with select()? Can you select a different set of variables?\n\n\nPract.A.1. You can exercise your skill by checking that you can view the BAFACALO_DATASET and select any variable you like. Be careful and make sure you go back to select the variables shown in the given code chunk: we will need them later.\n\n\n\n\nThere are several rows in the data-set with missing values. You need to remove these from the data before you get started with any calculations.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nbrazil &lt;- na.omit(brazil)\n\n\n\n\nWhat have you got?\nYou should always check the results of a tidying operation, to make sure that what you have got matches what you expected to get.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nsummary(brazil)\n\n  class_number participant_id    portuguese     english         math    \n M11    : 21   Min.   :  3.0   60     : 10   100    : 17   60     : 13  \n M15    : 20   1st Qu.: 77.5   76     :  9   79     : 11   69     :  9  \n M14    : 19   Median :144.5   65     :  8   96     :  8   70     :  9  \n M36    : 19   Mean   :146.6   73     :  8   81     :  7   62     :  8  \n M18    : 18   3rd Qu.:222.8   74     :  7   89     :  7   71     :  7  \n M21    : 17   Max.   :291.0   82     :  7   86     :  6   66     :  6  \n (Other):116                   (Other):181   (Other):174   (Other):178  \n    physics   \n 60     : 16  \n 60.1   :  5  \n 0      :  4  \n 61.8   :  4  \n 66     :  4  \n 74.2   :  4  \n (Other):193  \n\n\n\n\n\n\nPract.Q.2. Do you see the difference between the summary of the brazil data shown before and after you run na.omit()? What is it?\n\n\nPract.A.2. You should see that NAs are listed in some columns in the data before but not after you run na.omit().\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nbrazil$portuguese &lt;- as.numeric(brazil$portuguese)\nbrazil$english &lt;- as.numeric(brazil$english)\nbrazil$math &lt;- as.numeric(brazil$math)\nbrazil$physics &lt;- as.numeric(brazil$physics)\n\nbrazil$class_number &lt;- as.factor(brazil$class_number)\nbrazil$participant_id &lt;- as.factor(brazil$participant_id)\n\n\n\n\nWhat have you got?\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nsummary(brazil)\n\n  class_number participant_id   portuguese       english           math       \n M11    : 21   3      :  1    Min.   : 1.00   Min.   : 1.00   Min.   :  1.00  \n M15    : 20   4      :  1    1st Qu.:26.25   1st Qu.:29.00   1st Qu.: 39.00  \n M14    : 19   5      :  1    Median :42.00   Median :55.00   Median : 58.00  \n M36    : 19   6      :  1    Mean   :43.55   Mean   :50.25   Mean   : 58.17  \n M18    : 18   7      :  1    3rd Qu.:58.75   3rd Qu.:73.00   3rd Qu.: 79.75  \n M21    : 17   9      :  1    Max.   :83.00   Max.   :93.00   Max.   :104.00  \n (Other):116   (Other):224                                                    \n    physics      \n Min.   :  1.00  \n 1st Qu.: 34.00  \n Median : 71.50  \n Mean   : 71.57  \n 3rd Qu.:107.75  \n Max.   :148.00  \n                 \n\n\n\n\n\n\nPract.Q.3. Do you see the difference between the summary of the brazil data shown before and after you run as.numeric() or as.factor() function lines? What is it?\n\n\nPract.A.3. You should see that numeric variables like portuguese are listed with summary statistics like the mean after but not before you coerce the variables.\n\n\n\n\n\nTest out variable type using the is...() function for some of the variables\nTest out coercion – and its results – using the as...() function for some of the variables\nLook at the results using summary()\n\n\n\n\n\nHere, we are interested in the relationship between school grades, for the sample of Brazilian children for whom we have data.\n\nWhat is the relationship between portuguese and english grades?\n\n\n\nThis should be a revision exercise for you.\n\nDo you notice any changes in the way in which the plotting code is laid out?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nbrazil %&gt;%\n  ggplot(aes(x = portuguese, y = english)) +\n  geom_point(colour = \"black\", size = 2.5, alpha = .5) +\n  geom_smooth(method = \"lm\", size = 1.5, se = FALSE, colour = \"red\") +\n  xlab(\"Portuguese\") + ylab(\"English\") + theme_bw() \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChange the x and y variables to math and physics\nChange the theme from theme_bw() to something different\nChange the appearance of the points, try different colour, shape, size\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the ggplot reference documentation to help you make choices:\n\nhttps://ggplot2.tidyverse.org/reference/\n\nYou should be using webpages like the reference page often in your work.\n\n\n\n\n\n\n\nWe move next to analyzing the relationship between variation in grades in different subjects, across the children in our sample.\n\n\nYou should be able to reproduce the results shown in the slides and the book chapter.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nsummary(lm(english ~ portuguese, data = brazil))\n\n\nCall:\nlm(formula = english ~ portuguese, data = brazil)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-64.909 -17.573   2.782  20.042  53.292 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 29.77780    3.81426   7.807 2.11e-13 ***\nportuguese   0.47001    0.07897   5.952 9.91e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 25.02 on 228 degrees of freedom\nMultiple R-squared:  0.1345,    Adjusted R-squared:  0.1307 \nF-statistic: 35.43 on 1 and 228 DF,  p-value: 9.906e-09\n\n\n\n\n\n\n\n\nChange the outcome and predictor variables to math and physics.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nsummary(lm(physics ~ math, data = brazil))\n\n\nCall:\nlm(formula = physics ~ math, data = brazil)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-70.360 -14.265   0.437  14.797  78.784 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.83703    3.71858  -0.494    0.622    \nmath         1.26202    0.05795  21.777   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.81 on 228 degrees of freedom\nMultiple R-squared:  0.6753,    Adjusted R-squared:  0.6739 \nF-statistic: 474.2 on 1 and 228 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\nPract.Q.4. What is the estimated coefficient of the “effect” of math ability (grade) on physics grade?\n\n\nPract.A.4. The summary shows that physics grades are on average 1.3 higher for unit increase in math grade.\n\n\nPract.Q.5. Draw a scatterplot showing the relationship between math and physics grades. Does the trend you see in the plot reflect the coefficient you see in the linear model summary?\n\n\nPract.A.5. The plot shows the positive association between math and physics grade also indicated by the estimated coefficient of the math effect.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nbrazil %&gt;%\n  ggplot(aes(x = math, y = physics)) +\n  geom_point(colour = \"black\", size = 2.5, alpha = .5) +\n  geom_smooth(method = \"lm\", size = 1.5, se = FALSE, colour = \"red\") +\n  theme_bw() \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\nPract.Q.6. How does the strength of the math-physics relationship compare with the english-portuguese relationship?\n\n\nPract.A.6. Both the linear model and the plots indicate that the math-physics relationship is much stronger.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = brazil, aes(x = portuguese, y = english)) +\n  geom_point(colour = \"darkgrey\") + \n  geom_smooth(method = \"lm\", se = FALSE, colour = \"black\") +\n  xlab(\"Portuguese\") + ylab(\"English\") + theme_bw() +\n  scale_x_continuous(breaks=c(25,50,75)) + scale_y_continuous(breaks=c(0,50,100)) +\n  facet_wrap(~ class_number) \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChange the x and y variables to math and physics and draw a facetted scatterplot again\nExperiment with showing the differences between classes in a different way: instead of using facet_wrap(), in aes() add colour = class_number, and remove colour from geom_point and geom_smooth\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = brazil, aes(x = math, y = physics)) +\n  geom_point(colour = \"darkgrey\") + \n  geom_smooth(method = \"lm\", se = FALSE, colour = \"black\") +\n  facet_wrap(~ class_number) \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot(data = brazil, aes(x = math, y = physics, colour = class_number)) +\n  geom_point() + geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\nPract.Q.8. Evaluate the consistency between classes of the relationship between math and physics grades: what do the plots show? how does this compare with what you see of the relationship between english and portuguese grades?\n\n\nPract.A.8. The plots show that the relationship between math and physics is very consistent between classes, and more consistent than the relationship between english and portuguese grades appears to be.\n\n\n\n\n\n\n\nYou should be able to replicate the results shown in the slides and the book chapter.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nporto.lmer1 &lt;- lmer(english ~ portuguese +\n                      (portuguese + 1|class_number),\n                      data = brazil)\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.0391843 (tol = 0.002, component 1)\n\nsummary(porto.lmer1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: english ~ portuguese + (portuguese + 1 | class_number)\n   Data: brazil\n\nREML criterion at convergence: 2104.3\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.81240 -0.59545  0.04338  0.59975  2.23667 \n\nRandom effects:\n Groups       Name        Variance Std.Dev. Corr \n class_number (Intercept) 338.9057 18.4094       \n              portuguese    0.3278  0.5725  -0.98\n Residual                 493.2866 22.2101       \nNumber of obs: 230, groups:  class_number, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  25.2838     6.2522   4.044\nportuguese    0.6589     0.1726   3.817\n\nCorrelation of Fixed Effects:\n           (Intr)\nportuguese -0.943\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.0391843 (tol = 0.002, component 1)\n\n\n\n\n\n\n\n\n\nVary the random effects part of the model, while keeping this bit the same: lmer(english ~ portuguese + ...)\nChange the random effect from (portuguese + 1 | class_number) to (1 | class_number): what you are doing is asking R to ignore the differences in the slope of the effect of Portuguese grades.\nChange the random effect from (portuguese + 1 | class_number) to (portuguese + 0 | class_number): what you are doing is asking R to ignore the differences in the intercept\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nporto.lmer1 &lt;- lmer(english ~ portuguese +\n                      (portuguese + 1|class_number),\n                    data = brazil)\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.0391843 (tol = 0.002, component 1)\n\nsummary(porto.lmer1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: english ~ portuguese + (portuguese + 1 | class_number)\n   Data: brazil\n\nREML criterion at convergence: 2104.3\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.81240 -0.59545  0.04338  0.59975  2.23667 \n\nRandom effects:\n Groups       Name        Variance Std.Dev. Corr \n class_number (Intercept) 338.9057 18.4094       \n              portuguese    0.3278  0.5725  -0.98\n Residual                 493.2866 22.2101       \nNumber of obs: 230, groups:  class_number, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  25.2838     6.2522   4.044\nportuguese    0.6589     0.1726   3.817\n\nCorrelation of Fixed Effects:\n           (Intr)\nportuguese -0.943\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.0391843 (tol = 0.002, component 1)\n\n#\n\nporto.lmer2 &lt;- lmer(english ~ portuguese +\n                      (1|class_number),\n                    data = brazil)\nsummary(porto.lmer2)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: english ~ portuguese + (1 | class_number)\n   Data: brazil\n\nREML criterion at convergence: 2129.2\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.55966 -0.68483  0.08471  0.77210  2.51034 \n\nRandom effects:\n Groups       Name        Variance Std.Dev.\n class_number (Intercept)  47.59    6.899  \n Residual                 588.91   24.267  \nNumber of obs: 230, groups:  class_number, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 26.02493    4.60799   5.648\nportuguese   0.55060    0.08995   6.121\n\nCorrelation of Fixed Effects:\n           (Intr)\nportuguese -0.856\n\n#\n\nporto.lmer3 &lt;- lmer(english ~ portuguese +\n                      (portuguese + 0|class_number),\n                    data = brazil)\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.00236563 (tol = 0.002, component 1)\n\nsummary(porto.lmer3)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: english ~ portuguese + (portuguese + 0 | class_number)\n   Data: brazil\n\nREML criterion at convergence: 2116.1\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.74726 -0.65271  0.03263  0.64709  2.71650 \n\nRandom effects:\n Groups       Name       Variance Std.Dev.\n class_number portuguese   0.053   0.2302 \n Residual                535.157  23.1335 \nNumber of obs: 230, groups:  class_number, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  22.0168     4.0359   5.455\nportuguese    0.6813     0.1087   6.266\n\nCorrelation of Fixed Effects:\n           (Intr)\nportuguese -0.778\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.00236563 (tol = 0.002, component 1)\n\n\n\n\n\n\nPract.Q.9. Compare the results of the different versions of the model. Can you identify where the results are different?\n\n\nPract.A.9. It can be seen that Rhe estimated effect of portuguese varies between the models but the estimate is more similar, around .65, where the random effect is specified as (portuguese + 1|class_number) or (portuguese + 0|class_number). The residual variance term is different between the models. Which random effects variances are shown is also different. There is a convergence warning for:\n\nenglish ~ portuguese + (portuguese + 0 | class_number)\n\n\n\nChange the outcome (from english) and the predictor (from portuguese) – this is about changing the fixed effect part of the model.\n\nNote that you will need to change the random effect part as well.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nporto.lmer1 &lt;- lmer(physics ~ math +\n                      (math + 1|class_number),\n                    data = brazil)\n\nboundary (singular) fit: see help('isSingular')\n\nsummary(porto.lmer1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: physics ~ math + (math + 1 | class_number)\n   Data: brazil\n\nREML criterion at convergence: 2066\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.9688 -0.5956  0.0461  0.6276  3.4865 \n\nRandom effects:\n Groups       Name        Variance  Std.Dev. Corr\n class_number (Intercept) 8.827e+01  9.39520     \n              math        3.892e-03  0.06238 1.00\n Residual                 4.118e+02 20.29398     \nNumber of obs: 230, groups:  class_number, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  1.46514    4.10293   0.357\nmath         1.19196    0.05564  21.423\n\nCorrelation of Fixed Effects:\n     (Intr)\nmath -0.570\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\n\n\n\n\nPract.Q.10. What elements of the model summary stand out for you? It will help to see what you should notice if you compare the math-physics model with the first english-portuguese model.\n\n\nPract.A.10. You may notice that:\n\n\nThe summary comes with a fit is singular? warning.\nThe variance terms for intercept or the math effect by class number and the residual are very very small: much smaller than for the english-portuguese model.\n\n\n\n\n\n\n\nIn the lecture materials, I display a plot showing the estimated intercept and coefficient for each class, estimated using separate models for different classes.\n\nSome of you may be interested in how I did that, you can run the following code to see.\n\n\nUse the {dplyr} %&gt;% syntax to run a model for each class separately, collect together the results into a dataframe.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nbrazlm &lt;- brazil %&gt;% group_by(class_number) %&gt;% do(tidy(lm(english ~ portuguese, data=.)))\nbrazlm$term &lt;- as.factor(brazlm$term)\n\n\n\n\n\nExtract the per-class estimates of the intercepts and the ‘portuguese’ effect coefficient estimates.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nbrazlmint &lt;- filter(brazlm, term == '(Intercept)')\nbrazlmport &lt;- filter(brazlm, term == 'portuguese')\n\n\n\n\n\nPlot the estimates.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\npbrazlmint &lt;- ggplot(brazlmint, aes(x = class_number, y = estimate, ymin = estimate - std.error, ymax = estimate + std.error))\npbrazlmint &lt;- pbrazlmint + geom_point(size = 2) + geom_linerange() + theme_bw() \npbrazlmint &lt;- pbrazlmint + ggtitle(\"Intercept\") + ylab(\"Estimated coefficient +/- SE\") + xlab(\"Class\")\npbrazlmint &lt;- pbrazlmint + theme(axis.title.y = element_text(size = 10), axis.text.x = element_blank(), panel.grid = element_blank())\n# pbrazlmint\n\npbrazlmport &lt;- ggplot(brazlmport, aes(x = class_number, y = estimate, ymin = estimate - std.error, ymax = estimate + std.error))\npbrazlmport &lt;- pbrazlmport + geom_point(size = 2) + geom_linerange() + theme_bw() \npbrazlmport &lt;- pbrazlmport + ggtitle(\"Portuguese effect\") + ylab(\"Estimated coefficient +/- SE\") + xlab(\"Class\")\npbrazlmport &lt;- pbrazlmport + theme(axis.title.y = element_text(size = 10), axis.text.x = element_blank(), panel.grid = element_blank())\n# pbrazlmport\n\n# -- ask R to make a grid:\n\ngrid.arrange(pbrazlmint, pbrazlmport,\n             ncol = 2)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge: Can you change the code to show the estimates for the relationship between physics and math grades?\n\n\n\n\n\nAfter the practical class, we will reveal the answers that are currently hidden.\nThe answers version of the webpage will present my answers for questions, and some extra information where that is helpful.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 16. Workbook introduction to multilevel data"
    ]
  },
  {
    "objectID": "PSYC412/part2/01-multilevel-workbook.html#sec-multilevel-workbook-targets",
    "href": "PSYC412/part2/01-multilevel-workbook.html#sec-multilevel-workbook-targets",
    "title": "Week 16. Workbook introduction to multilevel data",
    "section": "",
    "text": "Our learning objectives include the development of key concepts and skills.\n\nconcepts – how data can have multilevel structures and what this requires in models\nskills – where skills comprise the capacity to:\n\n\nuse visualization to examine observations within groups\nrun linear models over all data and within each class\nuse the lmer() function to fit models of multilevel data\n\nWe are just getting started this week. Our plan will be to build depth and breadth in understanding as we progress over the next few weeks.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 16. Workbook introduction to multilevel data"
    ]
  },
  {
    "objectID": "PSYC412/part2/01-multilevel-workbook.html#sec-multilevel-workbook-resources",
    "href": "PSYC412/part2/01-multilevel-workbook.html#sec-multilevel-workbook-resources",
    "title": "Week 16. Workbook introduction to multilevel data",
    "section": "",
    "text": "You will see, next, the lectures we share to explain the concepts you will learn about, and the practical data analysis skills you will develop. Then you will see information about the practical materials you can use to build and practise your skills.\nEvery week, you will learn best if you first watch the lectures then do the practical exercises.\n\n\n\n\n\n\nLinked resources\n\n\n\nTo help your learning, you can read about the ideas and the practical coding required for analyses in the chapters I wrote for this course.\nThe chapter: Conceptual introduction to multilevel data\n\n\n\n\nThe lecture materials for this week are presented in three short parts.\nClick on a link and your browser should open a tab showing the Panopto video for the lecture part.\n\nPart 1 (17 minutes) Multilevel data: What do we mean when we talk about multilevel structured data and why it matters.\n\n\n\nPart 2 (13 minutes): Getting started in learning, working with an example from education, children clustered in classes, analyzing data using linear models, seeing the differences between different classes.\n\n\n\nPart 3 (13 minutes): Doing multilevel models in practice, coding models using lmer(), a first look at analysis results.\n\n\n\n\n\n\n\n\n\n\n\nDownload the lecture slides\n\n\n\nThe slides presented in the videos can be downloaded here:\n\n402-week-17-LME-1.pdf: high resolution .pdf, exactly as delivered [4 MB];\n402-week-17-LME-1_1pp.pdf: low resolution .pdf, printable version, one-slide-per-page [359 KB];\n402-week-17-LME-1_6pp.pdf: low resolution .pdf, printable version, six-slides-per-page [359 KB].\n\nThe high resolution version is the version delivered for the lecture recordings. Because the images are produced to be high resolution, the file size is quite big (4 MB) so, to make the slides easier to download, I produced low resolution versions: 1pp and 6pp. These should be easier to download and print out if that is what you want to do.\n\n\n\n\n\nWe will be working with data taken from a study on education outcomes in Brazilian children, reported by Golino & Gomes (2014).\nYou can read more about these data in Conceptual introduction to multilevel data.\n\n\n\n\n\n\nImportant\n\n\n\nGet the data: get the data file and the .R script you can use to do the exercises that will support your learning.\n\nYou can download the files folder for this chapter by clicking on the link 01-multilevel.zip.\n\n\n\nThe practical materials folder includes:\n\nBAFACALO_DATASET.RData\n402-01-multilevel-workbook.R the workbook you can use to do the practical exercises.\n\nThe data file is in the .RData format: .RData is R’s own file format so the code you use to load and access the data for analysis is a bit simpler than you are used to, as we will see.\n\n\n\n\n\n\nImportant\n\n\n\nYou can access the sign-in page for R-Studio Server here\n\n\n\n\nHere, our learning targets are:\n\nconcepts: multilevel data and multilevel modeling\nskills: visualization – examine overall and within-class trends\nskills: run linear models over all data – and within each class\nskills: use the lmer() function to fit models of multilevel data\n\nThe aims of the practical work are to:\n\nGet practice running the code so that you can reproduce the figures and results from the lecture and in the book chapter.\nExercise skills by varying code – changing variables, changing options – so that you can see how the code works.\nUse the opportunity to reflect on and evaluate results – so that we can support growth in development of understanding of main ideas.\n\n\n\n\nNow you will progress through a series of tasks, and challenges, to aid your learning.\n\n\n\n\n\n\nWarning\n\n\n\nWe will work with the data file:\n\nAFACALO_DATASET.RData\n\n\n\nWe again split the steps into into parts, tasks and questions.\nWe are going to work through the following workflow steps: each step is labelled as a practical part.\n\nSet-up\nLoad the data\nInspect the data\nVisualize relationships\nAnalyze relationships using linear models\nVisualize relationships for each class\nMixed-effects analysis\nOptional exercises: extensions\n\nIn the following, we will guide you through the tasks and questions step by step.\n\n\n\n\n\n\nImportant\n\n\n\nAn answers version of the workbook will be revealed after the practical class.\n\n\n\n\n\nTo begin, we set up our environment in R.\n\n\nUse the library() function to make the functions we need available to you.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlibrary(broom)\nlibrary(gridExtra)\nlibrary(lme4)\n\nLoading required package: Matrix\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::combine() masks gridExtra::combine()\n✖ tidyr::expand()  masks Matrix::expand()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ tidyr::pack()    masks Matrix::pack()\n✖ tidyr::unpack()  masks Matrix::unpack()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\n\n\n\n\nThe data file is called:\n\nBAFACALO_DATASET.RData\n\nUse the load() function to read the data files into R:\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nload(\"BAFACALO_DATASET.RData\")\n\n\n\n\nGet summary statistics to inspect what you have got.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nsummary(BAFACALO_DATASET)\n\n participant_id   scholarity_father scholarity_mother household_income\n Min.   :  1.00   Min.   :0.000     Min.   :0.000     Min.   :1.000   \n 1st Qu.: 73.75   1st Qu.:1.000     1st Qu.:1.000     1st Qu.:1.000   \n Median :146.50   Median :2.000     Median :2.000     Median :2.000   \n Mean   :146.50   Mean   :1.838     Mean   :1.879     Mean   :1.663   \n 3rd Qu.:219.25   3rd Qu.:2.000     3rd Qu.:3.000     3rd Qu.:2.000   \n Max.   :292.00   Max.   :3.000     Max.   :3.000     Max.   :3.000   \n                  NA's   :20        NA's   :19        NA's   :19      \n  class_number previous_school_type      sex              age       \n M11    : 21   Min.   :2            Min.   :0.0000   Min.   :14.00  \n M15    : 20   1st Qu.:2            1st Qu.:0.0000   1st Qu.:15.00  \n M14    : 19   Median :2            Median :1.0000   Median :16.00  \n M36    : 19   Mean   :2            Mean   :0.5348   Mean   :15.71  \n M18    : 18   3rd Qu.:2            3rd Qu.:1.0000   3rd Qu.:16.00  \n (Other):133   Max.   :2            Max.   :1.0000   Max.   :20.00  \n NA's   : 62   NA's   :62           NA's   :62       NA's   :83     \n     VZi01            VZi02            VZi03            VZi04       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.9308   Mean   :0.9062   Mean   :0.8252   Mean   :0.9158  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :3        NA's   :4        NA's   :6        NA's   :7       \n     VZi05            VZi06            VZi07            VZi08       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.8576   Mean   :0.9094   Mean   :0.5445   Mean   :0.8362  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :4        NA's   :5        NA's   :11       NA's   :5       \n     VZi09            VZi10            VZi11            VZi12       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.8403   Mean   :0.9303   Mean   :0.8433   Mean   :0.9041  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :4        NA's   :5        NA's   :24       NA's   :21      \n     VZi13            VZi14            VZi15            VZi16       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:1.0000  \n Median :0.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.4749   Mean   :0.5039   Mean   :0.8593   Mean   :0.9032  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :33       NA's   :38       NA's   :29       NA's   :44      \n     VZi17            VZi18           VZi19            VZi20       \n Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:1.000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.000   Median :1.0000   Median :1.0000  \n Mean   :0.7206   Mean   :0.902   Mean   :0.9421   Mean   :0.6911  \n 3rd Qu.:1.0000   3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.000   Max.   :1.0000   Max.   :1.0000  \n NA's   :45       NA's   :47      NA's   :50       NA's   :46      \n     VZi21            VZi22            VZi23            VZi24       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.7811   Mean   :0.9412   Mean   :0.7788   Mean   :0.9021  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :59       NA's   :54       NA's   :66       NA's   :57      \n     VZi25            VZi26            VZi27            VZi28       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.9788   Mean   :0.8426   Mean   :0.7944   Mean   :0.9673  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :56       NA's   :76       NA's   :78       NA's   :78      \n     VZi29            VZi30            CFi01            CFi02       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.8841   Mean   :0.8889   Mean   :0.8694   Mean   :0.9021  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :85       NA's   :76       NA's   :1        NA's   :6       \n     CFi03            CFi04            CFi05            CFi06      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:0.000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.000  \n Mean   :0.8905   Mean   :0.8298   Mean   :0.6629   Mean   :0.747  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.000  \n NA's   :9        NA's   :10       NA's   :28       NA's   :39     \n     CFi07            CFi08            CFi09            CFi10       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.7234   Mean   :0.7696   Mean   :0.9072   Mean   :0.8397  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :57       NA's   :62       NA's   :1        NA's   :5       \n     CFi11            CFi12            CFi13            CFi14       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.9537   Mean   :0.5933   Mean   :0.6877   Mean   :0.9733  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :11       NA's   :24       NA's   :23       NA's   :30      \n     CFi15            CFi16            CFi17            CFi18       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.7406   Mean   :0.8455   Mean   :0.6926   Mean   :0.9683  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :53       NA's   :59       NA's   :9        NA's   :8       \n     CFi19            CFi20            CFi21            CFi22       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.7782   Mean   :0.5927   Mean   :0.7546   Mean   :0.7683  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :17       NA's   :17       NA's   :23       NA's   :46      \n     CFi23            CFi24            CFi25            CFi26       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.7956   Mean   :0.6453   Mean   :0.8662   Mean   :0.8794  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :67       NA's   :58       NA's   :8        NA's   :10      \n     CFi27            CFi28            CFi29            CFi30      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:1.000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.000  \n Mean   :0.7319   Mean   :0.6444   Mean   :0.6279   Mean   :0.753  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.000  \n NA's   :16       NA's   :22       NA's   :34       NA's   :45     \n     CFi31            CFi32            V1i01          V1i02       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.00   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:0.75   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :1.00   Median :1.0000  \n Mean   :0.7902   Mean   :0.7061   Mean   :0.75   Mean   :0.6667  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.00   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.00   Max.   :1.0000  \n NA's   :68       NA's   :64                      NA's   :40      \n     V1i03            V1i04            V1i05            V1i06       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.6329   Mean   :0.5548   Mean   :0.9821   Mean   :0.6667  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :85       NA's   :146      NA's   :13       NA's   :175     \n     V1i07            V1i08            V1i09            V1i10      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.000  \n Median :1.0000   Median :0.0000   Median :0.0000   Median :1.000  \n Mean   :0.6371   Mean   :0.4663   Mean   :0.4351   Mean   :0.668  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.000  \n NA's   :168      NA's   :84       NA's   :138      NA's   :51     \n     V1i11            V1i12            V1i13            V1i14      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:1.000  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :1.000  \n Mean   :0.4675   Mean   :0.4074   Mean   :0.3582   Mean   :0.964  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.000  \n NA's   :138      NA's   :130      NA's   :225      NA's   :14     \n     V1i15            V1i16            V1i17           V1i18      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.000   Min.   :0.000  \n 1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:1.000   1st Qu.:0.000  \n Median :1.0000   Median :0.0000   Median :1.000   Median :1.000  \n Mean   :0.9819   Mean   :0.3636   Mean   :0.799   Mean   :0.659  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.000   3rd Qu.:1.000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.000   Max.   :1.000  \n NA's   :15       NA's   :215      NA's   :88      NA's   :75     \n     V1i19            V1i20            V1i21           V1i22       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000  \n Median :1.0000   Median :0.0000   Median :0.000   Median :0.0000  \n Mean   :0.8473   Mean   :0.4466   Mean   :0.181   Mean   :0.0885  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.000   Max.   :1.0000  \n NA's   :17       NA's   :189      NA's   :187     NA's   :66      \n     V1i23            V1i24           V2i01            V2i02       \n Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.000   Median :0.0000   Median :1.0000  \n Mean   :0.7011   Mean   :0.587   Mean   :0.4536   Mean   :0.7056  \n 3rd Qu.:1.0000   3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.000   Max.   :1.0000   Max.   :1.0000  \n NA's   :118      NA's   :200     NA's   :109      NA's   :95      \n     V2i03            V2i04            V2i05            V2i06      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.000  \n Median :1.0000   Median :0.0000   Median :0.0000   Median :0.000  \n Mean   :0.6078   Mean   :0.4796   Mean   :0.3387   Mean   :0.336  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.000  \n NA's   :60       NA's   :194      NA's   :230      NA's   :167    \n     V2i07            V2i08            V2i09            V2i10       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :0.0000   Median :1.0000   Median :1.0000   Median :0.0000  \n Mean   :0.3333   Mean   :0.7644   Mean   :0.8122   Mean   :0.3735  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :166      NA's   :101      NA's   :63       NA's   :209     \n     V2i11            V2i12            V2i13            V2i14       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :0.0000   Median :1.0000   Median :1.0000  \n Mean   :0.6455   Mean   :0.3617   Mean   :0.5645   Mean   :0.6035  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :103      NA's   :104      NA's   :168      NA's   :65      \n     V2i15            V2i16            V2i17            V2i18       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:1.0000  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :1.0000  \n Mean   :0.2807   Mean   :0.4789   Mean   :0.2206   Mean   :0.7716  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :178      NA's   :221      NA's   :224      NA's   :130     \n     V3i01            V3i02            V3i03            V3i04       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.7717   Mean   :0.8007   Mean   :0.6917   Mean   :0.8783  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :16       NA's   :6        NA's   :39       NA's   :29      \n     V3i05            V3i06            V3i07            V3i08       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :0.0000   Median :0.0000  \n Mean   :0.6637   Mean   :0.7143   Mean   :0.4083   Mean   :0.3417  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :69       NA's   :110      NA's   :172      NA's   :172     \n     V3i09            V3i10            V3i11            V3i12       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:1.0000  \n Median :0.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.1497   Mean   :0.5728   Mean   :0.6528   Mean   :0.8421  \n 3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :145      NA's   :189      NA's   :76       NA's   :64      \n     V3i13            V3i14            V3i15            V3i16       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.2333   Mean   :0.8537   Mean   :0.6475   Mean   :0.5361  \n 3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :172      NA's   :169      NA's   :153      NA's   :29      \n     V3i17            V3i18            MA1i01          MA1i02      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:1.000   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :1.000   Median :1.0000  \n Mean   :0.8112   Mean   :0.5152   Mean   :0.766   Mean   :0.7143  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.000   Max.   :1.0000  \n NA's   :43       NA's   :94       NA's   :104     NA's   :117     \n     MA1i03           MA1i04           MA1i05           MA1i06      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.7299   Mean   :0.7989   Mean   :0.7333   Mean   :0.6835  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :118      NA's   :103      NA's   :142      NA's   :134     \n     MA1i07           MA1i08           MA1i09         MA1i10      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.00   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.00   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :1.00   Median :1.0000  \n Mean   :0.8133   Mean   :0.8589   Mean   :0.85   Mean   :0.6835  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.00   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.00   Max.   :1.0000  \n NA's   :142      NA's   :51       NA's   :92     NA's   :153     \n     MA1i11          MA1i12           MA1i13           MA1i14      \n Min.   :0.000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :1.000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.698   Mean   :0.9123   Mean   :0.8664   Mean   :0.7321  \n 3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :143     NA's   :64       NA's   :60       NA's   :124     \n     MA1i15           MA2i01           MA2i02          MA2i03    \n Min.   :0.0000   Min.   :0.0000   Min.   :0.000   Min.   :0.00  \n 1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:1.000   1st Qu.:1.00  \n Median :1.0000   Median :1.0000   Median :1.000   Median :1.00  \n Mean   :0.7429   Mean   :0.8373   Mean   :0.934   Mean   :0.88  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.000   3rd Qu.:1.00  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.000   Max.   :1.00  \n NA's   :117      NA's   :126      NA's   :80      NA's   :167   \n     MA2i04           MA2i05           MA2i06           MA2i07      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.9118   Mean   :0.8705   Mean   :0.8919   Mean   :0.9264  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :122      NA's   :153      NA's   :144      NA's   :129     \n     MA2i08           MA2i09           MA2i10          MVi01       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :1.000   Median :1.0000  \n Mean   :0.9522   Mean   :0.8676   Mean   :0.872   Mean   :0.9084  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.000   Max.   :1.0000  \n NA's   :83       NA's   :141      NA's   :128     NA's   :19      \n     MViO2            MViO3            MVi02            MViO4      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.000  \n Mean   :0.8851   Mean   :0.9137   Mean   :0.8881   Mean   :0.876  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.000  \n NA's   :31       NA's   :14       NA's   :24       NA's   :34     \n     MViO5            MVi03            MViO6            MViO7      \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.000  \n Mean   :0.9504   Mean   :0.8606   Mean   :0.9825   Mean   :0.875  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.000  \n NA's   :10       NA's   :41       NA's   :6        NA's   :20     \n     MVi04            MViO8            MViO9            RGi01       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.8705   Mean   :0.8476   Mean   :0.8835   Mean   :0.7845  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :14       NA's   :23       NA's   :26       NA's   :9       \n     RGi02            RGi03            RGi04            RGi05       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.8143   Mean   :0.8185   Mean   :0.6392   Mean   :0.6875  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :12       NA's   :11       NA's   :37       NA's   :68      \n     RGi06            RGi07           RGi08            RGi09       \n Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:0.000   1st Qu.:1.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.000   Median :1.0000   Median :1.0000  \n Mean   :0.7578   Mean   :0.628   Mean   :0.8389   Mean   :0.8544  \n 3rd Qu.:1.0000   3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.000   Max.   :1.0000   Max.   :1.0000  \n NA's   :69       NA's   :85      NA's   :81       NA's   :86      \n     RGi10           RGi11            RGi12            RGi13       \n Min.   :0.000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.500   1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.000   Median :1.0000   Median :1.0000   Median :0.0000  \n Mean   :0.748   Mean   :0.8261   Mean   :0.7027   Mean   :0.4324  \n 3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :169     NA's   :154      NA's   :181      NA's   :255     \n     RGi14            RGi15         RLi01            RLi02       \n Min.   :0.0000   Min.   :0.0   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0   1st Qu.:1.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0   Median :1.0000   Median :1.0000  \n Mean   :0.9474   Mean   :0.8   Mean   :0.8897   Mean   :0.8828  \n 3rd Qu.:1.0000   3rd Qu.:1.0   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0   Max.   :1.0000   Max.   :1.0000  \n NA's   :197      NA's   :222   NA's   :2        NA's   :2       \n     RLi03            RLi04            RLi05            RLi06       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :0.0000   Median :1.0000  \n Mean   :0.8625   Mean   :0.6862   Mean   :0.4688   Mean   :0.8815  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :1        NA's   :2        NA's   :4        NA's   :5       \n     RLi07            RLi08            RLi09            RLi10       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.7042   Mean   :0.7163   Mean   :0.8505   Mean   :0.6725  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :8        NA's   :10       NA's   :11       NA's   :8       \n     RLi11            RLi12            RLi13            RLi14       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :0.0000   Median :1.0000   Median :0.0000  \n Mean   :0.6559   Mean   :0.4562   Mean   :0.6277   Mean   :0.2321  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :13       NA's   :18       NA's   :10       NA's   :12      \n     RLi15            RLi16            RLi17            RLi18       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :0.0000   Median :1.0000  \n Mean   :0.6431   Mean   :0.9381   Mean   :0.2962   Mean   :0.8676  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :9        NA's   :1        NA's   :5        NA's   :5       \n     RLi19           RLi20           RLi21            RLi22       \n Min.   :0.000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.000   1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.000   Median :0.000   Median :1.0000   Median :0.0000  \n Mean   :0.617   Mean   :0.259   Mean   :0.7367   Mean   :0.4488  \n 3rd Qu.:1.000   3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.000   Max.   :1.000   Max.   :1.0000   Max.   :1.0000  \n NA's   :10      NA's   :14      NA's   :11       NA's   :9       \n     RLi23            RLi24            RLi25            RLi26       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :0.0000   Median :0.0000  \n Mean   :0.8269   Mean   :0.5632   Mean   :0.3885   Mean   :0.3971  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :9        NA's   :15       NA's   :14       NA's   :15      \n     RLi27            RLi28            RLi29            RLi30       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.8885   Mean   :0.7201   Mean   :0.7426   Mean   :0.6527  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :14       NA's   :24       NA's   :20       NA's   :30      \n      Ii01             Ii02             Ii03             Ii04       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.9618   Mean   :0.8635   Mean   :0.7658   Mean   :0.8561  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :4        NA's   :43       NA's   :23       NA's   :21      \n      Ii05             Ii06             Ii07             Ii08       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.7961   Mean   :0.8352   Mean   :0.9299   Mean   :0.7371  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :37       NA's   :31       NA's   :21       NA's   :41      \n      Ii09            Ii10             Ii11             Ii12       \n Min.   :0.000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.786   Mean   :0.6161   Mean   :0.6284   Mean   :0.5353  \n 3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :35      NA's   :68       NA's   :74       NA's   :122     \n      Ii13             Ii14             Ii15              N        \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :13.00  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:27.00  \n Median :0.0000   Median :0.0000   Median :0.0000   Median :32.00  \n Mean   :0.2229   Mean   :0.4938   Mean   :0.4724   Mean   :31.59  \n 3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:36.00  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :58.00  \n NA's   :126      NA's   :130      NA's   :165      NA's   :62     \n       P1             P2              P3           FI1             FI2        \n Min.   : 6.0   Min.   : 3.00   48     : 14   Min.   : 0.00   Min.   : 1.000  \n 1st Qu.:22.0   1st Qu.:17.00   42     : 12   1st Qu.: 9.00   1st Qu.: 7.000  \n Median :26.5   Median :19.00   32     : 11   Median :12.00   Median : 9.000  \n Mean   :27.6   Mean   :19.36   40     : 10   Mean   :12.85   Mean   : 9.461  \n 3rd Qu.:33.0   3rd Qu.:22.00   39     :  9   3rd Qu.:15.00   3rd Qu.:12.000  \n Max.   :50.0   Max.   :32.00   (Other):174   Max.   :38.00   Max.   :22.000  \n NA's   :62     NA's   :62      NA's   : 62   NA's   :62      NA's   :62      \n       FF           portuguese     english         math        biology   \n Min.   : 2.000   60     : 10   100    : 17   60     : 13   80     : 15  \n 1st Qu.: 6.000   76     :  9   79     : 11   69     :  9   60     : 13  \n Median : 8.000   65     :  8   96     :  8   70     :  9   70     : 10  \n Mean   : 9.061   73     :  8   81     :  7   62     :  8   68.5   :  4  \n 3rd Qu.:11.750   74     :  7   89     :  7   71     :  7   75     :  4  \n Max.   :20.000   (Other):188   (Other):180   (Other):184   (Other):184  \n NA's   :62       NA's   : 62   NA's   : 62   NA's   : 62   NA's   : 62  \n    physics      chimestry     geography      history   \n 60     : 16   79.9   :  3   60     :  7   87     :  5  \n 60.1   :  5   84.1   :  3   63.5   :  5   88.5   :  4  \n 0      :  4   85     :  3   66     :  5   71     :  3  \n 61.8   :  4   85.6   :  3   0      :  3   75.5   :  3  \n 66     :  4   89     :  3   64     :  3   79.5   :  3  \n (Other):197   (Other):215   (Other): 98   (Other): 91  \n NA's   : 62   NA's   : 62   NA's   :171   NA's   :183  \n\n\n\n\n\n\n\n\n\n\n\nTidying the data involves a number of tasks, some essential and some things we do for our convenience.\nWe start by selecting the variables we want:\n\nclass_number, participant_id, portuguese, english, math, physics\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nWe use the {dplyr} function select() to pick just the variables needed.\nWe create a new data-set (call it what you like), from the original, to hold just the selection of variables we want to use in the analysis steps, following.\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nbrazil &lt;- BAFACALO_DATASET %&gt;%\n                              select(\n                              class_number, participant_id,\n                              portuguese, english, math, physics\n                              )\n\n\n\n\n\nPract.Q.1. Do you know what you are doing with select()? Can you select a different set of variables?\n\n\nPract.A.1. You can exercise your skill by checking that you can view the BAFACALO_DATASET and select any variable you like. Be careful and make sure you go back to select the variables shown in the given code chunk: we will need them later.\n\n\n\n\nThere are several rows in the data-set with missing values. You need to remove these from the data before you get started with any calculations.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nbrazil &lt;- na.omit(brazil)\n\n\n\n\nWhat have you got?\nYou should always check the results of a tidying operation, to make sure that what you have got matches what you expected to get.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nsummary(brazil)\n\n  class_number participant_id    portuguese     english         math    \n M11    : 21   Min.   :  3.0   60     : 10   100    : 17   60     : 13  \n M15    : 20   1st Qu.: 77.5   76     :  9   79     : 11   69     :  9  \n M14    : 19   Median :144.5   65     :  8   96     :  8   70     :  9  \n M36    : 19   Mean   :146.6   73     :  8   81     :  7   62     :  8  \n M18    : 18   3rd Qu.:222.8   74     :  7   89     :  7   71     :  7  \n M21    : 17   Max.   :291.0   82     :  7   86     :  6   66     :  6  \n (Other):116                   (Other):181   (Other):174   (Other):178  \n    physics   \n 60     : 16  \n 60.1   :  5  \n 0      :  4  \n 61.8   :  4  \n 66     :  4  \n 74.2   :  4  \n (Other):193  \n\n\n\n\n\n\nPract.Q.2. Do you see the difference between the summary of the brazil data shown before and after you run na.omit()? What is it?\n\n\nPract.A.2. You should see that NAs are listed in some columns in the data before but not after you run na.omit().\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nbrazil$portuguese &lt;- as.numeric(brazil$portuguese)\nbrazil$english &lt;- as.numeric(brazil$english)\nbrazil$math &lt;- as.numeric(brazil$math)\nbrazil$physics &lt;- as.numeric(brazil$physics)\n\nbrazil$class_number &lt;- as.factor(brazil$class_number)\nbrazil$participant_id &lt;- as.factor(brazil$participant_id)\n\n\n\n\nWhat have you got?\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nsummary(brazil)\n\n  class_number participant_id   portuguese       english           math       \n M11    : 21   3      :  1    Min.   : 1.00   Min.   : 1.00   Min.   :  1.00  \n M15    : 20   4      :  1    1st Qu.:26.25   1st Qu.:29.00   1st Qu.: 39.00  \n M14    : 19   5      :  1    Median :42.00   Median :55.00   Median : 58.00  \n M36    : 19   6      :  1    Mean   :43.55   Mean   :50.25   Mean   : 58.17  \n M18    : 18   7      :  1    3rd Qu.:58.75   3rd Qu.:73.00   3rd Qu.: 79.75  \n M21    : 17   9      :  1    Max.   :83.00   Max.   :93.00   Max.   :104.00  \n (Other):116   (Other):224                                                    \n    physics      \n Min.   :  1.00  \n 1st Qu.: 34.00  \n Median : 71.50  \n Mean   : 71.57  \n 3rd Qu.:107.75  \n Max.   :148.00  \n                 \n\n\n\n\n\n\nPract.Q.3. Do you see the difference between the summary of the brazil data shown before and after you run as.numeric() or as.factor() function lines? What is it?\n\n\nPract.A.3. You should see that numeric variables like portuguese are listed with summary statistics like the mean after but not before you coerce the variables.\n\n\n\n\n\nTest out variable type using the is...() function for some of the variables\nTest out coercion – and its results – using the as...() function for some of the variables\nLook at the results using summary()\n\n\n\n\n\nHere, we are interested in the relationship between school grades, for the sample of Brazilian children for whom we have data.\n\nWhat is the relationship between portuguese and english grades?\n\n\n\nThis should be a revision exercise for you.\n\nDo you notice any changes in the way in which the plotting code is laid out?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nbrazil %&gt;%\n  ggplot(aes(x = portuguese, y = english)) +\n  geom_point(colour = \"black\", size = 2.5, alpha = .5) +\n  geom_smooth(method = \"lm\", size = 1.5, se = FALSE, colour = \"red\") +\n  xlab(\"Portuguese\") + ylab(\"English\") + theme_bw() \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChange the x and y variables to math and physics\nChange the theme from theme_bw() to something different\nChange the appearance of the points, try different colour, shape, size\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the ggplot reference documentation to help you make choices:\n\nhttps://ggplot2.tidyverse.org/reference/\n\nYou should be using webpages like the reference page often in your work.\n\n\n\n\n\n\n\nWe move next to analyzing the relationship between variation in grades in different subjects, across the children in our sample.\n\n\nYou should be able to reproduce the results shown in the slides and the book chapter.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nsummary(lm(english ~ portuguese, data = brazil))\n\n\nCall:\nlm(formula = english ~ portuguese, data = brazil)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-64.909 -17.573   2.782  20.042  53.292 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 29.77780    3.81426   7.807 2.11e-13 ***\nportuguese   0.47001    0.07897   5.952 9.91e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 25.02 on 228 degrees of freedom\nMultiple R-squared:  0.1345,    Adjusted R-squared:  0.1307 \nF-statistic: 35.43 on 1 and 228 DF,  p-value: 9.906e-09\n\n\n\n\n\n\n\n\nChange the outcome and predictor variables to math and physics.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nsummary(lm(physics ~ math, data = brazil))\n\n\nCall:\nlm(formula = physics ~ math, data = brazil)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-70.360 -14.265   0.437  14.797  78.784 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.83703    3.71858  -0.494    0.622    \nmath         1.26202    0.05795  21.777   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.81 on 228 degrees of freedom\nMultiple R-squared:  0.6753,    Adjusted R-squared:  0.6739 \nF-statistic: 474.2 on 1 and 228 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\nPract.Q.4. What is the estimated coefficient of the “effect” of math ability (grade) on physics grade?\n\n\nPract.A.4. The summary shows that physics grades are on average 1.3 higher for unit increase in math grade.\n\n\nPract.Q.5. Draw a scatterplot showing the relationship between math and physics grades. Does the trend you see in the plot reflect the coefficient you see in the linear model summary?\n\n\nPract.A.5. The plot shows the positive association between math and physics grade also indicated by the estimated coefficient of the math effect.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nbrazil %&gt;%\n  ggplot(aes(x = math, y = physics)) +\n  geom_point(colour = \"black\", size = 2.5, alpha = .5) +\n  geom_smooth(method = \"lm\", size = 1.5, se = FALSE, colour = \"red\") +\n  theme_bw() \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\nPract.Q.6. How does the strength of the math-physics relationship compare with the english-portuguese relationship?\n\n\nPract.A.6. Both the linear model and the plots indicate that the math-physics relationship is much stronger.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = brazil, aes(x = portuguese, y = english)) +\n  geom_point(colour = \"darkgrey\") + \n  geom_smooth(method = \"lm\", se = FALSE, colour = \"black\") +\n  xlab(\"Portuguese\") + ylab(\"English\") + theme_bw() +\n  scale_x_continuous(breaks=c(25,50,75)) + scale_y_continuous(breaks=c(0,50,100)) +\n  facet_wrap(~ class_number) \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChange the x and y variables to math and physics and draw a facetted scatterplot again\nExperiment with showing the differences between classes in a different way: instead of using facet_wrap(), in aes() add colour = class_number, and remove colour from geom_point and geom_smooth\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = brazil, aes(x = math, y = physics)) +\n  geom_point(colour = \"darkgrey\") + \n  geom_smooth(method = \"lm\", se = FALSE, colour = \"black\") +\n  facet_wrap(~ class_number) \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot(data = brazil, aes(x = math, y = physics, colour = class_number)) +\n  geom_point() + geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\nPract.Q.8. Evaluate the consistency between classes of the relationship between math and physics grades: what do the plots show? how does this compare with what you see of the relationship between english and portuguese grades?\n\n\nPract.A.8. The plots show that the relationship between math and physics is very consistent between classes, and more consistent than the relationship between english and portuguese grades appears to be.\n\n\n\n\n\n\n\nYou should be able to replicate the results shown in the slides and the book chapter.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nporto.lmer1 &lt;- lmer(english ~ portuguese +\n                      (portuguese + 1|class_number),\n                      data = brazil)\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.0391843 (tol = 0.002, component 1)\n\nsummary(porto.lmer1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: english ~ portuguese + (portuguese + 1 | class_number)\n   Data: brazil\n\nREML criterion at convergence: 2104.3\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.81240 -0.59545  0.04338  0.59975  2.23667 \n\nRandom effects:\n Groups       Name        Variance Std.Dev. Corr \n class_number (Intercept) 338.9057 18.4094       \n              portuguese    0.3278  0.5725  -0.98\n Residual                 493.2866 22.2101       \nNumber of obs: 230, groups:  class_number, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  25.2838     6.2522   4.044\nportuguese    0.6589     0.1726   3.817\n\nCorrelation of Fixed Effects:\n           (Intr)\nportuguese -0.943\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.0391843 (tol = 0.002, component 1)\n\n\n\n\n\n\n\n\n\nVary the random effects part of the model, while keeping this bit the same: lmer(english ~ portuguese + ...)\nChange the random effect from (portuguese + 1 | class_number) to (1 | class_number): what you are doing is asking R to ignore the differences in the slope of the effect of Portuguese grades.\nChange the random effect from (portuguese + 1 | class_number) to (portuguese + 0 | class_number): what you are doing is asking R to ignore the differences in the intercept\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nporto.lmer1 &lt;- lmer(english ~ portuguese +\n                      (portuguese + 1|class_number),\n                    data = brazil)\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.0391843 (tol = 0.002, component 1)\n\nsummary(porto.lmer1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: english ~ portuguese + (portuguese + 1 | class_number)\n   Data: brazil\n\nREML criterion at convergence: 2104.3\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.81240 -0.59545  0.04338  0.59975  2.23667 \n\nRandom effects:\n Groups       Name        Variance Std.Dev. Corr \n class_number (Intercept) 338.9057 18.4094       \n              portuguese    0.3278  0.5725  -0.98\n Residual                 493.2866 22.2101       \nNumber of obs: 230, groups:  class_number, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  25.2838     6.2522   4.044\nportuguese    0.6589     0.1726   3.817\n\nCorrelation of Fixed Effects:\n           (Intr)\nportuguese -0.943\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.0391843 (tol = 0.002, component 1)\n\n#\n\nporto.lmer2 &lt;- lmer(english ~ portuguese +\n                      (1|class_number),\n                    data = brazil)\nsummary(porto.lmer2)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: english ~ portuguese + (1 | class_number)\n   Data: brazil\n\nREML criterion at convergence: 2129.2\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.55966 -0.68483  0.08471  0.77210  2.51034 \n\nRandom effects:\n Groups       Name        Variance Std.Dev.\n class_number (Intercept)  47.59    6.899  \n Residual                 588.91   24.267  \nNumber of obs: 230, groups:  class_number, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 26.02493    4.60799   5.648\nportuguese   0.55060    0.08995   6.121\n\nCorrelation of Fixed Effects:\n           (Intr)\nportuguese -0.856\n\n#\n\nporto.lmer3 &lt;- lmer(english ~ portuguese +\n                      (portuguese + 0|class_number),\n                    data = brazil)\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.00236563 (tol = 0.002, component 1)\n\nsummary(porto.lmer3)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: english ~ portuguese + (portuguese + 0 | class_number)\n   Data: brazil\n\nREML criterion at convergence: 2116.1\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.74726 -0.65271  0.03263  0.64709  2.71650 \n\nRandom effects:\n Groups       Name       Variance Std.Dev.\n class_number portuguese   0.053   0.2302 \n Residual                535.157  23.1335 \nNumber of obs: 230, groups:  class_number, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  22.0168     4.0359   5.455\nportuguese    0.6813     0.1087   6.266\n\nCorrelation of Fixed Effects:\n           (Intr)\nportuguese -0.778\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.00236563 (tol = 0.002, component 1)\n\n\n\n\n\n\nPract.Q.9. Compare the results of the different versions of the model. Can you identify where the results are different?\n\n\nPract.A.9. It can be seen that Rhe estimated effect of portuguese varies between the models but the estimate is more similar, around .65, where the random effect is specified as (portuguese + 1|class_number) or (portuguese + 0|class_number). The residual variance term is different between the models. Which random effects variances are shown is also different. There is a convergence warning for:\n\nenglish ~ portuguese + (portuguese + 0 | class_number)\n\n\n\nChange the outcome (from english) and the predictor (from portuguese) – this is about changing the fixed effect part of the model.\n\nNote that you will need to change the random effect part as well.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nporto.lmer1 &lt;- lmer(physics ~ math +\n                      (math + 1|class_number),\n                    data = brazil)\n\nboundary (singular) fit: see help('isSingular')\n\nsummary(porto.lmer1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: physics ~ math + (math + 1 | class_number)\n   Data: brazil\n\nREML criterion at convergence: 2066\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.9688 -0.5956  0.0461  0.6276  3.4865 \n\nRandom effects:\n Groups       Name        Variance  Std.Dev. Corr\n class_number (Intercept) 8.827e+01  9.39520     \n              math        3.892e-03  0.06238 1.00\n Residual                 4.118e+02 20.29398     \nNumber of obs: 230, groups:  class_number, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  1.46514    4.10293   0.357\nmath         1.19196    0.05564  21.423\n\nCorrelation of Fixed Effects:\n     (Intr)\nmath -0.570\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\n\n\n\n\nPract.Q.10. What elements of the model summary stand out for you? It will help to see what you should notice if you compare the math-physics model with the first english-portuguese model.\n\n\nPract.A.10. You may notice that:\n\n\nThe summary comes with a fit is singular? warning.\nThe variance terms for intercept or the math effect by class number and the residual are very very small: much smaller than for the english-portuguese model.\n\n\n\n\n\n\n\nIn the lecture materials, I display a plot showing the estimated intercept and coefficient for each class, estimated using separate models for different classes.\n\nSome of you may be interested in how I did that, you can run the following code to see.\n\n\nUse the {dplyr} %&gt;% syntax to run a model for each class separately, collect together the results into a dataframe.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nbrazlm &lt;- brazil %&gt;% group_by(class_number) %&gt;% do(tidy(lm(english ~ portuguese, data=.)))\nbrazlm$term &lt;- as.factor(brazlm$term)\n\n\n\n\n\nExtract the per-class estimates of the intercepts and the ‘portuguese’ effect coefficient estimates.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nbrazlmint &lt;- filter(brazlm, term == '(Intercept)')\nbrazlmport &lt;- filter(brazlm, term == 'portuguese')\n\n\n\n\n\nPlot the estimates.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\npbrazlmint &lt;- ggplot(brazlmint, aes(x = class_number, y = estimate, ymin = estimate - std.error, ymax = estimate + std.error))\npbrazlmint &lt;- pbrazlmint + geom_point(size = 2) + geom_linerange() + theme_bw() \npbrazlmint &lt;- pbrazlmint + ggtitle(\"Intercept\") + ylab(\"Estimated coefficient +/- SE\") + xlab(\"Class\")\npbrazlmint &lt;- pbrazlmint + theme(axis.title.y = element_text(size = 10), axis.text.x = element_blank(), panel.grid = element_blank())\n# pbrazlmint\n\npbrazlmport &lt;- ggplot(brazlmport, aes(x = class_number, y = estimate, ymin = estimate - std.error, ymax = estimate + std.error))\npbrazlmport &lt;- pbrazlmport + geom_point(size = 2) + geom_linerange() + theme_bw() \npbrazlmport &lt;- pbrazlmport + ggtitle(\"Portuguese effect\") + ylab(\"Estimated coefficient +/- SE\") + xlab(\"Class\")\npbrazlmport &lt;- pbrazlmport + theme(axis.title.y = element_text(size = 10), axis.text.x = element_blank(), panel.grid = element_blank())\n# pbrazlmport\n\n# -- ask R to make a grid:\n\ngrid.arrange(pbrazlmint, pbrazlmport,\n             ncol = 2)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge: Can you change the code to show the estimates for the relationship between physics and math grades?\n\n\n\n\n\nAfter the practical class, we will reveal the answers that are currently hidden.\nThe answers version of the webpage will present my answers for questions, and some extra information where that is helpful.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 16. Workbook introduction to multilevel data"
    ]
  },
  {
    "objectID": "PSYC412/part2/02-mixed.html",
    "href": "PSYC412/part2/02-mixed.html",
    "title": "Week 17. Conceptual introduction to mixed-effects models",
    "section": "",
    "text": "In the conceptual introduction to multilevel data and the workbook introduction to multilevel data chapters, we looked at a multilevel structured data-set in which there were observations about children’s grades, and it became evident that those children can be grouped by or under classes. As we discussed, this kind of data structure will come from studies with a very common design in which, for example, the researcher records observations about a sample of children who are members of a sample of classes. In working with these kind of data, it is common to say that the observations of children’s grades are nested within classes in a hierarchy.\nMany Psychologists conduct studies where observations are properly understood to be structured in groups of some form but where, nevertheless, it is inappropriate to think of the observations as being nested (Baayen et al., 2008). We are talking, here, about repeated-measures designs where the experimenter presents a sample of multiple stimuli for response to each participant in a sample of multiple participants. This is another very common experimental design in psychological science.\nStudies with repeated-measures designs will produce data with a structure that, also, requires the use of mixed-effects models but, as we shall see, the way we think about the structure will be a bit more complicated. We could say that observations of the responses made by participants to each stimulus can be grouped by participant: each person will tend to respond in similar ways to different stimuli. Or, we could say that observations of responses can be grouped by stimulus because each stimulus will tend to evoke similar kinds of responses in different people. Or, we could say that both forms of grouping should be taken into account at the same time.\nWe shall take the third position and this chapter will concern why, and how we will adapt our thinking and practice.\n\n\n\n\n\n\nQuick start: Workbook introduction to mixed-effects models\n\n\n\nThis chapter presents ideas and methods relating to the analysis of repeated-measures data using mixed-effects models in which we extend the ideas and the methods in depth.\n\nYou can find a quick start, focused on practical analysis steps, in the workbook introduction to mixed-effects models.\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nLinear mixed-effects models and multilevel models are basically the same.\n\n\nThis week, we again look at data with multilevel structure. But we are looking at data where participants were asked to respond to a set of stimuli (here, words) so that our observations consist of recordings made of the response made by each child to each stimulus. We use the same procedure we did for multilevel data but with one significant change which we shall identify and explain.\n\n\n\nOur learning objectives again include the development of both concepts and skills.\n\nskills – practice how to tidy experimental data for mixed-effects analysis.\nconcepts – begin to develop an understanding of crossed random effects of participants and stimuli.\nskills and concepts – practice fitting linear mixed-effects models incorporating random effects of participants and stimuli.\n\n\n\n\nI have provided a collection of materials you can use. Here, I explain what they are and how I suggest you use them.\n1. Video recordings of lectures\n1.1. I have recorded a lecture in three parts. The lectures should be accessible by anyone who has the link.\n\nPart 1 – about 8 minutes\nPart 2 – about 21 minutes\nPart 3 – about 15 minutes\n\n1.2. I suggest you watch the recordings then read the rest of this chapter.\n\nThe lectures provide a summary of the main points.\n\n1.3. You can download the lecture slides in three different versions:\n\n402-week-18-LME-2.pdf: high resolution .pdf, exactly as delivered [6 MB];\n402-week-18-LME-2_1pp-lo.pdf: lower resolution .pdf, printable version, one-slide-per-page [about 900 KB];\n402-week-18-LME-2_6pp-lo.pdf: lower resolution .pdf, printable version, six-slides-per-page [about 900 KB].\n\nThe high resolution version is the version delivered for the lecture recordings. Because the images are produced to be high resolution, the file size is quite big (6 MB) so, to make the slides easier to download, I produced low resolution versions: 1pp and 6pp. These should be easier to download and print out if that is what you want to do.\n2. Chapter: 02-mixed\n2.1. I have written this chapter to discuss the main ideas and set out the practical steps you can follow to start to develop the skills required to analyse multilevel structured data with crossed random effects.\n2.2. The practical elements include data tidying, visualization and analysis steps.\n2.3. You can read the chapter, run the code, and do the exercises.\n\nRead in the example CP reading study data-sets.\nIdentify how the data are structured by both participant and stimulus differences.\nUse visualizations to explore the impact of the structure.\nRun analyses using linear mixed-effects models involving multiple random effects.\nReview the recommended readings (Section 1.14).\n\n3. Practical materials\n3.1 In the following sections, I describe the practical steps, and associated practical materials (exercise workbooks and data), you can use for your learning.\n\n\n\nIn this chapter, we will be working with the CP reading study data-set. CP tested 62 children (aged 116-151 months) on reading aloud in English. In the experimental reading task, she presented 160 words as stimuli. The same 160 words were presented to all children. The words were presented one at a time on a computer screen. Each time a word was shown, each child had to read the word out loud and their response was recorded. Thus, the CP reading study data-set comprised observations about the responses made by 62 children to 160 words.\nIn addition to the reading task, CP administered tests of reading skill (TOWRE sight word and phonemic tests, Torgesen et al., 1999), reading experience (CART, Stainthorp, 1997), the Spoonerisms sub-test of the Phonological Awareness test Battery (PhAB, Frederickson et al., 1997), and an orthographic choice test measure of orthographic knowledge. She also recorded the gender and the handedness of the children.\nWe are going to use the CP study data to examine the answers to a research question similar to the question CP investigated:\n\n\n\n\n\n\nNote\n\n\n\n\nResearch question: What word properties influence responses to words in a test of reading aloud?\n\n\n\nWe can look at the answers to this question while also taking into account the impacts of random differences – between sampled participants or between sampled words – using mixed-effects models.\nUltimately, the CP data-set were incorporated in an analysis of the impact of age on reading skills over the life-span, reported by Davies et al. (2017). You can find more details on the data and the methods in that paper. (Data and analysis code are shared through the journal article webpage [paywalled] here, and a preprint version of the article can be accessed here.)\nThe CP study resembles many studies in psychological science. The critical features of the study are that:\n\nWe have an outcome measure – the reading response – observed multiple times.\n\n\nWe have multiple responses recorded for each participant: they make one response to each stimulus (here, each stimulus word), for the multiple stimuli that they see in the experimental reading task.\nAnd we have multiple responses recorded for each stimulus: one response is made to each stimulus by each participant, for all the participants who completed the task, in a sample of multiple participants.\n\nThe presence of these features is the reason why we need to use mixed-effects models in our analysis. These features are common across a range of study designs so the lessons we learn will apply frequently in psychological research. This is the reason why it is important we teach and learn how to use mixed-effects models.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nGet the data: get the data file and the .R script you can use to do the exercises that will support your learning.\n\nYou can download the files folder for this chapter by clicking on the link 02-mixed.zip.\n\n\n\nThe practical materials folder includes data files and an .R script:\n\nCP study word naming rt 180211.dat\nCP study word naming acc 180211.dat\nwords.items.5 120714 150916.csv\nall.subjects 110614-050316-290518.csv\nlong.all.noNAs.csv\n402-02-mixed-workbook.R the workbook you will need to do the practical exercises.\n\nThe .dat files are tab delimited files holding behavioural data: the latency or reaction time rt (in milliseconds) and the accuracy acc of response made by each participant to each stimulus.\nThe .csv files are comma separated values files. The words.items file holds information about the 160 stimulus words presented in the experimental reading (word naming) task. The all.subjects file holds information about the 62 participants who volunteered to take part in the experiment.\nSo: we will be working with multiple data files located in a .zip folder called 02-mixed. And, in this folder, we have got four files that we will need to import or read in to R.\nIn the following, I will describe a series of steps through which we get the data ready for analysis. However, as we shall see, you can avoid these steps by using the pre-tidied data-set:\n\nlong.all.noNAs.csv\n\nThe data files are collected together with the .R script:\n\n402-02-mixed-workbook.R.\n\nDuring practical sessions, each week, you can use the workbook to prompt your code construction and your thinking, with support.\n\n\nAfter practical sessions, you will be able to download an answers version of the workbook .R so check back here after the session.\n\n\n\n\n\n\nImportant\n\n\n\nGet the answers: get the data file and the .R script with answers.\n\nYou can download the files folder for this chapter by clicking on the link 02-mixed-answers.zip.\n\n\n\nThe link to this folder will not work until after a session has finished.\nWhen the link is live, you will be able to download a folder including:\n\n402-02-mixed-workbook-with-answers.R with answers to questions and example code for doing the exercises.\n\nBefore we do anything else, we need to talk about the messiness of real Psychological data and how we deal with it.\n\n\n\n\n\nOrdinarily, textbooks and guides to data analysis give you the data ready for analysis but this situation will never be true for your professional practice (at least, not at first). Instead of pretending that data arrive ready for analysis, we are going to look at the process of data tidying, step-by-step. This will help you to get ready for the same process when you have to develop and use it in your own research.\nWe are going to spend a bit of time looking at the data tidying process. This process involves identifying and resolving a series of challenges, in order. Looking at the tidying process will give you a concrete sense of the structure in the data. You should also take this opportunity to reflect on the nature of the process itself – what we have to do and why, in what order and why – so that you can develop a sense of the process you might need to build when the time comes for you to prepare your own data for analysis.\nThe time that we spend looking at data tidying is an investment in learning that will save you time later, in your professional work. If, however, you want to skip it, go to section Section 1.8.\n\n\nIn analyzing psychological data, the first step is usually to collect the data together. In psychological research, the data may exist, at first, in separate files. For the CP study, we have separate files for each of the pieces of information we need to use in our analyses:\n\nParticipant attributes: information about participants’ age, gender, identifier code, and abilities on various measures.\nStimulus attributes: information about stimulus items, e.g., the word, its item number, its value on each variable in a set of psycholinguistic properties (like word length, frequency).\nBehaviour: behavioural observations e.g. reaction time or accuracy of responses made by each participant to each stimulus word.\n\nOften, we need all these kinds of information for our analyses but different pieces of information are produced in separate ways and come to us in separate files. For example, we may collect experimental response data using software like PsychoPy, E-Prime, Qualtrics or DMDX. We may collect information about participant characteristics using standardized measures, or by asking participants to complete a set of questions on their age, gender, and so on.\n\n\n\nOften, the files we get are untidy: not in a useful or tidy format. For example, if you open the file CP_study_word_naming_rt_180211.dat (a .dat or tab delimited file) in Excel, you will see a spreadsheet that looks like Figure 1.\n\n\n\n\n\n\nFigure 1: CP study RTs .dat file\n\n\n\nTypical of the output from data collection software, we can see a data table with:\n\nin the top row, column header labels item_name, AislingoC, AllanaD ...;\nin the first (leftmost) column, row labels item_name, act, ask, both ...;\nfor each row, we see values equal to the reaction time (RT) observed for the response made to each stimulus (listed in the row labels);\nfor each column, we see values equal to the RTs observed for each person (listed in the column labels);\nand at each intersection of row and column (for each cell), we see the RT observed for a response made by a participant to a stimulus.\n\nData laid out like this are sometimes said to be in wide format. You can see that the data are wide because at least one variable – here, reading reaction time – is held not in one column but spread out over several columns, side-by-side. Thus, the data-set is wide with fewer rows and many columns.\nWe want the data in what is called the tidy format.\n\n\nThere are three inter-related rules which make data tidy (Grolemund & Wickham, n.d.).\n\n\n\n\n\n\nImportant\n\n\n\nIn tidy data:\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\n\n\nYou can read more about tidy data here.\nFor our purposes, the reason we want the data in tidy format is that it is required for the functions we are going to use for mixed-effects modelling. However, in general, tidy format is maximally flexible, and convenient, for use with different R functions.\n\n\n\n\n\nTo answer our research question, we will need to combine the behavioural data with information about the participants (age, gender …) and about the words (word, frequency …) We will need to ensure that the data-set we construct will be in tidy format. We will need to select variables (columns) to get just those required for our later analyses. And we will need to filter cases (rows), excluding errors or outliers.\nWe shall need to do this work in a series of processing steps:\n\nImport the data or read the data into R, see Section 1.7.1\nRestructure the data, see Section 1.7.3\nSelect or transform variables, see Section 1.7.6\nFilter observations, see Section 1.7.7\n\nWe will use {tidyverse} library functions from the beginning, starting with the import stage.\n\nlibrary(tidyverse)\n\n(Every step can also be done in alternative processing steps with the same result using base R code.)\n\n\nI am going to assume you have downloaded the data files, that they are all in the same folder, and that you know where they are on your computer or server. We need to use different versions of the read_ function to read all four files into R.\n\nbehaviour.rt &lt;- read_tsv(\"CP study word naming rt 180211.dat\", na = \"-999\")\nbehaviour.acc &lt;- read_tsv(\"CP study word naming acc 180211.dat\", na = \"-999\")\nsubjects &lt;- read_csv(\"all.subjects 110614-050316-290518.csv\", na = \"-999\")\nwords &lt;- read_csv(\"words.items.5 120714 150916.csv\", na = \"-999\")\n\nThese different versions respect the different ways in which the .dat and .csv file formats work. We need read_tsv() when data files consist of tab separated values. We need read_csv() when data files consist of comma separated values.\nYou can read more about the {tidyverse} {readr} library of helpful functions here\nIt is very common to get experimental data in all sorts of different formats. Learning to use tidyverse functions will make it easier to cope with this when you do research.\n\n\n\n\n\n\nTip\n\n\n\nWe use the read_ function to read in the data\n\nentering (here, at least) arguments – inside the brackets after the function name – to tell R what file we need and how missing values (NAs) are coded.\n\n\n\nIt will help your understanding to examine an example. Take a look at what this line of code includes, element by element.\n\nbehaviour.rt &lt;- read_tsv(\"CP study word naming rt 180211.dat\", na = \"-999\")\n\n\nWe write behaviour.rt &lt;- read_tsv(...) to create an object in the R environment, which we call behaviour.rt: the object with this name is the data-set we read into R using read_tsv(...).\nWhen we write the function read_tsv(...) we include two arguments inside it.\nread_tsv(\"CP study word naming rt 180211.dat\", ... first, the name of the file, given in quotes \"\" and then a comma.\nread_tsv(..., na = \"-999\") second, we tell R that there are some missing values na which are coded with the value \"-999\".\n\n\n\n\nIn the data-sets – typically, the spreadsheets – we create in our research, we will have values missing for different reasons. Take another look at the data spreadsheet you saw earlier, Figure 1.\n\n\n\n\n\n\nTip\n\n\n\nIn R, a missing value is said to be “not available”: NA.\n\n\nYou should be able to see that the spreadsheet holds information, as explained, about the RTs of the responses made by each child to each stimulus word. Each of the cells in the spreadsheet (i.e. the box where a column intersects with a row) includes a number value. Most of the values are positive numbers like 751.3: the reaction time of a response, recorded in milliseconds. The values have to be positive because they represent the length of time between the moment the stimulus word is presented on the test computer screen and the moment the child’s spoken word response has begun to be registered by the computer microphone and sound recording software.\nSome of the cells hold the value -999, however. Obviously, we cannot have negative RT. The value represents the fact that we have no data. Take a look at Figure 1: we have a -999 where we should have a RT for the response made by participant AllanaD to the word broad. This -999 is there because, for some reason, we did not record an RT or a response for that combination of participant and stimulus.\nWe can choose any value we like, as researchers, to code for missing data like this. Some researchers choose not to code for the absence of a response recording or leave the cell in a spreadsheet blank or empty where data are missing. This is bad practice though it is common.\nThere are a number of reasons why it is bad practice to just leave a cell empty when it is empty because no observation is to be recorded.\n\nData may be missing for different reasons: maybe a child did not make any response to a stimulus (often called a “null response”); or maybe a child made a response but there was a microphone or other technical fault; or maybe a child made a response but it was an error and (here) the corresponding performance measure (RT) cannot be counted.\nIf you do not code for missingness in the data then the software you use will do it for you, but you may not know how it does so, or where.\nIf you have missing data, you ought to be able to identify where the data are missing.\n\nI use -999 to code for missing values because you should never see a value like that in real reading RT data. You can use whatever value you like but you should make sure you do code for missing data somehow.\n\n\n\nWe are going to need to restructure these data from a wide format to a longer format. We need to restructure both behavioural data-sets, accuracy and RT. We do this using the pivot_longer() function.\n\nrt.long &lt;- behaviour.rt %&gt;%\n             pivot_longer(2:62, names_to = \"subjectID\", values_to = \"RT\")\n\nacc.long &lt;- behaviour.acc %&gt;%\n              pivot_longer(2:62, names_to = \"subjectID\", values_to = \"accuracy\")\n\nResearchers used to have to do this sort of thing by hand, using copying and pasting, in Excel or SPSS. Doing the process by hand takes many hours or days. And you always make errors.\n\n\n\n\n\n\nTip\n\n\n\nDoing data-set construction programmatically, using R functions, is generally faster and more reliable than doing it by hand.\n\n\nHere, we use a function you may have seen before: pivot_longer(). It will help your understanding to examine the code carefully.\n\nrt.long &lt;- behaviour.rt %&gt;%\n             pivot_longer(2:62, names_to = \"subjectID\", values_to = \"RT\")\n\nThe name of the function comes from the fact that we are starting with data in wide format e.g. behaviour.rt where we have what should be a single variable of observations (RTs) arranged in a wide series of multiple columns, side-by-side (one column for each participant). But we want to take those wide data and lengthen the data-set, increasing the number of rows and decreasing the number of columns.\nLet’s look at this line of code bit by bit. It includes a powerful function that accomplishes a lot of tasks, so it is worth explaining this function in some detail.\n\nrt.long &lt;- behaviour.rt %&gt;%\n\n\nAt the start, I tell R that I am going to create a new longer data-set (more rows, fewer columns) that I shall call rt.long.\nI will create this longer data-set from &lt;- the original wide data-set behaviour.rt.\nand I will create the new longer data-set by taking the original wide data-set and piping it %&gt;% to the pivot function coded on the next line:\n\n\npivot_longer(2:62, names_to = \"subjectID\", values_to = \"RT\")\n\n\nOn this next line, I tell R how to do the pivoting by using three arguments.\n\n\npivot_longer(2:62...)\n\n\nFirst, I tell R that I want to re-arrange all the columns that can be found in the data-set from the second column to the sixty-second column.\nIn a spreadsheet, we have a number of columns.\nColumns can be identified by their position in the spreadsheet.\nThe position of a column in a spreadsheet can be identified by number, from the leftmost column (column number 1) to the rightmost column (here, column number 62) in our data-set.\nSo this argument tells R exactly which columns I want to pivot.\n\n\npivot_longer(..., names_to = \"subjectID\", ...)\n\n\nSecond, I tell R that I want it to take the column labels and put them into a new column, called subjectID.\nIn the wide data-set behaviour.rt, each column holds a list of numbers (RTs) but begins with a word in the topmost cell, the name code for a participant, in the column label position.\nWe want to keep the information about which participant produces which response when we pivot the wide data to a longer structure.\nWe do this by asking R to take the column labels (the participant names) and listing them in a new column, called subjectID which now holds the names as participant ID codes.\n\n\npivot_longer(...values_to = \"RT\")\n\n\nThird, we tell R that all the RT values should be put in a single column.\nWe can understand that this new column RT will hold RT observations in a vertical stack, one cell for each response by a person to a word, with rows ordered by subjectID.\n\nThere are 61 columns of data listed by participant though 62 children were tested because we lost one child’s data through an administrative error. As a result, in the wide data sets there are 62 columns, with the first column holding item_name data.\nYou can find more information about pivoting data here\nAnd you can find more information specifically about the pivot_longer() operation here\n\n\n\nAs I noted, one problem with the wide format is that the data are structured so that the column names are not names of variables. In our example wide format data-set, behaviour.rt, the columns are headed by a participant identity code or name but a participant code is not the name of a variable, it is a value of the variable I call subjectID.\nIn the design of the CP reading study, we want to take into account the impact of differences between participants on response RT (so, we need to identify which participant makes which response). But we do not see the responses made by a participant as a predictor variable.\nA second problem is that, in a wide format file like behaviour.rt, information about the responses made to each stimulus word is all on the same row (that seems good) but in different columns. Each person responded to all the words. But the response made to a word e.g. act made by one participant is in a different column (e.g., 594.8ms, for AislingoC) from the response made to the same word by a different participant (e.g., 586ms, for AlexB). This means that information about the responses made to each stimulus word are spread out as values across multiple columns.\nYou can see this for yourself if you inspect the source rt data using head() to view the top four rows of the data-set.\n\n\n\n\n\nitem_name\nAislingoC\nAlexB\nAllanaD\nAmyR\nAndyD\nAnnaF\nAoifeH\nChloeBergin\nChloeF\nChloeS\nCianR\nConorF\nDavidL\nDillonF\nDJHerlihy\nEamonD\nEimearK\nEllenH\nEoinL\nGrainneH\nJackBr\nJackK\nJackS\nJamesoC\nJenniferoS\nKateF\nKayleighMc\nKenW\nKevinL\nKieranF\nKillianB\nKirstyC\nLeeJ\nMarkC\nMatthewC\nMeganoB\nMichaelaoD\nNataliaR\nNiallG\nNiallGavin\nNiallW\nOisinN\nOlaA\nOwenD\nPalomaM\nPauricT\nPerryD\nRachelD\nRebeccaGr\nRebeccaM\nRebeccaR\nRoisinF\nRonanT\nSarahP\nShaunaBr\nSiobhanR\nTaraB\nTeeTeeOj\nThomasK\nTristianT\nZainab\n\n\n\n\nact\n594.8\n586.0\nNA\n693.0\n597\n627.0\n649.0\n1081.0\n642.0\n622.7\n701.0\n686.0\n951.0\n661.0\n692.0\n670.0\n502.4\n578.4\n651.8\n441.6\n895.2\n529.0\n639.0\n809\n676.9\n568.8\n586.1\n591.3\n587\n586\n723.9\n1428.0\n557.0\n639.4\n676\n714.8\n623.3\n615.0\n796.0\n568.4\n800.9\n595\nNA\n574.9\n628\n797.0\n652\n757.0\n631.0\n520.5\n640.0\n733.0\n566\n758.0\n670.0\n532.4\n615.7\n540.0\n1390.0\n747\n651.0\n\n\nask\n481.5\n864.0\n1163.0\n694.4\n616\n631.0\n538.0\n799.3\n603.0\n526.0\n591.5\n699.6\n827.2\n635.0\n654.0\n508.0\n564.0\n822.0\n479.7\n600.0\n617.6\n555.9\n654.6\n765\n856.0\n576.7\n690.3\n501.9\n634\n626\n523.3\n640.7\n516.0\n625.7\n561\n698.2\n685.0\n606.0\n793.0\n551.9\n668.7\n722\n868.0\n633.0\n578\n660.2\n851\n640.3\n630.0\n535.0\n568.0\n579.7\n562\n747.0\n602.4\n558.4\n691.0\n580.8\n590.9\n795\n740.5\n\n\nboth\n457.5\n670.0\n1114.3\n980.0\n1019\n796.1\n545.2\nNA\n581.0\n568.4\n665.0\n751.0\n917.0\n808.1\n737.6\n597.0\n475.0\n608.0\n699.2\n600.6\n527.3\n601.0\n982.1\n917\n854.4\n571.6\n825.3\n584.0\n720\n571\n624.0\n853.4\nNA\n703.0\n781\n1065.8\n591.5\n559.2\n837.6\n612.0\n743.4\n743\n919.0\n883.0\n616\n734.6\n658\n574.4\n634.9\n559.7\n689.0\n997.8\n648\n753.4\n548.0\n530.5\n625.4\n552.0\n985.0\n640\n764.0\n\n\nbox\n546.0\n748.6\n975.0\n678.0\n589\n604.0\n574.0\n658.0\n688.7\n492.0\n641.0\n699.0\n824.0\n731.6\n634.0\n557.5\n520.1\n581.0\n1191.0\n612.0\n540.0\n540.0\n810.0\n885\n573.8\n614.9\n722.0\n594.6\n632\n549\n564.0\n632.6\n668.6\n627.0\n577\n785.9\n649.0\n616.0\n928.0\n608.7\n690.0\n857\n886.6\n544.0\n863\n726.0\n812\n640.5\n613.8\n395.1\n656.7\n604.7\n530\n667.4\n547.7\n570.0\n585.6\n532.0\n634.0\n764\n912.0\n\n\n\n\n\n\n\nThis structure is a problem for visualization and for analysis because the functions we will use require us to specify single columns for an outcome variable like reaction time.\nWe are looking at the process of tidying data because untidiness is very common. Learning how to deal with it will save you a lot of time and grief later.\nYou should check for yourself how subjectID and RT or accuracy scores get transposed from the old wide structure to the new long structure.\n\n# RT data\nhead(behaviour.rt)\nhead(rt.long)\n\n# accuracy data\nhead(behaviour.acc)\nhead(acc.long)\n\nIf you compare the rt.long or acc.long data with the data in the original wide format then you can see how – in going from wide – we have re-arranged the data to a longer and narrower set of columns:\n\none column listing each word;\none column for subjectID;\nand one column for RT or accuracy.\n\nWhat a check will show you is that we have multiple rows for responses to each item so that the item is repeated multiple times in different rows.\nThese data are now tidy.\n\nEach column has information about one variable\nAnd each row has information about one observation, here, the response made by a participant to a word\n\nThis is a big part of data tidying now done. However, these data are incomplete. Next we shall combine behavioural observations with data about stimulus words and about participants.\n\n\n\n\n\n\n\nTo answer our research question, we next need to combine the RT with the accuracy data, and then the combined behavioural data with participant information and stimulus information. This is because, as we have seen, information about behavioural responses, about participant attributes or stimulus word properties, are located in separate files.\nMany researchers have completed this kind of operation by hand. This involves copying and pasting bits of data in a spreadsheet. It can take hours or days. I know because I have done it, and I have seen others do it.\n\n\n\n\n\n\nWarning\n\n\n\nPlease do not try to combine data-sets through manual operations e.g. in Excel. I guarantee that:\n\nYou will make mistakes.\nYou will not know when or where those mistakes are in your data.\n\nThere are better ways to spend your time.\n\n\nWe can combine the data-sets, in the way that we need, using the {tidyverse} full_join() function. This gets the job done quickly, and accurately.\nFirst, we join RT and accuracy data together.\n\nlong &lt;- rt.long %&gt;% \n          full_join(acc.long)\n\nThen, we join subject and item information to the behavioural data.\n\nlong.subjects &lt;- long %&gt;% \n                   full_join(subjects, by = \"subjectID\")\n\nlong.all &lt;- long.subjects %&gt;%\n              full_join(words, by = \"item_name\")\n\nNotice, we can let R figure out how to join the pieces of data together. If we were doing this by hand then we would need to check very carefully the correspondences between observations in different data-sets.\nHere, in a series of steps, we take one data-set and join it (merge it) with the second data-set. Let’s look at an example element by element to better understand how this is accomplished.\n\nlong &lt;- rt.long %&gt;% \n           full_join(acc.long)\n\nThe code work as follows.\n\nlong &lt;- rt.long %&gt;%\n\n\nWe create a new data-set we call long.\nWe do this by taking one original data-set rt.long and %&gt;% piping it to the operation defined in the second step.\n\n\nfull_join(acc.long)\n\n\nIn this second step, we use the function full_join() to add observations from a second original data-set acc.long to those already from rt.long\n\nThe addition of observations from one database joining to those from another happens through a matching process.\n\nR looks at the data-sets being merged.\nIt identifies if the two data-sets have columns in common. Here, the data-sets have subjectID and item_name in common).\nR can use these common columns to identify rows of data. Here, each row of data will be identified by both subjectID and item_name i.e. as data about the response made by a participant to a word.\nR will then do a series of identity checks, comparing one data-set with the other and, row by row, looking for matching values in the columns that are common to both data-sets.\nIf there is a match then R joins the corresponding rows of data together.\nIf there isn’t a match then it creates NAs where there are missing entries in one row for one data-set which cannot be matched to a row from the joining data-set.\n\n\n\n\n\n\n\nTip\n\n\n\nNote that in one example, the example of code I discuss here, I did not specify identifying columns in common, allowing the function to do the work. In the other code chunks I did: long.all &lt;- long.subjects %&gt;% full_join(words, by = \"item_name\") using the by = ... argument.\n\nSometimes, you can vary in how you employ a _join() function.\nIt may help to specify the identifying column if you want to make explicit (to yourselves and others) how the process is to be completed.\n\n\n\n\n\nIn the {tidyverse} family of dplyr functions, when you work with multiple data-sets (tables of data), we call the data-sets relational data.\nThere are three families of functions (like verbs) designed to work with relational data:\n\nMutating joins, which add new variables to one data frame from matching observations in another.\nFiltering joins, which filter observations from one data frame based on whether or not they match an observation in the other table.\nSet operations, which treat observations as if they were set elements.\n\nWe can connect data-sets – relate them – according to shared variables like subjectID, item_name (for our data). In {tidyverse}, the variables that connect pairs of tables are called keys where, and this is what counts, key(-s) are variable(-s) that uniquely identify an observation.\nFor the experimental reading data, we have observations about each response made by a participant (one of 61 subjects) to an item (one of 160 words). For these data, we can match up a pair of RT and accuracy observations for each (unique) subjectID-item_name combination. If you reflect, we could not combine the RT and accuracy data correctly if we did not have both identifying variables in both data-sets, because both column variables are required to uniquely identify each observation.\nFurther, we could not combine the RT and accuracy data correctly if there were mismatches in values of the identifying variable. Sometimes, I have done this operation and it has gone wrong because a subjectID has been spelled one way in one data-set e.g. hugh and another way in the other data-set e.g. HughH. This leads me to share some advice.\n\n\n\n\n\n\nTip\n\n\n\n\nBe careful about spelling identifiers.\nAlways check your work after merger operations.\n\nYou can check your work by calculating data-set lengths to ensure the number of rows in the new data-set matches your expectations, given the study design and data collection procedure.\n\n\n\n\n\nWe used the full_join() function.\nThere are three kinds of joins.\n\nA left join keeps all observations in x.\nA right join keeps all observations in y.\nA full join keeps all observations in x and y.\n\nI used full_join() because I wanted to retain all observations from both data-sets, whether there was a match (as assumed) or not, in the identifying variables, between observations in each data-set.\n\n\n\n\nBreak the join: You can examine how the full_join() works by experimenting with stopping it from working.\n\nAs I discuss, you need to have matches in values on key (common) variables. If the subjectID is different on different data-sets, you will lose data that would otherwise be merged to form the merged or composite data-set.\nCheck what happens if you deliberately misspell one of the subjectID values in one of the original source wide behavioural data files.\nTo be safe, you might want to do this exercise with copies of the source files kept in a folder you create for this purpose. If it goes wrong, you can always re-access the source files and read them in again.\nYou can check what happens before and after you break the match by counting the number of rows in the data-set that results from the merger. We can count the number of rows in a data-set with:\n\nlength(long.all$RT)\n\n[1] 9762\n\n\nThis bit of code takes the length of the vector (i.e. variable column RT in data-set long.all), thus counting the number of rows in the data-set.\n\n\n\n\nOK, now we have all the data about everything all in one big, long and wide, data-set. But we do not actually require all of the data-set for the analyses we are going to do.\nWe next need to do two things. First, we need to get rid of variables we will not use: we do that by using select(). Then, we need to remove errors and outlying short RT observations: we do that by using filter() in Section Section 1.7.7.\nWe are going to select just the variables we need using the select() function.\n\nlong.all.select &lt;- long.all %&gt;% \n                        select(item_name, subjectID, RT, accuracy, \n                               Lg.UK.CDcount, brookesIMG, AoA_Kup_lem, \n                               Ortho_N, regularity, Length, BG_Mean, \n                               Voice,   Nasal,  Fricative,  Liquid_SV,\n                               Bilabials,   Labiodentals,   Alveolars,\n                               Palatals,    Velars, Glottals, \n                               age.months, TOWREW_skill, TOWRENW_skill, \n                               spoonerisms, CART_score)\n\n\n\n\n\n\n\nTip\n\n\n\nNotice that these variables do not have reader-friendly names: but naming things is important.\n\nCheck out the ever-useful Jenny Bryan’s advice.\n\n\n\nThe names we have in the CP study data were fine for internal use within my research group but we should be careful to ensure that variables have names that make sense to others and to our future selves. We can adjust variable names using the rename() function but I will leave that as an exercise for you to do.\n\n\n\nSelect different variables: You could analyze the CP study data for a research report.\n\nWhat if you wanted to analyze a different set of variables, could you select different variables?\n\n\n\n\nWe now have a tidy data-set long.all.select with 26 columns and 9762 rows.\nThe data-set includes missing values, designated NA. Here, every error (coded 0, in accuracy) corresponds to an NA in the RT column.\nThe data-set also includes outlier data. In this context, \\(RT &lt; 200\\) are probably response errors or equipment failures. We will want to analyse accuracy later, so we shall need to be careful about getting rid of NAs.\nAt this point, I am going to exclude two sets of observations only.\n\nobservations corresponding to correct response reaction times that are too short: \\(RT &lt; 200\\).\nplus observations corresponding to the word false which (because of stupid Excel auto-formatting) dropped item attribute data.\n\nWe can do this using the filter() function, setting conditions on rows, as arguments.\n\n# step 1\nlong.all.select.filter &lt;- long.all.select %&gt;% \n                            filter(item_name != 'FALSE')\n\n# step 2\nlong.all.select.filter &lt;- long.all.select.filter %&gt;%\n                            filter(RT &gt;= 200)\n\nHere, I am using the function filter() to …\n\nCreate a new data-set long.all.select.filter &lt;- ... by\nUsing functions to work on the data named immediately to the right of the assignment arrow: long.all.select\nAn observation is included in the new data-set if it matches the condition specified as an argument in the filter() function call, thus:\n\n\nfilter(item_name !='FALSE') means: include in the new data-set long.all.select.filter all observations from the old data-set long.all.select that are not != (! not = equal to) the value FALSE in the variable item_name,\nthen recreate the long.all.select.filter as a version of itself (with no name change) by including in the new version only those observations where RT was greater than or equal to 200ms using RT &gt;= 200.\n\n\n\n\n\n\n\nWarning\n\n\n\nThe difference between = and ==\nYou need to be careful to distinguish these signs.\n\n= assigns a value, so x = 2 means “x equals 2”\n== tests a match so x == 2 means: “is x equal to 2?”\n\n\n\n\n\nYou can supply multiple arguments to filter() and this may be helpful if (1.) you want to filter observations according to a match on condition-A and condition-B (logical “and” is coded with &) or (2.) you want to filter observations according to a match on condition-A or condition-B (logical “or” is coded |).\nYou can read more about using multiple arguments to filter observations here.\n\n\n\n\nVary the filter conditions: in different ways\n\n\nChange the threshold for including RTs from RT &gt;= 200 to something else\nCan you assess what impact the change has? Note that you can count the number of observations (rows) in a data-set using e.g. length(data.set.name$variable.name)\n\nFiltering or re-coding observations is an important element of the research workflow in psychological science, as I examine in the discussion of data multiverse analyses here. How we do or do not remove observations from original data may have an impact on our results (as explored by, e.g., Steegen et al., 2014). It is important, therefore, that we learn how to do this reproducibly using R scripts that we can share with our research reports.\nYou can read further information about filtering here.\n\n\n\n\nWe will be working with the long.all.select.filter.csv data-set collated from the experimental, subject ability scores, and item property data collected for the CP word naming study.\nFor convenience, I am going to remove missing values before we go any further, using the na.omit() function.\n\nlong.all.noNAs &lt;- na.omit(long.all.select.filter)\n\n\n\n\n\n\n\nTip\n\n\n\nThe na.omit() function is powerful.\n\nIn using this function, I am asking R to create a new data-set long.all.noNAs from the old data-set long.all.select.filter in a process in which the new data-set will have no rows in which there is a missing value NA in any column.\nYou need to be reasonably sure, when you use this function, where your NAs may be because, otherwise, you may end the process with a new filtered data-set that has many fewer rows in it than you expected.\n\n\n\n\n\n\n\nhead(long.all.noNAs, n = 10)\n\n# A tibble: 10 × 26\n   item_name subjectID      RT accuracy Lg.UK.CDcount brookesIMG AoA_Kup_lem\n   &lt;chr&gt;     &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n 1 act       AislingoC    595.        1          4.03          4        6.42\n 2 act       AlexB        586         1          4.03          4        6.42\n 3 act       AmyR         693         1          4.03          4        6.42\n 4 act       AndyD        597         1          4.03          4        6.42\n 5 act       AnnaF        627         1          4.03          4        6.42\n 6 act       AoifeH       649         1          4.03          4        6.42\n 7 act       ChloeBergin 1081         1          4.03          4        6.42\n 8 act       ChloeF       642         1          4.03          4        6.42\n 9 act       ChloeS       623.        1          4.03          4        6.42\n10 act       CianR        701         1          4.03          4        6.42\n# ℹ 19 more variables: Ortho_N &lt;dbl&gt;, regularity &lt;dbl&gt;, Length &lt;dbl&gt;,\n#   BG_Mean &lt;dbl&gt;, Voice &lt;dbl&gt;, Nasal &lt;dbl&gt;, Fricative &lt;dbl&gt;, Liquid_SV &lt;dbl&gt;,\n#   Bilabials &lt;dbl&gt;, Labiodentals &lt;dbl&gt;, Alveolars &lt;dbl&gt;, Palatals &lt;dbl&gt;,\n#   Velars &lt;dbl&gt;, Glottals &lt;dbl&gt;, age.months &lt;dbl&gt;, TOWREW_skill &lt;dbl&gt;,\n#   TOWRENW_skill &lt;dbl&gt;, spoonerisms &lt;dbl&gt;, CART_score &lt;dbl&gt;\n\n\nIf we inspect long.all.noNAs, we can see that we have now got a tidy data-set with all the data we need for our analyses:\n\nOne observation per row, corresponding to data about a response made by a participant to a stimulus in an experimental trial\nOne variable per column\nWe have information about the speed and accuracy of responses\nAnd we have information about the children and about the words.\n\nWe have removed the missing values and we have filtered outliers.\n\n\n\nHaving produced the tidy data-set, we may wish to share it, or save ourselves the trouble of going through the process again. We can do this by creating a .csv file.\n\nwrite_csv(long.all.noNAs, \"long.all.noNAs.csv\")\n\nThis function will create a .csv file from the data-set you name long.all.noNAs which R will put in your working directory.\n\n\n\nMost research work involving quantitative evidence requires a big chunk of data tidying or processing before you get to the statistics. Most of the time, this is work you will have to do. The lessons you can learn about the process will generalize to many future research scenarios.\n\nIt is a mistake to think of data tidying or wrangling as an inconvenience or as an extra task or something you need to do to get to the ‘good stuff’ (your results).\n\nAll analysis results follow from and thus are determined by the data processing steps that precede analysis.\nAnalysis results can and do vary, perhaps critically, depending on different processing decisions, and reasonable people may differ on key processing decisions.\nThe process of data tidying is frequently instructive of your data recording quality: you find things out about your field measurements or your instrument integrity or quality of your recordings, when you pay attention, when you process your data.\n\n\nIt is wise to see data tidying or processing as a key part of the data analysis workflow because, as I examine in the data multiverse discussion here, the choices you make or the integrity or quality of the actions you take, will have consequences for your analysis results or, more generally, for the quality of the evidence you share with others.\nHere is a nice substack post that links to some scholarly writing and makes some excellent points.\n\n\n\n\n\n\nTip\n\n\n\nA key recommendation is that you write code to tidy or process data, thus creating a self-documented auditable data tidying process in your analysis workflow.\n\nThis is simple to do in R and that is one important reason why we use and teach R.\n\n\n\n\n\n\n\nOur focus in this chapter is on analyzing data that come from studies with repeated-measures designs where the experimenter presents multiple stimuli for response to each participant.\nIn our working example, the CP reading study, CP asked all participants in her study to read a selection of words. All participants read the same selection of words, and every person read every word. For each participant, we have multiple observations and these (within-participant) observations will not be independent of each other. One participant will tend to be slower or less accurate compared to another participant, on average. Likewise, one participant’s responses will reveal a stronger (or weaker) impact of the effect of an experimental variable than another participant. These between-participant differences will tend to be apparent across the sample of participants.\nYou could say that the lowest trial-level observations can be grouped with respect to participants, that observations are nested within participant. But the data can also be grouped by stimuli. Remember that in the CP study, all participants read the same selection of words, and every person read every word. This means that for each stimulus word, there are multiple observations because all participants responded to each word, and these (within-item) observations will not be independent of each other. One word may prove to be more challenging compared to another, eliciting slower or less accurate responses, on average. Likewise, participants’ responses to a word will reveal a stronger (or weaker) impact of the effect of an experimental variable than the responses to another word. Again, these between-stimulus differences will tend to be apparent when you examine observations of responses across the sample of words.\nUnder these circumstances, are observations about the responses made by different participants nested under words, or are observations about the responses to different words nested under participants? We do not have to make a decision.\nGiven this common repeated-measures design, we can analyze the outcome variable in relation to:\n\nfixed effects: the impact of independent variables like participant reading skill or word frequency\nrandom effects: the impact of random or unexplained differences between participants and also between stimuli\n\nIn this situation, we can say that the random effects are crossed (Baayen et al., 2008). When multilevel models require the specification of crossed random effects, they tend to be called mixed-effects models.\n\n\n\nTo illustrate the approach, we examine observations from the CP study. We begin, as we did previously, by ignoring differences due to grouping variables (like participant or stimulus). We pretend that all observations are independent. In this fantasy situation, we address our research question.\n\n\n\n\n\n\nNote\n\n\n\n\nResearch question: What word properties influence responses to words in a test of reading aloud?\n\n\n\n\n\nIf you have not completed the process of tidying the CP study data then you can import the pre-tidied data here.\n\nlong.all.noNAs &lt;- read_csv(\"long.all.noNAs.csv\", \n                  col_types = cols(\n                    subjectID = col_factor(),\n                    item_name = col_factor()\n                    )\n                  ) \n\nNotice that I am using read_csv() with an additional argument col_types = cols(...).\n\nHere, I am requesting that read_csv() treats subjectID and item_name as factors.\n\n\n\n\n\n\n\nTip\n\n\n\nWe can use col_types = cols(...) to control how read_csv() interprets specific column variables in the data.\n\n\nControlling the way that read_csv() handles variables is a very useful capacity, and a more efficient way to work than, say, first reading in the data and then using coercion to ensure that variables are assigned appropriate types. You can read more about it here.\n\n\n\nWe begin our data analysis by asking if reading reaction time (RT) varies in association with word frequency. A scatterplot shows that response latencies decrease with increasing word frequency (Figure 2).\n\nlong.all.noNAs %&gt;%\nggplot(aes(x = Lg.UK.CDcount, y = RT)) +\n  geom_point(alpha = .2) + \n  geom_smooth(method = \"lm\", se = FALSE, size = 1.5, colour=\"red\") + \n  theme_bw() + \n  xlab(\"Word frequency: log context distinctiveness (CD) count\")\n\n\n\n\n\n\n\nFigure 2: Reading reaction time compared to word frequency, all data\n\n\n\n\n\nIn the plot, we see that the best fit line drawn with geom_smooth() trends downward for higher values of word frequency. This means that Figure 2 suggests that RT decreases with increasing word frequency. (I know there is a weird looking line of points around 0 but we can ignore that here.)\nWe can estimate the relationship between RT and word frequency using a linear model in which we ignore the possibility that there may be differences (between subjects, or between items) in the intercept or (between subjects) in the slope of the frequency effect. This simplified model can be stated as:\n\\[\nY_{ij} = \\beta_0 + \\beta_1X_j + e_{ij}\n\\]\n\nwhere \\(Y_{ij}\\) is the value of the observed outcome variable, the RT of the response made by the \\(i\\) participant to the \\(j\\) word;\n\\(\\beta_1X_j\\) refers to the fixed effect of the explanatory variable (here, word frequency), where the frequency value \\(X_j\\) is different for different words \\(j\\), and \\(\\beta_1\\) is the estimated coefficient of the effect due to the relationship between response RT and word frequency;\n\\(e_{ij}\\) is the residual error term, representing the differences between observed \\(Y_{ij}\\) and predicted values (given the model).\n\nThe linear model can be fit in R using the lm() function, as we have done previously.\n\n# label: lm-all-freq\nlm.all.1 &lt;- lm(RT ~  Lg.UK.CDcount, data = long.all.noNAs)\n\nsummary(lm.all.1)\n\n\nCall:\nlm(formula = RT ~ Lg.UK.CDcount, data = long.all.noNAs)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-346.62 -116.03  -38.37   62.05 1981.58 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    882.983     11.901   74.19   &lt;2e-16 ***\nLg.UK.CDcount  -53.375      3.067  -17.40   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 185.9 on 9083 degrees of freedom\nMultiple R-squared:  0.03227,   Adjusted R-squared:  0.03216 \nF-statistic: 302.8 on 1 and 9083 DF,  p-value: &lt; 2.2e-16\n\n\nIn the estimates from this linear model, we see an approximate first answer to our research question.\n\n\n\n\n\n\nNote\n\n\n\n\nResearch question: What word properties influence responses to words in a test of reading aloud?\nResult: Our analysis shows that the estimated effect of word frequency is \\(\\beta = -53.375\\). This means that, according to the linear model, RT decreases by about 53 milliseconds for each unit increase in log word frequency.\n\n\n\nNotice that, here, word frequency information is located in the Lg.UK.CDcount variable. In a common move for reading data analyses, we transformed the frequency estimate to the Log base 10 of the word frequency values, prior to analysis, in part because word frequency estimates are usually highly skewed.\nThe model does not explain much variance, as \\(Adjusted \\space R^2 = .03\\) but, no doubt due to the large sample, the regression model is significant overall \\(F(1,9083) = 302.8, p &lt; .001\\).\n\n\n\nVary the linear model: using different outcomes or different predictor variables.\n\nThe CP study data-set is rich with possibility. It would be useful to experiment with it.\n\nChange the predictor from frequency to something else: what do you see when you visualize the relationship between variables using scatterplots?\nSpecify linear models with different predictors: do the relationships you see in plots match the coefficients you see in the model estimates?\n\n\n\n\n\nThe problem is that, as we have discussed, we assume that observations are independent for the linear model yet we can suppose in advance that that assumption of independence will be questionable given the expectation that participants’ responses will differ in predictable ways: one participant’s responses will perhaps be slower or less accurate than another, perhaps more or less affected by word frequency than another.\nWe can examine that variation by estimating the intercept and the slope of the frequency effect separately using the data for each participant alone. We can start by visualizing the frequency effect for each child in a grid of plots, with each plot representing the \\(\\text{RT} \\sim \\text{frequency}\\) relationship for the data for a child (Figure 3).\nWe looked at how the plotting code works step-by-step in the conceptual introduction to multilevel data.\n\nlong.all.noNAs %&gt;%\n  ggplot(aes(x = Lg.UK.CDcount, y = RT)) +\n    geom_point(alpha = .2) + \n    geom_smooth(method = \"lm\", se = FALSE, size = 1.25, colour = \"red\") + \n    theme_bw() + \n    xlab(\"Word frequency (log10 UK SUBTLEX CD count)\") + \n    facet_wrap(~ subjectID)\n\n\n\n\n\n\n\nFigure 3: RT vs. word frequency, considered separately for data for each child\n\n\n\n\n\nFigure 3 shows how, on average, more frequent words are associated with shorter reaction time: faster responses. The plot further shows, however, that the effect of frequency varies considerably between children.\n\nSome children show little or no effect; the best fit line is practically level.\nSome children show a marked effect, with a steep fit line indicating a strong frequency effect.\n\nWe can get more insight into the differences between children if we plot the estimated intercept and frequency effect coefficients for each child directly. This allows more insight because it focuses the eye on the differences between children in the estimates. We do this next: see Figure 4. (I work through the code for generating the plot in: 02-mixed-workbook-with-answers.R)\n\n\n\n\n\n\n\n\nFigure 4: Estimated intercepts and frequency effect slopes calculated for each child’s data, with child data analysed separately for all children. Points represent estimates. Lines represent standard errors for estimates. Point estimates are presented in order of size.\n\n\n\n\n\nFigure 4 presents two plots showing the estimates of the intercept and the coefficient of the effect of word frequency on reading RT, calculated separately for each child. This means that we fitted a separate linear model, for the association between RT and frequency, using the data for just one child, for each child in our sample, for all the children.\nThe estimate for each child is shown as a black dot. The standard error of the estimate is shown as a black vertical line, shown above and below a point. You can say that where there is a longer line there we have more uncertainty about the location of the estimate.\nThe estimates calculated for each child are shown ordered from left to right in the plot by the size of the estimate. This adjustment to the plot reveals how the estimates of both the intercept and the slope of the frequency effect vary substantially between children.\n\n\n\n\n\n\nImportant\n\n\n\nThe first key observation is that if there is an average intercept for everyone in the sample or, better, an intercept we could estimate for everyone in the population, then the different intercepts we have estimated for each child would be distributed around that population-level average:\n\nSome children will have slower (here, larger) intercepts\nand other children will have faster (shorter) intercepts.\n\n\n\nHere, the intercept can be taken to be the average RT when all other effects in the model are set to zero. RT varies for this sample around somewhere like \\(\\beta_0 = 883ms\\) so a slower larger intercept might be e.g. \\(\\beta_0 = 1000ms\\).\n\n\n\n\n\n\nImportant\n\n\n\nThe second key observation is that if there is an average slope for the frequency effect, an effect of frequency on reading RT, averaged across everyone in the population, then, again, the different slopes we have estimated for each child would be distributed around that population-level effect.\n\nSome children will have larger (here, more negative) frequency effects\nand other children will have smaller (less negative) frequency effects.\n\n\n\nHere, the frequency effect is associated with a negative coefficient e.g. \\(\\beta_1 = -53\\) so a larger frequency effect will be a bigger negative number e.g. \\(\\beta_1 = -100\\).\n\n\n\nIn a mixed-effects model, we account for this variation: the differences between participants in intercepts and slopes.\nWe do this by modeling the intercept as two terms:\n\\[\n\\beta_{0i} = \\gamma_0 + U_{0i}\n\\]\n\nwhere \\(\\gamma_0\\) is the average intercept and \\(U_{0i}\\) is the difference for each \\(i\\) child between their intercept and the average intercept.\n\nWe model the frequency effect as two terms:\n\\[\n\\beta_{1i} = \\gamma_1 + U_{1i}\n\\]\n\nwhere \\(\\gamma_1\\) is the average slope and \\(U_{1i}\\) represents the difference for each \\(i\\) child between the slope of their frequency effect and the average slope.\n\nWe can then incorporate in a single model the fixed effects due to the average intercept and the average frequency effect, as well as the random effects, error variance due to unexplained differences between participants in intercepts and frequency effects:\n\\[\nY_{ij} = \\gamma_0 + \\gamma_1X_j + U_{0i}+ U_{1i}X_j + e_{ij}\n\\]\n\nwhere the outcome \\(Y_{ij}\\) is related to …\nthe average intercept \\(\\gamma_0\\) and differences between \\(i\\) children in the intercept \\(U_{0i}\\);\nthe average effect of the explanatory variable frequency \\(\\gamma_1X_j\\) and differences between \\(i\\) participants in the slope \\(U_{1i}X_j\\);\nin addition to residual error variance \\(e_{ij}\\).\n\n\n\nIn sections Section 1.9.7 and Section 1.11, we look at what exactly is captured in these random effects terms \\(U_{0i}, U_{1i}\\). Let’s first look at the practicalities of analysis then come back to deepen our understanding a bit more.\nRight now, it is important to understand that in our analysis we do not care about the differences between specific children. We care that there are differences. And we care how widely spread are the differences between child A and the average intercept (or slope), or between child B and the average intercept (or slope), or between child C … (you get the idea). Therefore, in our analysis, we estimate the spread of the differences as a variance term. We can see this when we look at the results of the mixed-effects model we specify, next.\n\n\n\n\nWe can fit a mixed-effects model of the \\(\\text{RT} \\sim \\text{frequency}\\) relationship, taking into account the random differences between participants. I first go through the model fitting code bit by bit. (I go through the output, the results, in Section 1.9.6.)\n\nlmer.all.1 &lt;- lmer(RT ~  Lg.UK.CDcount + \n                         (Lg.UK.CDcount + 1||subjectID),\n             \n             data = long.all.noNAs)\n\nsummary(lmer.all.1)\n\nYou have seen the lmer() function code before but practice makes perfect so we shall go through the code step by step, as we did previously.\nFirst, we have a chunk of code mostly similar to what we do when we do a regression analysis.\n\nlmer.all.1 &lt;- lmer(...) creates a linear mixed-effects model object using the lmer() function.\nRT ~  Lg.UK.CDcount is a formula expressing the model in which we estimate the fixed effect on the outcome or dependent variable RT (reaction time, in milliseconds) as predicted \\(\\sim\\) by the independent or predictor variable Lg.UK.CDcount (word frequency).\n...(..., data = long.all.noNAs) specifies the data-set in which you can find the variables named in the model fitting code.\nsummary(lmer.all.1) gets a summary of the fitted model object, showing you the results.\n\nSecond, we have the bit that is specific to multilevel or mixed-effects models.\n\nWe add (...||subjectID) to tell R about the random effects corresponding to random differences between sample groups (here, observations grouped by child) that are coded by the subjectID variable.\n(...1 ||subjectID) says that we want to estimate random differences between sample groups (observations by child) in intercepts, where the intercept is coded by 1.\n(Lg.UK.CDcount... ||subjectID) adds random differences between sample groups (observations by child) in slopes of the frequency effect coded using the Lg.UK.CDcount variable name.\n\n\n\nIt will help your learning if you now go back and compare this model with the model you saw in the conceptual introduction to multilevel data.\n\nIdentify what is different: data-set, variable names, and model formula.\nIdentify what stays the same: function name … the specification of both model and random effects.\n\nIf you can see what is different versus what stays the same then you learn what you can change when the time comes for your analysis with your data.\n\n\n\n\n\n\nTip\n\n\n\nLearning to look at example code so that you can identify how to adapt it for your own purposes is a key skill in psychological data science.\n\n\n\n\n\nBefore we move on, I want you to notice something that looks like nothing much: ||.\nWe are going to need to defer until later a (necessary) discussion of exactly why we need the two double lines. In short, the use of || asks R to fit a model in which we estimate random effects associated with:\n\nvariance due to differences in intercepts\nvariance due to differences in slopes\nbut not covariance between the two sets of differences\n\nI do this because otherwise the model I specify will not converge.\nWe shall need to discuss these things: convergence, and failures to converge; as well as random effects specification and simplification. We will discuss random effects covariance in Section 1.11. For now, the most important lesson is learnt by seeing how the analysis approach we saw last week can be extended to examining the effects of experimental variables in data from repeated measures design studies.\n\n\n\n\nThe lmer() model code we discussed in Section 1.9.5 gives us the following output.\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: RT ~ Lg.UK.CDcount + ((1 | subjectID) + (0 + Lg.UK.CDcount |  \n    subjectID))\n   Data: long.all.noNAs\n\nREML criterion at convergence: 117805.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.7839 -0.5568 -0.1659  0.3040 12.4850 \n\nRandom effects:\n Groups      Name          Variance Std.Dev.\n subjectID   (Intercept)   87575    295.93  \n subjectID.1 Lg.UK.CDcount  2657     51.55  \n Residual                  23734    154.06  \nNumber of obs: 9085, groups:  subjectID, 61\n\nFixed effects:\n              Estimate Std. Error t value\n(Intercept)    950.913     39.216  24.248\nLg.UK.CDcount  -67.980      7.092  -9.586\n\nCorrelation of Fixed Effects:\n            (Intr)\nLg.UK.CDcnt -0.093\n\n\nWe discussed the major elements of the results output last week. We expand on that discussion, a little, here.\nThe output from the model summary first gives us information about the model.\n\nFirst, we see information about the function used to fit the model, and the model object created by the lmer() function call.\nThen, we see the model formula RT ~  Lg.UK.CDcount + (Lg.UK.CDcount + 1|subjectID).\nThen, we see REML criterion at convergence about the model fitting process, which we can usually ignore.\nThen, we see information about the distribution of the model residuals.\nWe then see information listed under Random effects: this is where you can see information about the error variance terms estimated by the model.\n\n\nThe information is listed in four columns: 1. Groups; 2. Name; 3. Variance; and 4. Std.Dev.\n\nWe have discussed how observations can be grouped by participant (because we have multiple response observations for each person in the study) just as previously we identified how observations could be grouped by class (because we saw that children were nested under class). That is what we mean when we refer to Groups: we are identifying the grouping variables that give hierarchical structure to the data.\n\nThe Name lists whether the estimate we are looking at corresponds to, here, random differences between participants in intercepts (listed as (Intercept)), or in slopes (listed as Lg.UK.CDcount).\n\nAs we discuss later, in Section 1.11, mixed-effects models estimate the spread in random differences. We are not interested in the specific differences in intercept or slope between specific individuals. What we want is to be able to take into account the variance associated with those differences.\nThus, we see in the Random Effects section, the variances associated with:\n\nsubjectID Intercept) 87575, differences between participants in the intercepts;\nsubjectID.1 Lg.UK.CDcount 2657, differences between participants in the slopes of the frequency effect;\nAlongside Residual  23734, residuals where, just like a linear model, we have variance associated with differences between model estimates and observed RT, here, at the trial level.\n\nWe do not usually discuss the specific variance estimates in research reports. However, the relative size of the variances does provide useful information (Meteyard & Davies, 2020), as we shall see when we discuss the different estimates we get when we include a random effect due to differences between items (Section 1.10.2).\n\nLastly, we see estimates of the coefficients (of the slopes) of the fixed effects.\n\nIn this model, we see estimates of the fixed effects of the intercept and the slope of the RT ~ Lg.UK.CDcount model. We discuss these estimates next.\n\n\nRecall that the linear model yields the estimate for the frequency effect on reading RT such that RT decreases by about 53 ms for unit increase in log word frequency (\\(\\beta = -53.375\\)). Now, when we have taken random differences between participants into account, we see that the estimate of the effect for the mixed-effects model is \\(\\beta = -67.980\\). Taking into account random differences clearly has an impact on results.\n\n\n\n\n\n\nNote\n\n\n\n\nResearch question: What word properties influence responses to words in a test of reading aloud?\nResult: The mixed-effect analysis shows that RT decreases by about \\(68\\) milliseconds for each unit increase in log word frequency.\n\n\n\nWhich coefficient estimate should you trust? Well, it is obvious that the linear model and the linear mixed-effects model estimate are relatively similar. However, it is also obvious that the linear model makes an assumption – the assumption of independence of observations – that does not make sense theoretically (because reading responses will be similar for each child) and does not make sense empirically (because responses will differ between children, see Figure 4). Thus, we have good grounds for supposing that the linear mixed-effects model estimate for the frequency effect is likely to be closer to the true underlying population effect.\nIt is important to remember, however, that whatever estimate we can produce is the estimate given the sample of words we used, our information about word frequency, and our measurement of reading RT responses. How far our estimate actually generalizes to the wider population is not something we can judge in the context of a single study.\nFurther, we have not finished in our consideration of the random effects that the account should include. We need to do more work by thinking about the differences between stimuli (Section 1.10).\n\n\n\n\n\n\nWarning\n\n\n\nWhy aren’t there p-values?\nWe will come back to this (in a week) but note that if \\(t &gt; 2\\) we can suppose that an effect is significant at the \\(.05\\) significance level.\n\n\n\n\n\n\nWe have said that we can incorporate, in a mixed-effects model, fixed effects (e.g., the average frequency effect) and random effects, error variance due to unexplained differences between participants in intercepts and in frequency effects:\n\\[\nY_{ij} = \\gamma_0 + \\gamma_1X_j + U_{0i}+ U_{1i}X_j + e_{ij}\n\\]\nSo we distinguish:\n\nthe average intercept \\(\\gamma_0\\) and differences between \\(i\\) children in the intercept \\(U_{0i}\\);\nthe average effect of the explanatory variable frequency \\(\\gamma_1X_j\\) and differences between \\(i\\) participants in the slope \\(U_{1i}X_j\\).\n\nWhen we think about the differences between participants (or between the units of any grouping variable), in intercepts or in slopes, we typically assume that the differences are:\n\nrandom;\nshould be normally distributed;\nand are distributed around the population or average fixed effects.\n\nWe can say that the mixed-effects model sees the differences between participants relative to the fixed effect intercept or slope, that is, relative to the population level or average effects.\nWe can illustrate this by plotting, in Figure 5, the differences as estimated – technically, predicted – by the mixed-effects model that we have been examining. (The code for producing the plot can be found in 02-mixed-workbook-answers.R.)\n\n\n\n\n\n\n\n\nFigure 5: Plot showing histograms indicating the distribution of participant adjustments to account for between-child differences in intercept or slope (the Best Linear Unbiased Predictions).\n\n\n\n\n\nWhat you can see in Figure 5 are distributions, presented using histograms. The centers of the distributions are located at zero (shown by a red line). For each distribution (a. and b.), that is where the model estimate of the intercept or the slope of the frequency effect is located. Spread around that central point, you see the adjustments the model makes to account for differences between participants.\nNotice how, in Figure 5 (a.):\n\nSome children have intercepts that are smaller than the population-level or average intercept – so their adjustments are negative (to decrease their intercepts).\nSome children have intercepts that are larger than the population-level or average intercept – so their adjustments are positive (to increase their intercepts).\n\n\nStrikingly, you can see that a few children have intercepts that are as much as 1000ms larger than the population-level or average intercept: the bars representing the estimates for these children are far out on the right of the x-axis in Figure 5 (a.).\n\nNow notice how, in Figure 5 (b.):\n\nSome children have frequency effects (coefficients) that are smaller than the population-level or average frequency effect – so their adjustments are positive (to decrease their frequency effect, by making it less negative).\nSome children have frequency effects that are larger than the population-level or average frequency effect – so their adjustments are negative (to increase their frequency effect, by making it more negative).\n\n(When you look at Figure 5 (b.), remember that the estimated \\(\\beta\\) coefficient for the frequency effect is negative because higher word frequency is associated with smaller RT.)\n\nStrikingly, you can see that a few children have frequency effects that are as much as 200ms larger (see plot (b.) around \\(x = -200\\)) than the population-level or average effect.\n\nWhen a mixed-effects model is fitted to a data-set, its set of estimated parameters includes the coefficients for the fixed effects as well as the standard deviations for the random effects (Baayen et al., 2008). If you read the literature on mixed-effects models, you will see that the adjustments are called Best Linear Unbiased Predictors (BLUPs).\n\n\nMixed-effects modeling is hard to get used to at first. A bit more practice helps to show you how the different parts of the model work. We again focus on the random effects.\nIn the model we have seen so far, we specify (Lg.UK.CDcount + 1||subjectID)\n\nWe can change this part – and only this part – to see what happens to the results. Do it: rerun the model code, having changed the random effects part:\n\n\nlmer(RT ~  Lg.UK.CDcount + (1|subjectID)...) gives us a random intercepts model accounting for just random differences between participants in the intercept\nlmer(RT ~  Lg.UK.CDcount + (Lg.UK.CDcount + 0|subjectID)...) gives us a random slope model accounting for just random differences between participants in the slope of the frequency effect\nlmer(RT ~  Lg.UK.CDcount + (Lg.UK.CDcount + 1|subjectID)...) gives us a random intercepts and slopes model accounting for both random differences between participants in the intercept and in the slope, as well as covariance in these differences.\n\n\n\n\n\n\n\nTip\n\n\n\nTry out these variations and look carefully at the different results. Look, especially, at what happens to the Random effects part of the summary.\n\n\nThis will be an important and revealing exercise.\nWe can visualize the differences between the models in a plot showing the different predictions that the different models give us. Figure 6 shows what a mixed-effects model predicts are the effects of frequency on RT for different children in the CP study.\n\nThe predictions vary depending on the nature of the random effects we specify in the model.\n\n(Note that I figured out how to produce the plot from the information here.)\n\n\n\n\n\n\n\n\nFigure 6: Plot showing model predictions of the effect, for each individual, of word frequency on reading reaction time – predictions vary between models incorporating (a.) random effect of participants on intercepts only; (b.) random effect of participants on slopes only and (c.) random effect of participants on intercepts and on slopes.\n\n\n\n\n\nWe can see that:\n\nIf the model includes the random effect of participants on intercepts only then all the slopes are the same (the lines in the figure are parallel) because this model assumes that the only differences between participants are differences in the intercepts.\nIf the model includes the random effect of participants on slopes only then the slopes vary but they all have the same intercept. The plot does not show this but you can see how all the slopes are converging on one point somewhere on the left. This happens because this model assumes that the only differences between participants are differences in the slopes.\nIf the model includes the random effect of participants on intercepts and on slopes then we can see how the intercepts and the slopes vary. Given what we saw when we looked at the relation between frequency and RT for each participant considered separately we might argue that this model is much more realistic about the data.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is key to your skills development that you learn to make effective use of the warnings and error messages R can produce.\n\n\nYou do not have to just believe me when I say that || is in the model code to stop a problem appearing.\n\nExperiment – and see what happens when you change the code. Try this.\n\n\nlmer.all.1 &lt;- lmer(RT ~  Lg.UK.CDcount + (Lg.UK.CDcount + 1|subjectID),\n             \n             data = long.all.noNAs)\n\nsummary(lmer.all.1)\n\nDo you get an error message?\nA very useful trick is to learn to copy the error message you get into a search engine on your web browser. Do this and you will find useful help, as here\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nExperimental psychologists will often collect data in studies where they present some stimuli to a sample of participants.\n\n\nClark (1973) showed that the appropriate analysis of experimental effects for such data requires the researcher to take into account the error variance due to unexplained or random differences between sampled participants and also to random differences between sampled stimuli. This is true in the context of psycholinguistics but it is also true in the context of work in any field where the presented stimuli can be understood to constitute a sample from a wider population of potential stimuli (e.g., stories about social situations, Judd et al., 2012).\nIf we were to estimate the average latency of the responses made by different children to each word, in the CP study data, we would see that there is considerable variation between words. We do this in Figure 7. (I work through the code for producing the plot in 02-mixed-workbook-answers.R.)\n\nSome words elicit slower and some elicit faster responses on average.\nWe can also see that there is, again, variation in the uncertainty of estimates, as reflected in differences in the lengths of the error bars corresponding to the standard errors of the estimates.\n\n\n\n\n\n\n\n\n\nFigure 7: Estimated intercepts (with SEs) calculated for each stimulus word, with coefficients ordered by average latency for each word\n\n\n\n\n\nIn general, psychologists have been aware since Clark (1973) (if not earlier) that responses to experimental stimuli can vary because of random or unexplained differences between the stimuli: whether the stimuli are words, pictures or stories, etc. And researchers have been aware that if we did not take such variation into account, we might mistakenly detect an experimental effect, for example, as a significant difference between mean response in different conditions, simply because different stimuli presented in different conditions varied in some unknown way, randomly.\nFor many years, psychologists tried to take random differences between stimuli into account, alongside random differences between participants, using a variety of strategies with important limitations (see Baayen et al. (2008), for discussion). Clark (1973) suggested that researchers could calculate \\(minF'\\) (not F) when doing Analyses of Variance of experimental data\nThis involves a series of steps.\n\nYou start by aggregating your data\n\n\nBy-subjects data – for each subject, take the average of their responses to all the items\nBy-items data – for each item, take the average of all subjects’ responses to that item\n\n\nYou do separate ANOVAs, one for by-subjects (F1) data and one for by-items (F2) data\nYou put F1 and F2 together, calculating minF’\n\nAveraging data by-subjects or by-items is relatively simple. It is very common to see, in the literature, psychological reports in which F1 and F2 analysis results are presented (that is why I am explaining this).\nCalculating \\(minF'\\) is also relatively simple:\n\\[\nminF' = \\frac{MS_{effect}}{MS_{\\text{random-subject-effects}} + MS_{\\text{random-word-differences}}} = \\frac{F_1F_2}{F_1 + F_2}\n\\]\nHowever, after a while, psychologists stopped doing the extra step of the \\(minF'\\) calculation (Raaijmakers et al., 1999). They carried on calculating and reporting F1 and F2 ANOVA results but, as Baayen et al. (2008) discuss, that approach risks a high false positive error rate.\nPsychologists also found that while the \\(minF'\\) approach allowed them to take into account between-participant and between-stimulus differences it could not be applied where ANOVA could not be used. This stopped researchers from taking a comprehensive approach to error variance where they wanted to conduct multiple regression analyses.\nIn the psychological literature, you will often see multiple regression analyses of by-items data, where a sample of participants has been asked to respond to a sample of stimuli, and the analysis is of the effects of stimulus properties on outcomes averaged (over participants’ responses) to the mean outcome by item. The problem is that analyzing data only by-items ensures that we lose track of participant differences.\nLorch & Myers (1990) warn that analyzing only by-items mean RTs just assumes wrongly that subjects are a fixed effect. This approach, again, risks a higher rate of false positive errors.\n\n\nThe good thing is that, thanks to the advent of mixed-effects models, we now no longer need to tolerate these problems.\nIn the context of our working example, in our analysis of the CP study data, we can build up our mixed-effects model by adding a random effect to capture the impact of unexplained differences between stimuli.\nWe model the random effect of items on intercepts by modeling the intercept as two terms:\n\\[\n\\beta_{0j} = \\gamma_0 + W_{0j}\n\\]\n\nwhere \\(\\gamma_0\\) is the average intercept and \\(W_{0j}\\) represents the deviation, for each word, between the average intercept and the per-word intercept.\n\nOur model can now incorporate the additional random effect of items on intercepts:\n\\[\nY_{ij} = \\gamma_0 + \\gamma_1X_j + U_{0i}+ U_{1i}X_j + W_{0j} + e_{ij}\n\\]\nIn this model, the outcome \\(Y_{ij}\\) is related to:\n\nthe average intercept \\(\\gamma_0\\) and the word frequency effect \\(\\gamma_1X_j\\)\nplus random effects due to unexplained differences between participants in intercepts \\(U_{0i}\\) and the slope of the frequency effect \\(U_{1i}X_j\\)\nas well as random differences between items in intercepts \\(W_{0j}\\),\nin addition to the residual term \\(e_{ij}\\).\n\n\n\n\n\n\n\nWarning\n\n\n\nWhat about random effects associated with differences between stimulus items in the slopes of effects?\n\nJust as we may expect there to be between-participant differences in the slope of the word frequency effect, we may expect there to be between-stimulus differences in the slope of the effect of, e.g., participant age.\nRest assured, we will look at this question.\n\n\n\n\n\n\nWe can fit a mixed-effects model of the \\(\\text{RT} \\sim \\text{frequency}\\) relationship, taking into account the random differences between participants and now also the random differences between stimulus words.\n\nlmer.all.2 &lt;- lmer(RT ~  Lg.UK.CDcount + \n                         (Lg.UK.CDcount + 1||subjectID) +\n                         (1|item_name),\n             \n             data = long.all.noNAs)\n\nsummary(lmer.all.2)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: RT ~ Lg.UK.CDcount + ((1 | subjectID) + (0 + Lg.UK.CDcount |  \n    subjectID)) + (1 | item_name)\n   Data: long.all.noNAs\n\nREML criterion at convergence: 116976.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.1795 -0.5474 -0.1646  0.3058 12.9485 \n\nRandom effects:\n Groups      Name          Variance Std.Dev.\n item_name   (Intercept)     3397    58.29  \n subjectID   Lg.UK.CDcount   3624    60.20  \n subjectID.1 (Intercept)   112313   335.13  \n Residual                   20704   143.89  \nNumber of obs: 9085, groups:  item_name, 159; subjectID, 61\n\nFixed effects:\n              Estimate Std. Error t value\n(Intercept)     971.07      51.87  18.723\nLg.UK.CDcount   -72.33      10.79  -6.703\n\nCorrelation of Fixed Effects:\n            (Intr)\nLg.UK.CDcnt -0.388\n\n\nThis is the same mixed-effects model as the one we discussed in Section 1.9.5 and Section 1.9.6 but with one important addition.\n\nWe add (1|item_name) to take into account random differences between between words in intercepts.\n\n\n\nTake a look at the model results. You should notice three changes.\n\nYou can see that the estimate for the effect of word frequency on reading reaction time has changed again, it is now \\(\\beta = -72.33\\)\nitem_name   (Intercept)     3397 there is now an additional term in the list of random effects, giving the model estimate for variance associated with random differences between words in intercepts\nAnd you can see that the residual variance has changed. In the first model lmer.all.1 it was 23734, now it is 20704\n\n\n\n\n\n\n\nTip\n\n\n\nThe reduction in residual variance is one way in which we can judge how good a job the model is doing in accounting for the variance in the outcome, observed response reaction time.\n\n\nWe can see that by adding a term to account for differences between items we can reduce the amount by which the model estimates deviate from observed outcomes. This difference in error variance is, essentially, one basis for estimating how well the model fits the data, and a basis for estimating the variance explained by a model in terms of the \\(R^2\\) statistic you have seen before.\nWe will come back to this.\n\n\n\n\n\nAs I have said, we usually do not aim to examine the specific deviation from the average intercept or the average fixed effect slope for a participant or stimulus. We estimate just the spread of deviations by-participants or by-items.\nA mixed-effects model like our final model includes fixed effects corresponding to the intercept and the slope of the word frequency effect plus the variances:\n\n\\(var(U_{0i})\\) variance of deviations by-participants from the average intercept;\n\\(var(U_{1i}X_j)\\) variance of deviations by-participants from the average slope of the frequency effect;\n\\(var(W_{0j})\\) variance of deviations by-items from the average intercept;\n\\(var(e_{ij})\\) residuals, at the response level, after taking into account all other terms.\n\nBecause we have variances, we may expect the random effects of participants or items to covary, e.g., participants who are slow to respond may also be more susceptible to the frequency effect, as can be seen in Figure 8.\n\n\n\n\n\n\n\n\nFigure 8: Scatterplot showing the relationship between estimated coefficients for the intercept and for the frequency effect, for each child analysed separately\n\n\n\n\n\nThis is why it would often make sense to specify, among the random effects of the model, terms corresponding to the covariance of the random effects:\n\n\\(covar(U_{0i}, U_{1i}X_j)\\)\n\n\n\n\n\n\n\nWarning\n\n\n\nRemember we excluded random effects covariance using ||.\n\n\nIn Section 1.9.5.2, I noted how we used the || notation to stop the model estimating the covariance between differences between participants in intercepts and in slopes. The reason I did this is that if I had requested that the model estimate the covariance the model would have failed to converge. What this means depends on understanding how mixed-effects models are estimated. We shall have to return to a development of that understanding later. For now, it is enough to note that mixed-effects models fitted with lmer() often have more difficulty with random effects covariance estimates.\n\n\n\nThere is no official convention on what or how to report the results of a mixed-effects model. Lotte Meteyard and I suggest what psychologists should report in an article (Meteyard & Davies, 2020) that has been downloaded a few thousand times so, maybe, our advice will help to influence practice.\n\n\nExplain what you did, and why.\nExplain what you found, not just whether effects are significant or not.\n\n\n\nWe argue that researchers should explain what analysis they have done and, where space allows, should report both the estimates of the fixed effects and the estimates of the random effects.\nWe think you can report the model code (maybe in an appendix, maybe in a note under a tabled summary of results).\n\nA table summary presenting model results can look like this.\n\n\n\nCoefficients\nEstimate\nSE\nt\n\n\n\n\n\n(Intercept)\n971.1\n51.9\n18.7\n\n\n\nFrequency effect\n-72.3\n10.8\n-6.7\n\n\n\n\n\n\n\nGroups\nName\nVariance\nSD\n\n\n\n\n\nitem\n(Intercept)\n3397\n58.3\n\n\n\nparticipant\n(Intercept)\n112314\n335.1\n\n\n\nparticipant\nFrequency\n3624\n60.2\n\n\n\nresidual\n\n20704\n143.9\n\n\n\n\nNote: lmer(RT ~  Lg.UK.CDcount + (Lg.UK.CDcount + 1||subjectID) + (1|item_name)\nBarr et al. (2013) argued that choices about random effects structure affect the generalizability of the estimates of fixed effects. In particular, it seems sensible to examine the possibility that the slope of the effect of an explanatory variable may vary at random between participants or between stimuli. Correspondingly, researchers should report and explain their decisions about the inclusion of random effects.\n\n\n\n\n\n\nTip\n\n\n\nResearchers should report their modelling in sufficient detail that their results can be reproduced by others.\n\n\nIt is normal practice in psychology to report the p-values associated with null hypothesis significance tests of effects when reporting analysis. Performing hypothesis tests using t- or F-distributions depends on the calculation of degrees of freedom yet it is uncertain how degrees of freedom should be counted when analyzing multilevel data (Baayen et al. (2008)). In most software applications, however, p-values associated with fixed effects may be calculated using an approximation for denominator degrees of freedom.\nWe will come back to how we should report the results of mixed-effects models because, here, too, in learning about writing, we can benefit by developing our approach, in depth, step by step.\n\n\n\nA large proportion of psychological studies involves scenarios in which the researcher samples both participants and some kind of stimuli. Often, the researcher will present the stimuli to the participants for response in some version of a range of possible designs:\n\nall participants see and respond to all stimuli;\nparticipants respond to different sub-sets of stimuli in different conditions (or in different groups) but they see and respond to all stimuli in a sub-set;\nparticipants are allocated to respond to stimulus sub-sets according to a counter balancing scheme (e.g., through the use of Latin squares).\n\nWhatever version of this scenario, if participants are responding to multiple stimuli and if multiple partcipants respond to each stimulus, then the data will have a multilevel structure such that each observation can be grouped both by participant and by stimulus.\nWe are interested in taking into account the random effects associated with unexplained or random differences between participants or between stimuli. We often discuss the accounting of these effects in terms of the estimation of error variances associated with the random differences, calling the effects of the differences random effects. Where we have to deal with both samples of participants and samples of stimuli, we can talk about crossed random effects.\nThe terms are not that important. The insight is.\n\n\n\n\n\n\nImportant\n\n\n\nIn general, in experimental psychological science, when we do data analysis, if we want to estimate effects of experimental variables more accurately then our models need to incorporate terms to capture the impact on observed outcomes of sampled participants and sampled stimuli.\n\n\nHistorically, we have, as a field, learned to take into account these sampling effects. Now, and most likely, more and more commonly in the future, we are learning to use multilevel or mixed-effects models to do this.\n\n\n\n\nWe discussed the way that data are structured when they come from studies with repeated measures designs. Critically, we examined data from a common study design where a sample of stimulus items are presented for response to members of a participant sample. This means that each observation can be grouped by participant and, also, by stimulus. The possibility that observations can be grouped means that the data have a multilevel structure.\nThe multilevel structure requires the use of linear mixed-effects models when we seek to estimate the effects of experimental variables. The fact that data can be grouped both by participant and by stimulus means that the model can incorporate random effects to capture random between-participant differences as well as between-stimulus differences.\nThe use of mixed-effects models has meant that psychologists no longer need to adopt compromise solutions which have important limitations, like by-items and by-subjects analyses.\n\n\n\nWe reviewed the ways that experimental data can be untidy. And we outlined the steps that may be required to process untidy data into a tidy format suitable for analysis. As is typical for the data analysis we need to do for experimental psychological science, getting data ready for analysis requires a series of steps including: access; import; restructure; select variables; and filter observations.\nWe then developed a mixed-effects model to answer the question:\n\n\n\n\n\n\nNote\n\n\n\n\nResearch question: What word properties influence responses to words in a test of reading aloud?\n\n\n\nOur analysis focused on the relationship between reading response reaction time (RT, in ms) and the predictor word frequency. We examined how the effect of word frequency was estimated in a linear model ignoring the multilevel structure and then in mixed-effects models which incorporated terms to capture:\n\nvariance associated with random differences between participants in intercepts or in the slope of the frequency effect,\nvariance associated with random differences between items in intercepts.\n\nWe saw that estimates of the frequency effect differed between different models.\n\n\n\n\nWe used a number of functions to tidy and visualize the CP study data.\n\nread_csv() and read_csv() to load source data files into the R workspace\npivot_longer() to restructure data from wide to long\nfull_join() to put together data from separate data-sets; in our example, from data-sets holding information about participant attributes, stimulus word properties, and participant behaviours\nselect() to select the variables we need\nfilter() to filter observations based on conditions\nna.omit() to remove missing values\nFor visualisation, we used facet_wrap() to show plots of the relationship between outcome and predictor variables separately for different groups (by participant, or by item)\n\nFor our analyses:\n\nWe used lmer() to fit a multilevel model.\n\nWe used the summary() function to get model results for both linear models and for the mulilevel or liner mixed-effects model.\n\n\n\n\n(Baayen et al., 2008; see also Barr et al., 2013; Judd et al., 2012) discuss mixed-effects models with crossed random effects in a variety of contexts in psychological science. The explanations are clear and the examples are often helpful.\nI wrote a tutorial article on mixed-effects models with Lotte Meteyard (Meteyard & Davies, 2020). We discuss how important the approach now is for psychological science, what researchers worry about when they use it, and what they should do and report when they use the method.\n\n\n\n\nBaayen, R. H., Davidson, D. J., & Bates, D. M. (2008). Mixed-effects modeling with crossed random effects for subjects and items. Journal of Memory and Language, 59(4), 390–412. https://doi.org/10.1016/j.jml.2007.12.005\n\n\nBarr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of Memory and Language, 68, 255–278.\n\n\nClark, H. (Stanford. U. (1973). Clark_1973_LanguageAsAFixedEffectFallacy.pdf.\n\n\nDavies, R. A. I., Birchenough, J. M. H., Arnell, R., Grimmond, D., & Houlson, S. (2017). Reading through the life span: Individual differences in psycholinguistic effects. Journal of Experimental Psychology: Learning Memory and Cognition, 43(8). https://doi.org/10.1037/xlm0000366\n\n\nFrederickson, N., Frith, U., & Reason, R. (1997). Phonological assessment battery [PhAB]: Manual and test materials. nfer Nelson Publishing Company Ltd.\n\n\nGrolemund, G., & Wickham, H. (n.d.). R for Data Science [Book]. https://www.oreilly.com/library/view/r-for-data/9781491910382/\n\n\nJudd, C. M., Westfall, J., & Kenny, D. A. (2012). Treating stimuli as a random factor in social psychology : A new and comprehensive solution to a pervasive but largely ignored problem. 103(1), 54–69. https://doi.org/10.1037/a0028347\n\n\nLorch, R. F., & Myers, J. L. (1990). Regression analyses of repeated measures data in cognitive research. Journal of Experimental Psychology: Learning, Memory, and Cognition, 16(1), 149–157. https://doi.org/10.1037/0278-7393.16.1.149\n\n\nMeteyard, L., & Davies, R. A. I. (2020). Best practice guidance for linear mixed-effects models in psychological science. Journal of Memory and Language, 112, 104092. https://doi.org/10.1016/j.jml.2020.104092\n\n\nRaaijmakers, J. G. W., Schrijnemakers, J. M. C., & Gremmen, F. (1999). How to deal with \"the language-as-fixed-effect fallacy\": Common misconceptions and alternative solutions. Journal of Memory and Language, 41(3), 416–426. https://doi.org/10.1006/jmla.1999.2650\n\n\nStainthorp, R. (1997). A children’s author recognition test: A useful tool in reading research. Journal of Research in Reading, 20(2), 148158.\n\n\nTorgesen, J. K., Rashotte, C. A., & Wagner, R. K. (1999). TOWRE: Test of word reading efficiency. Pro-ed Austin, TX.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 17. Conceptual introduction to mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/02-mixed.html#sec-intro-motivations",
    "href": "PSYC412/part2/02-mixed.html#sec-intro-motivations",
    "title": "Week 17. Conceptual introduction to mixed-effects models",
    "section": "",
    "text": "In the conceptual introduction to multilevel data and the workbook introduction to multilevel data chapters, we looked at a multilevel structured data-set in which there were observations about children’s grades, and it became evident that those children can be grouped by or under classes. As we discussed, this kind of data structure will come from studies with a very common design in which, for example, the researcher records observations about a sample of children who are members of a sample of classes. In working with these kind of data, it is common to say that the observations of children’s grades are nested within classes in a hierarchy.\nMany Psychologists conduct studies where observations are properly understood to be structured in groups of some form but where, nevertheless, it is inappropriate to think of the observations as being nested (Baayen et al., 2008). We are talking, here, about repeated-measures designs where the experimenter presents a sample of multiple stimuli for response to each participant in a sample of multiple participants. This is another very common experimental design in psychological science.\nStudies with repeated-measures designs will produce data with a structure that, also, requires the use of mixed-effects models but, as we shall see, the way we think about the structure will be a bit more complicated. We could say that observations of the responses made by participants to each stimulus can be grouped by participant: each person will tend to respond in similar ways to different stimuli. Or, we could say that observations of responses can be grouped by stimulus because each stimulus will tend to evoke similar kinds of responses in different people. Or, we could say that both forms of grouping should be taken into account at the same time.\nWe shall take the third position and this chapter will concern why, and how we will adapt our thinking and practice.\n\n\n\n\n\n\nQuick start: Workbook introduction to mixed-effects models\n\n\n\nThis chapter presents ideas and methods relating to the analysis of repeated-measures data using mixed-effects models in which we extend the ideas and the methods in depth.\n\nYou can find a quick start, focused on practical analysis steps, in the workbook introduction to mixed-effects models.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 17. Conceptual introduction to mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/02-mixed.html#sec-intro-key",
    "href": "PSYC412/part2/02-mixed.html#sec-intro-key",
    "title": "Week 17. Conceptual introduction to mixed-effects models",
    "section": "",
    "text": "Important\n\n\n\nLinear mixed-effects models and multilevel models are basically the same.\n\n\nThis week, we again look at data with multilevel structure. But we are looking at data where participants were asked to respond to a set of stimuli (here, words) so that our observations consist of recordings made of the response made by each child to each stimulus. We use the same procedure we did for multilevel data but with one significant change which we shall identify and explain.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 17. Conceptual introduction to mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/02-mixed.html#sec-intro-targets",
    "href": "PSYC412/part2/02-mixed.html#sec-intro-targets",
    "title": "Week 17. Conceptual introduction to mixed-effects models",
    "section": "",
    "text": "Our learning objectives again include the development of both concepts and skills.\n\nskills – practice how to tidy experimental data for mixed-effects analysis.\nconcepts – begin to develop an understanding of crossed random effects of participants and stimuli.\nskills and concepts – practice fitting linear mixed-effects models incorporating random effects of participants and stimuli.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 17. Conceptual introduction to mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/02-mixed.html#sec-intro-mixed-guide",
    "href": "PSYC412/part2/02-mixed.html#sec-intro-mixed-guide",
    "title": "Week 17. Conceptual introduction to mixed-effects models",
    "section": "",
    "text": "I have provided a collection of materials you can use. Here, I explain what they are and how I suggest you use them.\n1. Video recordings of lectures\n1.1. I have recorded a lecture in three parts. The lectures should be accessible by anyone who has the link.\n\nPart 1 – about 8 minutes\nPart 2 – about 21 minutes\nPart 3 – about 15 minutes\n\n1.2. I suggest you watch the recordings then read the rest of this chapter.\n\nThe lectures provide a summary of the main points.\n\n1.3. You can download the lecture slides in three different versions:\n\n402-week-18-LME-2.pdf: high resolution .pdf, exactly as delivered [6 MB];\n402-week-18-LME-2_1pp-lo.pdf: lower resolution .pdf, printable version, one-slide-per-page [about 900 KB];\n402-week-18-LME-2_6pp-lo.pdf: lower resolution .pdf, printable version, six-slides-per-page [about 900 KB].\n\nThe high resolution version is the version delivered for the lecture recordings. Because the images are produced to be high resolution, the file size is quite big (6 MB) so, to make the slides easier to download, I produced low resolution versions: 1pp and 6pp. These should be easier to download and print out if that is what you want to do.\n2. Chapter: 02-mixed\n2.1. I have written this chapter to discuss the main ideas and set out the practical steps you can follow to start to develop the skills required to analyse multilevel structured data with crossed random effects.\n2.2. The practical elements include data tidying, visualization and analysis steps.\n2.3. You can read the chapter, run the code, and do the exercises.\n\nRead in the example CP reading study data-sets.\nIdentify how the data are structured by both participant and stimulus differences.\nUse visualizations to explore the impact of the structure.\nRun analyses using linear mixed-effects models involving multiple random effects.\nReview the recommended readings (Section 1.14).\n\n3. Practical materials\n3.1 In the following sections, I describe the practical steps, and associated practical materials (exercise workbooks and data), you can use for your learning.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 17. Conceptual introduction to mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/02-mixed.html#sec-intro-mixed-data",
    "href": "PSYC412/part2/02-mixed.html#sec-intro-mixed-data",
    "title": "Week 17. Conceptual introduction to mixed-effects models",
    "section": "",
    "text": "In this chapter, we will be working with the CP reading study data-set. CP tested 62 children (aged 116-151 months) on reading aloud in English. In the experimental reading task, she presented 160 words as stimuli. The same 160 words were presented to all children. The words were presented one at a time on a computer screen. Each time a word was shown, each child had to read the word out loud and their response was recorded. Thus, the CP reading study data-set comprised observations about the responses made by 62 children to 160 words.\nIn addition to the reading task, CP administered tests of reading skill (TOWRE sight word and phonemic tests, Torgesen et al., 1999), reading experience (CART, Stainthorp, 1997), the Spoonerisms sub-test of the Phonological Awareness test Battery (PhAB, Frederickson et al., 1997), and an orthographic choice test measure of orthographic knowledge. She also recorded the gender and the handedness of the children.\nWe are going to use the CP study data to examine the answers to a research question similar to the question CP investigated:\n\n\n\n\n\n\nNote\n\n\n\n\nResearch question: What word properties influence responses to words in a test of reading aloud?\n\n\n\nWe can look at the answers to this question while also taking into account the impacts of random differences – between sampled participants or between sampled words – using mixed-effects models.\nUltimately, the CP data-set were incorporated in an analysis of the impact of age on reading skills over the life-span, reported by Davies et al. (2017). You can find more details on the data and the methods in that paper. (Data and analysis code are shared through the journal article webpage [paywalled] here, and a preprint version of the article can be accessed here.)\nThe CP study resembles many studies in psychological science. The critical features of the study are that:\n\nWe have an outcome measure – the reading response – observed multiple times.\n\n\nWe have multiple responses recorded for each participant: they make one response to each stimulus (here, each stimulus word), for the multiple stimuli that they see in the experimental reading task.\nAnd we have multiple responses recorded for each stimulus: one response is made to each stimulus by each participant, for all the participants who completed the task, in a sample of multiple participants.\n\nThe presence of these features is the reason why we need to use mixed-effects models in our analysis. These features are common across a range of study designs so the lessons we learn will apply frequently in psychological research. This is the reason why it is important we teach and learn how to use mixed-effects models.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nGet the data: get the data file and the .R script you can use to do the exercises that will support your learning.\n\nYou can download the files folder for this chapter by clicking on the link 02-mixed.zip.\n\n\n\nThe practical materials folder includes data files and an .R script:\n\nCP study word naming rt 180211.dat\nCP study word naming acc 180211.dat\nwords.items.5 120714 150916.csv\nall.subjects 110614-050316-290518.csv\nlong.all.noNAs.csv\n402-02-mixed-workbook.R the workbook you will need to do the practical exercises.\n\nThe .dat files are tab delimited files holding behavioural data: the latency or reaction time rt (in milliseconds) and the accuracy acc of response made by each participant to each stimulus.\nThe .csv files are comma separated values files. The words.items file holds information about the 160 stimulus words presented in the experimental reading (word naming) task. The all.subjects file holds information about the 62 participants who volunteered to take part in the experiment.\nSo: we will be working with multiple data files located in a .zip folder called 02-mixed. And, in this folder, we have got four files that we will need to import or read in to R.\nIn the following, I will describe a series of steps through which we get the data ready for analysis. However, as we shall see, you can avoid these steps by using the pre-tidied data-set:\n\nlong.all.noNAs.csv\n\nThe data files are collected together with the .R script:\n\n402-02-mixed-workbook.R.\n\nDuring practical sessions, each week, you can use the workbook to prompt your code construction and your thinking, with support.\n\n\nAfter practical sessions, you will be able to download an answers version of the workbook .R so check back here after the session.\n\n\n\n\n\n\nImportant\n\n\n\nGet the answers: get the data file and the .R script with answers.\n\nYou can download the files folder for this chapter by clicking on the link 02-mixed-answers.zip.\n\n\n\nThe link to this folder will not work until after a session has finished.\nWhen the link is live, you will be able to download a folder including:\n\n402-02-mixed-workbook-with-answers.R with answers to questions and example code for doing the exercises.\n\nBefore we do anything else, we need to talk about the messiness of real Psychological data and how we deal with it.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 17. Conceptual introduction to mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/02-mixed.html#sec-intro-mixed-data-untidy",
    "href": "PSYC412/part2/02-mixed.html#sec-intro-mixed-data-untidy",
    "title": "Week 17. Conceptual introduction to mixed-effects models",
    "section": "",
    "text": "Ordinarily, textbooks and guides to data analysis give you the data ready for analysis but this situation will never be true for your professional practice (at least, not at first). Instead of pretending that data arrive ready for analysis, we are going to look at the process of data tidying, step-by-step. This will help you to get ready for the same process when you have to develop and use it in your own research.\nWe are going to spend a bit of time looking at the data tidying process. This process involves identifying and resolving a series of challenges, in order. Looking at the tidying process will give you a concrete sense of the structure in the data. You should also take this opportunity to reflect on the nature of the process itself – what we have to do and why, in what order and why – so that you can develop a sense of the process you might need to build when the time comes for you to prepare your own data for analysis.\nThe time that we spend looking at data tidying is an investment in learning that will save you time later, in your professional work. If, however, you want to skip it, go to section Section 1.8.\n\n\nIn analyzing psychological data, the first step is usually to collect the data together. In psychological research, the data may exist, at first, in separate files. For the CP study, we have separate files for each of the pieces of information we need to use in our analyses:\n\nParticipant attributes: information about participants’ age, gender, identifier code, and abilities on various measures.\nStimulus attributes: information about stimulus items, e.g., the word, its item number, its value on each variable in a set of psycholinguistic properties (like word length, frequency).\nBehaviour: behavioural observations e.g. reaction time or accuracy of responses made by each participant to each stimulus word.\n\nOften, we need all these kinds of information for our analyses but different pieces of information are produced in separate ways and come to us in separate files. For example, we may collect experimental response data using software like PsychoPy, E-Prime, Qualtrics or DMDX. We may collect information about participant characteristics using standardized measures, or by asking participants to complete a set of questions on their age, gender, and so on.\n\n\n\nOften, the files we get are untidy: not in a useful or tidy format. For example, if you open the file CP_study_word_naming_rt_180211.dat (a .dat or tab delimited file) in Excel, you will see a spreadsheet that looks like Figure 1.\n\n\n\n\n\n\nFigure 1: CP study RTs .dat file\n\n\n\nTypical of the output from data collection software, we can see a data table with:\n\nin the top row, column header labels item_name, AislingoC, AllanaD ...;\nin the first (leftmost) column, row labels item_name, act, ask, both ...;\nfor each row, we see values equal to the reaction time (RT) observed for the response made to each stimulus (listed in the row labels);\nfor each column, we see values equal to the RTs observed for each person (listed in the column labels);\nand at each intersection of row and column (for each cell), we see the RT observed for a response made by a participant to a stimulus.\n\nData laid out like this are sometimes said to be in wide format. You can see that the data are wide because at least one variable – here, reading reaction time – is held not in one column but spread out over several columns, side-by-side. Thus, the data-set is wide with fewer rows and many columns.\nWe want the data in what is called the tidy format.\n\n\nThere are three inter-related rules which make data tidy (Grolemund & Wickham, n.d.).\n\n\n\n\n\n\nImportant\n\n\n\nIn tidy data:\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\n\n\nYou can read more about tidy data here.\nFor our purposes, the reason we want the data in tidy format is that it is required for the functions we are going to use for mixed-effects modelling. However, in general, tidy format is maximally flexible, and convenient, for use with different R functions.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 17. Conceptual introduction to mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/02-mixed.html#sec-intro-mixed-tidy-data",
    "href": "PSYC412/part2/02-mixed.html#sec-intro-mixed-tidy-data",
    "title": "Week 17. Conceptual introduction to mixed-effects models",
    "section": "",
    "text": "To answer our research question, we will need to combine the behavioural data with information about the participants (age, gender …) and about the words (word, frequency …) We will need to ensure that the data-set we construct will be in tidy format. We will need to select variables (columns) to get just those required for our later analyses. And we will need to filter cases (rows), excluding errors or outliers.\nWe shall need to do this work in a series of processing steps:\n\nImport the data or read the data into R, see Section 1.7.1\nRestructure the data, see Section 1.7.3\nSelect or transform variables, see Section 1.7.6\nFilter observations, see Section 1.7.7\n\nWe will use {tidyverse} library functions from the beginning, starting with the import stage.\n\nlibrary(tidyverse)\n\n(Every step can also be done in alternative processing steps with the same result using base R code.)\n\n\nI am going to assume you have downloaded the data files, that they are all in the same folder, and that you know where they are on your computer or server. We need to use different versions of the read_ function to read all four files into R.\n\nbehaviour.rt &lt;- read_tsv(\"CP study word naming rt 180211.dat\", na = \"-999\")\nbehaviour.acc &lt;- read_tsv(\"CP study word naming acc 180211.dat\", na = \"-999\")\nsubjects &lt;- read_csv(\"all.subjects 110614-050316-290518.csv\", na = \"-999\")\nwords &lt;- read_csv(\"words.items.5 120714 150916.csv\", na = \"-999\")\n\nThese different versions respect the different ways in which the .dat and .csv file formats work. We need read_tsv() when data files consist of tab separated values. We need read_csv() when data files consist of comma separated values.\nYou can read more about the {tidyverse} {readr} library of helpful functions here\nIt is very common to get experimental data in all sorts of different formats. Learning to use tidyverse functions will make it easier to cope with this when you do research.\n\n\n\n\n\n\nTip\n\n\n\nWe use the read_ function to read in the data\n\nentering (here, at least) arguments – inside the brackets after the function name – to tell R what file we need and how missing values (NAs) are coded.\n\n\n\nIt will help your understanding to examine an example. Take a look at what this line of code includes, element by element.\n\nbehaviour.rt &lt;- read_tsv(\"CP study word naming rt 180211.dat\", na = \"-999\")\n\n\nWe write behaviour.rt &lt;- read_tsv(...) to create an object in the R environment, which we call behaviour.rt: the object with this name is the data-set we read into R using read_tsv(...).\nWhen we write the function read_tsv(...) we include two arguments inside it.\nread_tsv(\"CP study word naming rt 180211.dat\", ... first, the name of the file, given in quotes \"\" and then a comma.\nread_tsv(..., na = \"-999\") second, we tell R that there are some missing values na which are coded with the value \"-999\".\n\n\n\n\nIn the data-sets – typically, the spreadsheets – we create in our research, we will have values missing for different reasons. Take another look at the data spreadsheet you saw earlier, Figure 1.\n\n\n\n\n\n\nTip\n\n\n\nIn R, a missing value is said to be “not available”: NA.\n\n\nYou should be able to see that the spreadsheet holds information, as explained, about the RTs of the responses made by each child to each stimulus word. Each of the cells in the spreadsheet (i.e. the box where a column intersects with a row) includes a number value. Most of the values are positive numbers like 751.3: the reaction time of a response, recorded in milliseconds. The values have to be positive because they represent the length of time between the moment the stimulus word is presented on the test computer screen and the moment the child’s spoken word response has begun to be registered by the computer microphone and sound recording software.\nSome of the cells hold the value -999, however. Obviously, we cannot have negative RT. The value represents the fact that we have no data. Take a look at Figure 1: we have a -999 where we should have a RT for the response made by participant AllanaD to the word broad. This -999 is there because, for some reason, we did not record an RT or a response for that combination of participant and stimulus.\nWe can choose any value we like, as researchers, to code for missing data like this. Some researchers choose not to code for the absence of a response recording or leave the cell in a spreadsheet blank or empty where data are missing. This is bad practice though it is common.\nThere are a number of reasons why it is bad practice to just leave a cell empty when it is empty because no observation is to be recorded.\n\nData may be missing for different reasons: maybe a child did not make any response to a stimulus (often called a “null response”); or maybe a child made a response but there was a microphone or other technical fault; or maybe a child made a response but it was an error and (here) the corresponding performance measure (RT) cannot be counted.\nIf you do not code for missingness in the data then the software you use will do it for you, but you may not know how it does so, or where.\nIf you have missing data, you ought to be able to identify where the data are missing.\n\nI use -999 to code for missing values because you should never see a value like that in real reading RT data. You can use whatever value you like but you should make sure you do code for missing data somehow.\n\n\n\nWe are going to need to restructure these data from a wide format to a longer format. We need to restructure both behavioural data-sets, accuracy and RT. We do this using the pivot_longer() function.\n\nrt.long &lt;- behaviour.rt %&gt;%\n             pivot_longer(2:62, names_to = \"subjectID\", values_to = \"RT\")\n\nacc.long &lt;- behaviour.acc %&gt;%\n              pivot_longer(2:62, names_to = \"subjectID\", values_to = \"accuracy\")\n\nResearchers used to have to do this sort of thing by hand, using copying and pasting, in Excel or SPSS. Doing the process by hand takes many hours or days. And you always make errors.\n\n\n\n\n\n\nTip\n\n\n\nDoing data-set construction programmatically, using R functions, is generally faster and more reliable than doing it by hand.\n\n\nHere, we use a function you may have seen before: pivot_longer(). It will help your understanding to examine the code carefully.\n\nrt.long &lt;- behaviour.rt %&gt;%\n             pivot_longer(2:62, names_to = \"subjectID\", values_to = \"RT\")\n\nThe name of the function comes from the fact that we are starting with data in wide format e.g. behaviour.rt where we have what should be a single variable of observations (RTs) arranged in a wide series of multiple columns, side-by-side (one column for each participant). But we want to take those wide data and lengthen the data-set, increasing the number of rows and decreasing the number of columns.\nLet’s look at this line of code bit by bit. It includes a powerful function that accomplishes a lot of tasks, so it is worth explaining this function in some detail.\n\nrt.long &lt;- behaviour.rt %&gt;%\n\n\nAt the start, I tell R that I am going to create a new longer data-set (more rows, fewer columns) that I shall call rt.long.\nI will create this longer data-set from &lt;- the original wide data-set behaviour.rt.\nand I will create the new longer data-set by taking the original wide data-set and piping it %&gt;% to the pivot function coded on the next line:\n\n\npivot_longer(2:62, names_to = \"subjectID\", values_to = \"RT\")\n\n\nOn this next line, I tell R how to do the pivoting by using three arguments.\n\n\npivot_longer(2:62...)\n\n\nFirst, I tell R that I want to re-arrange all the columns that can be found in the data-set from the second column to the sixty-second column.\nIn a spreadsheet, we have a number of columns.\nColumns can be identified by their position in the spreadsheet.\nThe position of a column in a spreadsheet can be identified by number, from the leftmost column (column number 1) to the rightmost column (here, column number 62) in our data-set.\nSo this argument tells R exactly which columns I want to pivot.\n\n\npivot_longer(..., names_to = \"subjectID\", ...)\n\n\nSecond, I tell R that I want it to take the column labels and put them into a new column, called subjectID.\nIn the wide data-set behaviour.rt, each column holds a list of numbers (RTs) but begins with a word in the topmost cell, the name code for a participant, in the column label position.\nWe want to keep the information about which participant produces which response when we pivot the wide data to a longer structure.\nWe do this by asking R to take the column labels (the participant names) and listing them in a new column, called subjectID which now holds the names as participant ID codes.\n\n\npivot_longer(...values_to = \"RT\")\n\n\nThird, we tell R that all the RT values should be put in a single column.\nWe can understand that this new column RT will hold RT observations in a vertical stack, one cell for each response by a person to a word, with rows ordered by subjectID.\n\nThere are 61 columns of data listed by participant though 62 children were tested because we lost one child’s data through an administrative error. As a result, in the wide data sets there are 62 columns, with the first column holding item_name data.\nYou can find more information about pivoting data here\nAnd you can find more information specifically about the pivot_longer() operation here\n\n\n\nAs I noted, one problem with the wide format is that the data are structured so that the column names are not names of variables. In our example wide format data-set, behaviour.rt, the columns are headed by a participant identity code or name but a participant code is not the name of a variable, it is a value of the variable I call subjectID.\nIn the design of the CP reading study, we want to take into account the impact of differences between participants on response RT (so, we need to identify which participant makes which response). But we do not see the responses made by a participant as a predictor variable.\nA second problem is that, in a wide format file like behaviour.rt, information about the responses made to each stimulus word is all on the same row (that seems good) but in different columns. Each person responded to all the words. But the response made to a word e.g. act made by one participant is in a different column (e.g., 594.8ms, for AislingoC) from the response made to the same word by a different participant (e.g., 586ms, for AlexB). This means that information about the responses made to each stimulus word are spread out as values across multiple columns.\nYou can see this for yourself if you inspect the source rt data using head() to view the top four rows of the data-set.\n\n\n\n\n\nitem_name\nAislingoC\nAlexB\nAllanaD\nAmyR\nAndyD\nAnnaF\nAoifeH\nChloeBergin\nChloeF\nChloeS\nCianR\nConorF\nDavidL\nDillonF\nDJHerlihy\nEamonD\nEimearK\nEllenH\nEoinL\nGrainneH\nJackBr\nJackK\nJackS\nJamesoC\nJenniferoS\nKateF\nKayleighMc\nKenW\nKevinL\nKieranF\nKillianB\nKirstyC\nLeeJ\nMarkC\nMatthewC\nMeganoB\nMichaelaoD\nNataliaR\nNiallG\nNiallGavin\nNiallW\nOisinN\nOlaA\nOwenD\nPalomaM\nPauricT\nPerryD\nRachelD\nRebeccaGr\nRebeccaM\nRebeccaR\nRoisinF\nRonanT\nSarahP\nShaunaBr\nSiobhanR\nTaraB\nTeeTeeOj\nThomasK\nTristianT\nZainab\n\n\n\n\nact\n594.8\n586.0\nNA\n693.0\n597\n627.0\n649.0\n1081.0\n642.0\n622.7\n701.0\n686.0\n951.0\n661.0\n692.0\n670.0\n502.4\n578.4\n651.8\n441.6\n895.2\n529.0\n639.0\n809\n676.9\n568.8\n586.1\n591.3\n587\n586\n723.9\n1428.0\n557.0\n639.4\n676\n714.8\n623.3\n615.0\n796.0\n568.4\n800.9\n595\nNA\n574.9\n628\n797.0\n652\n757.0\n631.0\n520.5\n640.0\n733.0\n566\n758.0\n670.0\n532.4\n615.7\n540.0\n1390.0\n747\n651.0\n\n\nask\n481.5\n864.0\n1163.0\n694.4\n616\n631.0\n538.0\n799.3\n603.0\n526.0\n591.5\n699.6\n827.2\n635.0\n654.0\n508.0\n564.0\n822.0\n479.7\n600.0\n617.6\n555.9\n654.6\n765\n856.0\n576.7\n690.3\n501.9\n634\n626\n523.3\n640.7\n516.0\n625.7\n561\n698.2\n685.0\n606.0\n793.0\n551.9\n668.7\n722\n868.0\n633.0\n578\n660.2\n851\n640.3\n630.0\n535.0\n568.0\n579.7\n562\n747.0\n602.4\n558.4\n691.0\n580.8\n590.9\n795\n740.5\n\n\nboth\n457.5\n670.0\n1114.3\n980.0\n1019\n796.1\n545.2\nNA\n581.0\n568.4\n665.0\n751.0\n917.0\n808.1\n737.6\n597.0\n475.0\n608.0\n699.2\n600.6\n527.3\n601.0\n982.1\n917\n854.4\n571.6\n825.3\n584.0\n720\n571\n624.0\n853.4\nNA\n703.0\n781\n1065.8\n591.5\n559.2\n837.6\n612.0\n743.4\n743\n919.0\n883.0\n616\n734.6\n658\n574.4\n634.9\n559.7\n689.0\n997.8\n648\n753.4\n548.0\n530.5\n625.4\n552.0\n985.0\n640\n764.0\n\n\nbox\n546.0\n748.6\n975.0\n678.0\n589\n604.0\n574.0\n658.0\n688.7\n492.0\n641.0\n699.0\n824.0\n731.6\n634.0\n557.5\n520.1\n581.0\n1191.0\n612.0\n540.0\n540.0\n810.0\n885\n573.8\n614.9\n722.0\n594.6\n632\n549\n564.0\n632.6\n668.6\n627.0\n577\n785.9\n649.0\n616.0\n928.0\n608.7\n690.0\n857\n886.6\n544.0\n863\n726.0\n812\n640.5\n613.8\n395.1\n656.7\n604.7\n530\n667.4\n547.7\n570.0\n585.6\n532.0\n634.0\n764\n912.0\n\n\n\n\n\n\n\nThis structure is a problem for visualization and for analysis because the functions we will use require us to specify single columns for an outcome variable like reaction time.\nWe are looking at the process of tidying data because untidiness is very common. Learning how to deal with it will save you a lot of time and grief later.\nYou should check for yourself how subjectID and RT or accuracy scores get transposed from the old wide structure to the new long structure.\n\n# RT data\nhead(behaviour.rt)\nhead(rt.long)\n\n# accuracy data\nhead(behaviour.acc)\nhead(acc.long)\n\nIf you compare the rt.long or acc.long data with the data in the original wide format then you can see how – in going from wide – we have re-arranged the data to a longer and narrower set of columns:\n\none column listing each word;\none column for subjectID;\nand one column for RT or accuracy.\n\nWhat a check will show you is that we have multiple rows for responses to each item so that the item is repeated multiple times in different rows.\nThese data are now tidy.\n\nEach column has information about one variable\nAnd each row has information about one observation, here, the response made by a participant to a word\n\nThis is a big part of data tidying now done. However, these data are incomplete. Next we shall combine behavioural observations with data about stimulus words and about participants.\n\n\n\n\n\n\n\nTo answer our research question, we next need to combine the RT with the accuracy data, and then the combined behavioural data with participant information and stimulus information. This is because, as we have seen, information about behavioural responses, about participant attributes or stimulus word properties, are located in separate files.\nMany researchers have completed this kind of operation by hand. This involves copying and pasting bits of data in a spreadsheet. It can take hours or days. I know because I have done it, and I have seen others do it.\n\n\n\n\n\n\nWarning\n\n\n\nPlease do not try to combine data-sets through manual operations e.g. in Excel. I guarantee that:\n\nYou will make mistakes.\nYou will not know when or where those mistakes are in your data.\n\nThere are better ways to spend your time.\n\n\nWe can combine the data-sets, in the way that we need, using the {tidyverse} full_join() function. This gets the job done quickly, and accurately.\nFirst, we join RT and accuracy data together.\n\nlong &lt;- rt.long %&gt;% \n          full_join(acc.long)\n\nThen, we join subject and item information to the behavioural data.\n\nlong.subjects &lt;- long %&gt;% \n                   full_join(subjects, by = \"subjectID\")\n\nlong.all &lt;- long.subjects %&gt;%\n              full_join(words, by = \"item_name\")\n\nNotice, we can let R figure out how to join the pieces of data together. If we were doing this by hand then we would need to check very carefully the correspondences between observations in different data-sets.\nHere, in a series of steps, we take one data-set and join it (merge it) with the second data-set. Let’s look at an example element by element to better understand how this is accomplished.\n\nlong &lt;- rt.long %&gt;% \n           full_join(acc.long)\n\nThe code work as follows.\n\nlong &lt;- rt.long %&gt;%\n\n\nWe create a new data-set we call long.\nWe do this by taking one original data-set rt.long and %&gt;% piping it to the operation defined in the second step.\n\n\nfull_join(acc.long)\n\n\nIn this second step, we use the function full_join() to add observations from a second original data-set acc.long to those already from rt.long\n\nThe addition of observations from one database joining to those from another happens through a matching process.\n\nR looks at the data-sets being merged.\nIt identifies if the two data-sets have columns in common. Here, the data-sets have subjectID and item_name in common).\nR can use these common columns to identify rows of data. Here, each row of data will be identified by both subjectID and item_name i.e. as data about the response made by a participant to a word.\nR will then do a series of identity checks, comparing one data-set with the other and, row by row, looking for matching values in the columns that are common to both data-sets.\nIf there is a match then R joins the corresponding rows of data together.\nIf there isn’t a match then it creates NAs where there are missing entries in one row for one data-set which cannot be matched to a row from the joining data-set.\n\n\n\n\n\n\n\nTip\n\n\n\nNote that in one example, the example of code I discuss here, I did not specify identifying columns in common, allowing the function to do the work. In the other code chunks I did: long.all &lt;- long.subjects %&gt;% full_join(words, by = \"item_name\") using the by = ... argument.\n\nSometimes, you can vary in how you employ a _join() function.\nIt may help to specify the identifying column if you want to make explicit (to yourselves and others) how the process is to be completed.\n\n\n\n\n\nIn the {tidyverse} family of dplyr functions, when you work with multiple data-sets (tables of data), we call the data-sets relational data.\nThere are three families of functions (like verbs) designed to work with relational data:\n\nMutating joins, which add new variables to one data frame from matching observations in another.\nFiltering joins, which filter observations from one data frame based on whether or not they match an observation in the other table.\nSet operations, which treat observations as if they were set elements.\n\nWe can connect data-sets – relate them – according to shared variables like subjectID, item_name (for our data). In {tidyverse}, the variables that connect pairs of tables are called keys where, and this is what counts, key(-s) are variable(-s) that uniquely identify an observation.\nFor the experimental reading data, we have observations about each response made by a participant (one of 61 subjects) to an item (one of 160 words). For these data, we can match up a pair of RT and accuracy observations for each (unique) subjectID-item_name combination. If you reflect, we could not combine the RT and accuracy data correctly if we did not have both identifying variables in both data-sets, because both column variables are required to uniquely identify each observation.\nFurther, we could not combine the RT and accuracy data correctly if there were mismatches in values of the identifying variable. Sometimes, I have done this operation and it has gone wrong because a subjectID has been spelled one way in one data-set e.g. hugh and another way in the other data-set e.g. HughH. This leads me to share some advice.\n\n\n\n\n\n\nTip\n\n\n\n\nBe careful about spelling identifiers.\nAlways check your work after merger operations.\n\nYou can check your work by calculating data-set lengths to ensure the number of rows in the new data-set matches your expectations, given the study design and data collection procedure.\n\n\n\n\n\nWe used the full_join() function.\nThere are three kinds of joins.\n\nA left join keeps all observations in x.\nA right join keeps all observations in y.\nA full join keeps all observations in x and y.\n\nI used full_join() because I wanted to retain all observations from both data-sets, whether there was a match (as assumed) or not, in the identifying variables, between observations in each data-set.\n\n\n\n\nBreak the join: You can examine how the full_join() works by experimenting with stopping it from working.\n\nAs I discuss, you need to have matches in values on key (common) variables. If the subjectID is different on different data-sets, you will lose data that would otherwise be merged to form the merged or composite data-set.\nCheck what happens if you deliberately misspell one of the subjectID values in one of the original source wide behavioural data files.\nTo be safe, you might want to do this exercise with copies of the source files kept in a folder you create for this purpose. If it goes wrong, you can always re-access the source files and read them in again.\nYou can check what happens before and after you break the match by counting the number of rows in the data-set that results from the merger. We can count the number of rows in a data-set with:\n\nlength(long.all$RT)\n\n[1] 9762\n\n\nThis bit of code takes the length of the vector (i.e. variable column RT in data-set long.all), thus counting the number of rows in the data-set.\n\n\n\n\nOK, now we have all the data about everything all in one big, long and wide, data-set. But we do not actually require all of the data-set for the analyses we are going to do.\nWe next need to do two things. First, we need to get rid of variables we will not use: we do that by using select(). Then, we need to remove errors and outlying short RT observations: we do that by using filter() in Section Section 1.7.7.\nWe are going to select just the variables we need using the select() function.\n\nlong.all.select &lt;- long.all %&gt;% \n                        select(item_name, subjectID, RT, accuracy, \n                               Lg.UK.CDcount, brookesIMG, AoA_Kup_lem, \n                               Ortho_N, regularity, Length, BG_Mean, \n                               Voice,   Nasal,  Fricative,  Liquid_SV,\n                               Bilabials,   Labiodentals,   Alveolars,\n                               Palatals,    Velars, Glottals, \n                               age.months, TOWREW_skill, TOWRENW_skill, \n                               spoonerisms, CART_score)\n\n\n\n\n\n\n\nTip\n\n\n\nNotice that these variables do not have reader-friendly names: but naming things is important.\n\nCheck out the ever-useful Jenny Bryan’s advice.\n\n\n\nThe names we have in the CP study data were fine for internal use within my research group but we should be careful to ensure that variables have names that make sense to others and to our future selves. We can adjust variable names using the rename() function but I will leave that as an exercise for you to do.\n\n\n\nSelect different variables: You could analyze the CP study data for a research report.\n\nWhat if you wanted to analyze a different set of variables, could you select different variables?\n\n\n\n\nWe now have a tidy data-set long.all.select with 26 columns and 9762 rows.\nThe data-set includes missing values, designated NA. Here, every error (coded 0, in accuracy) corresponds to an NA in the RT column.\nThe data-set also includes outlier data. In this context, \\(RT &lt; 200\\) are probably response errors or equipment failures. We will want to analyse accuracy later, so we shall need to be careful about getting rid of NAs.\nAt this point, I am going to exclude two sets of observations only.\n\nobservations corresponding to correct response reaction times that are too short: \\(RT &lt; 200\\).\nplus observations corresponding to the word false which (because of stupid Excel auto-formatting) dropped item attribute data.\n\nWe can do this using the filter() function, setting conditions on rows, as arguments.\n\n# step 1\nlong.all.select.filter &lt;- long.all.select %&gt;% \n                            filter(item_name != 'FALSE')\n\n# step 2\nlong.all.select.filter &lt;- long.all.select.filter %&gt;%\n                            filter(RT &gt;= 200)\n\nHere, I am using the function filter() to …\n\nCreate a new data-set long.all.select.filter &lt;- ... by\nUsing functions to work on the data named immediately to the right of the assignment arrow: long.all.select\nAn observation is included in the new data-set if it matches the condition specified as an argument in the filter() function call, thus:\n\n\nfilter(item_name !='FALSE') means: include in the new data-set long.all.select.filter all observations from the old data-set long.all.select that are not != (! not = equal to) the value FALSE in the variable item_name,\nthen recreate the long.all.select.filter as a version of itself (with no name change) by including in the new version only those observations where RT was greater than or equal to 200ms using RT &gt;= 200.\n\n\n\n\n\n\n\nWarning\n\n\n\nThe difference between = and ==\nYou need to be careful to distinguish these signs.\n\n= assigns a value, so x = 2 means “x equals 2”\n== tests a match so x == 2 means: “is x equal to 2?”\n\n\n\n\n\nYou can supply multiple arguments to filter() and this may be helpful if (1.) you want to filter observations according to a match on condition-A and condition-B (logical “and” is coded with &) or (2.) you want to filter observations according to a match on condition-A or condition-B (logical “or” is coded |).\nYou can read more about using multiple arguments to filter observations here.\n\n\n\n\nVary the filter conditions: in different ways\n\n\nChange the threshold for including RTs from RT &gt;= 200 to something else\nCan you assess what impact the change has? Note that you can count the number of observations (rows) in a data-set using e.g. length(data.set.name$variable.name)\n\nFiltering or re-coding observations is an important element of the research workflow in psychological science, as I examine in the discussion of data multiverse analyses here. How we do or do not remove observations from original data may have an impact on our results (as explored by, e.g., Steegen et al., 2014). It is important, therefore, that we learn how to do this reproducibly using R scripts that we can share with our research reports.\nYou can read further information about filtering here.\n\n\n\n\nWe will be working with the long.all.select.filter.csv data-set collated from the experimental, subject ability scores, and item property data collected for the CP word naming study.\nFor convenience, I am going to remove missing values before we go any further, using the na.omit() function.\n\nlong.all.noNAs &lt;- na.omit(long.all.select.filter)\n\n\n\n\n\n\n\nTip\n\n\n\nThe na.omit() function is powerful.\n\nIn using this function, I am asking R to create a new data-set long.all.noNAs from the old data-set long.all.select.filter in a process in which the new data-set will have no rows in which there is a missing value NA in any column.\nYou need to be reasonably sure, when you use this function, where your NAs may be because, otherwise, you may end the process with a new filtered data-set that has many fewer rows in it than you expected.\n\n\n\n\n\n\n\nhead(long.all.noNAs, n = 10)\n\n# A tibble: 10 × 26\n   item_name subjectID      RT accuracy Lg.UK.CDcount brookesIMG AoA_Kup_lem\n   &lt;chr&gt;     &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n 1 act       AislingoC    595.        1          4.03          4        6.42\n 2 act       AlexB        586         1          4.03          4        6.42\n 3 act       AmyR         693         1          4.03          4        6.42\n 4 act       AndyD        597         1          4.03          4        6.42\n 5 act       AnnaF        627         1          4.03          4        6.42\n 6 act       AoifeH       649         1          4.03          4        6.42\n 7 act       ChloeBergin 1081         1          4.03          4        6.42\n 8 act       ChloeF       642         1          4.03          4        6.42\n 9 act       ChloeS       623.        1          4.03          4        6.42\n10 act       CianR        701         1          4.03          4        6.42\n# ℹ 19 more variables: Ortho_N &lt;dbl&gt;, regularity &lt;dbl&gt;, Length &lt;dbl&gt;,\n#   BG_Mean &lt;dbl&gt;, Voice &lt;dbl&gt;, Nasal &lt;dbl&gt;, Fricative &lt;dbl&gt;, Liquid_SV &lt;dbl&gt;,\n#   Bilabials &lt;dbl&gt;, Labiodentals &lt;dbl&gt;, Alveolars &lt;dbl&gt;, Palatals &lt;dbl&gt;,\n#   Velars &lt;dbl&gt;, Glottals &lt;dbl&gt;, age.months &lt;dbl&gt;, TOWREW_skill &lt;dbl&gt;,\n#   TOWRENW_skill &lt;dbl&gt;, spoonerisms &lt;dbl&gt;, CART_score &lt;dbl&gt;\n\n\nIf we inspect long.all.noNAs, we can see that we have now got a tidy data-set with all the data we need for our analyses:\n\nOne observation per row, corresponding to data about a response made by a participant to a stimulus in an experimental trial\nOne variable per column\nWe have information about the speed and accuracy of responses\nAnd we have information about the children and about the words.\n\nWe have removed the missing values and we have filtered outliers.\n\n\n\nHaving produced the tidy data-set, we may wish to share it, or save ourselves the trouble of going through the process again. We can do this by creating a .csv file.\n\nwrite_csv(long.all.noNAs, \"long.all.noNAs.csv\")\n\nThis function will create a .csv file from the data-set you name long.all.noNAs which R will put in your working directory.\n\n\n\nMost research work involving quantitative evidence requires a big chunk of data tidying or processing before you get to the statistics. Most of the time, this is work you will have to do. The lessons you can learn about the process will generalize to many future research scenarios.\n\nIt is a mistake to think of data tidying or wrangling as an inconvenience or as an extra task or something you need to do to get to the ‘good stuff’ (your results).\n\nAll analysis results follow from and thus are determined by the data processing steps that precede analysis.\nAnalysis results can and do vary, perhaps critically, depending on different processing decisions, and reasonable people may differ on key processing decisions.\nThe process of data tidying is frequently instructive of your data recording quality: you find things out about your field measurements or your instrument integrity or quality of your recordings, when you pay attention, when you process your data.\n\n\nIt is wise to see data tidying or processing as a key part of the data analysis workflow because, as I examine in the data multiverse discussion here, the choices you make or the integrity or quality of the actions you take, will have consequences for your analysis results or, more generally, for the quality of the evidence you share with others.\nHere is a nice substack post that links to some scholarly writing and makes some excellent points.\n\n\n\n\n\n\nTip\n\n\n\nA key recommendation is that you write code to tidy or process data, thus creating a self-documented auditable data tidying process in your analysis workflow.\n\nThis is simple to do in R and that is one important reason why we use and teach R.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 17. Conceptual introduction to mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/02-mixed.html#sec-intro-mixed-crossed-random",
    "href": "PSYC412/part2/02-mixed.html#sec-intro-mixed-crossed-random",
    "title": "Week 17. Conceptual introduction to mixed-effects models",
    "section": "",
    "text": "Our focus in this chapter is on analyzing data that come from studies with repeated-measures designs where the experimenter presents multiple stimuli for response to each participant.\nIn our working example, the CP reading study, CP asked all participants in her study to read a selection of words. All participants read the same selection of words, and every person read every word. For each participant, we have multiple observations and these (within-participant) observations will not be independent of each other. One participant will tend to be slower or less accurate compared to another participant, on average. Likewise, one participant’s responses will reveal a stronger (or weaker) impact of the effect of an experimental variable than another participant. These between-participant differences will tend to be apparent across the sample of participants.\nYou could say that the lowest trial-level observations can be grouped with respect to participants, that observations are nested within participant. But the data can also be grouped by stimuli. Remember that in the CP study, all participants read the same selection of words, and every person read every word. This means that for each stimulus word, there are multiple observations because all participants responded to each word, and these (within-item) observations will not be independent of each other. One word may prove to be more challenging compared to another, eliciting slower or less accurate responses, on average. Likewise, participants’ responses to a word will reveal a stronger (or weaker) impact of the effect of an experimental variable than the responses to another word. Again, these between-stimulus differences will tend to be apparent when you examine observations of responses across the sample of words.\nUnder these circumstances, are observations about the responses made by different participants nested under words, or are observations about the responses to different words nested under participants? We do not have to make a decision.\nGiven this common repeated-measures design, we can analyze the outcome variable in relation to:\n\nfixed effects: the impact of independent variables like participant reading skill or word frequency\nrandom effects: the impact of random or unexplained differences between participants and also between stimuli\n\nIn this situation, we can say that the random effects are crossed (Baayen et al., 2008). When multilevel models require the specification of crossed random effects, they tend to be called mixed-effects models.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 17. Conceptual introduction to mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/02-mixed.html#sec-intro-mixed-working-models",
    "href": "PSYC412/part2/02-mixed.html#sec-intro-mixed-working-models",
    "title": "Week 17. Conceptual introduction to mixed-effects models",
    "section": "",
    "text": "To illustrate the approach, we examine observations from the CP study. We begin, as we did previously, by ignoring differences due to grouping variables (like participant or stimulus). We pretend that all observations are independent. In this fantasy situation, we address our research question.\n\n\n\n\n\n\nNote\n\n\n\n\nResearch question: What word properties influence responses to words in a test of reading aloud?\n\n\n\n\n\nIf you have not completed the process of tidying the CP study data then you can import the pre-tidied data here.\n\nlong.all.noNAs &lt;- read_csv(\"long.all.noNAs.csv\", \n                  col_types = cols(\n                    subjectID = col_factor(),\n                    item_name = col_factor()\n                    )\n                  ) \n\nNotice that I am using read_csv() with an additional argument col_types = cols(...).\n\nHere, I am requesting that read_csv() treats subjectID and item_name as factors.\n\n\n\n\n\n\n\nTip\n\n\n\nWe can use col_types = cols(...) to control how read_csv() interprets specific column variables in the data.\n\n\nControlling the way that read_csv() handles variables is a very useful capacity, and a more efficient way to work than, say, first reading in the data and then using coercion to ensure that variables are assigned appropriate types. You can read more about it here.\n\n\n\nWe begin our data analysis by asking if reading reaction time (RT) varies in association with word frequency. A scatterplot shows that response latencies decrease with increasing word frequency (Figure 2).\n\nlong.all.noNAs %&gt;%\nggplot(aes(x = Lg.UK.CDcount, y = RT)) +\n  geom_point(alpha = .2) + \n  geom_smooth(method = \"lm\", se = FALSE, size = 1.5, colour=\"red\") + \n  theme_bw() + \n  xlab(\"Word frequency: log context distinctiveness (CD) count\")\n\n\n\n\n\n\n\nFigure 2: Reading reaction time compared to word frequency, all data\n\n\n\n\n\nIn the plot, we see that the best fit line drawn with geom_smooth() trends downward for higher values of word frequency. This means that Figure 2 suggests that RT decreases with increasing word frequency. (I know there is a weird looking line of points around 0 but we can ignore that here.)\nWe can estimate the relationship between RT and word frequency using a linear model in which we ignore the possibility that there may be differences (between subjects, or between items) in the intercept or (between subjects) in the slope of the frequency effect. This simplified model can be stated as:\n\\[\nY_{ij} = \\beta_0 + \\beta_1X_j + e_{ij}\n\\]\n\nwhere \\(Y_{ij}\\) is the value of the observed outcome variable, the RT of the response made by the \\(i\\) participant to the \\(j\\) word;\n\\(\\beta_1X_j\\) refers to the fixed effect of the explanatory variable (here, word frequency), where the frequency value \\(X_j\\) is different for different words \\(j\\), and \\(\\beta_1\\) is the estimated coefficient of the effect due to the relationship between response RT and word frequency;\n\\(e_{ij}\\) is the residual error term, representing the differences between observed \\(Y_{ij}\\) and predicted values (given the model).\n\nThe linear model can be fit in R using the lm() function, as we have done previously.\n\n# label: lm-all-freq\nlm.all.1 &lt;- lm(RT ~  Lg.UK.CDcount, data = long.all.noNAs)\n\nsummary(lm.all.1)\n\n\nCall:\nlm(formula = RT ~ Lg.UK.CDcount, data = long.all.noNAs)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-346.62 -116.03  -38.37   62.05 1981.58 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    882.983     11.901   74.19   &lt;2e-16 ***\nLg.UK.CDcount  -53.375      3.067  -17.40   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 185.9 on 9083 degrees of freedom\nMultiple R-squared:  0.03227,   Adjusted R-squared:  0.03216 \nF-statistic: 302.8 on 1 and 9083 DF,  p-value: &lt; 2.2e-16\n\n\nIn the estimates from this linear model, we see an approximate first answer to our research question.\n\n\n\n\n\n\nNote\n\n\n\n\nResearch question: What word properties influence responses to words in a test of reading aloud?\nResult: Our analysis shows that the estimated effect of word frequency is \\(\\beta = -53.375\\). This means that, according to the linear model, RT decreases by about 53 milliseconds for each unit increase in log word frequency.\n\n\n\nNotice that, here, word frequency information is located in the Lg.UK.CDcount variable. In a common move for reading data analyses, we transformed the frequency estimate to the Log base 10 of the word frequency values, prior to analysis, in part because word frequency estimates are usually highly skewed.\nThe model does not explain much variance, as \\(Adjusted \\space R^2 = .03\\) but, no doubt due to the large sample, the regression model is significant overall \\(F(1,9083) = 302.8, p &lt; .001\\).\n\n\n\nVary the linear model: using different outcomes or different predictor variables.\n\nThe CP study data-set is rich with possibility. It would be useful to experiment with it.\n\nChange the predictor from frequency to something else: what do you see when you visualize the relationship between variables using scatterplots?\nSpecify linear models with different predictors: do the relationships you see in plots match the coefficients you see in the model estimates?\n\n\n\n\n\nThe problem is that, as we have discussed, we assume that observations are independent for the linear model yet we can suppose in advance that that assumption of independence will be questionable given the expectation that participants’ responses will differ in predictable ways: one participant’s responses will perhaps be slower or less accurate than another, perhaps more or less affected by word frequency than another.\nWe can examine that variation by estimating the intercept and the slope of the frequency effect separately using the data for each participant alone. We can start by visualizing the frequency effect for each child in a grid of plots, with each plot representing the \\(\\text{RT} \\sim \\text{frequency}\\) relationship for the data for a child (Figure 3).\nWe looked at how the plotting code works step-by-step in the conceptual introduction to multilevel data.\n\nlong.all.noNAs %&gt;%\n  ggplot(aes(x = Lg.UK.CDcount, y = RT)) +\n    geom_point(alpha = .2) + \n    geom_smooth(method = \"lm\", se = FALSE, size = 1.25, colour = \"red\") + \n    theme_bw() + \n    xlab(\"Word frequency (log10 UK SUBTLEX CD count)\") + \n    facet_wrap(~ subjectID)\n\n\n\n\n\n\n\nFigure 3: RT vs. word frequency, considered separately for data for each child\n\n\n\n\n\nFigure 3 shows how, on average, more frequent words are associated with shorter reaction time: faster responses. The plot further shows, however, that the effect of frequency varies considerably between children.\n\nSome children show little or no effect; the best fit line is practically level.\nSome children show a marked effect, with a steep fit line indicating a strong frequency effect.\n\nWe can get more insight into the differences between children if we plot the estimated intercept and frequency effect coefficients for each child directly. This allows more insight because it focuses the eye on the differences between children in the estimates. We do this next: see Figure 4. (I work through the code for generating the plot in: 02-mixed-workbook-with-answers.R)\n\n\n\n\n\n\n\n\nFigure 4: Estimated intercepts and frequency effect slopes calculated for each child’s data, with child data analysed separately for all children. Points represent estimates. Lines represent standard errors for estimates. Point estimates are presented in order of size.\n\n\n\n\n\nFigure 4 presents two plots showing the estimates of the intercept and the coefficient of the effect of word frequency on reading RT, calculated separately for each child. This means that we fitted a separate linear model, for the association between RT and frequency, using the data for just one child, for each child in our sample, for all the children.\nThe estimate for each child is shown as a black dot. The standard error of the estimate is shown as a black vertical line, shown above and below a point. You can say that where there is a longer line there we have more uncertainty about the location of the estimate.\nThe estimates calculated for each child are shown ordered from left to right in the plot by the size of the estimate. This adjustment to the plot reveals how the estimates of both the intercept and the slope of the frequency effect vary substantially between children.\n\n\n\n\n\n\nImportant\n\n\n\nThe first key observation is that if there is an average intercept for everyone in the sample or, better, an intercept we could estimate for everyone in the population, then the different intercepts we have estimated for each child would be distributed around that population-level average:\n\nSome children will have slower (here, larger) intercepts\nand other children will have faster (shorter) intercepts.\n\n\n\nHere, the intercept can be taken to be the average RT when all other effects in the model are set to zero. RT varies for this sample around somewhere like \\(\\beta_0 = 883ms\\) so a slower larger intercept might be e.g. \\(\\beta_0 = 1000ms\\).\n\n\n\n\n\n\nImportant\n\n\n\nThe second key observation is that if there is an average slope for the frequency effect, an effect of frequency on reading RT, averaged across everyone in the population, then, again, the different slopes we have estimated for each child would be distributed around that population-level effect.\n\nSome children will have larger (here, more negative) frequency effects\nand other children will have smaller (less negative) frequency effects.\n\n\n\nHere, the frequency effect is associated with a negative coefficient e.g. \\(\\beta_1 = -53\\) so a larger frequency effect will be a bigger negative number e.g. \\(\\beta_1 = -100\\).\n\n\n\nIn a mixed-effects model, we account for this variation: the differences between participants in intercepts and slopes.\nWe do this by modeling the intercept as two terms:\n\\[\n\\beta_{0i} = \\gamma_0 + U_{0i}\n\\]\n\nwhere \\(\\gamma_0\\) is the average intercept and \\(U_{0i}\\) is the difference for each \\(i\\) child between their intercept and the average intercept.\n\nWe model the frequency effect as two terms:\n\\[\n\\beta_{1i} = \\gamma_1 + U_{1i}\n\\]\n\nwhere \\(\\gamma_1\\) is the average slope and \\(U_{1i}\\) represents the difference for each \\(i\\) child between the slope of their frequency effect and the average slope.\n\nWe can then incorporate in a single model the fixed effects due to the average intercept and the average frequency effect, as well as the random effects, error variance due to unexplained differences between participants in intercepts and frequency effects:\n\\[\nY_{ij} = \\gamma_0 + \\gamma_1X_j + U_{0i}+ U_{1i}X_j + e_{ij}\n\\]\n\nwhere the outcome \\(Y_{ij}\\) is related to …\nthe average intercept \\(\\gamma_0\\) and differences between \\(i\\) children in the intercept \\(U_{0i}\\);\nthe average effect of the explanatory variable frequency \\(\\gamma_1X_j\\) and differences between \\(i\\) participants in the slope \\(U_{1i}X_j\\);\nin addition to residual error variance \\(e_{ij}\\).\n\n\n\nIn sections Section 1.9.7 and Section 1.11, we look at what exactly is captured in these random effects terms \\(U_{0i}, U_{1i}\\). Let’s first look at the practicalities of analysis then come back to deepen our understanding a bit more.\nRight now, it is important to understand that in our analysis we do not care about the differences between specific children. We care that there are differences. And we care how widely spread are the differences between child A and the average intercept (or slope), or between child B and the average intercept (or slope), or between child C … (you get the idea). Therefore, in our analysis, we estimate the spread of the differences as a variance term. We can see this when we look at the results of the mixed-effects model we specify, next.\n\n\n\n\nWe can fit a mixed-effects model of the \\(\\text{RT} \\sim \\text{frequency}\\) relationship, taking into account the random differences between participants. I first go through the model fitting code bit by bit. (I go through the output, the results, in Section 1.9.6.)\n\nlmer.all.1 &lt;- lmer(RT ~  Lg.UK.CDcount + \n                         (Lg.UK.CDcount + 1||subjectID),\n             \n             data = long.all.noNAs)\n\nsummary(lmer.all.1)\n\nYou have seen the lmer() function code before but practice makes perfect so we shall go through the code step by step, as we did previously.\nFirst, we have a chunk of code mostly similar to what we do when we do a regression analysis.\n\nlmer.all.1 &lt;- lmer(...) creates a linear mixed-effects model object using the lmer() function.\nRT ~  Lg.UK.CDcount is a formula expressing the model in which we estimate the fixed effect on the outcome or dependent variable RT (reaction time, in milliseconds) as predicted \\(\\sim\\) by the independent or predictor variable Lg.UK.CDcount (word frequency).\n...(..., data = long.all.noNAs) specifies the data-set in which you can find the variables named in the model fitting code.\nsummary(lmer.all.1) gets a summary of the fitted model object, showing you the results.\n\nSecond, we have the bit that is specific to multilevel or mixed-effects models.\n\nWe add (...||subjectID) to tell R about the random effects corresponding to random differences between sample groups (here, observations grouped by child) that are coded by the subjectID variable.\n(...1 ||subjectID) says that we want to estimate random differences between sample groups (observations by child) in intercepts, where the intercept is coded by 1.\n(Lg.UK.CDcount... ||subjectID) adds random differences between sample groups (observations by child) in slopes of the frequency effect coded using the Lg.UK.CDcount variable name.\n\n\n\nIt will help your learning if you now go back and compare this model with the model you saw in the conceptual introduction to multilevel data.\n\nIdentify what is different: data-set, variable names, and model formula.\nIdentify what stays the same: function name … the specification of both model and random effects.\n\nIf you can see what is different versus what stays the same then you learn what you can change when the time comes for your analysis with your data.\n\n\n\n\n\n\nTip\n\n\n\nLearning to look at example code so that you can identify how to adapt it for your own purposes is a key skill in psychological data science.\n\n\n\n\n\nBefore we move on, I want you to notice something that looks like nothing much: ||.\nWe are going to need to defer until later a (necessary) discussion of exactly why we need the two double lines. In short, the use of || asks R to fit a model in which we estimate random effects associated with:\n\nvariance due to differences in intercepts\nvariance due to differences in slopes\nbut not covariance between the two sets of differences\n\nI do this because otherwise the model I specify will not converge.\nWe shall need to discuss these things: convergence, and failures to converge; as well as random effects specification and simplification. We will discuss random effects covariance in Section 1.11. For now, the most important lesson is learnt by seeing how the analysis approach we saw last week can be extended to examining the effects of experimental variables in data from repeated measures design studies.\n\n\n\n\nThe lmer() model code we discussed in Section 1.9.5 gives us the following output.\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: RT ~ Lg.UK.CDcount + ((1 | subjectID) + (0 + Lg.UK.CDcount |  \n    subjectID))\n   Data: long.all.noNAs\n\nREML criterion at convergence: 117805.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.7839 -0.5568 -0.1659  0.3040 12.4850 \n\nRandom effects:\n Groups      Name          Variance Std.Dev.\n subjectID   (Intercept)   87575    295.93  \n subjectID.1 Lg.UK.CDcount  2657     51.55  \n Residual                  23734    154.06  \nNumber of obs: 9085, groups:  subjectID, 61\n\nFixed effects:\n              Estimate Std. Error t value\n(Intercept)    950.913     39.216  24.248\nLg.UK.CDcount  -67.980      7.092  -9.586\n\nCorrelation of Fixed Effects:\n            (Intr)\nLg.UK.CDcnt -0.093\n\n\nWe discussed the major elements of the results output last week. We expand on that discussion, a little, here.\nThe output from the model summary first gives us information about the model.\n\nFirst, we see information about the function used to fit the model, and the model object created by the lmer() function call.\nThen, we see the model formula RT ~  Lg.UK.CDcount + (Lg.UK.CDcount + 1|subjectID).\nThen, we see REML criterion at convergence about the model fitting process, which we can usually ignore.\nThen, we see information about the distribution of the model residuals.\nWe then see information listed under Random effects: this is where you can see information about the error variance terms estimated by the model.\n\n\nThe information is listed in four columns: 1. Groups; 2. Name; 3. Variance; and 4. Std.Dev.\n\nWe have discussed how observations can be grouped by participant (because we have multiple response observations for each person in the study) just as previously we identified how observations could be grouped by class (because we saw that children were nested under class). That is what we mean when we refer to Groups: we are identifying the grouping variables that give hierarchical structure to the data.\n\nThe Name lists whether the estimate we are looking at corresponds to, here, random differences between participants in intercepts (listed as (Intercept)), or in slopes (listed as Lg.UK.CDcount).\n\nAs we discuss later, in Section 1.11, mixed-effects models estimate the spread in random differences. We are not interested in the specific differences in intercept or slope between specific individuals. What we want is to be able to take into account the variance associated with those differences.\nThus, we see in the Random Effects section, the variances associated with:\n\nsubjectID Intercept) 87575, differences between participants in the intercepts;\nsubjectID.1 Lg.UK.CDcount 2657, differences between participants in the slopes of the frequency effect;\nAlongside Residual  23734, residuals where, just like a linear model, we have variance associated with differences between model estimates and observed RT, here, at the trial level.\n\nWe do not usually discuss the specific variance estimates in research reports. However, the relative size of the variances does provide useful information (Meteyard & Davies, 2020), as we shall see when we discuss the different estimates we get when we include a random effect due to differences between items (Section 1.10.2).\n\nLastly, we see estimates of the coefficients (of the slopes) of the fixed effects.\n\nIn this model, we see estimates of the fixed effects of the intercept and the slope of the RT ~ Lg.UK.CDcount model. We discuss these estimates next.\n\n\nRecall that the linear model yields the estimate for the frequency effect on reading RT such that RT decreases by about 53 ms for unit increase in log word frequency (\\(\\beta = -53.375\\)). Now, when we have taken random differences between participants into account, we see that the estimate of the effect for the mixed-effects model is \\(\\beta = -67.980\\). Taking into account random differences clearly has an impact on results.\n\n\n\n\n\n\nNote\n\n\n\n\nResearch question: What word properties influence responses to words in a test of reading aloud?\nResult: The mixed-effect analysis shows that RT decreases by about \\(68\\) milliseconds for each unit increase in log word frequency.\n\n\n\nWhich coefficient estimate should you trust? Well, it is obvious that the linear model and the linear mixed-effects model estimate are relatively similar. However, it is also obvious that the linear model makes an assumption – the assumption of independence of observations – that does not make sense theoretically (because reading responses will be similar for each child) and does not make sense empirically (because responses will differ between children, see Figure 4). Thus, we have good grounds for supposing that the linear mixed-effects model estimate for the frequency effect is likely to be closer to the true underlying population effect.\nIt is important to remember, however, that whatever estimate we can produce is the estimate given the sample of words we used, our information about word frequency, and our measurement of reading RT responses. How far our estimate actually generalizes to the wider population is not something we can judge in the context of a single study.\nFurther, we have not finished in our consideration of the random effects that the account should include. We need to do more work by thinking about the differences between stimuli (Section 1.10).\n\n\n\n\n\n\nWarning\n\n\n\nWhy aren’t there p-values?\nWe will come back to this (in a week) but note that if \\(t &gt; 2\\) we can suppose that an effect is significant at the \\(.05\\) significance level.\n\n\n\n\n\n\nWe have said that we can incorporate, in a mixed-effects model, fixed effects (e.g., the average frequency effect) and random effects, error variance due to unexplained differences between participants in intercepts and in frequency effects:\n\\[\nY_{ij} = \\gamma_0 + \\gamma_1X_j + U_{0i}+ U_{1i}X_j + e_{ij}\n\\]\nSo we distinguish:\n\nthe average intercept \\(\\gamma_0\\) and differences between \\(i\\) children in the intercept \\(U_{0i}\\);\nthe average effect of the explanatory variable frequency \\(\\gamma_1X_j\\) and differences between \\(i\\) participants in the slope \\(U_{1i}X_j\\).\n\nWhen we think about the differences between participants (or between the units of any grouping variable), in intercepts or in slopes, we typically assume that the differences are:\n\nrandom;\nshould be normally distributed;\nand are distributed around the population or average fixed effects.\n\nWe can say that the mixed-effects model sees the differences between participants relative to the fixed effect intercept or slope, that is, relative to the population level or average effects.\nWe can illustrate this by plotting, in Figure 5, the differences as estimated – technically, predicted – by the mixed-effects model that we have been examining. (The code for producing the plot can be found in 02-mixed-workbook-answers.R.)\n\n\n\n\n\n\n\n\nFigure 5: Plot showing histograms indicating the distribution of participant adjustments to account for between-child differences in intercept or slope (the Best Linear Unbiased Predictions).\n\n\n\n\n\nWhat you can see in Figure 5 are distributions, presented using histograms. The centers of the distributions are located at zero (shown by a red line). For each distribution (a. and b.), that is where the model estimate of the intercept or the slope of the frequency effect is located. Spread around that central point, you see the adjustments the model makes to account for differences between participants.\nNotice how, in Figure 5 (a.):\n\nSome children have intercepts that are smaller than the population-level or average intercept – so their adjustments are negative (to decrease their intercepts).\nSome children have intercepts that are larger than the population-level or average intercept – so their adjustments are positive (to increase their intercepts).\n\n\nStrikingly, you can see that a few children have intercepts that are as much as 1000ms larger than the population-level or average intercept: the bars representing the estimates for these children are far out on the right of the x-axis in Figure 5 (a.).\n\nNow notice how, in Figure 5 (b.):\n\nSome children have frequency effects (coefficients) that are smaller than the population-level or average frequency effect – so their adjustments are positive (to decrease their frequency effect, by making it less negative).\nSome children have frequency effects that are larger than the population-level or average frequency effect – so their adjustments are negative (to increase their frequency effect, by making it more negative).\n\n(When you look at Figure 5 (b.), remember that the estimated \\(\\beta\\) coefficient for the frequency effect is negative because higher word frequency is associated with smaller RT.)\n\nStrikingly, you can see that a few children have frequency effects that are as much as 200ms larger (see plot (b.) around \\(x = -200\\)) than the population-level or average effect.\n\nWhen a mixed-effects model is fitted to a data-set, its set of estimated parameters includes the coefficients for the fixed effects as well as the standard deviations for the random effects (Baayen et al., 2008). If you read the literature on mixed-effects models, you will see that the adjustments are called Best Linear Unbiased Predictors (BLUPs).\n\n\nMixed-effects modeling is hard to get used to at first. A bit more practice helps to show you how the different parts of the model work. We again focus on the random effects.\nIn the model we have seen so far, we specify (Lg.UK.CDcount + 1||subjectID)\n\nWe can change this part – and only this part – to see what happens to the results. Do it: rerun the model code, having changed the random effects part:\n\n\nlmer(RT ~  Lg.UK.CDcount + (1|subjectID)...) gives us a random intercepts model accounting for just random differences between participants in the intercept\nlmer(RT ~  Lg.UK.CDcount + (Lg.UK.CDcount + 0|subjectID)...) gives us a random slope model accounting for just random differences between participants in the slope of the frequency effect\nlmer(RT ~  Lg.UK.CDcount + (Lg.UK.CDcount + 1|subjectID)...) gives us a random intercepts and slopes model accounting for both random differences between participants in the intercept and in the slope, as well as covariance in these differences.\n\n\n\n\n\n\n\nTip\n\n\n\nTry out these variations and look carefully at the different results. Look, especially, at what happens to the Random effects part of the summary.\n\n\nThis will be an important and revealing exercise.\nWe can visualize the differences between the models in a plot showing the different predictions that the different models give us. Figure 6 shows what a mixed-effects model predicts are the effects of frequency on RT for different children in the CP study.\n\nThe predictions vary depending on the nature of the random effects we specify in the model.\n\n(Note that I figured out how to produce the plot from the information here.)\n\n\n\n\n\n\n\n\nFigure 6: Plot showing model predictions of the effect, for each individual, of word frequency on reading reaction time – predictions vary between models incorporating (a.) random effect of participants on intercepts only; (b.) random effect of participants on slopes only and (c.) random effect of participants on intercepts and on slopes.\n\n\n\n\n\nWe can see that:\n\nIf the model includes the random effect of participants on intercepts only then all the slopes are the same (the lines in the figure are parallel) because this model assumes that the only differences between participants are differences in the intercepts.\nIf the model includes the random effect of participants on slopes only then the slopes vary but they all have the same intercept. The plot does not show this but you can see how all the slopes are converging on one point somewhere on the left. This happens because this model assumes that the only differences between participants are differences in the slopes.\nIf the model includes the random effect of participants on intercepts and on slopes then we can see how the intercepts and the slopes vary. Given what we saw when we looked at the relation between frequency and RT for each participant considered separately we might argue that this model is much more realistic about the data.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is key to your skills development that you learn to make effective use of the warnings and error messages R can produce.\n\n\nYou do not have to just believe me when I say that || is in the model code to stop a problem appearing.\n\nExperiment – and see what happens when you change the code. Try this.\n\n\nlmer.all.1 &lt;- lmer(RT ~  Lg.UK.CDcount + (Lg.UK.CDcount + 1|subjectID),\n             \n             data = long.all.noNAs)\n\nsummary(lmer.all.1)\n\nDo you get an error message?\nA very useful trick is to learn to copy the error message you get into a search engine on your web browser. Do this and you will find useful help, as here",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 17. Conceptual introduction to mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/02-mixed.html#sec-intro-mixed-fixed-effect-fallacy",
    "href": "PSYC412/part2/02-mixed.html#sec-intro-mixed-fixed-effect-fallacy",
    "title": "Week 17. Conceptual introduction to mixed-effects models",
    "section": "",
    "text": "Important\n\n\n\nExperimental psychologists will often collect data in studies where they present some stimuli to a sample of participants.\n\n\nClark (1973) showed that the appropriate analysis of experimental effects for such data requires the researcher to take into account the error variance due to unexplained or random differences between sampled participants and also to random differences between sampled stimuli. This is true in the context of psycholinguistics but it is also true in the context of work in any field where the presented stimuli can be understood to constitute a sample from a wider population of potential stimuli (e.g., stories about social situations, Judd et al., 2012).\nIf we were to estimate the average latency of the responses made by different children to each word, in the CP study data, we would see that there is considerable variation between words. We do this in Figure 7. (I work through the code for producing the plot in 02-mixed-workbook-answers.R.)\n\nSome words elicit slower and some elicit faster responses on average.\nWe can also see that there is, again, variation in the uncertainty of estimates, as reflected in differences in the lengths of the error bars corresponding to the standard errors of the estimates.\n\n\n\n\n\n\n\n\n\nFigure 7: Estimated intercepts (with SEs) calculated for each stimulus word, with coefficients ordered by average latency for each word\n\n\n\n\n\nIn general, psychologists have been aware since Clark (1973) (if not earlier) that responses to experimental stimuli can vary because of random or unexplained differences between the stimuli: whether the stimuli are words, pictures or stories, etc. And researchers have been aware that if we did not take such variation into account, we might mistakenly detect an experimental effect, for example, as a significant difference between mean response in different conditions, simply because different stimuli presented in different conditions varied in some unknown way, randomly.\nFor many years, psychologists tried to take random differences between stimuli into account, alongside random differences between participants, using a variety of strategies with important limitations (see Baayen et al. (2008), for discussion). Clark (1973) suggested that researchers could calculate \\(minF'\\) (not F) when doing Analyses of Variance of experimental data\nThis involves a series of steps.\n\nYou start by aggregating your data\n\n\nBy-subjects data – for each subject, take the average of their responses to all the items\nBy-items data – for each item, take the average of all subjects’ responses to that item\n\n\nYou do separate ANOVAs, one for by-subjects (F1) data and one for by-items (F2) data\nYou put F1 and F2 together, calculating minF’\n\nAveraging data by-subjects or by-items is relatively simple. It is very common to see, in the literature, psychological reports in which F1 and F2 analysis results are presented (that is why I am explaining this).\nCalculating \\(minF'\\) is also relatively simple:\n\\[\nminF' = \\frac{MS_{effect}}{MS_{\\text{random-subject-effects}} + MS_{\\text{random-word-differences}}} = \\frac{F_1F_2}{F_1 + F_2}\n\\]\nHowever, after a while, psychologists stopped doing the extra step of the \\(minF'\\) calculation (Raaijmakers et al., 1999). They carried on calculating and reporting F1 and F2 ANOVA results but, as Baayen et al. (2008) discuss, that approach risks a high false positive error rate.\nPsychologists also found that while the \\(minF'\\) approach allowed them to take into account between-participant and between-stimulus differences it could not be applied where ANOVA could not be used. This stopped researchers from taking a comprehensive approach to error variance where they wanted to conduct multiple regression analyses.\nIn the psychological literature, you will often see multiple regression analyses of by-items data, where a sample of participants has been asked to respond to a sample of stimuli, and the analysis is of the effects of stimulus properties on outcomes averaged (over participants’ responses) to the mean outcome by item. The problem is that analyzing data only by-items ensures that we lose track of participant differences.\nLorch & Myers (1990) warn that analyzing only by-items mean RTs just assumes wrongly that subjects are a fixed effect. This approach, again, risks a higher rate of false positive errors.\n\n\nThe good thing is that, thanks to the advent of mixed-effects models, we now no longer need to tolerate these problems.\nIn the context of our working example, in our analysis of the CP study data, we can build up our mixed-effects model by adding a random effect to capture the impact of unexplained differences between stimuli.\nWe model the random effect of items on intercepts by modeling the intercept as two terms:\n\\[\n\\beta_{0j} = \\gamma_0 + W_{0j}\n\\]\n\nwhere \\(\\gamma_0\\) is the average intercept and \\(W_{0j}\\) represents the deviation, for each word, between the average intercept and the per-word intercept.\n\nOur model can now incorporate the additional random effect of items on intercepts:\n\\[\nY_{ij} = \\gamma_0 + \\gamma_1X_j + U_{0i}+ U_{1i}X_j + W_{0j} + e_{ij}\n\\]\nIn this model, the outcome \\(Y_{ij}\\) is related to:\n\nthe average intercept \\(\\gamma_0\\) and the word frequency effect \\(\\gamma_1X_j\\)\nplus random effects due to unexplained differences between participants in intercepts \\(U_{0i}\\) and the slope of the frequency effect \\(U_{1i}X_j\\)\nas well as random differences between items in intercepts \\(W_{0j}\\),\nin addition to the residual term \\(e_{ij}\\).\n\n\n\n\n\n\n\nWarning\n\n\n\nWhat about random effects associated with differences between stimulus items in the slopes of effects?\n\nJust as we may expect there to be between-participant differences in the slope of the word frequency effect, we may expect there to be between-stimulus differences in the slope of the effect of, e.g., participant age.\nRest assured, we will look at this question.\n\n\n\n\n\n\nWe can fit a mixed-effects model of the \\(\\text{RT} \\sim \\text{frequency}\\) relationship, taking into account the random differences between participants and now also the random differences between stimulus words.\n\nlmer.all.2 &lt;- lmer(RT ~  Lg.UK.CDcount + \n                         (Lg.UK.CDcount + 1||subjectID) +\n                         (1|item_name),\n             \n             data = long.all.noNAs)\n\nsummary(lmer.all.2)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: RT ~ Lg.UK.CDcount + ((1 | subjectID) + (0 + Lg.UK.CDcount |  \n    subjectID)) + (1 | item_name)\n   Data: long.all.noNAs\n\nREML criterion at convergence: 116976.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.1795 -0.5474 -0.1646  0.3058 12.9485 \n\nRandom effects:\n Groups      Name          Variance Std.Dev.\n item_name   (Intercept)     3397    58.29  \n subjectID   Lg.UK.CDcount   3624    60.20  \n subjectID.1 (Intercept)   112313   335.13  \n Residual                   20704   143.89  \nNumber of obs: 9085, groups:  item_name, 159; subjectID, 61\n\nFixed effects:\n              Estimate Std. Error t value\n(Intercept)     971.07      51.87  18.723\nLg.UK.CDcount   -72.33      10.79  -6.703\n\nCorrelation of Fixed Effects:\n            (Intr)\nLg.UK.CDcnt -0.388\n\n\nThis is the same mixed-effects model as the one we discussed in Section 1.9.5 and Section 1.9.6 but with one important addition.\n\nWe add (1|item_name) to take into account random differences between between words in intercepts.\n\n\n\nTake a look at the model results. You should notice three changes.\n\nYou can see that the estimate for the effect of word frequency on reading reaction time has changed again, it is now \\(\\beta = -72.33\\)\nitem_name   (Intercept)     3397 there is now an additional term in the list of random effects, giving the model estimate for variance associated with random differences between words in intercepts\nAnd you can see that the residual variance has changed. In the first model lmer.all.1 it was 23734, now it is 20704\n\n\n\n\n\n\n\nTip\n\n\n\nThe reduction in residual variance is one way in which we can judge how good a job the model is doing in accounting for the variance in the outcome, observed response reaction time.\n\n\nWe can see that by adding a term to account for differences between items we can reduce the amount by which the model estimates deviate from observed outcomes. This difference in error variance is, essentially, one basis for estimating how well the model fits the data, and a basis for estimating the variance explained by a model in terms of the \\(R^2\\) statistic you have seen before.\nWe will come back to this.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 17. Conceptual introduction to mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/02-mixed.html#sec-intro-mixed-variance-covariance",
    "href": "PSYC412/part2/02-mixed.html#sec-intro-mixed-variance-covariance",
    "title": "Week 17. Conceptual introduction to mixed-effects models",
    "section": "",
    "text": "As I have said, we usually do not aim to examine the specific deviation from the average intercept or the average fixed effect slope for a participant or stimulus. We estimate just the spread of deviations by-participants or by-items.\nA mixed-effects model like our final model includes fixed effects corresponding to the intercept and the slope of the word frequency effect plus the variances:\n\n\\(var(U_{0i})\\) variance of deviations by-participants from the average intercept;\n\\(var(U_{1i}X_j)\\) variance of deviations by-participants from the average slope of the frequency effect;\n\\(var(W_{0j})\\) variance of deviations by-items from the average intercept;\n\\(var(e_{ij})\\) residuals, at the response level, after taking into account all other terms.\n\nBecause we have variances, we may expect the random effects of participants or items to covary, e.g., participants who are slow to respond may also be more susceptible to the frequency effect, as can be seen in Figure 8.\n\n\n\n\n\n\n\n\nFigure 8: Scatterplot showing the relationship between estimated coefficients for the intercept and for the frequency effect, for each child analysed separately\n\n\n\n\n\nThis is why it would often make sense to specify, among the random effects of the model, terms corresponding to the covariance of the random effects:\n\n\\(covar(U_{0i}, U_{1i}X_j)\\)\n\n\n\n\n\n\n\nWarning\n\n\n\nRemember we excluded random effects covariance using ||.\n\n\nIn Section 1.9.5.2, I noted how we used the || notation to stop the model estimating the covariance between differences between participants in intercepts and in slopes. The reason I did this is that if I had requested that the model estimate the covariance the model would have failed to converge. What this means depends on understanding how mixed-effects models are estimated. We shall have to return to a development of that understanding later. For now, it is enough to note that mixed-effects models fitted with lmer() often have more difficulty with random effects covariance estimates.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 17. Conceptual introduction to mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/02-mixed.html#sec-intro-mixed-lmer-results-reporting",
    "href": "PSYC412/part2/02-mixed.html#sec-intro-mixed-lmer-results-reporting",
    "title": "Week 17. Conceptual introduction to mixed-effects models",
    "section": "",
    "text": "There is no official convention on what or how to report the results of a mixed-effects model. Lotte Meteyard and I suggest what psychologists should report in an article (Meteyard & Davies, 2020) that has been downloaded a few thousand times so, maybe, our advice will help to influence practice.\n\n\nExplain what you did, and why.\nExplain what you found, not just whether effects are significant or not.\n\n\n\nWe argue that researchers should explain what analysis they have done and, where space allows, should report both the estimates of the fixed effects and the estimates of the random effects.\nWe think you can report the model code (maybe in an appendix, maybe in a note under a tabled summary of results).\n\nA table summary presenting model results can look like this.\n\n\n\nCoefficients\nEstimate\nSE\nt\n\n\n\n\n\n(Intercept)\n971.1\n51.9\n18.7\n\n\n\nFrequency effect\n-72.3\n10.8\n-6.7\n\n\n\n\n\n\n\nGroups\nName\nVariance\nSD\n\n\n\n\n\nitem\n(Intercept)\n3397\n58.3\n\n\n\nparticipant\n(Intercept)\n112314\n335.1\n\n\n\nparticipant\nFrequency\n3624\n60.2\n\n\n\nresidual\n\n20704\n143.9\n\n\n\n\nNote: lmer(RT ~  Lg.UK.CDcount + (Lg.UK.CDcount + 1||subjectID) + (1|item_name)\nBarr et al. (2013) argued that choices about random effects structure affect the generalizability of the estimates of fixed effects. In particular, it seems sensible to examine the possibility that the slope of the effect of an explanatory variable may vary at random between participants or between stimuli. Correspondingly, researchers should report and explain their decisions about the inclusion of random effects.\n\n\n\n\n\n\nTip\n\n\n\nResearchers should report their modelling in sufficient detail that their results can be reproduced by others.\n\n\nIt is normal practice in psychology to report the p-values associated with null hypothesis significance tests of effects when reporting analysis. Performing hypothesis tests using t- or F-distributions depends on the calculation of degrees of freedom yet it is uncertain how degrees of freedom should be counted when analyzing multilevel data (Baayen et al. (2008)). In most software applications, however, p-values associated with fixed effects may be calculated using an approximation for denominator degrees of freedom.\nWe will come back to how we should report the results of mixed-effects models because, here, too, in learning about writing, we can benefit by developing our approach, in depth, step by step.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 17. Conceptual introduction to mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/02-mixed.html#sec-intro-mixed-conclusions",
    "href": "PSYC412/part2/02-mixed.html#sec-intro-mixed-conclusions",
    "title": "Week 17. Conceptual introduction to mixed-effects models",
    "section": "",
    "text": "A large proportion of psychological studies involves scenarios in which the researcher samples both participants and some kind of stimuli. Often, the researcher will present the stimuli to the participants for response in some version of a range of possible designs:\n\nall participants see and respond to all stimuli;\nparticipants respond to different sub-sets of stimuli in different conditions (or in different groups) but they see and respond to all stimuli in a sub-set;\nparticipants are allocated to respond to stimulus sub-sets according to a counter balancing scheme (e.g., through the use of Latin squares).\n\nWhatever version of this scenario, if participants are responding to multiple stimuli and if multiple partcipants respond to each stimulus, then the data will have a multilevel structure such that each observation can be grouped both by participant and by stimulus.\nWe are interested in taking into account the random effects associated with unexplained or random differences between participants or between stimuli. We often discuss the accounting of these effects in terms of the estimation of error variances associated with the random differences, calling the effects of the differences random effects. Where we have to deal with both samples of participants and samples of stimuli, we can talk about crossed random effects.\nThe terms are not that important. The insight is.\n\n\n\n\n\n\nImportant\n\n\n\nIn general, in experimental psychological science, when we do data analysis, if we want to estimate effects of experimental variables more accurately then our models need to incorporate terms to capture the impact on observed outcomes of sampled participants and sampled stimuli.\n\n\nHistorically, we have, as a field, learned to take into account these sampling effects. Now, and most likely, more and more commonly in the future, we are learning to use multilevel or mixed-effects models to do this.\n\n\n\n\nWe discussed the way that data are structured when they come from studies with repeated measures designs. Critically, we examined data from a common study design where a sample of stimulus items are presented for response to members of a participant sample. This means that each observation can be grouped by participant and, also, by stimulus. The possibility that observations can be grouped means that the data have a multilevel structure.\nThe multilevel structure requires the use of linear mixed-effects models when we seek to estimate the effects of experimental variables. The fact that data can be grouped both by participant and by stimulus means that the model can incorporate random effects to capture random between-participant differences as well as between-stimulus differences.\nThe use of mixed-effects models has meant that psychologists no longer need to adopt compromise solutions which have important limitations, like by-items and by-subjects analyses.\n\n\n\nWe reviewed the ways that experimental data can be untidy. And we outlined the steps that may be required to process untidy data into a tidy format suitable for analysis. As is typical for the data analysis we need to do for experimental psychological science, getting data ready for analysis requires a series of steps including: access; import; restructure; select variables; and filter observations.\nWe then developed a mixed-effects model to answer the question:\n\n\n\n\n\n\nNote\n\n\n\n\nResearch question: What word properties influence responses to words in a test of reading aloud?\n\n\n\nOur analysis focused on the relationship between reading response reaction time (RT, in ms) and the predictor word frequency. We examined how the effect of word frequency was estimated in a linear model ignoring the multilevel structure and then in mixed-effects models which incorporated terms to capture:\n\nvariance associated with random differences between participants in intercepts or in the slope of the frequency effect,\nvariance associated with random differences between items in intercepts.\n\nWe saw that estimates of the frequency effect differed between different models.\n\n\n\n\nWe used a number of functions to tidy and visualize the CP study data.\n\nread_csv() and read_csv() to load source data files into the R workspace\npivot_longer() to restructure data from wide to long\nfull_join() to put together data from separate data-sets; in our example, from data-sets holding information about participant attributes, stimulus word properties, and participant behaviours\nselect() to select the variables we need\nfilter() to filter observations based on conditions\nna.omit() to remove missing values\nFor visualisation, we used facet_wrap() to show plots of the relationship between outcome and predictor variables separately for different groups (by participant, or by item)\n\nFor our analyses:\n\nWe used lmer() to fit a multilevel model.\n\nWe used the summary() function to get model results for both linear models and for the mulilevel or liner mixed-effects model.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 17. Conceptual introduction to mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/02-mixed.html#sec-intro-mixed-recommended-reading",
    "href": "PSYC412/part2/02-mixed.html#sec-intro-mixed-recommended-reading",
    "title": "Week 17. Conceptual introduction to mixed-effects models",
    "section": "",
    "text": "(Baayen et al., 2008; see also Barr et al., 2013; Judd et al., 2012) discuss mixed-effects models with crossed random effects in a variety of contexts in psychological science. The explanations are clear and the examples are often helpful.\nI wrote a tutorial article on mixed-effects models with Lotte Meteyard (Meteyard & Davies, 2020). We discuss how important the approach now is for psychological science, what researchers worry about when they use it, and what they should do and report when they use the method.\n\n\n\n\nBaayen, R. H., Davidson, D. J., & Bates, D. M. (2008). Mixed-effects modeling with crossed random effects for subjects and items. Journal of Memory and Language, 59(4), 390–412. https://doi.org/10.1016/j.jml.2007.12.005\n\n\nBarr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of Memory and Language, 68, 255–278.\n\n\nClark, H. (Stanford. U. (1973). Clark_1973_LanguageAsAFixedEffectFallacy.pdf.\n\n\nDavies, R. A. I., Birchenough, J. M. H., Arnell, R., Grimmond, D., & Houlson, S. (2017). Reading through the life span: Individual differences in psycholinguistic effects. Journal of Experimental Psychology: Learning Memory and Cognition, 43(8). https://doi.org/10.1037/xlm0000366\n\n\nFrederickson, N., Frith, U., & Reason, R. (1997). Phonological assessment battery [PhAB]: Manual and test materials. nfer Nelson Publishing Company Ltd.\n\n\nGrolemund, G., & Wickham, H. (n.d.). R for Data Science [Book]. https://www.oreilly.com/library/view/r-for-data/9781491910382/\n\n\nJudd, C. M., Westfall, J., & Kenny, D. A. (2012). Treating stimuli as a random factor in social psychology : A new and comprehensive solution to a pervasive but largely ignored problem. 103(1), 54–69. https://doi.org/10.1037/a0028347\n\n\nLorch, R. F., & Myers, J. L. (1990). Regression analyses of repeated measures data in cognitive research. Journal of Experimental Psychology: Learning, Memory, and Cognition, 16(1), 149–157. https://doi.org/10.1037/0278-7393.16.1.149\n\n\nMeteyard, L., & Davies, R. A. I. (2020). Best practice guidance for linear mixed-effects models in psychological science. Journal of Memory and Language, 112, 104092. https://doi.org/10.1016/j.jml.2020.104092\n\n\nRaaijmakers, J. G. W., Schrijnemakers, J. M. C., & Gremmen, F. (1999). How to deal with \"the language-as-fixed-effect fallacy\": Common misconceptions and alternative solutions. Journal of Memory and Language, 41(3), 416–426. https://doi.org/10.1006/jmla.1999.2650\n\n\nStainthorp, R. (1997). A children’s author recognition test: A useful tool in reading research. Journal of Research in Reading, 20(2), 148158.\n\n\nTorgesen, J. K., Rashotte, C. A., & Wagner, R. K. (1999). TOWRE: Test of word reading efficiency. Pro-ed Austin, TX.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 17. Conceptual introduction to mixed-effects models"
    ]
  },
  {
    "objectID": "PSYC412/part2/04-glmm.html",
    "href": "PSYC412/part2/04-glmm.html",
    "title": "Week 19. Conceptual introduction to Generalized Linear Mixed-effects Models",
    "section": "",
    "text": "We have been discussing how we can use Linear Mixed-effects models to analyze multilevel structured data, the kind of data that we commonly acquire in experimental psychological studies, for example, when our studies have repeated measures designs. The use of Linear Mixed-effects models is appropriate where the outcome variable is a continuous numeric variable like reaction time. In this chapter, we extend our understanding and skills by moving to examine data where the outcome variable is categorical: this is a context that requires the use of Generalized Linear Mixed-effects Models (GLMMs).\nWe will begin by looking at the motivations for using GLMMs. We will then look at a practical example of a GLMM analysis, in an exploration in which we shall reveal some of the challenges that can arise in such work. The R code to do the modeling is very similar to the code we have used before. The way we can understand the models is also similar but with one critical difference. We start to understand that difference here.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nCategorical outcomes cannot be analyzed using linear models, in whatever form, without having to make some important compromises.\n\n\nYou need to do something about the categorical nature of the outcome.\n\n\n\nIn this chapter, we look at Generalized Linear Mixed-effects Models (GLMMs): we can use these models to analyze outcome variables of different kinds, including outcome variables like response accuracy that are coded using discrete categories (e.g. correct vs. incorrect). Our aims are to:\n\nRecognize the limitations of alternative methods for analyzing such outcomes, Section 1.8.1.\nUnderstand practically the reasons for using GLMMs when we analyze discrete outcome variables, Section 1.8.2.\nPractice running GLMMs with varying random effects structures.\nPractice reporting the results of GLMMs, including through the use of model plots.\n\n\n\n\nI have provided a collection of materials you can use. Here, I explain what they are and how I suggest you use them.\n\n\n\n\n\n\nLinked resources\n\n\n\n\nWe learned about multilevel structured data in the conceptual introduction to multilevel data and the workbook introduction to multilevel data.\nWe then deepened our understanding by looking at the analysis of data from studies with repeated-measures designs in the conceptual introduction to linear mixed-effects models and the workbook introduction to mixed-effects models.\nWe further extended our understanding and practice skills in the conceptual introduction to developing linear mixed-effects models and the corresponding workbook.\n\nWe present the workbook introduction to **Generalized Linear Mixed-effects Models where the explanation of ideas or of practical analysis steps is cut down, to focus attention on practical action steps.\n\n\n1. Video recordings of lectures\n1.1. I have recorded a lecture in three parts. The lectures should be accessible by anyone who has the link.\n\nPart 1 – about 20 minutes\nPart 2 – about 16 minutes\nPart 3 – about 19 minutes\n\n1.2. I suggest you watch the recordings then read the rest of this chapter.\n\nThe lectures provide a summary of the main points.\n\n1.3. You can download the lecture slides in two different versions:\n\n402-week-20-GLMM.pdf: exactly as delivered [700 KB];\n402-week-20-GLMM_6pp.pdf: printable version, six-slides-per-page [850 KB].\n\nThe GLMM.pdf version is the version delivered for the lecture recordings. To make the slides easier to download, I produced a six-slide-per-page version, GLMM_6pp.pdf. This should be easier to download and print out if that is what you want to do.\n2. Chapter: 04-glmm\n2.1. I have written this chapter to discuss the main ideas and set out the practical steps you can follow to start to develop the skills required to work with categorical outcomes data using GLMMs.\n2.2. The practical elements include data tidying, visualization and analysis steps.\n2.3. You can read the chapter, run the code, and do the exercises.\n\nRead in the example word learning data-set.\nExperiment with the .R code used to work with the example data.\nRun GLMMs of demonstration data.\nRun GLMMs of alternate data sets.\nReview the recommended readings (Section 1.13).\n\n3. Practical materials\n3.1 In the following sections, I describe the practical steps, and associated practical materials (exercise workbooks and data), you can use for your learning.\n\n\n\nWe will be working with data collected for a study investigating word learning in children, reported by Ricketts et al. (2021). You will see that the study design has both a repeated measures aspect because each child is asked to respond to multiple stimuli, and a longitudinal aspect because responses are recorded at two time points. Because responses were observed to multiple stimuli for each child, and because responses were recorded at multiple time points, the data have a multilevel structure. These features require the use of mixed-effects models for analysis.\nWe will see, also, that the study involves the factorial manipulation of learning conditions. This means that, when you see the description of the study design, you will see embedded in it the 2 x 2 factorial design beloved of psychologists. You will be able to generalize from our work this week to many other research contexts where psychologists conduct experiments in which conditions are manipulated according to a factorial design.\nHowever, our focus here is on the fact that the outcome for analysis is the accuracy of the responses made by children to word targets in a spelling task. The categorical nature of accuracy as an outcome is the reason why we now turn to use Generalized Linear Mixed-effects Models.\n\n\nI am going to present the study information in some detail, in part, to enable you to make sense of the analysis aims and results and, in part, so that we can simulate results reporting in a meaningful context.\n\n\nVocabulary knowledge is essential for processing language in everyday life and it is vital that we know how to optimize vocabulary teaching. One strategy with growing empirical support is orthographic facilitation: children and adults are more likely to learn new spoken words that are taught with their orthography [visual word forms; for a systematic review, see Colenbrander et al. (2019)]. Why might orthographic facilitation occur? Compared to spoken inputs, written inputs are less transient across time and less variable across contexts. In addition, orthography is more clearly marked (e.g., the ends of letters and words) than the continuous speech stream. Therefore, orthographic forms may be more readily learned than phonological forms.\nRicketts et al. (2021) investigated how school-aged children learn words. We conducted two studies in which children learned phonological forms and meanings of 16 polysyllabic words in the same experimental paradigm. To test whether orthographic facilitation would occur, half of the words were taught with access to the orthographic form (orthography present condition) and the other half were taught without orthographic forms (orthography absent condition). In addition, we manipulated the instructions that children received: approximately half of the children were told that some words would appear with their written form (explicit group); the remaining children did not receive these instructions (incidental group). Finally, we investigated the impact of spelling-sound consistency of word targets for learning, by including words that varied continuously on a measure of pronunciation consistency (after Mousikou et al., 2017).\nThe quality of lexical representations was measured in two ways. A cuing hierarchical response task (definition, cued definition, recognition) was used to elicit semantic knowledge from the phonological forms, providing a fine-grained measure of semantic learning. A spelling task indexed the extent of orthographic learning for each word. We focus on the analysis of the spelling task responses in this chapter (you may be interested in reviewing our other analyses, see Section 1.5.2).\nRicketts et al. (2021) reported two studies. We focus on Study 1, in which Ricketts et al. (2021) measured knowledge of newly learned words at two intervals: first one week and then, again, eight months after training. Longitudinal studies of word learning are rare and this is the first longitudinal investigation of orthographic facilitation.\nWe addressed three research questions and tested predictions in relation to each question.\n\n\n\n\n\n\nNote\n\n\n\n\nDoes the presence of orthography promote greater word learning?\n\n\nWe predicted that children would demonstrate greater orthographic learning for words that they had seen (orthography present condition) versus not seen (orthography absent condition).\n\n\nWill orthographic facilitation be greater when the presence of orthography is emphasized explicitly during teaching?\n\n\nWe expected to observe an interaction between instructions and orthography, with the highest levels of learning when the orthography present condition was combined with explicit instructions.\n\n\nDoes word consistency moderate the orthographic facilitation effect?\n\n\nFor orthographic learning, we expected that the presence of orthography might be particularly beneficial for words with higher spelling-sound consistency, with learning highest when children saw and heard the word, and these codes provided overlapping information.\n\n\n\n\n\n\nChildren were taught 16 novel words in a \\(2 \\times 2\\) factorial design. The presence of orthography (orthography absent vs. orthography present) was manipulated within participants: for all children, eight of the words were taught with orthography present and eight with orthography absent. Instructions (incidental vs. explicit) were manipulated between participants such that children in the explicit condition were alerted to the presence of orthography whereas children in the incidental condition were not.\n\n\n\nIn Study 1, 41 children aged 9-10 years completed the word learning task and completed semantic and orthographic assessments one week after learning (Time 1), and eight months later (Time 2). We tested children from one socially mixed school in the South-East of England (\\(M_{age} = 9.95, SD = .53\\)).\n\n\n\nStimuli comprised 16 polysyllabic words, all of which were nouns. We indexed consistency at the whole word level using the H uncertainty statistic (Mousikou et al., 2017). An H value of 0 would indicate a consistent item (all participants producing the same pronunciation), with values \\(&gt;0\\) indicating greater inconsistency (pronunciation variability) with increasing magnitude.\n\n\n\nA ‘pre-test’ was conducted to establish participants’ knowledge of the stimulus words before i.e. pre- training was administered. Then, each child was seen for three 45-minute sessions to complete training (Sessions 1 and 2) and post-tests (Session 3).\nIn Study 1, longitudinal post-test data were collected because children were post-tested at two time points. (Here, we refer to ‘post-tests’ as the tests done to test learning, after i.e. post training.) Children were given post-tests in Session 3, as noted: this was Time 1. They were then given post-tests again, about eight months later at Time 2.\n\n\n\nThe Orthographic post-test was used to examine orthographic knowledge after training. Children were asked to spell each word to dictation and spelling productions were transcribed for scoring. For the purposes of our learning in this chapter, we focus on the accuracy of responses. Each response made by a child to a target word was coded as correct or incorrect.\nA more sensitive outcome measure of orthographic knowledge was also taken. Responses were also scored using a Levenshtein distance measure, using the {stringdist} library (Loo et al., 2022). This score indexes the number of letter deletions, insertions and substitutions that distinguish between the target and child’s response. In the published report (Ricketts et al. (2021)) we focus our analysis of the orthographic outcome on the Levenshtein distance measure of response spelling accuracy, and further details on the analysis approach (Poisson rather than Binomial Generalized Linear Mixed-effects Models) can be found in the paper.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nGet the data: get the data file and the .R script you can use to do the exercises that will support your learning.\n\nYou can download the files folder for this chapter by clicking on the link 04-glmm.zip.\n\n\n\nThe practical materials folder includes data files and an .R script:\nIn this chapter, we will be working with the data about the orthographic post-test outcome for the Ricketts word learning study:\n\nlong.orth_2020-08-11.csv\n\nThe data file is collected together with the .R script:\n\n04-glmm-workbook.R the workbook you will need to do the practical exercises.\n\nThe data come from the Ricketts et al. (2021) study, and you can access the analysis code and data for that study, in full, at the OSF repository here\nIn addition to these data, you will notice that I refer, Section 1.8.1.1, to another data-set analyzed by Monaghan et al. (2015). I enclose the data referenced:\n\nnoun-verb-learning-study.csv\n\nAlong with additional .R you can work with to develop your skills:\n\n402-04-GLMM-exercise-gavagai-data-analysis-notes.R\n\nThe Monaghan et al. (2015) paper can be accessed here.\nDuring practical sessions, each week, you can use the workbook to prompt your code construction and your thinking, with support.\n\n\nAfter practical sessions, you can download an answers version of the workbook .R to check what you did against how I would approach tasks.\n\n\n\n\n\n\nImportant\n\n\n\nGet the answers: get the data file and the .R script with answers.\n\nYou can download the files folder for this chapter by clicking on the link 04-glmm-answers.zip.\n\n\n\nYou will be able to download a folder including:\n\n402-04-glmm-workbook-with-answers.R with answers to questions and example code for doing the exercises.\n\n\n\n\n\nI am going to assume you have downloaded the data file, and that you know where it is. We use read_csv to read the data file into R.\n\nlong.orth &lt;- read_csv(\"long.orth_2020-08-11.csv\", \n                      col_types = cols(\n                        Participant = col_factor(),\n                        Time = col_factor(),\n                        Study = col_factor(),\n                        Instructions = col_factor(),\n                        Version = col_factor(),\n                        Word = col_factor(),\n                        Orthography = col_factor(),\n                        Measure = col_factor(),\n                        Spelling.transcription = col_factor()\n                      )\n                    )\n\nYou can see, here, that within the read_csv() function call, I specify col_types, instructing R how to treat a number of different variables. Recall that in the discussion of the cols() function, we saw how we can determine how R treats variables at the read-in stage, using the col_types = specification. You can read more about this here.\n\n\n\nIt is always a good to inspect what you have got when you read a data file in to R.\n\nsummary(long.orth)\n\nSome of the variables included in the .csv file are listed, following, with information about value coding or calculation.\n\nParticipant – Participant identity codes were used to anonymize participation.\nTime – Test time was coded 1 (time 1) or 2 (time 2). For the Study 1 longitudinal data, it can be seen that each participant identity code is associated with observations taken at test times 1 and 2.\nInstructions – Variable coding for whether participants undertook training in the explicit} or incidental} conditions.\nWord – Letter string values showing the words presented as stimuli to the children.\nOrthography – Variable coding for whether participants had seen a word in training in the orthography absent or present conditions.\nConsistency-H – Calculated orthography-to-phonology consistency value for each word. -zConsistency-H – Standardized Consistency H scores\nScore – Outcome variable – for the orthographic post-test, responses were scored as 1 (correct, if the target spelling was produced in full) or 0 (incorrect, if the target spelling was not produced).\n\nThe summary will show you that we have a number of other variables available, including measures of individual differences in reading or reading-related abilities or knowledge, but we do not need to pay attention to them, for our exercises. If you are interested in the data-set, you can find more information about the variables in the Appendix for this chapter (Section 1.14) and, of course, in Ricketts et al. (2021).\n\n\n\n\nThe data are already tidy: each column in long.orth_2020-08-11.csv corresponds to a variable and each row corresponds to an observation. However, we need to do a bit of work, before we can run any analyses, to fix the coding of the categorical predictor (or independent) variables, the factors Orthography, Instructions, and Time.\n\n\nBy default, R will dummy code observations at different levels of a factor. So, for a factor or a categorical variable like Orthography (present, absent), R will code one level name e.g. absent as 0 and the other e.g. present as 1. The 0-coded level is termed the reference level, which you could call the baseline level, and by default R will code the level with the name appearing earlier in the alphabet as the reference level.\nAll this is usually not important. When you specify a model in R where you are asking to estimate the effect of a categorical variable like Orthography (present, absent) then, by default, what you will get is an estimate of the average difference in outcome, when all other factors are set to zero, estimated as the difference in outcomes comparing the reference level and the other level or levels of the factor. This will be presented, for example, like the output shown following, for a Generalized Linear Model (i.e., a logistic regression) analysis of the effect of Orthography condition, ignoring the random effects:\n\nsummary(glm(Score ~ Orthography, family = \"binomial\", data = long.orth))\n\n\nCall:\nglm(formula = Score ~ Orthography, family = \"binomial\", data = long.orth)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        -1.35879    0.09871 -13.765  &lt; 2e-16 ***\nOrthographypresent  0.52951    0.13124   4.035 5.47e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1431.9  on 1262  degrees of freedom\nResidual deviance: 1415.4  on 1261  degrees of freedom\nAIC: 1419.4\n\nNumber of Fisher Scoring iterations: 4\n\n\nYou can see that you have an estimate, in the summary, of the effect of orthographic condition shown as:\nOrthographypresent  0.52951\nThis model (and default coding) gives us an estimate of how the log odds of a child getting a response correct changes if we compare the responses in the absent condition (here, treated as the baseline or reference level) with responses in the present condition.\n\n\n\n\n\n\nTip\n\n\n\nR tells us about the estimate by adding the name of the factor level that is not the reference level, here, present to the name of the variable Orthography whose effect is being estimated.\n\n\nWe can see that the log odds of a correct response increase by \\(0.52951\\) when the orthography (visual word form or spelling) of a word is present during learning trials.\nHowever, as Dale Barr explains, it is better not to use R’s default dummy coding scheme if we are analyzing data where the data come from a study involving two or more factors, and where we want to estimate not just the main effects of the factors but also the effect of the interaction between the factors.\nIn our analyses, we want the coding that allows us to get estimates of the main effects of factors, and of the interaction effects, somewhat like what we would get from an ANOVA. This requires us to use effect coding.\nWe can code whether a response was recorded in the absent or present condition using numbers. In dummy coding, for any observation, we would use a column of zeroes or ones to code condition: i.e., absent (0) or present (1). In effect coding, for any observation, we would use a column of ones or minus ones to code condition: i.e., absent (-1) or present (1). (With a factor with more than two levels, we would use more than one column to do the coding: the number of columns we would use would equal the number of factor condition levels minus one.) In effect coding, observations coded -1 are in the reference level.\nWith effect coding, the constant (i.e., the intercept for our model) is equal to the grand mean of all the observed responses. And the coefficient of each of the effect variables is equal to the difference between the mean of the group coded 1 and the grand mean.\nYou can read more about effect coding here or here.\n\n\n\nWe follow recommendations to use sum contrast coding for the experimental factors. Further, to make interpretation easier, we want the coding to work so that for both orthography and presentation conditions, doing something is the “high” level in the factor – hence:\n\nOrthography, absent (-1) vs. present (+1)\nInstructions, incidental (-1) vs. explicit (+1)\nTime, test time 1 (-1) vs. time 2 (+1)\n\nWe use a modified version of the contr.sum() function (provided in the {memisc} library) that allows us to define the base or reference level for the factor manually (see documentation).\n\nlibrary(memisc)\n\n\n\n\n\n\n\nTip\n\n\n\nWe sometimes see that we cannot appear to load library(memisc) and library(tidyverse) at the same time without getting weird warnings.\n\nI would load library(memisc) after I have loaded library(tidyverse)\nand maybe then unload it afterwards: just click on the button next to the package or library name in R-Studio to detach the library (i.e., stop it from being available in the R session).\n\n\n\nIn the following sequence, I first check how R codes the levels of each factor by default, I then change the coding, and check that the change gets me what I want.\nWe want effects coding for the orthography condition factor, with orthography condition coded as -1, +1. Check the coding.\n\ncontrasts(long.orth$Orthography)\n\n        present\nabsent        0\npresent       1\n\n\nYou can see that Orthography condition is initially coded, by default, using dummy coding: absent (0); present (1). We want to change the coding, then check that we have got what we want.\n\ncontrasts(long.orth$Orthography) &lt;- contr.sum(2, base = 1)\ncontrasts(long.orth$Orthography)\n\n         2\nabsent  -1\npresent  1\n\n\nWe want effects coding for the presentation condition factor, with presentation condition coded as -1, +1. Check the coding.\n\ncontrasts(long.orth$Instructions)\n\n           incidental\nexplicit            0\nincidental          1\n\n\nChange it.\n\ncontrasts(long.orth$Instructions) &lt;- contr.sum(2, base = 2)\ncontrasts(long.orth$Instructions)\n\n            1\nexplicit    1\nincidental -1\n\n\nWe want effects coding for the Time factor, with Time coded as -1, +1 Check the coding.\n\ncontrasts(long.orth$Time)\n\n  2\n1 0\n2 1\n\n\nChange it.\n\ncontrasts(long.orth$Time) &lt;- contr.sum(2, base = 1)\ncontrasts(long.orth$Time)\n\n   2\n1 -1\n2  1\n\n\nIn these chunks of code, I use contr.sum(a, base = b) to do the coding, where:\n\na is the number of levels in a factor (replace a with the right number)\nb tells R which level to use as the baseline or reference level (replace b with the right number).\n\nI usually need to check the coding before and after I specify it.\n\n\n\n\n\n\n\nWe often need to analyze outcome or dependent variables which comprise observations of responses that are discrete or categorical. We need to learn to recognize research contexts that require GLMMs. Categorical outcome variables can include any of the following:\n\nThe accuracy of each of a series of responses in some task, e.g., whether a recorded response is correct or incorrect.\nThe location of a recorded eye movement, e.g., a fixation to the left or to the right visual field.\nThe membership of one group out of two possible groups, e.g., is a participant impaired or unimpaired?\nThe membership of one group out of multiple possible groups, e.g., is a participant a member of one out of some number of groups, say, a member of a religious or ethnic group?\nResponses that can be coded in terms of ordered categories, e.g., a response on a (Likert) ratings scale.\nOutcomes like the frequency of occurrence of an event, e.g., how many arrests are made at a particular city location.\n\nIn this chapter, we will analyze accuracy data: where the outcome variable consists of responses observed in a behavioural task, the accuracy of responses was recorded, and responses could either be correct or incorrect. The accuracy of a response is, here, coded under a binary or dichotomous classification though we can imagine situations when a response is coded in multiple different ways.\nThose interested in analyzing outcome data from ratings scales, that is, ordered categorical outcome variables, often called ordinal data, may wish to read about ordinal regression analyses, which you can do in R using functions from the {ordinal} library.\n\nWe will take a closer look at ordinal data analyses in a following week.\n\nThose interested in analyzing outcome data composed of counts may wish to read about poisson regression analyses in Gelman & Hill (2007a).\nIt will be apparent in our discussion that researchers have used, and will continue to use, a number of ‘traditional’ methods to analyze categorical outcome variables when really they should be using GLMMs. We will talk about these alternatives, next, so that you recognize what is being done when you read research articles. Critically, we will discuss the limitations of such methods because these limitations explain why we bother to learn about GLMMs.\n\n\nIf you want to analyze data from a study where responses can be either correct or incorrect but not both (and not anything else), then your outcome variable is categorical, and your analysis approach ought to respect that. However, if you read enough psychological research articles then you will see many reports of data analyses in which the researchers collected data on the accuracy of responses but then present the results of analyses that ignored the binary or dichotomous nature of accuracy. We often see response accuracy analyzed using an approach that looks something like the following:\n\nThe accuracy of responses (correct vs. incorrect) is counted, e.g., as the number of correct responses or the number of errors.\nThe percentage, or the proportion, of responses that are correct or incorrect is calculated, for each participant, for each level of each experimental condition or factor.\nThe percentage or proportion values are then entered as the outcome or dependent variable in ANOVA or t-test or linear model (multiple regression) analyses of response accuracy.\n\nYou will see many reports of ANOVA or t-test or linear model analyses of accuracy.\n\n\n\n\n\n\nTip\n\n\n\nWhy can’t we follow these examples, and save ourselves the effort of learning how to use GLMMs?\n\nThe reason is that these analyses are, at best, approximations to more appropriate methods.\nTheir results can be expected to be questionable, or misleading, for reasons that we discuss next.\n\n\n\n\n\nTo illustrate the problems associated with using traditional analysis methods (like ANOVA or multiple regression), when working with accuracy as an outcome, we start by looking at data from an artificial vocabulary learning study (reported by Monaghan et al., 2015). Monaghan et al. (2015) recorded responses made by participants to stimuli in a test where the response was correct (coded 1) or incorrect (coded 0). In our study, we directly compared learning of noun-object pairings, verb-motion pairings, and learning of both noun and verb pairings simultaneously, using a cross-situational learning task. (Those interested in this data-set can read more about it at the online repository associated with this chapter.) The data will have a multilevel structure because you will have multiple responses recorded for each person, and for each stimulus. But what concerns us is that if you attempt to use a linear model to analyze the effects of the experimental variables then you will see some paradoxical results that are easily demonstrated.\nLet’s imagine that we wish to estimate the effects of experimental variables like learning condition: learning trial block (1-12); or vocabulary condition (noun-only, noun-verb, verb-only). We can calculate the proportion of responses correct made by each person for each condition and learning trial block. We can then plot the regression best fit lines indicating how proportion of responses correct varies by person and condition. Figure 1 shows the results.\nLook at where the best fit lines go.\n\n\n\n\n\n\n\n\nFigure 1: Monaghan et al. (2015) artificial word learning study: plot showing the proportion of responses correct for each participant, in each of 12 blocks of 24 learning trials, in each learning condition; each grey line shows the linear model prediction of the proportion correct, for each person, by learning block, in each condition; black lines show the average prediction of the proportion correct, by learning block, in each condition. The position of points has been jittered.\n\n\n\n\n\nFigure 1 shows how variation in the outcome, here, the proportion of responses that are correct, is bounded between the y-axis limits of 0 and 1 while the best fit lines exceed those limits. Clearly, if you consider the accuracy of a person’s responses in any set of trials, for any condition in an experiment, the proportion of responses that can be correct can vary only between 0 (no responses are correct) and 1 (all responses are correct). There are no inbuilt or intrinsic limits to the proportion of responses that a linear model can predict would be correct. According to linear model predictions, if you follow the best fit lines in Figure 1 then there are conditions, or there are participants, in which the proportion of a person’s responses that could be correct will be greater than 1. That is impossible.\n\n\n\nThe other fundamental problem with using analysis approaches like ANOVA or regression to analyze categorical outcomes like accuracy is that we cannot assume that the variance in accuracy of responses will be homogenous across different experimental conditions.\nThe logic of the problem can be set out as follows:\n\nGiven a binary outcome, e.g., where the response is correct or incorrect, for every trial, there is a probability \\(p\\) that the response is correct.\nThe variance of the proportion of trials (per condition) with correct responses is dependent on \\(p\\), and it is greater when \\(p \\sim .5\\), the probability that a response will be correct.\n\nJaeger (2008) (p. 3) then explains the problem like this. If the probability of a binomially distributed outcome like response accuracy differs between two conditions (call them conditions 1 and 2), the variances will only be identical if \\(p1\\) (the proportion of correct responses in condition 1) and p2 (the proportion of correct responses in condition 1) are equally distant from 0.5 (e.g. \\(p1 = .4\\) and \\(p2 = .6\\)). The bigger the difference in distance from 0.5, comparing the conditions, the less similar the variances will be.\nSample proportions between 0.3 and 0.7 are considered close enough to 0.5 to assume homogeneous variances (Agresti, 2002). Unfortunately, we usually cannot determine a priori the range of sample proportions in our experiment.\nIn general, variances in two binomially distributed conditions will not be homogeneous but, as you will recall, in both ANOVA and regression analysis, we assume homogeneity of variance in the outcome variable when we compare the effect of differences (in the mean outcome) between the different levels of a factor. This means that if we design a study in which the outcome variable is the accuracy of responses in different experimental conditions and we plan to use ANOVA or regression to estimate the effect of variation in experimental conditions on response accuracy then unless we get lucky our estimation of the experimental effect will take place under circumstances in which the application of the analysis method (ANOVA or regression) and thus the analysis results will be invalid.\n\n\n\nThe application of traditional (parametric) analysis methods like ANOVA or regression to categorical outcome variables like accuracy is very common in the psychological literature. The problem is that these approaches can give us misleading results.\n\nLinear models assume outcomes are unbounded so allow predictions that are impossible when outcomes are, in fact, bounded as is the case for accuracy or other categorical variables.\nLinear models assume homogeneity of variance but that is unlikely and anyway cannot be predicted in advance when outcomes are categorical variables.\nIf we are interested in the effect of an interaction between two effects, using ANOVA or linear models on accuracy (proportions of responses correct) can tell you, wrongly, that the interaction is significant.\n\nTraditionally, researchers have recognized the limitations attached to using methods like ANOVA or regression to analyze categorical outcomes like accuracy and have applied remedies, transforming the outcome variables, e.g. the arcsine root transformation, to render them ‘more normal’. However, as Jaeger (2008) demonstrates, the remedies like the arcsine transformation that have traditionally been applied are often not likely to succeed. Jaeger (2008) completed a comparison of the results of analyses of accuracy data, where outcomes are raw values for the proportions of correct responses, or arcsine transformed values for proportions correct. His comparison demonstrated that the traditional techniques will show either that effects are significant when they are not or that effects are not significant when they are.\n\n\n\n\nWhat we need, then, is a method that allows us to analyze categorical outcomes. We find the appropriate method in Generalized Linear Models, and in Generalized Linear Mixed-effects Models for repeated measures or multilevel structured data. We can understand these methods, as their name suggests, as generalizations of linear models or linear mixed-effects models: generalizations that allow for the categorical nature of some outcome data.\nYou can understand how Generalized Linear Mixed-effects Models work by seeing them as analyses of categorical outcome data like accuracy where the outcome variable is transformed, as I explain next (see Baguley (2012) for a nice clear explanation, which I summarize here).\nOur problems begin with the need to estimate effects on a bounded outcome like accuracy with a linear model which, as we have seen, will yield unbounded predictions.\nThe logistic transformation takes \\(p\\) the probability of an event with two possible outcomes, and turns it into a logit: the natural logarithm of the odds of the event. The effect of this transformation is to turn a discrete binary bounded outcome into a continuous unbounded outcome.\n\nTransforming a probability to odds \\(o = \\frac{p}{1-p}\\) is a partial solution.\n\n\nOdds are, for example, the ratio of the probability of the occurrence of an event compared to the probability of the non-occurrence of an event, or, in terms of a response accuracy variable, the ratio of the probability of the response being correct compared to the probability of the response being incorrect.\nAnd odds are continuous numeric quantities that are scaled from zero to infinity.\nYou can see how this works if you run the calculations using the equation \\(o = \\frac{p}{1-p}\\) in R as odds &lt;- p/(1-p): replacing p with various numbers (e.g. p = 0.1, 0.01, 0.001).\n\n\nWe can then use the (natural) logarithm of the odds \\(logit = ln\\frac{p}{1-p}\\) because using the logarithm removes the boundary at zero because log odds range from negative to positive infinity.\n\n\nYou can see how this works if you run the calculations using the equation \\(logit = ln\\frac{p}{1-p}\\) in R as logit &lt;- log(p/(1-p)): replacing p with smaller and smaller numbers (e.g. p = 0.1, 0.01, 0.001) gets you increasing negative log odds.\n\nWhen we model the log odds (logit) that a response will be correct, the model is called a logistic regression or logistic model. We can think of logistic models as working like linear models with log-odds outcomes.\n\\[\nln\\frac{p}{1-p} = logitp = \\beta_0 + \\beta_1X_1 \\dots\n\\]\nWe can describe the predicted log odds of a response of one type as the linear sum of the estimated effects of the included predictor variables. In a logistic regression, the predictors have an additive relationship with respect to the log odds outcome, just like in an ordinary linear model. The log odds range from negative to positive infinity; logit of 0 corresponds to proportion of .5.\nBaguley (2012) notes that is it advantageous that odds and probabilities are both directly interpretable. We are used to seeing and thinking in everyday life about the chances that some event will occur.\n\n\n\n\nA small change in R lmer code allows us to extend what we know about linear mixed-effects models to conduct Generalized Linear Mixed-effects Models. We change the function call from lmer() to glmer(). However, we have to make some other changes, as we detail in the following sections.\nWe will be examining the impact of the experimental effects, that is, the fixed effects associated with the impacts on the outcome Score (accuracy of response in the word spelling test) associated with the following comparisons:\n\nTime: time 1 versus time 2\nOrthography: present versus absent conditions\nInstructions: explicit versus incidental conditions\nStandardized spelling-sound consistency\nInteraction between the effects of Orthography and Instructions\nInteraction between the effects of Orthography and consistency\n\nWe will begin by keeping the random effects structure simple.\n\n\nIn our first model, we will specify just random effects of participants and items on intercepts.\n\nlong.orth.min.glmer &lt;- glmer(Score ~ \n                               \n                          Time + Orthography + Instructions + zConsistency_H + \n                               \n                          Orthography:Instructions +\n                               \n                          Orthography:zConsistency_H +\n                               \n                          (1 | Participant) + \n                               \n                          (1 | Word),\n                             \n                    family = \"binomial\", \n                    glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)),\n                             \n                    data = long.orth)\n\nsummary(long.orth.min.glmer)\n\nThe code works as follows.\nFirst, we have a chunk of code mostly similar to what we have done before, but changing the function.\n\nglmer() the function name changes because now we want a generalized linear mixed-effects model of accuracy.\n\nThe model specification includes information about fixed effects and about random effects.\n\nWith (1 | Participant) we include random effects of participants on on intercepts.\nWith (1 | Word) we include random effects of stimulus on on intercepts.\n\nSecond, we have the bit that is specific to generalized models.\n\nfamily = \"binomial\" is entered because accuracy is a binary outcome variable (correct, incorrect) so we assume a binomial probability distribution.\n\nWe then specify:\n\nglmerControl(optimizer=\"bobyqa\", ...) to change the underlying mathematical engine (the optimizer) to cope with greater model complexity,\nand we allow the model fitting functions to take longer to find estimates with optCtrl=list(maxfun=2e5).\n\nNotice how we specify the fixed effects. We want glmer() to estimate “main effects and interactions” that we hypothesized.\nWe specify the main effects with:\n\nTime + Orthography + Instructions + zConsistency_H +\n\nWe specify the interaction effects with:\n\nOrthography:Instructions +\n                               \nOrthography:zConsistency_H +\n\nWhere we ask for estimates of the fixed effects associated with:\n\nOrthography:Instructions the interaction between the effects of Orthography and Instructions;\nOrthography:zConsistency-H the interaction between the effects of Orthography and consistency.\n\n\n\nThere are two forms of notation we can use to specify interactions in R. The simplest form is to use something like this:\n\nOrthography*Instructions\n\nThis will get you estimates of:\n\nThe effect of Orthography: present versus absent conditions.\nThe effect of Instructions: explicit versus incidental conditions.\nAnd the effect of Orthography x Instructions: the interaction between the effects of Orthography and Instructions.\n\nSo, in general, if you want estimates of the effects of variables A, B and the interaction A x B, then you write A*B.\nWe can also use the colon symbol to specify only the interaction, i.e., ignoring main effects, so if you specify A:B then you will get an estimate of the interaction A x B but not the effects A, B.\nWith the coding:\n\nScore ~ Orthography + Instructions + Orthography:Instructions\n\nI would be making explicit that I want estimates for the effects of Orthography, Instruction and the interaction between the effects of Orthography and Instructions.\n\n\n\n\nIf you run the model code, you will get the results shown in the output.\n\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (1 |  \n    Participant) + (1 | Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1040.4    1086.7    -511.2    1022.4      1254 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.0994 -0.4083 -0.2018  0.2019  7.4940 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n Participant (Intercept) 1.840    1.357   \n Word        (Intercept) 2.224    1.491   \nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.878464   0.443942  -4.231 2.32e-05 ***\nTime2                        0.050136   0.083325   0.602    0.547    \nOrthography2                 0.455009   0.086813   5.241 1.59e-07 ***\nInstructions1                0.042290   0.230335   0.184    0.854    \nzConsistency_H              -0.618092   0.384002  -1.610    0.107    \nOrthography2:Instructions1   0.005786   0.083187   0.070    0.945    \nOrthography2:zConsistency_H  0.014611   0.083105   0.176    0.860    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.008                                   \nOrthogrphy2 -0.059  0.008                            \nInstructns1  0.014  0.025  0.001                     \nzCnsstncy_H  0.016 -0.002 -0.029 -0.001              \nOrthgrp2:I1 -0.002 -0.001  0.049 -0.045  0.000       \nOrthgr2:C_H -0.027  0.001  0.179  0.000 -0.035 -0.007\n\n\nThe output from the model summary first gives us information about the model.\n\nFirst, we see information about the function used to fit the model, and the model object created by the lmer() function call.\nThen, we see the model formula including main effects Score ~ Time + Orthography + Instructions + zConsistencyH,\nAs well as the interactions Orthography:Instructions + Orthography:zConsistencyH,\nAnd the random effects (1 | Participant) + (1 |Word).\nThen we see information about the model algorithm\nThen we see model fit statistics, including AIC BIC logLik\nThen we see information about the distribution of residuals\n\nWe then see information listed under Random Effects: this is where you can see information about the error variance terms estimated by the model.\n\nThe information is listed in four columns: 1. Groups; 2. Name; 3. Variance; and 4. Std.Dev.\n\nWe see in the Random Effects section, the variances associated with:\n\nThe random differences between the average intercept (over all data) and the intercept for each participant;\nThe random differences between the average intercept (over all data) and the intercept for responses to each word stimulus.\n\n\nLastly, just as for linear models, we see estimates of the coefficients of the fixed effects, the intercept and the slopes of the experimental variables.\n\nNote that we get p-values (Pr(&gt;|z|)) for what are called Wald null hypothesis significance tests on the coefficients.\nWe can see that one effect is significant. The estimate for the effect of the presence compared to the absence of Orthography is 0.455009. The positive coefficient tells us that the log odds that a response will be correct is higher when Orthography is present compared to when it is absent.\nWe can also see an effect of consistency that can (conventionally) be considered to be marginal or near-significant. The estimate for the effect of zConsistency_H is -0.618093 indicating that the log odds of a response being correct decrease for unit increase in the standardized H consistency measure.\n\n\nAs we discussed in the last chapter, we can conduct null hypothesis significance tests by comparing models that differ in the presence or absence of a fixed effect or a random effect, using the Likelihood Ratio Test. In the results output for a GLMM by the glmer() function, you can see that alongside the estimates of the coefficients (and standard error) for the fixed effects we also have z and p-values. Wald z tests for GLMMs test the null hypothesis of no effect by comparing the effect estimate with their standard error, and comparing the resulting test statistic to zero (Bolker et al., 2009).\n\n\n\n\nWe usually want to do more than just report whether experimental effects are or are not significant. It helps us to present and interpret the estimates from a model if we can visualize the model prediction. There are a variety of tools that help us to do this.\n\n\nWe can use the plot_model function from the {sjPlot} library. The following sequence of code takes information from the model we have just run, then generates model predictions, of change in the probablity of a correct response (Score) for different levels of the Orthography factor and the consistency variable. I chose these variables because they are the significant or near-significant effects.\n\nporth &lt;- plot_model(long.orth.min.glmer,\n           type=\"pred\",\n           terms = \"Orthography\") +\n         theme_bw() +\n         ggtitle(\"Predicted probability\") +\n         ylim(0,1)\n\npzconsH &lt;- plot_model(long.orth.min.glmer,\n           type=\"pred\",\n           terms = \"zConsistency_H\") +\n         theme_bw() +\n         ggtitle(\"Predicted probability\") +\n         ylim(0,1)\n\ngrid.arrange(porth, pzconsH,\n            ncol=2)\n\n\n\n\n\n\n\nFigure 2: Effect of orthography condition (present versus absent) on probability of a response being correct\n\n\n\n\n\nThe plots in Figure 2 show clearly how the probability of a correct response is greater for the conditions where Orthography had been present (versus absent) during the word learning phase of the study. We can also see a trend such that the probability of a response being correct decreases as the (in-)consistency of a target word tends to increase.\nTo produce this plot, you will need to install the {sjPlot} library first, and then run the command library(sjPlot) before creating your plot.\nNotice:\n\nplot_model() produces the plot\nplot_modellong.orth.min.glmer) specifies that the plot should be produced given information about the previously fitted model long.orth.min.glmer\ntype = \"pred\" tells R that you want a plot showing the model predictions, of the effect of, e.g., Orthography: condition\n\nThe function outputs an object whose appearance can be edited as a ggplot object.\nMore information can be found here or here about the {sjPlot} library and plotting model estimates of effects.\n\n\n\n\n\nSo far, we have been considering the results of a random intercepts model in which we take into account the random effects of participants and stimulus word differences on intercepts. We have ignored the possibility that the slopes of the experimental variables might vary between participants or between words.\n\n\n\n\n\n\nTip\n\n\n\nWe now need to examine the question: What random effects should we specify?\n\n\nI must warn you that the question and the answer are complex but the coding is quite simple, and the approaches you can take to address the question are, now, quite well recognized by the psychological community. In other words, the community recognizes the nature of the problem, and recognizes the methods you can potentially follow to solve the problem.\nThe complexity, for us, lies not in the mathematics: the coding is simple and the glmer() function does the work.\nI think the complexity lies, firstly, in how we have to think about the study design, what gets manipulated or allowed to vary. I find it very helpful to sketch out, by hand, what the study design means in relation to who does what in an experiment.\nThe complexity lies, secondly, and in how we have to translate our understanding of the study design to a specification of random effects. We can master that aspect of the challenge through practice.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe question is:\n\nSo, what random effects should we include?\n\n\n\nIf you go back to the description of the study design, then you will be able to see that a number of possibilities follow, theoretically, from the design.\nA currently influential set of recommendations Matuschek et al. (2017) has been labeled Keep it maximal.\nIn this approach, if you are testing effects manipulated according to a pre-specified design then you should:\n\nTest random intercepts – due to random differences between subjects or between items (or other sample grouping variables);\nTest random slopes for all within-subjects or within-items (or other) fixed effects.\n\nThis means that specification of the random effects structure requires two sets of information – corresponding to the answers to two questions:\n\nWhat are the fixed effects?\nWhat are the grouping variables?\n\nAs we have seen, we can rephrase the second question in terms: did you test multiple participants using multiple stimuli (e.g., words …) or did you test participants under multiple different conditions (e.g., levels of experimental condition factors)?\nOur answers to these questions then dictate potentially how we specify the random effects structure for our model.\nWe can consider that we should certainly include a random effect of each grouping variable (e.g., participants, stimulus word) on intercepts. We can reflect that we should also include a random effect of a grouping variable e.g. participant on the slope of each variable that was manipulated within the units of that variable.\nWhen we specify models, we should remember that by default if you specify (1 + something | participant) then you are specifying that you want to take into account:\n\nvariance due to the random differences between participants in intercepts (1 ...| participant),\nplus variance due to the random differences between participants in slopes (... something | participant),\nplus the covariance (or correlation) between the random intercepts and the random slopes.\n\nYou will have become familiar with the practice of referring to effects as within-subjects or between-subjects previously, in working with ANOVA. Here, whether an effect is within-subjects or within-items or not has relevance to whether we can or should specify a random effect of subjects or of items on the slope of a fixed effect.\nIn deciding what random effects we should specify, we need to think about what response data we have recorded, for each of the experimental variables, given the study design. This is because if we want to specify a random effect of participants (or stimulus words) on the slope of an experimental condition then we need to have data, for each person, on their responses under all levels of the condition.\nIf we want to estimate the effect of the experimental manipulation of learning condition, for example, the impact of the presence of orthography, for a person, we need to have data for both levels of the condition (orthography absent and orthography present) for that person. If you think about it, we cannot estimate the slope of the effect of the presence of orthography without response data recorded under both the orthography absent condition and the orthography present condition. If we can estimate the slope of the effect then we can estimate how the slope of the effect deviates between participants.\nWe can spell out how the experimental conditions were manipulated for the example study, as follows. (Writing out this kind of account, for yourself, will be helpful perhaps when you are planning an analysis and have to work out what the random effects could be.)\n\nThe effect of Orthography was manipulated within participants and within stimulus words. This is because the presence of orthography (orthography absent versus orthography present) was manipulated so that we have data about test responses to each word under both Orthography conditions, and data about responses from each child under both conditions.\nThe effect of Instructions was manipulated between participants. This is because Instructions (incidental vs. explicit) were manipulated between participants such that children in the explicit condition were alerted to the presence of orthography whereas children in the incidental condition were not.\nWe can say that the effects of Orthography and of Instructions are both manipulated within words. Items were counterbalanced across instruction and orthography conditions, with all words appearing in both orthography conditions for approximately the same number of children within the explicit and incidental groups.\nThe effect of spelling-sound consistency varies between words because different words have different consistency values but the effect of consistency varies within} participants because we have response data for each participant for their responses to words of different levels of consistency.\nWe recorded responses for all participants and all words so we can say that the effect of Time (test time 1 versus time 2) can also be understood to vary within} both participants and words. This means that, for each person’s response to each word on which they are tested, we have response data recorded at both test times.\n\nThese considerations suggests that we should specify a model with the random effects:\n\nThe random effects of participants on intercepts, and on the slopes of the effects of Time, Orthography and spelling-sound consistency, as well as all corresponding covariances.\nThe random effects of stimulus words on intercepts, and on the slopes of the effects of Time, Orthography and Instructions, as well as all corresponding covariances.\n\nThis is simple to do using the code shown following.\n\nlong.orth.max.glmer &lt;- glmer(Score ~ \n                           \n                      Time + Orthography + Instructions + zConsistency_H + \n                           \n                      Orthography:Instructions +\n                           \n                      Orthography:zConsistency_H +\n                           \n                      (Time + Orthography + zConsistency_H + 1 | Participant) + \n                           \n                      (Time + Orthography + Instructions + 1 |Word),\n                         \n                      family = \"binomial\",\n                      glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)),\n                         \n                         data = long.orth)\n\nsummary(long.orth.max.glmer)\n\nWhere we have the critical terms:\n\n(Time + Orthography + zConsistency_H + 1 | Participant) to account for the random effects of participants on intercepts, and on the slopes of the effects of Time, Orthography and spelling-sound consistency, as well as all corresponding covariances.\n(Time + Orthography + Instructions + 1 |Word) to account for the random effects of stimulus words on intercepts, and on the slopes of the effects of Time, Orthography and Instructions, as well as all corresponding covariances.\n\nIf you run this code, however, you will see that you get warnings along with your estimates.\n\n\nboundary (singular) fit: see help('isSingular')\n\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (Time +  \n    Orthography + zConsistency_H + 1 | Participant) + (Time +  \n    Orthography + Instructions + 1 | Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1053.6    1192.4    -499.8     999.6      1236 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.1014 -0.4027 -0.1723  0.2037  7.0331 \n\nRandom effects:\n Groups      Name           Variance Std.Dev. Corr             \n Participant (Intercept)    2.043127 1.42938                   \n             Time2          0.005675 0.07533   0.62            \n             Orthography2   0.079980 0.28281   0.78 -0.01      \n             zConsistency_H 0.065576 0.25608   0.49  0.99 -0.16\n Word        (Intercept)    2.793448 1.67136                   \n             Time2          0.046736 0.21618   0.14            \n             Orthography2   0.093740 0.30617  -0.68 -0.81      \n             Instructions1  0.212706 0.46120  -0.74 -0.05  0.38\nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -2.10099    0.49589  -4.237 2.27e-05 ***\nTime2                        0.02077    0.12285   0.169 0.865740    \nOrthography2                 0.52480    0.15496   3.387 0.000708 ***\nInstructions1                0.24467    0.27281   0.897 0.369805    \nzConsistency_H              -0.67818    0.36311  -1.868 0.061803 .  \nOrthography2:Instructions1  -0.05133    0.10004  -0.513 0.607907    \nOrthography2:zConsistency_H  0.05850    0.11634   0.503 0.615064    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.097                                   \nOrthogrphy2 -0.280 -0.187                            \nInstructns1 -0.278  0.023  0.109                     \nzCnsstncy_H  0.062  0.005 -0.064  0.120              \nOrthgrp2:I1  0.024  0.005 -0.071  0.212 -0.065       \nOrthgr2:C_H -0.062  0.049  0.246 -0.031 -0.440  0.001\noptimizer (bobyqa) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nNotice, especially, the warning:\n\nboundary (singular) fit: see ?isSingular\n\nIf you put the warning message text into a search engine, then you will get directed to a variety of discussions about what they mean and what you should do about them.\nA highly instructive blog post by (I think) Ben Bolker provides some very useful advice here.\n\nCheck singularity\nIf the fit is singular or near-singular, there might be a higher chance of a false positive (we’re not necessarily screening out gradient and Hessian checking on singular directions properly); a higher chance that the model has actually misconverged (because the optimization problem is difficult on the boundary); and a reasonable argument that the random effects model should be simplified.\nThe definition of singularity is that some of the constrained parameters of the random effects theta parameters are on the boundary (equal to zero, or very very close to zero …)\n\n(Emphasis added.)\nI am going to take his advice and simplify the random effects part of the model. We know that the random intercepts model converges fine and now we know that the maximal model does not. Thus, our task is now to identify a model that includes random effects of participants or items on slopes and still converges without warnings.\n\n\n\nI am just going to assume we need both random effects of subjects and of items on intercepts so I focus on random slopes here. (This assumption may not always be true but is often useful.)\nWe can fit a series of models as follows. Note that I will not show the results for every model, to save space, but you should run the code to see what happens. Look out for convergence or singularity warnings, where they appear.\n\n\nIn the first model, we have just random effects of participants or items on intercepts. This is where we started (earlier).\n\nlong.orth.min.glmer &lt;- glmer(Score ~ \n                               \n                        Time + Orthography + Instructions + zConsistency_H + \n                               \n                        Orthography:Instructions +\n                               \n                        Orthography:zConsistency_H +\n                               \n                        (1 | Participant) + \n                               \n                        (1 | Word),\n                             \n                      family = \"binomial\", \n                      glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)),\n                             \n                      data = long.orth)\n\nsummary(long.orth.min.glmer)\n\nWe saw that the model converged, and we looked at the results previously.\nWhat next? A simple approach we can take is to see if we can add each fixed effect to be included in our random effects terms, one effect at a time.\n\n\n\nIn our second model, we change the random effects terms so that we can account for the random effects of participants and of items on intercepts as well as on the slopes of the Orthography effect. (The Orthography effect is both within-subjects and within-items.)\n\nlong.orth.2.glmer &lt;- glmer(Score ~ \n                             \n                      Time + Orthography + Instructions + zConsistency_H + \n                             \n                      Orthography:Instructions +\n                             \n                      Orthography:zConsistency_H +\n                             \n                      (dummy(Orthography) + 1 || Participant) + \n                             \n                      (dummy(Orthography) + 1 || Word),\n                           \n                      family = \"binomial\", \n                      glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)),\n                           \n                      data = long.orth)\n\nsummary(long.orth.2.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) +  \n    1 || Participant) + (dummy(Orthography) + 1 || Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1041.0    1097.6    -509.5    1019.0      1252 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9373 -0.4132 -0.1952  0.1826  6.9614 \n\nRandom effects:\n Groups        Name               Variance Std.Dev.\n Participant   (Intercept)        1.57092  1.2534  \n Participant.1 dummy(Orthography) 0.57624  0.7591  \n Word          (Intercept)        2.36284  1.5372  \n Word.1        dummy(Orthography) 0.02101  0.1450  \nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.9025027  0.4512443  -4.216 2.49e-05 ***\nTime2                        0.0501974  0.0837186   0.600 0.548775    \nOrthography2                 0.4135727  0.1120792   3.690 0.000224 ***\nInstructions1                0.0455920  0.2234615   0.204 0.838333    \nzConsistency_H              -0.6254414  0.3958971  -1.580 0.114151    \nOrthography2:Instructions1   0.0019343  0.1038694   0.019 0.985142    \nOrthography2:zConsistency_H -0.0007112  0.0877740  -0.008 0.993535    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.007                                   \nOrthogrphy2  0.034  0.007                            \nInstructns1  0.013  0.025  0.007                     \nzCnsstncy_H  0.017 -0.002 -0.024 -0.002              \nOrthgrp2:I1  0.003  0.001  0.043  0.126  0.000       \nOrthgr2:C_H -0.029  0.001  0.182  0.001 -0.024 -0.011\n\n\nThis model converges without warnings.\nNotice that we specify that we || do not want random covariances; we are keeping things simple in each step.\nNote the use of dummy() inside the random effects terms. The ‘dummy’ is a mis-leading name; we are not talking about dummy coding (as above). Here, the dummy() stops R from mis-interpreting the requirement to estimate the effect of the differences between category levels, within random effects.\nThe reason is explained here.\nYou can see what impact it has by specifying, instead, the naked random effect:\n(Orthography + 1 || Participant).\n\n\n\nNext we can add Instructions to take into account random differences between words in the slope of this effect. We show the results for this model as they are instructive.\n\nlong.orth.3.glmer &lt;- glmer(Score ~ \n                             \n                     Time + Orthography + Instructions + zConsistency_H + \n                             \n                     Orthography:Instructions +\n                             \n                     Orthography:zConsistency_H +\n                             \n                     (dummy(Orthography) + 1 || Participant) + \n                             \n                     (dummy(Orthography) + dummy(Instructions) + 1 || Word),\n                           \n                     family = \"binomial\", \n                     glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)),\n                           \n                     data = long.orth)\n\nsummary(long.orth.3.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) +  \n    1 || Participant) + (dummy(Orthography) + dummy(Instructions) +  \n    1 || Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1036.5    1098.2    -506.2    1012.5      1251 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9951 -0.4068 -0.1920  0.1838  5.9308 \n\nRandom effects:\n Groups        Name                Variance Std.Dev.\n Participant   (Intercept)         1.64393  1.2822  \n Participant.1 dummy(Orthography)  0.55604  0.7457  \n Word          (Intercept)         1.94313  1.3940  \n Word.1        dummy(Orthography)  0.01607  0.1268  \n Word.2        dummy(Instructions) 0.86694  0.9311  \nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.9621261  0.4400649  -4.459 8.25e-06 ***\nTime2                        0.0507347  0.0843237   0.602  0.54740    \nOrthography2                 0.4263703  0.1114244   3.827  0.00013 ***\nInstructions1                0.1907423  0.2632605   0.725  0.46874    \nzConsistency_H              -0.6270900  0.3669904  -1.709  0.08750 .  \nOrthography2:Instructions1  -0.0265729  0.1048392  -0.253  0.79991    \nOrthography2:zConsistency_H -0.0006305  0.0878823  -0.007  0.99428    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.007                                   \nOrthogrphy2  0.028  0.008                            \nInstructns1 -0.127  0.022  0.019                     \nzCnsstncy_H  0.017 -0.002 -0.026  0.011              \nOrthgrp2:I1  0.013  0.001  0.003  0.068 -0.009       \nOrthgr2:C_H -0.030  0.002  0.182  0.002 -0.026 -0.018\n\n\nThis model also converges without warnings.\nTake a look at the random effects summary. You can see:\n\nParticipant (Intercept) 1.64393 estimated variance due to random effect of participants on intercepts\nParticipant.1 dummy(Orthography) 0.55604 variance due to random effect of participants on the slope of the Orthography effect\nWord (Intercept) 1.94314 variance due to random effect of words on intercepts\nWord.1 dummy(Orthography) 0.01607 variance due to random effect of words on the slope of the Orthography effect\nWord.2 dummy(Instructions) 0.86694 variance due to random effect of words on the slope of the Instructions effect\n\nWe do not see correlations (random effects covariances) because we use the || notation to stop them being estimated. We want to stop them being estimated because we want to see what we gain from adding just the requirement, first, to estimate the variance associated with random effects of participants or words on the slopes of the experimental variables.\nAlso, we can suspect that adding the requirement to estimate covariances will blow the model up for two reasons. The maximal model, including random slopes variances and covariances clearly did not converge. Secondly, at least two of the correlations listed in the random effects, for the maximal model, were pretty extreme with Corr \\(=-0.01\\) and \\(=0.99\\); such extreme values (\\(r \\sim \\pm 1\\)) are bad signs; see the discussion in Section 1.10.2.\n\n\n\nIn the following models, because we can see that we can get a model to converge with random effects of participants or items on Orthography, and random effect of participants on Instructions, I am going to keep these random effects in the model. I will check if adding further effects is OK too, in terms of successful convergence. I am going to treat all the following models as variations on a theme, the theme being: can we add anything else to:\n\n(dummy(Orthography) + 1 || Participant) + \n                             \n(dummy(Orthography) + dummy(Instructions) + 1 || Word),\n\n\n\n\nNext we see if we can add zConsistency_H.\n\nlong.orth.4.a.glmer &lt;- glmer(Score ~ \n                             \n                        Time + Orthography + Instructions + zConsistency_H + \n                             \n                        Orthography:Instructions +\n                             \n                        Orthography:zConsistency_H +\n                             \n                        (dummy(Orthography) + zConsistency_H + 1 || Participant) + \n                             \n                        (dummy(Orthography) + dummy(Instructions) + 1 || Word),\n                           \n                        family = \"binomial\", \n                        glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)),\n                           \n                        data = long.orth)\n\nboundary (singular) fit: see help('isSingular')\n\nsummary(long.orth.4.a.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) +  \n    zConsistency_H + 1 || Participant) + (dummy(Orthography) +  \n    dummy(Instructions) + 1 || Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1038.5    1105.3    -506.2    1012.5      1250 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9952 -0.4068 -0.1920  0.1838  5.9309 \n\nRandom effects:\n Groups        Name                Variance  Std.Dev. \n Participant   (Intercept)         1.644e+00 1.282e+00\n Participant.1 dummy(Orthography)  5.560e-01 7.456e-01\n Participant.2 zConsistency_H      2.090e-10 1.446e-05\n Word          (Intercept)         1.943e+00 1.394e+00\n Word.1        dummy(Orthography)  1.604e-02 1.266e-01\n Word.2        dummy(Instructions) 8.669e-01 9.311e-01\nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.9621306  0.4400477  -4.459 8.24e-06 ***\nTime2                        0.0507346  0.0843236   0.602  0.54740    \nOrthography2                 0.4263730  0.1114188   3.827  0.00013 ***\nInstructions1                0.1907330  0.2632583   0.725  0.46875    \nzConsistency_H              -0.6270898  0.3669803  -1.709  0.08749 .  \nOrthography2:Instructions1  -0.0265706  0.1048370  -0.253  0.79992    \nOrthography2:zConsistency_H -0.0006352  0.0878782  -0.007  0.99423    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.007                                   \nOrthogrphy2  0.028  0.008                            \nInstructns1 -0.127  0.022  0.019                     \nzCnsstncy_H  0.017 -0.002 -0.026  0.011              \nOrthgrp2:I1  0.013  0.001  0.003  0.068 -0.009       \nOrthgr2:C_H -0.030  0.002  0.182  0.002 -0.026 -0.018\noptimizer (bobyqa) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nWe see two useful pieces of information when we run this model:\n\nWe get the warning boundary (singular) fit: see ?isSingular that tells us the model algorithm could not converge on effects estimates, given the model we specify, given the data.\nNote, also, we see the random effects variance estimate Participant.2 zConsistency_H 1.259e-10.\n\nThese two things are possibly connected: the singularity warning; and the estimated variance of \\(1.259e-10\\) (i.e. a very very small number) associated with the random effect of participants on the slope of the zConsistency_H. We can expect that the model fitting algorithm is going to have difficulty estimating nothing, or something close to nothing: here, the very very small variance associated with the between-participant differences in the slope of the non-significant effect of word spelling-sound consistency on response accuracy.\n\n\n\nWhat about the random effects of participants or of words on the slope of the effect of Time?\n\nlong.orth.4.b.glmer &lt;- glmer(Score ~ \n                             \n            Time + Orthography + Instructions + zConsistency_H + \n                             \n            Orthography:Instructions +\n                             \n            Orthography:zConsistency_H +\n                             \n            (dummy(Orthography) + dummy(Time) + 1 || Participant) + \n                             \n            (dummy(Orthography) + dummy(Instructions) + dummy(Time) + 1 || Word),\n                           \n            family = \"binomial\", \n            glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)),\n                           \n            data = long.orth)\n\nboundary (singular) fit: see help('isSingular')\n\nsummary(long.orth.4.b.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) +  \n    dummy(Time) + 1 || Participant) + (dummy(Orthography) + dummy(Instructions) +  \n    dummy(Time) + 1 || Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1040.5    1112.5    -506.2    1012.5      1249 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9952 -0.4068 -0.1920  0.1838  5.9309 \n\nRandom effects:\n Groups        Name                Variance  Std.Dev. \n Participant   (Intercept)         1.644e+00 1.2821917\n Participant.1 dummy(Orthography)  5.560e-01 0.7456314\n Participant.2 dummy(Time)         0.000e+00 0.0000000\n Word          (Intercept)         1.943e+00 1.3939636\n Word.1        dummy(Orthography)  1.604e-02 0.1266475\n Word.2        dummy(Instructions) 8.669e-01 0.9310619\n Word.3        dummy(Time)         1.992e-08 0.0001411\nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.9621304  0.4400607  -4.459 8.24e-06 ***\nTime2                        0.0507347  0.0843238   0.602  0.54740    \nOrthography2                 0.4263730  0.1114191   3.827  0.00013 ***\nInstructions1                0.1907329  0.2632608   0.725  0.46876    \nzConsistency_H              -0.6270910  0.3669875  -1.709  0.08750 .  \nOrthography2:Instructions1  -0.0265706  0.1048370  -0.253  0.79992    \nOrthography2:zConsistency_H -0.0006352  0.0878785  -0.007  0.99423    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.007                                   \nOrthogrphy2  0.028  0.008                            \nInstructns1 -0.127  0.022  0.019                     \nzCnsstncy_H  0.017 -0.002 -0.026  0.011              \nOrthgrp2:I1  0.013  0.001  0.003  0.068 -0.009       \nOrthgr2:C_H -0.030  0.002  0.182  0.002 -0.026 -0.018\noptimizer (bobyqa) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nNope.\nWe again see two useful pieces of information when we run this model:\n\nWe get the warning boundary (singular) fit: see ?isSingular that tells us the model algorithm could not converge on effects estimates, given the model we specify, given the data\nNote, also, we see the random effects variance estimate Participant.2 dummy(Time) 0.000e+00.\nAnd we see the variance estimate Word.3 dummy(Time) 2.161e-07.\n\nAgain, we can surmise that a mixed-effects model will get into trouble – and we will see convergence warnings – where we have a fixed effect with little impact (like, here, Time) and we are asking for the estimation of variance associated with random differences in slopes where there may be, in fact, little random variation. Possibly, these two things are connected too: we are perhaps unlikely to see random differences in the slope of the effect of an experimental variable if the effect is at or near zero. Possibly, we may see the effect of an experimental variable which is very very consistent. Think back to the conceptual introduction to multilevel data and the effect associated with the relation between maths and physics scores where there seemed to be little variation between classes in the slope representing the relation.\n\n\n\n\nWe can see that a model has difficulty if we see things like:\n\nConvergence warnings, obviously\nVery very small random effects variances\nExtreme random effects correlations of \\(\\pm 1.00\\)\n\nIf we see a warning that the model fitting algorithm nearly failed to converge: boundary (singular) fit: see ?isSingular or failed to converge then this tells us that, given the data, the mathematical engine (optimizer) underlying the lmer() function got into trouble because, in short, it was trying to find estimates for effects that were close to not being there at all.\nIf the variances for the random effects of participants or stimulus items on the slopes of an experimental variable are very small this suggests that the level of complexity in the model cannot really be justified or that the model will have difficulty estimating it. Extreme correlations (near 0 or 1) between random effects on intercepts and on slopes of fixed effects suggest the level of complexity in the model cannot really be justified (see also the discussion in Bates et al. (2015), Eager & Roy (n.d.) and Matuschek et al. (2017)).\n\n\n\nThere is no point comparing the models that do not converge, so we focus on those that do converge.\nDoes the addition of random slopes improve model fit? We can compare the model in pairs, as follows, to test whether each addition in model complexity improves model fit. We run the code for the model comparisons as follows.\nFirst, we compare the model long.orth.min.glmer (just random intercepts) with long.orth.2.glmer to check if increasing model complexity, by accounting for random differences between participants or words in the slope of the Orthography effect improves model fit to data.\n\nanova(long.orth.min.glmer, long.orth.2.glmer)\n\nData: long.orth\nModels:\nlong.orth.min.glmer: Score ~ Time + Orthography + Instructions + zConsistency_H + Orthography:Instructions + Orthography:zConsistency_H + (1 | Participant) + (1 | Word)\nlong.orth.2.glmer: Score ~ Time + Orthography + Instructions + zConsistency_H + Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) + 1 || Participant) + (dummy(Orthography) + 1 || Word)\n                    npar    AIC    BIC  logLik -2*log(L)  Chisq Df Pr(&gt;Chisq)\nlong.orth.min.glmer    9 1040.4 1086.7 -511.20    1022.4                     \nlong.orth.2.glmer     11 1041.0 1097.6 -509.51    1019.0 3.3909  2     0.1835\n\n\nSecond, we compare the model long.orth.min.glmer (just random intercepts) with long.orth.3.glmer to check if increasing model complexity, by accounting for random differences between participants or words in the slope of the Instructions effect improves model fit to data.\n\nanova(long.orth.min.glmer, long.orth.3.glmer)\n\nData: long.orth\nModels:\nlong.orth.min.glmer: Score ~ Time + Orthography + Instructions + zConsistency_H + Orthography:Instructions + Orthography:zConsistency_H + (1 | Participant) + (1 | Word)\nlong.orth.3.glmer: Score ~ Time + Orthography + Instructions + zConsistency_H + Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) + 1 || Participant) + (dummy(Orthography) + dummy(Instructions) + 1 || Word)\n                    npar    AIC    BIC  logLik -2*log(L)  Chisq Df Pr(&gt;Chisq)  \nlong.orth.min.glmer    9 1040.4 1086.7 -511.20    1022.4                       \nlong.orth.3.glmer     12 1036.5 1098.2 -506.24    1012.5 9.9115  3    0.01933 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nI think this justifies reporting the model long.orth.3.glmer.\nIf you look at the outputs, the addition of the random effects of participants and words on the slope of the effect of Orthography cannot be justified (\\(\\chi^2 = 3.3909, 2 df, p = .1835\\)) by improved model fit, in comparison to a model with the random effects of participants and words on intercepts.\nThe model comparison summary indicates that the addition of a random effect of words on the slope of the Instructions effect is justified by significantly improved model fit to data (\\(\\chi^2 = 9.9115, 3 df, p = 0.01933\\)).\nNotice I take a bit of a short-cut here, by adding both random effects for Orthography and Instructions. Logically, if adding the random effect for Orthography does not improve model fit but adding the random effects for both random effects for Orthography and Instructions does then it is the addition of Instructions that is doing the work. However, I should prefer to see as complex a random effects structure as possible, provided a model converges.\nWhile adding the random effect of Orthography does not improve model fit significantly, you see researchers allowing a generous p-value threshold for inclusion of terms (i.e. it is ok to add variables up to where \\(.p &lt; .2\\)). Matuschek et al. (2017) argue that when we are engaged in model selection – here, this is what we are doing because we are trying to figure out what model (with what random effects) we should use – then we should resist the reflex to choose the model that seems justified because the LRT \\(\\chi^2, p &lt; .05\\).\nThe \\(\\chi^2\\) alpha level cannot be interpreted as the expected model selection Type I error rate (in Null Hypothesis Significance Test terms) but rather as the relative weight of model complexity and goodness-of-fit (look back at the discussion of model comparison in the previous chapter). In this sense, setting a threshold such that we include an effect only if \\(\\chi^2, p &lt; .05\\) will always tend to penalize model complexity, and tend therefore to lead us to choose simpler (perhaps too simple) models.\n\n\n\nSometimes (g)lmer() has difficulty finding estimates for effects in a model given a data-set. If it encounters problems, the problems are expressed as warnings about convergence failures. Convergence failures typically arise when the model is too complicated for the data (see the discussion in Bates et al., 2015; Eager & Roy, n.d.; Matuschek et al., 2017; and Meteyard & Davies, 2020). As we have seen, problems can occur if you are trying to estimate (or predict) random effects terms that are very small – that do not really explain much variance in performance. Problems can also occur if variables have very large or very small ranges.\nWe can detect and address these problems in a number of ways:\n\nSometimes convergence problems can be fixed by switching the optimizer used to fit the model – we can do this by adding the argument: glmerControl(optimize = \"bobyqa\") to the glmer() function call, as I did for the class example. Switching optimizers is a quick solution to a common problem: models can fail to converge for a number of different reasons. In short, there may not be enough data for the model fitting process to settle on appropriate estimates for fixed or random effects.\nSometimes, a warning message advises us to consider rescaling the continuous numeric predictor variable. For this reason, and others, I usually standardize numeric predictor variables, as a default, before the analysis.\nSometimes, the warnings tell us that we need to simplify the random effects part of the model. We can simplify the random effects structure of a mixed-effects model in a number of ways:\n\n\nAs we examine the estimates resulting from a model fit we can consider whether the variance and covariance terms are small or large.\nUltimately, I decided that the effects of subjects and items on intercepts was important, as was, to some extent, the effect of words on the slope of the effect of Instructions.\n\nThe approach we have progressed through is widely used (see discussions in Baayen et al., 2008; Barr et al., 2013; Matuschek et al., 2017; Meteyard & Davies, 2020).\nMuch useful advice is set out in the troubleshooting guide by Ben Bolker here.\n\n\nTo demonstrate the impact of these adjustments, you could refit the models discussed in the foregoing:\n\nWithout using the standardized consistency variable as a predictor;\nWithout using the modification to the model fitting code, i.e. deleting the line that includes glmerControl(optimizer = \"bobyqa\");\nMaking the last most complex model more complex by adding further random effects of subjects or items on the slopes of both main effects and the interaction.\n\n\n\n\nMy advice, then, is to consider whether random effects should be included in a model based on:\n\nTheoretical reasons, in terms of what your understanding of a study design allows and requires, with respect to random differences between groups (classes, participants, stimuli etc.) or stimuli;\nModel convergence, as when models do or do not converge;\nOver a series of model comparisons, an evaluation of whether model fit is improved by the inclusion of the random effect.\n\nThis sounds like it involves work, judgment, and a process. It also sounds like people may disagree on the judgment or the process so that you shall have to share data and code, to enable others to check if the results vary depending on different decisions or different approaches. And it sounds like you will need to not only figure out what to do but also justify the approach you take when you report the results.\nI think all these things are true.\nThis is why Lotte Meteyard and I advise that researchers need to explain their approach, and share their data and code, when they report their analyses. Our best practice guidance for reporting mixed-effects models includes, among other things, the advice that in reports …\n\nRandom effects are explicitly specified according to sampling units (e.g., participants, items), the data structure (e.g., repeated measures) and anticipated interactions between fixed effects and sampling units (e.g., intercepts only or intercepts and slopes). Fixed effects and covariates are specified from explicitly stated research questions and/or hypotheses.\nReport the size of the sample analysed in terms of total number of data points and of sampling units (e.g., number of participants, number of items, number of other groups specified as random effects, such as classes of children).\nA clear statement of the methods by which models are compared/selected; e.g., simple to complex, covariates first, random effects first, fixed effects first etc.\nReport comparison method (LRT, AIC, BIC) and justify the choice.\nA complete report of all models compared (e.g., in appendices/supplementary data/analysis scripts) with model equations and the result of comparisons.\nIf models fail to converge, the approach taken to manage this should be comprehensively reported. This should include the formula for each model that did or did not converge and a rationale for a) the simplification method used and b) the final model reported. This may be most easily presented in an analysis script.\n\n\n\n\n\n\n\nTip\n\n\n\nThis looks like a lot of work.\n\nWhy bother?\n\n\n\nI think it is always worth asking this question.\nThe first answer is that it is all relative. In my own experience, a lot of the effort spent in the research workflow used to be occupied by face-to-face data collection: weeks or months of testing; now all data get collected online, and it gets finished overnight. Considerable time and effort (as Hadley Wickham’s joke runs, 80% of analysis effort) was spent on tidying the data before analysis; I still do this work but it is now much faster and less effort, thanks to {tidyverse}. A lot of effort used to be spent by me or colleagues on the literature review, the power analysis, or the stimulus preparation: that still happens. And a lot of effort used to be spent on doing the analysis and figuring out what the results mean: that, too, still happens. It is up to you if you want to spend ten months on data collection and five minutes on data analysis (as another joke has it, a million bucks on the data and a nickel on the statistics).\nI think we do need to work at understanding the most appropriate analysis for our data, based on both our theoretical expectations and a data-driven evaluation. No-one is going to help us unsee multilevel structure in the data, or save us from the obligation to take into account random effects. Matuschek et al. (2017) (2017; p.312) argue that:\n\nThe goal of model selection is not to obtain a significant p-value; the goal is to identify the most parsimonious model that can be assumed to have generated the data.\n\nThis makes sense to me. And their analyses show that determining a parsimonious model with a standard model selection criterion is a defensible choice, a way to take into account random effects, while controlling for both the risk of false positives, and the risk of false negatives.\n\n\n\n\n\nWe have discussed how to report the results of mixed-effects models previously. The same conceptual structure, and similar language, can be used to report the results of Generalized Linear Mixed-effects Models (GLMMs).\nI think you need to think about reporting the analysis as a task in which you first prepare the reader, explaining the motivation for using GLMMs, then present the analysis you did (the process, in outline), then present the results you shall later discuss.\n\nStart by explaining the study design: outline the fixed effects that have to be estimated.\nExplain how random effects structure was selected – be prepared to present a short version of the story in the main part of the report – sharing your code, in an appendix, to illustrate the steps in full.\n\nLotte Meteyard and I recommend that results reporting should:\n\nProvide equation(s) that transparently define the reported model(s). An elegant way to do this is providing the model equation with the table that reports the model output.\nAnd that final model(s) should be reported in a table that includes all parameter estimates for fixed effects (coefficients, standard errors and/or confidence intervals, associated test statistics and p-values if used), random effects (standard deviation and/or variance for each random effect, correlations/covariances if modelled) and some measure of model fit (e.g. R-squared, correlation between fitted values and data).\nWhile researchers should be able to share the coding script used to complete the analysis and, wherever possible, share data that generated the reported results.\n\nFor the word learning study we have been working through, the Results section for the report would include the following elements:\n\nExplain approach\n\n\nWe used mixed-effects models to analyse data because this approach permits modelling of both participant- and item-level variability simultaneously, unlike more traditional approaches such as ANOVA. In this study, multiple participants responded to multiple items, meaning that both participants and items were sources of nonindependence in our data (i.e. responses from the same participant are likely to be correlated, as are responses to the same item). Compared to ANOVA, mixed-effects models offer a more flexible approach, and are better able to handle missing data without significant loss of statistical power (Baayen, Davidson, & Bates, 2008).\n\n\nExplain how you get from the study design to the model you use to test or estimate key effects\n\n\nWe took a hypothesis driven approach, estimating the fixed effects of time (Time 1 versus Time 2), Orthography (absent versus present), Instructions (incidental versus explicit) and consistency (standardized H), as well as the interaction between orthography and instructions and the interaction between orthography and consistency. Different levels of the three binary fixed effects were sum coded… Consistency H, as a numeric predictor variable, was standardized to z scores before entry to models as a predictor. scores before entry to models as predictors. –&gt;\n\n\nOutline the model comparison or model selection work\n\n\nThe models were initially fitted specifying just random effects to account for variation by participants and stimuli in accuracy (random intercepts) plus terms to estimate the fixed effects of the experimental conditions ([name them]), and the interactions [name them]. Following the recommendations of Barr, Levy, Scheepers, and Tily (2013; see also Baayen, 2008; Matuschek et al., 2017), we fitted further models adding both random intercepts and random slopes for the random effects. Likelihood ratio test comparison of models showed that a model with both random intercepts and slopes … fit the data better than a model with just random intercepts \\((\\chi^2(df) = ..., p = ...)\\).\n\n\nUse appendices or supplementary materials\n\n\nTo give the reader full information on models fit, model comparisons.\nTo Help the reader with a concise summary of estimates.\n\nAs I have advised for reporting linear models, I included a tabled summary of coefficient estimates, presenting fixed and random effects (see e.g. Davies et al., 2013; Monaghan et al., 2015)\n\nShow and tell\n\nUse figures – model prediction plots, as seen – to help the reader to see what the fixed effects estimates imply.\n\n\n\n\n\n\nTip\n\n\n\nWhich model do we report?\n\nNote that given the model comparison results we have seen, I would probably report the estimates from long.orth.3.glmer. The model appears to include the most comprehensive account of random effects while still being capable of converging.\n\n\n\n\n\n\nWe focused on the need to use Generalized Linear Mixed-effects Models (GLMMs). We identified the kind of outcome data (like response accuracy) that requires analysis using GLMMs. Alternative methods, and their limitations, were discussed.\nWe examined a study that incorporates repeated measures (participants respond to multiple stimuli), a 2 x 2 factorial design, and a longitudinal aspect (participants tested at two time points), the word learning study (Ricketts et al., 2021).\nWe discussed the need to use effect coding for categorical predictor variables (factors). We work through example code to set factor level coding as required.\nWe worked through a random intercepts GLMM, and identified the critical elements of the model code, and of the results summary, including hypothesis test p-values. We examined how to present visualizations of fixed effects estimates (model predictions) using different libraries.\nWe then moved on to considering the question of what random effects we should include in the model. We considered the study design in some depth, and explored what random effects we could, in theory, expect to require. We then worked through a model comparison approach. We looked at some warning signs, what they indicate, and how to deal with them.\nWe considered how to report the model selection (or comparison, or building) process, and how to report the model for presentation of results.\n\n\nWe used two functions to fit and evaluate mixed-effects models.\n\nWe used glmer() to fit a mixed-effects model\nWe used anova() to compare two or more models using AIC, BIC and the Likelihood Ratio Test\n\n\n\n\n\nThe example studies referred to in this chapter are published in (Monaghan et al., 2015; Ricketts et al., 2021).\nBen Bolker provides a very readable introduction to Generalized Linear Mixed-effects Models (Bolker et al., 2009; see also Jaeger, 2008).\n(Baayen et al., 2008; see also Barr et al., 2013) discuss mixed-effects models with crossed random effects.\nThe issue of model comparison or model selection, and the appropriate choice of random effects structure is discussed helpfully by Baayen et al. (2008), Bates et al. (2015), Barr et al. (2013), Eager & Roy (n.d.) and Matuschek et al. (2017).\nI wrote a tutorial article on mixed-effects models with Lotte Meteyard (Meteyard & Davies, 2020). We discuss how important the approach now is for psychological science, what researchers worry about when they use it, and what they should do and report when they use the method.\nAccessible ook length introductions are provided by Snijders & Bosker (2004) and Gelman & Hill (2007b).\n\n\nCan be found here.\nOther helpful online advice by Ben Bolker (besides his numerous helpful interventions on StackOverflow) can be found here and here.\n\n\n\n\nFurther information about the variables in the long.orth_2020-08-11.csv data-set is listed following.\n\nParticipant\n\nCell values comprise character strings coding for participant. Participant identity codes were used to anonymise participation. Children included in studies 1 and 2 – participants in the longitudinal data collection – were coded EOF[number]. Children included in Study 2 only (i.e., the older, additional, sample) were coded ND[number].\n\nTime\n\nTest time was coded 1 (time 1) or 2 (time 2). For the Study 1 longitudinal data, it can be seen that each participant identity code is associated with observations taken at test times 1 and 2.\n\nStudy\n\nObservations taken for children included in studies 1 and 2 – participants in the longitudinal daa collection – were coded Study1&2. Children included in Study 2 only (i.e., the older, additional, sample) were coded Study2.\n\nInstructions – Variable coding for whether participants undertook training in the explicit} or incidental} conditions.\nVersion – Experiment administration coding\nWord – Letter string values show the words presented as stimuli to children.\nConsistency-H – Calculated orthography-to-phonology value for each word.\nOrthography – Variable coding for whether participants had seen a word in training in the orthography absent} or present} conditions.\nMeasure – Variable coding for the post-test measure: Sem_all: if the semantic post-test;Orth_sp: if the orthographic post-test.\nScore – Variable coding for response category. For the semantic (sequential or dynamic) post-test, responses were scored as corresponding to:\n3 – correct response in the definition task\n2 – correct response in the cued definition task\n1 – correct response in the recognition task\n0 – if the item wasn’t correctly defined or recognised\nFor the orthographic post-test, responses were scored as:\n1 – correct, if the target spelling was produced in full\n0 – incorrect\nWASImRS Raw score – Matrix Reasoning subtest of the Wechsler Abbreviated Scale of Intelligence\nTOWREsweRS Raw score – Sight Word Efficiency (SWE) subtest of the Test of Word Reading Efficiency; number of words read correctly in 45 seconds.\nTOWREpdeRS Raw score – Phonemic Decoding Efficiency (PDE) subtest of the Test of Word Reading Efficiency; number of nonwords read correctly in 45 seconds.\nCC2regRS Raw score – Castles and Coltheart Test 2; number of regular words read correctly\nCC2irregRS Raw score – Castles and Coltheart Test 2; number of irregular words read correctly\nCC2nwRS Raw score – Castles and Coltheart Test 2; number of nonwords read correctly\nWASIvRS Raw score – vocabulary knowledge indexed by the Vocabulary subtest of the WASI-II\nBPVSRS Raw score – vocabulary knowledge indexed by the British Picture Vocabulary Scale – Third Edition\nSpelling.transcription Transcription of the spelling response produced by children in the orthographic post-test\nLevenshtein.Score Children were asked to spell each word to dictation and spelling productions were transcribed for scoring. Responses were scored using a Levenshtein distance measure, using the `stringdist: library (van der Loo, 2019). This score indexes the number of letter deletions, insertions and substitutions that distinguish between the target and child’s response. For example, the response ‘epegram’ for target ‘epigram’ attracts a Levenshtein score of 1 (one substitution). Thus, this score gives credit for partially correct responses, as well as entirely correct responses. The maximum score is 0, with higher scores indicating less accurate responses.\nzTOWREsweRS We standardized TOWREsweRS values, calculating the z score as \\(z = \\frac{x - \\bar{x}}{sd_x}\\), over all observations in the longitudinal} (Study 1) or concurrent} (Study 2) data-set, using the scale() function in R.\nzTOWREpdeRS Standardized TOWREpdeRS scores\nzCC2regRS] Standardized CC2regRS scores\nzCC2irregRSStandardized CC2irregRS scores\nzCC2nwRS Standardized CC2nwRS scores\nzWASIvRS Standardized WASIvRS scores\nzBPVSRSStandardized BPVSRS scores\n\n\n\n\n\n\n\nAgresti, A. (2002). Categorical data analysis. Wiley Series in Probability and Statistics. https://doi.org/10.1002/0471249688\n\n\nBaayen, R. H., Davidson, D. J., & Bates, D. M. (2008). Mixed-effects modeling with crossed random effects for subjects and items. Journal of Memory and Language, 59(4), 390–412. https://doi.org/10.1016/j.jml.2007.12.005\n\n\nBaguley, T. (2012). Modeling discrete outcomes (pp. 667–723). Macmillan Education UK. https://doi.org/10.1007/978-0-230-36355-7_17\n\n\nBarr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of Memory and Language, 68, 255–278.\n\n\nBates, D., Kliegl, R., Vasishth, S., & Baayen, H. (2015). Parsimonious mixed models. arXiv Preprint arXiv:1506.04967.\n\n\nBolker, B. M., Brooks, M. E., Clark, C. J., Geange, S. W., Poulsen, J. R., Stevens, M. H. H., & White, J.-S. S. (2009). Generalized linear mixed models: a practical guide for ecology and evolution. Trends in Ecology & Evolution, 24(3), 127–135. https://doi.org/10.1016/j.tree.2008.10.008\n\n\nColenbrander, D., Miles, K. P., & Ricketts, J. (2019). To See or Not to See: How Does Seeing Spellings Support Vocabulary Learning? Language, Speech, and Hearing Services in Schools, 50(4), 609–628. https://doi.org/10.1044/2019_lshss-voia-18-0135\n\n\nEager, C., & Roy, J. (n.d.). Mixed effects models are sometimes terrible. https://doi.org/10.48550/arXiv.1701.04858\n\n\nGelman, A., & Hill, J. (2007b). Data analysis using regression and multilevel/hierarchical models. Cambridge University Press.\n\n\nGelman, A., & Hill, J. (2007a). Data analysis using regression and multilevel/hierarchical models. Cambridge University Press.\n\n\nJaeger, T. F. (2008). Categorical data analysis: Away from ANOVAs (transformation or not) and towards logit mixed models. Journal of Memory and Language, 59(4), 434–446. https://doi.org/10.1016/j.jml.2007.11.007\n\n\nLoo, M. van der, Laan, J. van der, R Core Team, Logan, N., Muir, C., Gruber, J., & Ripley, B. (2022). Stringdist: Approximate string matching, fuzzy text search, and string distance functions. https://CRAN.R-project.org/package=stringdist\n\n\nMatuschek, H., Kliegl, R., Vasishth, S., Baayen, H., & Bates, D. (2017). Balancing type i error and power in linear mixed models. Journal of Memory and Language, 94, 305–315. https://doi.org/10.1016/j.jml.2017.01.001\n\n\nMeteyard, L., & Davies, R. A. I. (2020). Best practice guidance for linear mixed-effects models in psychological science. Journal of Memory and Language, 112, 104092. https://doi.org/10.1016/j.jml.2020.104092\n\n\nMonaghan, P., Mattock, K., Davies, R. A. I., & Smith, A. C. (2015). Gavagai is as gavagai does: Learning nouns and verbs from cross-situational statistics. Cognitive Science, 39(5), 1099–1112. https://doi.org/10.1111/cogs.12186\n\n\nMousikou, P., Sadat, J., Lucas, R., & Rastle, K. (2017). Moving beyond the monosyllable in models of skilled reading: Mega-study of disyllabic nonword reading. Journal of Memory and Language, 93, 169–192. https://doi.org/10.1016/j.jml.2016.09.003\n\n\nRicketts, J., Dawson, N., & Davies, R. (2021). The hidden depths of new word knowledge: Using graded measures of orthographic and semantic learning to measure vocabulary acquisition. Learning and Instruction, 74, 101468. https://doi.org/10.1016/j.learninstruc.2021.101468\n\n\nSnijders, T. A. B., & Bosker, R. J. (2004). Multilevel analysis: An introduction to basic and advanced multilevel modeling. Sage Publications Ltd.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 19. Conceptual introduction to Generalized Linear Mixed-effects Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/04-glmm.html#sec-glmm-motivations",
    "href": "PSYC412/part2/04-glmm.html#sec-glmm-motivations",
    "title": "Week 19. Conceptual introduction to Generalized Linear Mixed-effects Models",
    "section": "",
    "text": "We have been discussing how we can use Linear Mixed-effects models to analyze multilevel structured data, the kind of data that we commonly acquire in experimental psychological studies, for example, when our studies have repeated measures designs. The use of Linear Mixed-effects models is appropriate where the outcome variable is a continuous numeric variable like reaction time. In this chapter, we extend our understanding and skills by moving to examine data where the outcome variable is categorical: this is a context that requires the use of Generalized Linear Mixed-effects Models (GLMMs).\nWe will begin by looking at the motivations for using GLMMs. We will then look at a practical example of a GLMM analysis, in an exploration in which we shall reveal some of the challenges that can arise in such work. The R code to do the modeling is very similar to the code we have used before. The way we can understand the models is also similar but with one critical difference. We start to understand that difference here.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 19. Conceptual introduction to Generalized Linear Mixed-effects Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/04-glmm.html#sec-glmm-ideas",
    "href": "PSYC412/part2/04-glmm.html#sec-glmm-ideas",
    "title": "Week 19. Conceptual introduction to Generalized Linear Mixed-effects Models",
    "section": "",
    "text": "Important\n\n\n\nCategorical outcomes cannot be analyzed using linear models, in whatever form, without having to make some important compromises.\n\n\nYou need to do something about the categorical nature of the outcome.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 19. Conceptual introduction to Generalized Linear Mixed-effects Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/04-glmm.html#sec-glmm-targets",
    "href": "PSYC412/part2/04-glmm.html#sec-glmm-targets",
    "title": "Week 19. Conceptual introduction to Generalized Linear Mixed-effects Models",
    "section": "",
    "text": "In this chapter, we look at Generalized Linear Mixed-effects Models (GLMMs): we can use these models to analyze outcome variables of different kinds, including outcome variables like response accuracy that are coded using discrete categories (e.g. correct vs. incorrect). Our aims are to:\n\nRecognize the limitations of alternative methods for analyzing such outcomes, Section 1.8.1.\nUnderstand practically the reasons for using GLMMs when we analyze discrete outcome variables, Section 1.8.2.\nPractice running GLMMs with varying random effects structures.\nPractice reporting the results of GLMMs, including through the use of model plots.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 19. Conceptual introduction to Generalized Linear Mixed-effects Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/04-glmm.html#sec-glmm-study-guide",
    "href": "PSYC412/part2/04-glmm.html#sec-glmm-study-guide",
    "title": "Week 19. Conceptual introduction to Generalized Linear Mixed-effects Models",
    "section": "",
    "text": "I have provided a collection of materials you can use. Here, I explain what they are and how I suggest you use them.\n\n\n\n\n\n\nLinked resources\n\n\n\n\nWe learned about multilevel structured data in the conceptual introduction to multilevel data and the workbook introduction to multilevel data.\nWe then deepened our understanding by looking at the analysis of data from studies with repeated-measures designs in the conceptual introduction to linear mixed-effects models and the workbook introduction to mixed-effects models.\nWe further extended our understanding and practice skills in the conceptual introduction to developing linear mixed-effects models and the corresponding workbook.\n\nWe present the workbook introduction to **Generalized Linear Mixed-effects Models where the explanation of ideas or of practical analysis steps is cut down, to focus attention on practical action steps.\n\n\n1. Video recordings of lectures\n1.1. I have recorded a lecture in three parts. The lectures should be accessible by anyone who has the link.\n\nPart 1 – about 20 minutes\nPart 2 – about 16 minutes\nPart 3 – about 19 minutes\n\n1.2. I suggest you watch the recordings then read the rest of this chapter.\n\nThe lectures provide a summary of the main points.\n\n1.3. You can download the lecture slides in two different versions:\n\n402-week-20-GLMM.pdf: exactly as delivered [700 KB];\n402-week-20-GLMM_6pp.pdf: printable version, six-slides-per-page [850 KB].\n\nThe GLMM.pdf version is the version delivered for the lecture recordings. To make the slides easier to download, I produced a six-slide-per-page version, GLMM_6pp.pdf. This should be easier to download and print out if that is what you want to do.\n2. Chapter: 04-glmm\n2.1. I have written this chapter to discuss the main ideas and set out the practical steps you can follow to start to develop the skills required to work with categorical outcomes data using GLMMs.\n2.2. The practical elements include data tidying, visualization and analysis steps.\n2.3. You can read the chapter, run the code, and do the exercises.\n\nRead in the example word learning data-set.\nExperiment with the .R code used to work with the example data.\nRun GLMMs of demonstration data.\nRun GLMMs of alternate data sets.\nReview the recommended readings (Section 1.13).\n\n3. Practical materials\n3.1 In the following sections, I describe the practical steps, and associated practical materials (exercise workbooks and data), you can use for your learning.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 19. Conceptual introduction to Generalized Linear Mixed-effects Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/04-glmm.html#sec-glmm-word-learning-data",
    "href": "PSYC412/part2/04-glmm.html#sec-glmm-word-learning-data",
    "title": "Week 19. Conceptual introduction to Generalized Linear Mixed-effects Models",
    "section": "",
    "text": "We will be working with data collected for a study investigating word learning in children, reported by Ricketts et al. (2021). You will see that the study design has both a repeated measures aspect because each child is asked to respond to multiple stimuli, and a longitudinal aspect because responses are recorded at two time points. Because responses were observed to multiple stimuli for each child, and because responses were recorded at multiple time points, the data have a multilevel structure. These features require the use of mixed-effects models for analysis.\nWe will see, also, that the study involves the factorial manipulation of learning conditions. This means that, when you see the description of the study design, you will see embedded in it the 2 x 2 factorial design beloved of psychologists. You will be able to generalize from our work this week to many other research contexts where psychologists conduct experiments in which conditions are manipulated according to a factorial design.\nHowever, our focus here is on the fact that the outcome for analysis is the accuracy of the responses made by children to word targets in a spelling task. The categorical nature of accuracy as an outcome is the reason why we now turn to use Generalized Linear Mixed-effects Models.\n\n\nI am going to present the study information in some detail, in part, to enable you to make sense of the analysis aims and results and, in part, so that we can simulate results reporting in a meaningful context.\n\n\nVocabulary knowledge is essential for processing language in everyday life and it is vital that we know how to optimize vocabulary teaching. One strategy with growing empirical support is orthographic facilitation: children and adults are more likely to learn new spoken words that are taught with their orthography [visual word forms; for a systematic review, see Colenbrander et al. (2019)]. Why might orthographic facilitation occur? Compared to spoken inputs, written inputs are less transient across time and less variable across contexts. In addition, orthography is more clearly marked (e.g., the ends of letters and words) than the continuous speech stream. Therefore, orthographic forms may be more readily learned than phonological forms.\nRicketts et al. (2021) investigated how school-aged children learn words. We conducted two studies in which children learned phonological forms and meanings of 16 polysyllabic words in the same experimental paradigm. To test whether orthographic facilitation would occur, half of the words were taught with access to the orthographic form (orthography present condition) and the other half were taught without orthographic forms (orthography absent condition). In addition, we manipulated the instructions that children received: approximately half of the children were told that some words would appear with their written form (explicit group); the remaining children did not receive these instructions (incidental group). Finally, we investigated the impact of spelling-sound consistency of word targets for learning, by including words that varied continuously on a measure of pronunciation consistency (after Mousikou et al., 2017).\nThe quality of lexical representations was measured in two ways. A cuing hierarchical response task (definition, cued definition, recognition) was used to elicit semantic knowledge from the phonological forms, providing a fine-grained measure of semantic learning. A spelling task indexed the extent of orthographic learning for each word. We focus on the analysis of the spelling task responses in this chapter (you may be interested in reviewing our other analyses, see Section 1.5.2).\nRicketts et al. (2021) reported two studies. We focus on Study 1, in which Ricketts et al. (2021) measured knowledge of newly learned words at two intervals: first one week and then, again, eight months after training. Longitudinal studies of word learning are rare and this is the first longitudinal investigation of orthographic facilitation.\nWe addressed three research questions and tested predictions in relation to each question.\n\n\n\n\n\n\nNote\n\n\n\n\nDoes the presence of orthography promote greater word learning?\n\n\nWe predicted that children would demonstrate greater orthographic learning for words that they had seen (orthography present condition) versus not seen (orthography absent condition).\n\n\nWill orthographic facilitation be greater when the presence of orthography is emphasized explicitly during teaching?\n\n\nWe expected to observe an interaction between instructions and orthography, with the highest levels of learning when the orthography present condition was combined with explicit instructions.\n\n\nDoes word consistency moderate the orthographic facilitation effect?\n\n\nFor orthographic learning, we expected that the presence of orthography might be particularly beneficial for words with higher spelling-sound consistency, with learning highest when children saw and heard the word, and these codes provided overlapping information.\n\n\n\n\n\n\nChildren were taught 16 novel words in a \\(2 \\times 2\\) factorial design. The presence of orthography (orthography absent vs. orthography present) was manipulated within participants: for all children, eight of the words were taught with orthography present and eight with orthography absent. Instructions (incidental vs. explicit) were manipulated between participants such that children in the explicit condition were alerted to the presence of orthography whereas children in the incidental condition were not.\n\n\n\nIn Study 1, 41 children aged 9-10 years completed the word learning task and completed semantic and orthographic assessments one week after learning (Time 1), and eight months later (Time 2). We tested children from one socially mixed school in the South-East of England (\\(M_{age} = 9.95, SD = .53\\)).\n\n\n\nStimuli comprised 16 polysyllabic words, all of which were nouns. We indexed consistency at the whole word level using the H uncertainty statistic (Mousikou et al., 2017). An H value of 0 would indicate a consistent item (all participants producing the same pronunciation), with values \\(&gt;0\\) indicating greater inconsistency (pronunciation variability) with increasing magnitude.\n\n\n\nA ‘pre-test’ was conducted to establish participants’ knowledge of the stimulus words before i.e. pre- training was administered. Then, each child was seen for three 45-minute sessions to complete training (Sessions 1 and 2) and post-tests (Session 3).\nIn Study 1, longitudinal post-test data were collected because children were post-tested at two time points. (Here, we refer to ‘post-tests’ as the tests done to test learning, after i.e. post training.) Children were given post-tests in Session 3, as noted: this was Time 1. They were then given post-tests again, about eight months later at Time 2.\n\n\n\nThe Orthographic post-test was used to examine orthographic knowledge after training. Children were asked to spell each word to dictation and spelling productions were transcribed for scoring. For the purposes of our learning in this chapter, we focus on the accuracy of responses. Each response made by a child to a target word was coded as correct or incorrect.\nA more sensitive outcome measure of orthographic knowledge was also taken. Responses were also scored using a Levenshtein distance measure, using the {stringdist} library (Loo et al., 2022). This score indexes the number of letter deletions, insertions and substitutions that distinguish between the target and child’s response. In the published report (Ricketts et al. (2021)) we focus our analysis of the orthographic outcome on the Levenshtein distance measure of response spelling accuracy, and further details on the analysis approach (Poisson rather than Binomial Generalized Linear Mixed-effects Models) can be found in the paper.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nGet the data: get the data file and the .R script you can use to do the exercises that will support your learning.\n\nYou can download the files folder for this chapter by clicking on the link 04-glmm.zip.\n\n\n\nThe practical materials folder includes data files and an .R script:\nIn this chapter, we will be working with the data about the orthographic post-test outcome for the Ricketts word learning study:\n\nlong.orth_2020-08-11.csv\n\nThe data file is collected together with the .R script:\n\n04-glmm-workbook.R the workbook you will need to do the practical exercises.\n\nThe data come from the Ricketts et al. (2021) study, and you can access the analysis code and data for that study, in full, at the OSF repository here\nIn addition to these data, you will notice that I refer, Section 1.8.1.1, to another data-set analyzed by Monaghan et al. (2015). I enclose the data referenced:\n\nnoun-verb-learning-study.csv\n\nAlong with additional .R you can work with to develop your skills:\n\n402-04-GLMM-exercise-gavagai-data-analysis-notes.R\n\nThe Monaghan et al. (2015) paper can be accessed here.\nDuring practical sessions, each week, you can use the workbook to prompt your code construction and your thinking, with support.\n\n\nAfter practical sessions, you can download an answers version of the workbook .R to check what you did against how I would approach tasks.\n\n\n\n\n\n\nImportant\n\n\n\nGet the answers: get the data file and the .R script with answers.\n\nYou can download the files folder for this chapter by clicking on the link 04-glmm-answers.zip.\n\n\n\nYou will be able to download a folder including:\n\n402-04-glmm-workbook-with-answers.R with answers to questions and example code for doing the exercises.\n\n\n\n\n\nI am going to assume you have downloaded the data file, and that you know where it is. We use read_csv to read the data file into R.\n\nlong.orth &lt;- read_csv(\"long.orth_2020-08-11.csv\", \n                      col_types = cols(\n                        Participant = col_factor(),\n                        Time = col_factor(),\n                        Study = col_factor(),\n                        Instructions = col_factor(),\n                        Version = col_factor(),\n                        Word = col_factor(),\n                        Orthography = col_factor(),\n                        Measure = col_factor(),\n                        Spelling.transcription = col_factor()\n                      )\n                    )\n\nYou can see, here, that within the read_csv() function call, I specify col_types, instructing R how to treat a number of different variables. Recall that in the discussion of the cols() function, we saw how we can determine how R treats variables at the read-in stage, using the col_types = specification. You can read more about this here.\n\n\n\nIt is always a good to inspect what you have got when you read a data file in to R.\n\nsummary(long.orth)\n\nSome of the variables included in the .csv file are listed, following, with information about value coding or calculation.\n\nParticipant – Participant identity codes were used to anonymize participation.\nTime – Test time was coded 1 (time 1) or 2 (time 2). For the Study 1 longitudinal data, it can be seen that each participant identity code is associated with observations taken at test times 1 and 2.\nInstructions – Variable coding for whether participants undertook training in the explicit} or incidental} conditions.\nWord – Letter string values showing the words presented as stimuli to the children.\nOrthography – Variable coding for whether participants had seen a word in training in the orthography absent or present conditions.\nConsistency-H – Calculated orthography-to-phonology consistency value for each word. -zConsistency-H – Standardized Consistency H scores\nScore – Outcome variable – for the orthographic post-test, responses were scored as 1 (correct, if the target spelling was produced in full) or 0 (incorrect, if the target spelling was not produced).\n\nThe summary will show you that we have a number of other variables available, including measures of individual differences in reading or reading-related abilities or knowledge, but we do not need to pay attention to them, for our exercises. If you are interested in the data-set, you can find more information about the variables in the Appendix for this chapter (Section 1.14) and, of course, in Ricketts et al. (2021).",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 19. Conceptual introduction to Generalized Linear Mixed-effects Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/04-glmm.html#sec-glmm-data-tidy",
    "href": "PSYC412/part2/04-glmm.html#sec-glmm-data-tidy",
    "title": "Week 19. Conceptual introduction to Generalized Linear Mixed-effects Models",
    "section": "",
    "text": "The data are already tidy: each column in long.orth_2020-08-11.csv corresponds to a variable and each row corresponds to an observation. However, we need to do a bit of work, before we can run any analyses, to fix the coding of the categorical predictor (or independent) variables, the factors Orthography, Instructions, and Time.\n\n\nBy default, R will dummy code observations at different levels of a factor. So, for a factor or a categorical variable like Orthography (present, absent), R will code one level name e.g. absent as 0 and the other e.g. present as 1. The 0-coded level is termed the reference level, which you could call the baseline level, and by default R will code the level with the name appearing earlier in the alphabet as the reference level.\nAll this is usually not important. When you specify a model in R where you are asking to estimate the effect of a categorical variable like Orthography (present, absent) then, by default, what you will get is an estimate of the average difference in outcome, when all other factors are set to zero, estimated as the difference in outcomes comparing the reference level and the other level or levels of the factor. This will be presented, for example, like the output shown following, for a Generalized Linear Model (i.e., a logistic regression) analysis of the effect of Orthography condition, ignoring the random effects:\n\nsummary(glm(Score ~ Orthography, family = \"binomial\", data = long.orth))\n\n\nCall:\nglm(formula = Score ~ Orthography, family = \"binomial\", data = long.orth)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        -1.35879    0.09871 -13.765  &lt; 2e-16 ***\nOrthographypresent  0.52951    0.13124   4.035 5.47e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1431.9  on 1262  degrees of freedom\nResidual deviance: 1415.4  on 1261  degrees of freedom\nAIC: 1419.4\n\nNumber of Fisher Scoring iterations: 4\n\n\nYou can see that you have an estimate, in the summary, of the effect of orthographic condition shown as:\nOrthographypresent  0.52951\nThis model (and default coding) gives us an estimate of how the log odds of a child getting a response correct changes if we compare the responses in the absent condition (here, treated as the baseline or reference level) with responses in the present condition.\n\n\n\n\n\n\nTip\n\n\n\nR tells us about the estimate by adding the name of the factor level that is not the reference level, here, present to the name of the variable Orthography whose effect is being estimated.\n\n\nWe can see that the log odds of a correct response increase by \\(0.52951\\) when the orthography (visual word form or spelling) of a word is present during learning trials.\nHowever, as Dale Barr explains, it is better not to use R’s default dummy coding scheme if we are analyzing data where the data come from a study involving two or more factors, and where we want to estimate not just the main effects of the factors but also the effect of the interaction between the factors.\nIn our analyses, we want the coding that allows us to get estimates of the main effects of factors, and of the interaction effects, somewhat like what we would get from an ANOVA. This requires us to use effect coding.\nWe can code whether a response was recorded in the absent or present condition using numbers. In dummy coding, for any observation, we would use a column of zeroes or ones to code condition: i.e., absent (0) or present (1). In effect coding, for any observation, we would use a column of ones or minus ones to code condition: i.e., absent (-1) or present (1). (With a factor with more than two levels, we would use more than one column to do the coding: the number of columns we would use would equal the number of factor condition levels minus one.) In effect coding, observations coded -1 are in the reference level.\nWith effect coding, the constant (i.e., the intercept for our model) is equal to the grand mean of all the observed responses. And the coefficient of each of the effect variables is equal to the difference between the mean of the group coded 1 and the grand mean.\nYou can read more about effect coding here or here.\n\n\n\nWe follow recommendations to use sum contrast coding for the experimental factors. Further, to make interpretation easier, we want the coding to work so that for both orthography and presentation conditions, doing something is the “high” level in the factor – hence:\n\nOrthography, absent (-1) vs. present (+1)\nInstructions, incidental (-1) vs. explicit (+1)\nTime, test time 1 (-1) vs. time 2 (+1)\n\nWe use a modified version of the contr.sum() function (provided in the {memisc} library) that allows us to define the base or reference level for the factor manually (see documentation).\n\nlibrary(memisc)\n\n\n\n\n\n\n\nTip\n\n\n\nWe sometimes see that we cannot appear to load library(memisc) and library(tidyverse) at the same time without getting weird warnings.\n\nI would load library(memisc) after I have loaded library(tidyverse)\nand maybe then unload it afterwards: just click on the button next to the package or library name in R-Studio to detach the library (i.e., stop it from being available in the R session).\n\n\n\nIn the following sequence, I first check how R codes the levels of each factor by default, I then change the coding, and check that the change gets me what I want.\nWe want effects coding for the orthography condition factor, with orthography condition coded as -1, +1. Check the coding.\n\ncontrasts(long.orth$Orthography)\n\n        present\nabsent        0\npresent       1\n\n\nYou can see that Orthography condition is initially coded, by default, using dummy coding: absent (0); present (1). We want to change the coding, then check that we have got what we want.\n\ncontrasts(long.orth$Orthography) &lt;- contr.sum(2, base = 1)\ncontrasts(long.orth$Orthography)\n\n         2\nabsent  -1\npresent  1\n\n\nWe want effects coding for the presentation condition factor, with presentation condition coded as -1, +1. Check the coding.\n\ncontrasts(long.orth$Instructions)\n\n           incidental\nexplicit            0\nincidental          1\n\n\nChange it.\n\ncontrasts(long.orth$Instructions) &lt;- contr.sum(2, base = 2)\ncontrasts(long.orth$Instructions)\n\n            1\nexplicit    1\nincidental -1\n\n\nWe want effects coding for the Time factor, with Time coded as -1, +1 Check the coding.\n\ncontrasts(long.orth$Time)\n\n  2\n1 0\n2 1\n\n\nChange it.\n\ncontrasts(long.orth$Time) &lt;- contr.sum(2, base = 1)\ncontrasts(long.orth$Time)\n\n   2\n1 -1\n2  1\n\n\nIn these chunks of code, I use contr.sum(a, base = b) to do the coding, where:\n\na is the number of levels in a factor (replace a with the right number)\nb tells R which level to use as the baseline or reference level (replace b with the right number).\n\nI usually need to check the coding before and after I specify it.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 19. Conceptual introduction to Generalized Linear Mixed-effects Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/04-glmm.html#sec-glmm-categorical-outcomes",
    "href": "PSYC412/part2/04-glmm.html#sec-glmm-categorical-outcomes",
    "title": "Week 19. Conceptual introduction to Generalized Linear Mixed-effects Models",
    "section": "",
    "text": "We often need to analyze outcome or dependent variables which comprise observations of responses that are discrete or categorical. We need to learn to recognize research contexts that require GLMMs. Categorical outcome variables can include any of the following:\n\nThe accuracy of each of a series of responses in some task, e.g., whether a recorded response is correct or incorrect.\nThe location of a recorded eye movement, e.g., a fixation to the left or to the right visual field.\nThe membership of one group out of two possible groups, e.g., is a participant impaired or unimpaired?\nThe membership of one group out of multiple possible groups, e.g., is a participant a member of one out of some number of groups, say, a member of a religious or ethnic group?\nResponses that can be coded in terms of ordered categories, e.g., a response on a (Likert) ratings scale.\nOutcomes like the frequency of occurrence of an event, e.g., how many arrests are made at a particular city location.\n\nIn this chapter, we will analyze accuracy data: where the outcome variable consists of responses observed in a behavioural task, the accuracy of responses was recorded, and responses could either be correct or incorrect. The accuracy of a response is, here, coded under a binary or dichotomous classification though we can imagine situations when a response is coded in multiple different ways.\nThose interested in analyzing outcome data from ratings scales, that is, ordered categorical outcome variables, often called ordinal data, may wish to read about ordinal regression analyses, which you can do in R using functions from the {ordinal} library.\n\nWe will take a closer look at ordinal data analyses in a following week.\n\nThose interested in analyzing outcome data composed of counts may wish to read about poisson regression analyses in Gelman & Hill (2007a).\nIt will be apparent in our discussion that researchers have used, and will continue to use, a number of ‘traditional’ methods to analyze categorical outcome variables when really they should be using GLMMs. We will talk about these alternatives, next, so that you recognize what is being done when you read research articles. Critically, we will discuss the limitations of such methods because these limitations explain why we bother to learn about GLMMs.\n\n\nIf you want to analyze data from a study where responses can be either correct or incorrect but not both (and not anything else), then your outcome variable is categorical, and your analysis approach ought to respect that. However, if you read enough psychological research articles then you will see many reports of data analyses in which the researchers collected data on the accuracy of responses but then present the results of analyses that ignored the binary or dichotomous nature of accuracy. We often see response accuracy analyzed using an approach that looks something like the following:\n\nThe accuracy of responses (correct vs. incorrect) is counted, e.g., as the number of correct responses or the number of errors.\nThe percentage, or the proportion, of responses that are correct or incorrect is calculated, for each participant, for each level of each experimental condition or factor.\nThe percentage or proportion values are then entered as the outcome or dependent variable in ANOVA or t-test or linear model (multiple regression) analyses of response accuracy.\n\nYou will see many reports of ANOVA or t-test or linear model analyses of accuracy.\n\n\n\n\n\n\nTip\n\n\n\nWhy can’t we follow these examples, and save ourselves the effort of learning how to use GLMMs?\n\nThe reason is that these analyses are, at best, approximations to more appropriate methods.\nTheir results can be expected to be questionable, or misleading, for reasons that we discuss next.\n\n\n\n\n\nTo illustrate the problems associated with using traditional analysis methods (like ANOVA or multiple regression), when working with accuracy as an outcome, we start by looking at data from an artificial vocabulary learning study (reported by Monaghan et al., 2015). Monaghan et al. (2015) recorded responses made by participants to stimuli in a test where the response was correct (coded 1) or incorrect (coded 0). In our study, we directly compared learning of noun-object pairings, verb-motion pairings, and learning of both noun and verb pairings simultaneously, using a cross-situational learning task. (Those interested in this data-set can read more about it at the online repository associated with this chapter.) The data will have a multilevel structure because you will have multiple responses recorded for each person, and for each stimulus. But what concerns us is that if you attempt to use a linear model to analyze the effects of the experimental variables then you will see some paradoxical results that are easily demonstrated.\nLet’s imagine that we wish to estimate the effects of experimental variables like learning condition: learning trial block (1-12); or vocabulary condition (noun-only, noun-verb, verb-only). We can calculate the proportion of responses correct made by each person for each condition and learning trial block. We can then plot the regression best fit lines indicating how proportion of responses correct varies by person and condition. Figure 1 shows the results.\nLook at where the best fit lines go.\n\n\n\n\n\n\n\n\nFigure 1: Monaghan et al. (2015) artificial word learning study: plot showing the proportion of responses correct for each participant, in each of 12 blocks of 24 learning trials, in each learning condition; each grey line shows the linear model prediction of the proportion correct, for each person, by learning block, in each condition; black lines show the average prediction of the proportion correct, by learning block, in each condition. The position of points has been jittered.\n\n\n\n\n\nFigure 1 shows how variation in the outcome, here, the proportion of responses that are correct, is bounded between the y-axis limits of 0 and 1 while the best fit lines exceed those limits. Clearly, if you consider the accuracy of a person’s responses in any set of trials, for any condition in an experiment, the proportion of responses that can be correct can vary only between 0 (no responses are correct) and 1 (all responses are correct). There are no inbuilt or intrinsic limits to the proportion of responses that a linear model can predict would be correct. According to linear model predictions, if you follow the best fit lines in Figure 1 then there are conditions, or there are participants, in which the proportion of a person’s responses that could be correct will be greater than 1. That is impossible.\n\n\n\nThe other fundamental problem with using analysis approaches like ANOVA or regression to analyze categorical outcomes like accuracy is that we cannot assume that the variance in accuracy of responses will be homogenous across different experimental conditions.\nThe logic of the problem can be set out as follows:\n\nGiven a binary outcome, e.g., where the response is correct or incorrect, for every trial, there is a probability \\(p\\) that the response is correct.\nThe variance of the proportion of trials (per condition) with correct responses is dependent on \\(p\\), and it is greater when \\(p \\sim .5\\), the probability that a response will be correct.\n\nJaeger (2008) (p. 3) then explains the problem like this. If the probability of a binomially distributed outcome like response accuracy differs between two conditions (call them conditions 1 and 2), the variances will only be identical if \\(p1\\) (the proportion of correct responses in condition 1) and p2 (the proportion of correct responses in condition 1) are equally distant from 0.5 (e.g. \\(p1 = .4\\) and \\(p2 = .6\\)). The bigger the difference in distance from 0.5, comparing the conditions, the less similar the variances will be.\nSample proportions between 0.3 and 0.7 are considered close enough to 0.5 to assume homogeneous variances (Agresti, 2002). Unfortunately, we usually cannot determine a priori the range of sample proportions in our experiment.\nIn general, variances in two binomially distributed conditions will not be homogeneous but, as you will recall, in both ANOVA and regression analysis, we assume homogeneity of variance in the outcome variable when we compare the effect of differences (in the mean outcome) between the different levels of a factor. This means that if we design a study in which the outcome variable is the accuracy of responses in different experimental conditions and we plan to use ANOVA or regression to estimate the effect of variation in experimental conditions on response accuracy then unless we get lucky our estimation of the experimental effect will take place under circumstances in which the application of the analysis method (ANOVA or regression) and thus the analysis results will be invalid.\n\n\n\nThe application of traditional (parametric) analysis methods like ANOVA or regression to categorical outcome variables like accuracy is very common in the psychological literature. The problem is that these approaches can give us misleading results.\n\nLinear models assume outcomes are unbounded so allow predictions that are impossible when outcomes are, in fact, bounded as is the case for accuracy or other categorical variables.\nLinear models assume homogeneity of variance but that is unlikely and anyway cannot be predicted in advance when outcomes are categorical variables.\nIf we are interested in the effect of an interaction between two effects, using ANOVA or linear models on accuracy (proportions of responses correct) can tell you, wrongly, that the interaction is significant.\n\nTraditionally, researchers have recognized the limitations attached to using methods like ANOVA or regression to analyze categorical outcomes like accuracy and have applied remedies, transforming the outcome variables, e.g. the arcsine root transformation, to render them ‘more normal’. However, as Jaeger (2008) demonstrates, the remedies like the arcsine transformation that have traditionally been applied are often not likely to succeed. Jaeger (2008) completed a comparison of the results of analyses of accuracy data, where outcomes are raw values for the proportions of correct responses, or arcsine transformed values for proportions correct. His comparison demonstrated that the traditional techniques will show either that effects are significant when they are not or that effects are not significant when they are.\n\n\n\n\nWhat we need, then, is a method that allows us to analyze categorical outcomes. We find the appropriate method in Generalized Linear Models, and in Generalized Linear Mixed-effects Models for repeated measures or multilevel structured data. We can understand these methods, as their name suggests, as generalizations of linear models or linear mixed-effects models: generalizations that allow for the categorical nature of some outcome data.\nYou can understand how Generalized Linear Mixed-effects Models work by seeing them as analyses of categorical outcome data like accuracy where the outcome variable is transformed, as I explain next (see Baguley (2012) for a nice clear explanation, which I summarize here).\nOur problems begin with the need to estimate effects on a bounded outcome like accuracy with a linear model which, as we have seen, will yield unbounded predictions.\nThe logistic transformation takes \\(p\\) the probability of an event with two possible outcomes, and turns it into a logit: the natural logarithm of the odds of the event. The effect of this transformation is to turn a discrete binary bounded outcome into a continuous unbounded outcome.\n\nTransforming a probability to odds \\(o = \\frac{p}{1-p}\\) is a partial solution.\n\n\nOdds are, for example, the ratio of the probability of the occurrence of an event compared to the probability of the non-occurrence of an event, or, in terms of a response accuracy variable, the ratio of the probability of the response being correct compared to the probability of the response being incorrect.\nAnd odds are continuous numeric quantities that are scaled from zero to infinity.\nYou can see how this works if you run the calculations using the equation \\(o = \\frac{p}{1-p}\\) in R as odds &lt;- p/(1-p): replacing p with various numbers (e.g. p = 0.1, 0.01, 0.001).\n\n\nWe can then use the (natural) logarithm of the odds \\(logit = ln\\frac{p}{1-p}\\) because using the logarithm removes the boundary at zero because log odds range from negative to positive infinity.\n\n\nYou can see how this works if you run the calculations using the equation \\(logit = ln\\frac{p}{1-p}\\) in R as logit &lt;- log(p/(1-p)): replacing p with smaller and smaller numbers (e.g. p = 0.1, 0.01, 0.001) gets you increasing negative log odds.\n\nWhen we model the log odds (logit) that a response will be correct, the model is called a logistic regression or logistic model. We can think of logistic models as working like linear models with log-odds outcomes.\n\\[\nln\\frac{p}{1-p} = logitp = \\beta_0 + \\beta_1X_1 \\dots\n\\]\nWe can describe the predicted log odds of a response of one type as the linear sum of the estimated effects of the included predictor variables. In a logistic regression, the predictors have an additive relationship with respect to the log odds outcome, just like in an ordinary linear model. The log odds range from negative to positive infinity; logit of 0 corresponds to proportion of .5.\nBaguley (2012) notes that is it advantageous that odds and probabilities are both directly interpretable. We are used to seeing and thinking in everyday life about the chances that some event will occur.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 19. Conceptual introduction to Generalized Linear Mixed-effects Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/04-glmm.html#sec-glmm-working-models",
    "href": "PSYC412/part2/04-glmm.html#sec-glmm-working-models",
    "title": "Week 19. Conceptual introduction to Generalized Linear Mixed-effects Models",
    "section": "",
    "text": "A small change in R lmer code allows us to extend what we know about linear mixed-effects models to conduct Generalized Linear Mixed-effects Models. We change the function call from lmer() to glmer(). However, we have to make some other changes, as we detail in the following sections.\nWe will be examining the impact of the experimental effects, that is, the fixed effects associated with the impacts on the outcome Score (accuracy of response in the word spelling test) associated with the following comparisons:\n\nTime: time 1 versus time 2\nOrthography: present versus absent conditions\nInstructions: explicit versus incidental conditions\nStandardized spelling-sound consistency\nInteraction between the effects of Orthography and Instructions\nInteraction between the effects of Orthography and consistency\n\nWe will begin by keeping the random effects structure simple.\n\n\nIn our first model, we will specify just random effects of participants and items on intercepts.\n\nlong.orth.min.glmer &lt;- glmer(Score ~ \n                               \n                          Time + Orthography + Instructions + zConsistency_H + \n                               \n                          Orthography:Instructions +\n                               \n                          Orthography:zConsistency_H +\n                               \n                          (1 | Participant) + \n                               \n                          (1 | Word),\n                             \n                    family = \"binomial\", \n                    glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)),\n                             \n                    data = long.orth)\n\nsummary(long.orth.min.glmer)\n\nThe code works as follows.\nFirst, we have a chunk of code mostly similar to what we have done before, but changing the function.\n\nglmer() the function name changes because now we want a generalized linear mixed-effects model of accuracy.\n\nThe model specification includes information about fixed effects and about random effects.\n\nWith (1 | Participant) we include random effects of participants on on intercepts.\nWith (1 | Word) we include random effects of stimulus on on intercepts.\n\nSecond, we have the bit that is specific to generalized models.\n\nfamily = \"binomial\" is entered because accuracy is a binary outcome variable (correct, incorrect) so we assume a binomial probability distribution.\n\nWe then specify:\n\nglmerControl(optimizer=\"bobyqa\", ...) to change the underlying mathematical engine (the optimizer) to cope with greater model complexity,\nand we allow the model fitting functions to take longer to find estimates with optCtrl=list(maxfun=2e5).\n\nNotice how we specify the fixed effects. We want glmer() to estimate “main effects and interactions” that we hypothesized.\nWe specify the main effects with:\n\nTime + Orthography + Instructions + zConsistency_H +\n\nWe specify the interaction effects with:\n\nOrthography:Instructions +\n                               \nOrthography:zConsistency_H +\n\nWhere we ask for estimates of the fixed effects associated with:\n\nOrthography:Instructions the interaction between the effects of Orthography and Instructions;\nOrthography:zConsistency-H the interaction between the effects of Orthography and consistency.\n\n\n\nThere are two forms of notation we can use to specify interactions in R. The simplest form is to use something like this:\n\nOrthography*Instructions\n\nThis will get you estimates of:\n\nThe effect of Orthography: present versus absent conditions.\nThe effect of Instructions: explicit versus incidental conditions.\nAnd the effect of Orthography x Instructions: the interaction between the effects of Orthography and Instructions.\n\nSo, in general, if you want estimates of the effects of variables A, B and the interaction A x B, then you write A*B.\nWe can also use the colon symbol to specify only the interaction, i.e., ignoring main effects, so if you specify A:B then you will get an estimate of the interaction A x B but not the effects A, B.\nWith the coding:\n\nScore ~ Orthography + Instructions + Orthography:Instructions\n\nI would be making explicit that I want estimates for the effects of Orthography, Instruction and the interaction between the effects of Orthography and Instructions.\n\n\n\n\nIf you run the model code, you will get the results shown in the output.\n\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (1 |  \n    Participant) + (1 | Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1040.4    1086.7    -511.2    1022.4      1254 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.0994 -0.4083 -0.2018  0.2019  7.4940 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n Participant (Intercept) 1.840    1.357   \n Word        (Intercept) 2.224    1.491   \nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.878464   0.443942  -4.231 2.32e-05 ***\nTime2                        0.050136   0.083325   0.602    0.547    \nOrthography2                 0.455009   0.086813   5.241 1.59e-07 ***\nInstructions1                0.042290   0.230335   0.184    0.854    \nzConsistency_H              -0.618092   0.384002  -1.610    0.107    \nOrthography2:Instructions1   0.005786   0.083187   0.070    0.945    \nOrthography2:zConsistency_H  0.014611   0.083105   0.176    0.860    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.008                                   \nOrthogrphy2 -0.059  0.008                            \nInstructns1  0.014  0.025  0.001                     \nzCnsstncy_H  0.016 -0.002 -0.029 -0.001              \nOrthgrp2:I1 -0.002 -0.001  0.049 -0.045  0.000       \nOrthgr2:C_H -0.027  0.001  0.179  0.000 -0.035 -0.007\n\n\nThe output from the model summary first gives us information about the model.\n\nFirst, we see information about the function used to fit the model, and the model object created by the lmer() function call.\nThen, we see the model formula including main effects Score ~ Time + Orthography + Instructions + zConsistencyH,\nAs well as the interactions Orthography:Instructions + Orthography:zConsistencyH,\nAnd the random effects (1 | Participant) + (1 |Word).\nThen we see information about the model algorithm\nThen we see model fit statistics, including AIC BIC logLik\nThen we see information about the distribution of residuals\n\nWe then see information listed under Random Effects: this is where you can see information about the error variance terms estimated by the model.\n\nThe information is listed in four columns: 1. Groups; 2. Name; 3. Variance; and 4. Std.Dev.\n\nWe see in the Random Effects section, the variances associated with:\n\nThe random differences between the average intercept (over all data) and the intercept for each participant;\nThe random differences between the average intercept (over all data) and the intercept for responses to each word stimulus.\n\n\nLastly, just as for linear models, we see estimates of the coefficients of the fixed effects, the intercept and the slopes of the experimental variables.\n\nNote that we get p-values (Pr(&gt;|z|)) for what are called Wald null hypothesis significance tests on the coefficients.\nWe can see that one effect is significant. The estimate for the effect of the presence compared to the absence of Orthography is 0.455009. The positive coefficient tells us that the log odds that a response will be correct is higher when Orthography is present compared to when it is absent.\nWe can also see an effect of consistency that can (conventionally) be considered to be marginal or near-significant. The estimate for the effect of zConsistency_H is -0.618093 indicating that the log odds of a response being correct decrease for unit increase in the standardized H consistency measure.\n\n\nAs we discussed in the last chapter, we can conduct null hypothesis significance tests by comparing models that differ in the presence or absence of a fixed effect or a random effect, using the Likelihood Ratio Test. In the results output for a GLMM by the glmer() function, you can see that alongside the estimates of the coefficients (and standard error) for the fixed effects we also have z and p-values. Wald z tests for GLMMs test the null hypothesis of no effect by comparing the effect estimate with their standard error, and comparing the resulting test statistic to zero (Bolker et al., 2009).\n\n\n\n\nWe usually want to do more than just report whether experimental effects are or are not significant. It helps us to present and interpret the estimates from a model if we can visualize the model prediction. There are a variety of tools that help us to do this.\n\n\nWe can use the plot_model function from the {sjPlot} library. The following sequence of code takes information from the model we have just run, then generates model predictions, of change in the probablity of a correct response (Score) for different levels of the Orthography factor and the consistency variable. I chose these variables because they are the significant or near-significant effects.\n\nporth &lt;- plot_model(long.orth.min.glmer,\n           type=\"pred\",\n           terms = \"Orthography\") +\n         theme_bw() +\n         ggtitle(\"Predicted probability\") +\n         ylim(0,1)\n\npzconsH &lt;- plot_model(long.orth.min.glmer,\n           type=\"pred\",\n           terms = \"zConsistency_H\") +\n         theme_bw() +\n         ggtitle(\"Predicted probability\") +\n         ylim(0,1)\n\ngrid.arrange(porth, pzconsH,\n            ncol=2)\n\n\n\n\n\n\n\nFigure 2: Effect of orthography condition (present versus absent) on probability of a response being correct\n\n\n\n\n\nThe plots in Figure 2 show clearly how the probability of a correct response is greater for the conditions where Orthography had been present (versus absent) during the word learning phase of the study. We can also see a trend such that the probability of a response being correct decreases as the (in-)consistency of a target word tends to increase.\nTo produce this plot, you will need to install the {sjPlot} library first, and then run the command library(sjPlot) before creating your plot.\nNotice:\n\nplot_model() produces the plot\nplot_modellong.orth.min.glmer) specifies that the plot should be produced given information about the previously fitted model long.orth.min.glmer\ntype = \"pred\" tells R that you want a plot showing the model predictions, of the effect of, e.g., Orthography: condition\n\nThe function outputs an object whose appearance can be edited as a ggplot object.\nMore information can be found here or here about the {sjPlot} library and plotting model estimates of effects.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 19. Conceptual introduction to Generalized Linear Mixed-effects Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/04-glmm.html#sec-glmm-random-effects",
    "href": "PSYC412/part2/04-glmm.html#sec-glmm-random-effects",
    "title": "Week 19. Conceptual introduction to Generalized Linear Mixed-effects Models",
    "section": "",
    "text": "So far, we have been considering the results of a random intercepts model in which we take into account the random effects of participants and stimulus word differences on intercepts. We have ignored the possibility that the slopes of the experimental variables might vary between participants or between words.\n\n\n\n\n\n\nTip\n\n\n\nWe now need to examine the question: What random effects should we specify?\n\n\nI must warn you that the question and the answer are complex but the coding is quite simple, and the approaches you can take to address the question are, now, quite well recognized by the psychological community. In other words, the community recognizes the nature of the problem, and recognizes the methods you can potentially follow to solve the problem.\nThe complexity, for us, lies not in the mathematics: the coding is simple and the glmer() function does the work.\nI think the complexity lies, firstly, in how we have to think about the study design, what gets manipulated or allowed to vary. I find it very helpful to sketch out, by hand, what the study design means in relation to who does what in an experiment.\nThe complexity lies, secondly, and in how we have to translate our understanding of the study design to a specification of random effects. We can master that aspect of the challenge through practice.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe question is:\n\nSo, what random effects should we include?\n\n\n\nIf you go back to the description of the study design, then you will be able to see that a number of possibilities follow, theoretically, from the design.\nA currently influential set of recommendations Matuschek et al. (2017) has been labeled Keep it maximal.\nIn this approach, if you are testing effects manipulated according to a pre-specified design then you should:\n\nTest random intercepts – due to random differences between subjects or between items (or other sample grouping variables);\nTest random slopes for all within-subjects or within-items (or other) fixed effects.\n\nThis means that specification of the random effects structure requires two sets of information – corresponding to the answers to two questions:\n\nWhat are the fixed effects?\nWhat are the grouping variables?\n\nAs we have seen, we can rephrase the second question in terms: did you test multiple participants using multiple stimuli (e.g., words …) or did you test participants under multiple different conditions (e.g., levels of experimental condition factors)?\nOur answers to these questions then dictate potentially how we specify the random effects structure for our model.\nWe can consider that we should certainly include a random effect of each grouping variable (e.g., participants, stimulus word) on intercepts. We can reflect that we should also include a random effect of a grouping variable e.g. participant on the slope of each variable that was manipulated within the units of that variable.\nWhen we specify models, we should remember that by default if you specify (1 + something | participant) then you are specifying that you want to take into account:\n\nvariance due to the random differences between participants in intercepts (1 ...| participant),\nplus variance due to the random differences between participants in slopes (... something | participant),\nplus the covariance (or correlation) between the random intercepts and the random slopes.\n\nYou will have become familiar with the practice of referring to effects as within-subjects or between-subjects previously, in working with ANOVA. Here, whether an effect is within-subjects or within-items or not has relevance to whether we can or should specify a random effect of subjects or of items on the slope of a fixed effect.\nIn deciding what random effects we should specify, we need to think about what response data we have recorded, for each of the experimental variables, given the study design. This is because if we want to specify a random effect of participants (or stimulus words) on the slope of an experimental condition then we need to have data, for each person, on their responses under all levels of the condition.\nIf we want to estimate the effect of the experimental manipulation of learning condition, for example, the impact of the presence of orthography, for a person, we need to have data for both levels of the condition (orthography absent and orthography present) for that person. If you think about it, we cannot estimate the slope of the effect of the presence of orthography without response data recorded under both the orthography absent condition and the orthography present condition. If we can estimate the slope of the effect then we can estimate how the slope of the effect deviates between participants.\nWe can spell out how the experimental conditions were manipulated for the example study, as follows. (Writing out this kind of account, for yourself, will be helpful perhaps when you are planning an analysis and have to work out what the random effects could be.)\n\nThe effect of Orthography was manipulated within participants and within stimulus words. This is because the presence of orthography (orthography absent versus orthography present) was manipulated so that we have data about test responses to each word under both Orthography conditions, and data about responses from each child under both conditions.\nThe effect of Instructions was manipulated between participants. This is because Instructions (incidental vs. explicit) were manipulated between participants such that children in the explicit condition were alerted to the presence of orthography whereas children in the incidental condition were not.\nWe can say that the effects of Orthography and of Instructions are both manipulated within words. Items were counterbalanced across instruction and orthography conditions, with all words appearing in both orthography conditions for approximately the same number of children within the explicit and incidental groups.\nThe effect of spelling-sound consistency varies between words because different words have different consistency values but the effect of consistency varies within} participants because we have response data for each participant for their responses to words of different levels of consistency.\nWe recorded responses for all participants and all words so we can say that the effect of Time (test time 1 versus time 2) can also be understood to vary within} both participants and words. This means that, for each person’s response to each word on which they are tested, we have response data recorded at both test times.\n\nThese considerations suggests that we should specify a model with the random effects:\n\nThe random effects of participants on intercepts, and on the slopes of the effects of Time, Orthography and spelling-sound consistency, as well as all corresponding covariances.\nThe random effects of stimulus words on intercepts, and on the slopes of the effects of Time, Orthography and Instructions, as well as all corresponding covariances.\n\nThis is simple to do using the code shown following.\n\nlong.orth.max.glmer &lt;- glmer(Score ~ \n                           \n                      Time + Orthography + Instructions + zConsistency_H + \n                           \n                      Orthography:Instructions +\n                           \n                      Orthography:zConsistency_H +\n                           \n                      (Time + Orthography + zConsistency_H + 1 | Participant) + \n                           \n                      (Time + Orthography + Instructions + 1 |Word),\n                         \n                      family = \"binomial\",\n                      glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)),\n                         \n                         data = long.orth)\n\nsummary(long.orth.max.glmer)\n\nWhere we have the critical terms:\n\n(Time + Orthography + zConsistency_H + 1 | Participant) to account for the random effects of participants on intercepts, and on the slopes of the effects of Time, Orthography and spelling-sound consistency, as well as all corresponding covariances.\n(Time + Orthography + Instructions + 1 |Word) to account for the random effects of stimulus words on intercepts, and on the slopes of the effects of Time, Orthography and Instructions, as well as all corresponding covariances.\n\nIf you run this code, however, you will see that you get warnings along with your estimates.\n\n\nboundary (singular) fit: see help('isSingular')\n\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (Time +  \n    Orthography + zConsistency_H + 1 | Participant) + (Time +  \n    Orthography + Instructions + 1 | Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1053.6    1192.4    -499.8     999.6      1236 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.1014 -0.4027 -0.1723  0.2037  7.0331 \n\nRandom effects:\n Groups      Name           Variance Std.Dev. Corr             \n Participant (Intercept)    2.043127 1.42938                   \n             Time2          0.005675 0.07533   0.62            \n             Orthography2   0.079980 0.28281   0.78 -0.01      \n             zConsistency_H 0.065576 0.25608   0.49  0.99 -0.16\n Word        (Intercept)    2.793448 1.67136                   \n             Time2          0.046736 0.21618   0.14            \n             Orthography2   0.093740 0.30617  -0.68 -0.81      \n             Instructions1  0.212706 0.46120  -0.74 -0.05  0.38\nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -2.10099    0.49589  -4.237 2.27e-05 ***\nTime2                        0.02077    0.12285   0.169 0.865740    \nOrthography2                 0.52480    0.15496   3.387 0.000708 ***\nInstructions1                0.24467    0.27281   0.897 0.369805    \nzConsistency_H              -0.67818    0.36311  -1.868 0.061803 .  \nOrthography2:Instructions1  -0.05133    0.10004  -0.513 0.607907    \nOrthography2:zConsistency_H  0.05850    0.11634   0.503 0.615064    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.097                                   \nOrthogrphy2 -0.280 -0.187                            \nInstructns1 -0.278  0.023  0.109                     \nzCnsstncy_H  0.062  0.005 -0.064  0.120              \nOrthgrp2:I1  0.024  0.005 -0.071  0.212 -0.065       \nOrthgr2:C_H -0.062  0.049  0.246 -0.031 -0.440  0.001\noptimizer (bobyqa) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nNotice, especially, the warning:\n\nboundary (singular) fit: see ?isSingular\n\nIf you put the warning message text into a search engine, then you will get directed to a variety of discussions about what they mean and what you should do about them.\nA highly instructive blog post by (I think) Ben Bolker provides some very useful advice here.\n\nCheck singularity\nIf the fit is singular or near-singular, there might be a higher chance of a false positive (we’re not necessarily screening out gradient and Hessian checking on singular directions properly); a higher chance that the model has actually misconverged (because the optimization problem is difficult on the boundary); and a reasonable argument that the random effects model should be simplified.\nThe definition of singularity is that some of the constrained parameters of the random effects theta parameters are on the boundary (equal to zero, or very very close to zero …)\n\n(Emphasis added.)\nI am going to take his advice and simplify the random effects part of the model. We know that the random intercepts model converges fine and now we know that the maximal model does not. Thus, our task is now to identify a model that includes random effects of participants or items on slopes and still converges without warnings.\n\n\n\nI am just going to assume we need both random effects of subjects and of items on intercepts so I focus on random slopes here. (This assumption may not always be true but is often useful.)\nWe can fit a series of models as follows. Note that I will not show the results for every model, to save space, but you should run the code to see what happens. Look out for convergence or singularity warnings, where they appear.\n\n\nIn the first model, we have just random effects of participants or items on intercepts. This is where we started (earlier).\n\nlong.orth.min.glmer &lt;- glmer(Score ~ \n                               \n                        Time + Orthography + Instructions + zConsistency_H + \n                               \n                        Orthography:Instructions +\n                               \n                        Orthography:zConsistency_H +\n                               \n                        (1 | Participant) + \n                               \n                        (1 | Word),\n                             \n                      family = \"binomial\", \n                      glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)),\n                             \n                      data = long.orth)\n\nsummary(long.orth.min.glmer)\n\nWe saw that the model converged, and we looked at the results previously.\nWhat next? A simple approach we can take is to see if we can add each fixed effect to be included in our random effects terms, one effect at a time.\n\n\n\nIn our second model, we change the random effects terms so that we can account for the random effects of participants and of items on intercepts as well as on the slopes of the Orthography effect. (The Orthography effect is both within-subjects and within-items.)\n\nlong.orth.2.glmer &lt;- glmer(Score ~ \n                             \n                      Time + Orthography + Instructions + zConsistency_H + \n                             \n                      Orthography:Instructions +\n                             \n                      Orthography:zConsistency_H +\n                             \n                      (dummy(Orthography) + 1 || Participant) + \n                             \n                      (dummy(Orthography) + 1 || Word),\n                           \n                      family = \"binomial\", \n                      glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)),\n                           \n                      data = long.orth)\n\nsummary(long.orth.2.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) +  \n    1 || Participant) + (dummy(Orthography) + 1 || Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1041.0    1097.6    -509.5    1019.0      1252 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9373 -0.4132 -0.1952  0.1826  6.9614 \n\nRandom effects:\n Groups        Name               Variance Std.Dev.\n Participant   (Intercept)        1.57092  1.2534  \n Participant.1 dummy(Orthography) 0.57624  0.7591  \n Word          (Intercept)        2.36284  1.5372  \n Word.1        dummy(Orthography) 0.02101  0.1450  \nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.9025027  0.4512443  -4.216 2.49e-05 ***\nTime2                        0.0501974  0.0837186   0.600 0.548775    \nOrthography2                 0.4135727  0.1120792   3.690 0.000224 ***\nInstructions1                0.0455920  0.2234615   0.204 0.838333    \nzConsistency_H              -0.6254414  0.3958971  -1.580 0.114151    \nOrthography2:Instructions1   0.0019343  0.1038694   0.019 0.985142    \nOrthography2:zConsistency_H -0.0007112  0.0877740  -0.008 0.993535    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.007                                   \nOrthogrphy2  0.034  0.007                            \nInstructns1  0.013  0.025  0.007                     \nzCnsstncy_H  0.017 -0.002 -0.024 -0.002              \nOrthgrp2:I1  0.003  0.001  0.043  0.126  0.000       \nOrthgr2:C_H -0.029  0.001  0.182  0.001 -0.024 -0.011\n\n\nThis model converges without warnings.\nNotice that we specify that we || do not want random covariances; we are keeping things simple in each step.\nNote the use of dummy() inside the random effects terms. The ‘dummy’ is a mis-leading name; we are not talking about dummy coding (as above). Here, the dummy() stops R from mis-interpreting the requirement to estimate the effect of the differences between category levels, within random effects.\nThe reason is explained here.\nYou can see what impact it has by specifying, instead, the naked random effect:\n(Orthography + 1 || Participant).\n\n\n\nNext we can add Instructions to take into account random differences between words in the slope of this effect. We show the results for this model as they are instructive.\n\nlong.orth.3.glmer &lt;- glmer(Score ~ \n                             \n                     Time + Orthography + Instructions + zConsistency_H + \n                             \n                     Orthography:Instructions +\n                             \n                     Orthography:zConsistency_H +\n                             \n                     (dummy(Orthography) + 1 || Participant) + \n                             \n                     (dummy(Orthography) + dummy(Instructions) + 1 || Word),\n                           \n                     family = \"binomial\", \n                     glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)),\n                           \n                     data = long.orth)\n\nsummary(long.orth.3.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) +  \n    1 || Participant) + (dummy(Orthography) + dummy(Instructions) +  \n    1 || Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1036.5    1098.2    -506.2    1012.5      1251 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9951 -0.4068 -0.1920  0.1838  5.9308 \n\nRandom effects:\n Groups        Name                Variance Std.Dev.\n Participant   (Intercept)         1.64393  1.2822  \n Participant.1 dummy(Orthography)  0.55604  0.7457  \n Word          (Intercept)         1.94313  1.3940  \n Word.1        dummy(Orthography)  0.01607  0.1268  \n Word.2        dummy(Instructions) 0.86694  0.9311  \nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.9621261  0.4400649  -4.459 8.25e-06 ***\nTime2                        0.0507347  0.0843237   0.602  0.54740    \nOrthography2                 0.4263703  0.1114244   3.827  0.00013 ***\nInstructions1                0.1907423  0.2632605   0.725  0.46874    \nzConsistency_H              -0.6270900  0.3669904  -1.709  0.08750 .  \nOrthography2:Instructions1  -0.0265729  0.1048392  -0.253  0.79991    \nOrthography2:zConsistency_H -0.0006305  0.0878823  -0.007  0.99428    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.007                                   \nOrthogrphy2  0.028  0.008                            \nInstructns1 -0.127  0.022  0.019                     \nzCnsstncy_H  0.017 -0.002 -0.026  0.011              \nOrthgrp2:I1  0.013  0.001  0.003  0.068 -0.009       \nOrthgr2:C_H -0.030  0.002  0.182  0.002 -0.026 -0.018\n\n\nThis model also converges without warnings.\nTake a look at the random effects summary. You can see:\n\nParticipant (Intercept) 1.64393 estimated variance due to random effect of participants on intercepts\nParticipant.1 dummy(Orthography) 0.55604 variance due to random effect of participants on the slope of the Orthography effect\nWord (Intercept) 1.94314 variance due to random effect of words on intercepts\nWord.1 dummy(Orthography) 0.01607 variance due to random effect of words on the slope of the Orthography effect\nWord.2 dummy(Instructions) 0.86694 variance due to random effect of words on the slope of the Instructions effect\n\nWe do not see correlations (random effects covariances) because we use the || notation to stop them being estimated. We want to stop them being estimated because we want to see what we gain from adding just the requirement, first, to estimate the variance associated with random effects of participants or words on the slopes of the experimental variables.\nAlso, we can suspect that adding the requirement to estimate covariances will blow the model up for two reasons. The maximal model, including random slopes variances and covariances clearly did not converge. Secondly, at least two of the correlations listed in the random effects, for the maximal model, were pretty extreme with Corr \\(=-0.01\\) and \\(=0.99\\); such extreme values (\\(r \\sim \\pm 1\\)) are bad signs; see the discussion in Section 1.10.2.\n\n\n\nIn the following models, because we can see that we can get a model to converge with random effects of participants or items on Orthography, and random effect of participants on Instructions, I am going to keep these random effects in the model. I will check if adding further effects is OK too, in terms of successful convergence. I am going to treat all the following models as variations on a theme, the theme being: can we add anything else to:\n\n(dummy(Orthography) + 1 || Participant) + \n                             \n(dummy(Orthography) + dummy(Instructions) + 1 || Word),\n\n\n\n\nNext we see if we can add zConsistency_H.\n\nlong.orth.4.a.glmer &lt;- glmer(Score ~ \n                             \n                        Time + Orthography + Instructions + zConsistency_H + \n                             \n                        Orthography:Instructions +\n                             \n                        Orthography:zConsistency_H +\n                             \n                        (dummy(Orthography) + zConsistency_H + 1 || Participant) + \n                             \n                        (dummy(Orthography) + dummy(Instructions) + 1 || Word),\n                           \n                        family = \"binomial\", \n                        glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)),\n                           \n                        data = long.orth)\n\nboundary (singular) fit: see help('isSingular')\n\nsummary(long.orth.4.a.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) +  \n    zConsistency_H + 1 || Participant) + (dummy(Orthography) +  \n    dummy(Instructions) + 1 || Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1038.5    1105.3    -506.2    1012.5      1250 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9952 -0.4068 -0.1920  0.1838  5.9309 \n\nRandom effects:\n Groups        Name                Variance  Std.Dev. \n Participant   (Intercept)         1.644e+00 1.282e+00\n Participant.1 dummy(Orthography)  5.560e-01 7.456e-01\n Participant.2 zConsistency_H      2.090e-10 1.446e-05\n Word          (Intercept)         1.943e+00 1.394e+00\n Word.1        dummy(Orthography)  1.604e-02 1.266e-01\n Word.2        dummy(Instructions) 8.669e-01 9.311e-01\nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.9621306  0.4400477  -4.459 8.24e-06 ***\nTime2                        0.0507346  0.0843236   0.602  0.54740    \nOrthography2                 0.4263730  0.1114188   3.827  0.00013 ***\nInstructions1                0.1907330  0.2632583   0.725  0.46875    \nzConsistency_H              -0.6270898  0.3669803  -1.709  0.08749 .  \nOrthography2:Instructions1  -0.0265706  0.1048370  -0.253  0.79992    \nOrthography2:zConsistency_H -0.0006352  0.0878782  -0.007  0.99423    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.007                                   \nOrthogrphy2  0.028  0.008                            \nInstructns1 -0.127  0.022  0.019                     \nzCnsstncy_H  0.017 -0.002 -0.026  0.011              \nOrthgrp2:I1  0.013  0.001  0.003  0.068 -0.009       \nOrthgr2:C_H -0.030  0.002  0.182  0.002 -0.026 -0.018\noptimizer (bobyqa) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nWe see two useful pieces of information when we run this model:\n\nWe get the warning boundary (singular) fit: see ?isSingular that tells us the model algorithm could not converge on effects estimates, given the model we specify, given the data.\nNote, also, we see the random effects variance estimate Participant.2 zConsistency_H 1.259e-10.\n\nThese two things are possibly connected: the singularity warning; and the estimated variance of \\(1.259e-10\\) (i.e. a very very small number) associated with the random effect of participants on the slope of the zConsistency_H. We can expect that the model fitting algorithm is going to have difficulty estimating nothing, or something close to nothing: here, the very very small variance associated with the between-participant differences in the slope of the non-significant effect of word spelling-sound consistency on response accuracy.\n\n\n\nWhat about the random effects of participants or of words on the slope of the effect of Time?\n\nlong.orth.4.b.glmer &lt;- glmer(Score ~ \n                             \n            Time + Orthography + Instructions + zConsistency_H + \n                             \n            Orthography:Instructions +\n                             \n            Orthography:zConsistency_H +\n                             \n            (dummy(Orthography) + dummy(Time) + 1 || Participant) + \n                             \n            (dummy(Orthography) + dummy(Instructions) + dummy(Time) + 1 || Word),\n                           \n            family = \"binomial\", \n            glmerControl(optimizer=\"bobyqa\", optCtrl=list(maxfun=2e5)),\n                           \n            data = long.orth)\n\nboundary (singular) fit: see help('isSingular')\n\nsummary(long.orth.4.b.glmer)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Score ~ Time + Orthography + Instructions + zConsistency_H +  \n    Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) +  \n    dummy(Time) + 1 || Participant) + (dummy(Orthography) + dummy(Instructions) +  \n    dummy(Time) + 1 || Word)\n   Data: long.orth\nControl: glmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e+05))\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n   1040.5    1112.5    -506.2    1012.5      1249 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9952 -0.4068 -0.1920  0.1838  5.9309 \n\nRandom effects:\n Groups        Name                Variance  Std.Dev. \n Participant   (Intercept)         1.644e+00 1.2821917\n Participant.1 dummy(Orthography)  5.560e-01 0.7456314\n Participant.2 dummy(Time)         0.000e+00 0.0000000\n Word          (Intercept)         1.943e+00 1.3939636\n Word.1        dummy(Orthography)  1.604e-02 0.1266475\n Word.2        dummy(Instructions) 8.669e-01 0.9310619\n Word.3        dummy(Time)         1.992e-08 0.0001411\nNumber of obs: 1263, groups:  Participant, 41; Word, 16\n\nFixed effects:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.9621304  0.4400607  -4.459 8.24e-06 ***\nTime2                        0.0507347  0.0843238   0.602  0.54740    \nOrthography2                 0.4263730  0.1114191   3.827  0.00013 ***\nInstructions1                0.1907329  0.2632608   0.725  0.46876    \nzConsistency_H              -0.6270910  0.3669875  -1.709  0.08750 .  \nOrthography2:Instructions1  -0.0265706  0.1048370  -0.253  0.79992    \nOrthography2:zConsistency_H -0.0006352  0.0878785  -0.007  0.99423    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) Time2  Orthg2 Instr1 zCns_H Or2:I1\nTime2        0.007                                   \nOrthogrphy2  0.028  0.008                            \nInstructns1 -0.127  0.022  0.019                     \nzCnsstncy_H  0.017 -0.002 -0.026  0.011              \nOrthgrp2:I1  0.013  0.001  0.003  0.068 -0.009       \nOrthgr2:C_H -0.030  0.002  0.182  0.002 -0.026 -0.018\noptimizer (bobyqa) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nNope.\nWe again see two useful pieces of information when we run this model:\n\nWe get the warning boundary (singular) fit: see ?isSingular that tells us the model algorithm could not converge on effects estimates, given the model we specify, given the data\nNote, also, we see the random effects variance estimate Participant.2 dummy(Time) 0.000e+00.\nAnd we see the variance estimate Word.3 dummy(Time) 2.161e-07.\n\nAgain, we can surmise that a mixed-effects model will get into trouble – and we will see convergence warnings – where we have a fixed effect with little impact (like, here, Time) and we are asking for the estimation of variance associated with random differences in slopes where there may be, in fact, little random variation. Possibly, these two things are connected too: we are perhaps unlikely to see random differences in the slope of the effect of an experimental variable if the effect is at or near zero. Possibly, we may see the effect of an experimental variable which is very very consistent. Think back to the conceptual introduction to multilevel data and the effect associated with the relation between maths and physics scores where there seemed to be little variation between classes in the slope representing the relation.\n\n\n\n\nWe can see that a model has difficulty if we see things like:\n\nConvergence warnings, obviously\nVery very small random effects variances\nExtreme random effects correlations of \\(\\pm 1.00\\)\n\nIf we see a warning that the model fitting algorithm nearly failed to converge: boundary (singular) fit: see ?isSingular or failed to converge then this tells us that, given the data, the mathematical engine (optimizer) underlying the lmer() function got into trouble because, in short, it was trying to find estimates for effects that were close to not being there at all.\nIf the variances for the random effects of participants or stimulus items on the slopes of an experimental variable are very small this suggests that the level of complexity in the model cannot really be justified or that the model will have difficulty estimating it. Extreme correlations (near 0 or 1) between random effects on intercepts and on slopes of fixed effects suggest the level of complexity in the model cannot really be justified (see also the discussion in Bates et al. (2015), Eager & Roy (n.d.) and Matuschek et al. (2017)).\n\n\n\nThere is no point comparing the models that do not converge, so we focus on those that do converge.\nDoes the addition of random slopes improve model fit? We can compare the model in pairs, as follows, to test whether each addition in model complexity improves model fit. We run the code for the model comparisons as follows.\nFirst, we compare the model long.orth.min.glmer (just random intercepts) with long.orth.2.glmer to check if increasing model complexity, by accounting for random differences between participants or words in the slope of the Orthography effect improves model fit to data.\n\nanova(long.orth.min.glmer, long.orth.2.glmer)\n\nData: long.orth\nModels:\nlong.orth.min.glmer: Score ~ Time + Orthography + Instructions + zConsistency_H + Orthography:Instructions + Orthography:zConsistency_H + (1 | Participant) + (1 | Word)\nlong.orth.2.glmer: Score ~ Time + Orthography + Instructions + zConsistency_H + Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) + 1 || Participant) + (dummy(Orthography) + 1 || Word)\n                    npar    AIC    BIC  logLik -2*log(L)  Chisq Df Pr(&gt;Chisq)\nlong.orth.min.glmer    9 1040.4 1086.7 -511.20    1022.4                     \nlong.orth.2.glmer     11 1041.0 1097.6 -509.51    1019.0 3.3909  2     0.1835\n\n\nSecond, we compare the model long.orth.min.glmer (just random intercepts) with long.orth.3.glmer to check if increasing model complexity, by accounting for random differences between participants or words in the slope of the Instructions effect improves model fit to data.\n\nanova(long.orth.min.glmer, long.orth.3.glmer)\n\nData: long.orth\nModels:\nlong.orth.min.glmer: Score ~ Time + Orthography + Instructions + zConsistency_H + Orthography:Instructions + Orthography:zConsistency_H + (1 | Participant) + (1 | Word)\nlong.orth.3.glmer: Score ~ Time + Orthography + Instructions + zConsistency_H + Orthography:Instructions + Orthography:zConsistency_H + (dummy(Orthography) + 1 || Participant) + (dummy(Orthography) + dummy(Instructions) + 1 || Word)\n                    npar    AIC    BIC  logLik -2*log(L)  Chisq Df Pr(&gt;Chisq)  \nlong.orth.min.glmer    9 1040.4 1086.7 -511.20    1022.4                       \nlong.orth.3.glmer     12 1036.5 1098.2 -506.24    1012.5 9.9115  3    0.01933 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nI think this justifies reporting the model long.orth.3.glmer.\nIf you look at the outputs, the addition of the random effects of participants and words on the slope of the effect of Orthography cannot be justified (\\(\\chi^2 = 3.3909, 2 df, p = .1835\\)) by improved model fit, in comparison to a model with the random effects of participants and words on intercepts.\nThe model comparison summary indicates that the addition of a random effect of words on the slope of the Instructions effect is justified by significantly improved model fit to data (\\(\\chi^2 = 9.9115, 3 df, p = 0.01933\\)).\nNotice I take a bit of a short-cut here, by adding both random effects for Orthography and Instructions. Logically, if adding the random effect for Orthography does not improve model fit but adding the random effects for both random effects for Orthography and Instructions does then it is the addition of Instructions that is doing the work. However, I should prefer to see as complex a random effects structure as possible, provided a model converges.\nWhile adding the random effect of Orthography does not improve model fit significantly, you see researchers allowing a generous p-value threshold for inclusion of terms (i.e. it is ok to add variables up to where \\(.p &lt; .2\\)). Matuschek et al. (2017) argue that when we are engaged in model selection – here, this is what we are doing because we are trying to figure out what model (with what random effects) we should use – then we should resist the reflex to choose the model that seems justified because the LRT \\(\\chi^2, p &lt; .05\\).\nThe \\(\\chi^2\\) alpha level cannot be interpreted as the expected model selection Type I error rate (in Null Hypothesis Significance Test terms) but rather as the relative weight of model complexity and goodness-of-fit (look back at the discussion of model comparison in the previous chapter). In this sense, setting a threshold such that we include an effect only if \\(\\chi^2, p &lt; .05\\) will always tend to penalize model complexity, and tend therefore to lead us to choose simpler (perhaps too simple) models.\n\n\n\nSometimes (g)lmer() has difficulty finding estimates for effects in a model given a data-set. If it encounters problems, the problems are expressed as warnings about convergence failures. Convergence failures typically arise when the model is too complicated for the data (see the discussion in Bates et al., 2015; Eager & Roy, n.d.; Matuschek et al., 2017; and Meteyard & Davies, 2020). As we have seen, problems can occur if you are trying to estimate (or predict) random effects terms that are very small – that do not really explain much variance in performance. Problems can also occur if variables have very large or very small ranges.\nWe can detect and address these problems in a number of ways:\n\nSometimes convergence problems can be fixed by switching the optimizer used to fit the model – we can do this by adding the argument: glmerControl(optimize = \"bobyqa\") to the glmer() function call, as I did for the class example. Switching optimizers is a quick solution to a common problem: models can fail to converge for a number of different reasons. In short, there may not be enough data for the model fitting process to settle on appropriate estimates for fixed or random effects.\nSometimes, a warning message advises us to consider rescaling the continuous numeric predictor variable. For this reason, and others, I usually standardize numeric predictor variables, as a default, before the analysis.\nSometimes, the warnings tell us that we need to simplify the random effects part of the model. We can simplify the random effects structure of a mixed-effects model in a number of ways:\n\n\nAs we examine the estimates resulting from a model fit we can consider whether the variance and covariance terms are small or large.\nUltimately, I decided that the effects of subjects and items on intercepts was important, as was, to some extent, the effect of words on the slope of the effect of Instructions.\n\nThe approach we have progressed through is widely used (see discussions in Baayen et al., 2008; Barr et al., 2013; Matuschek et al., 2017; Meteyard & Davies, 2020).\nMuch useful advice is set out in the troubleshooting guide by Ben Bolker here.\n\n\nTo demonstrate the impact of these adjustments, you could refit the models discussed in the foregoing:\n\nWithout using the standardized consistency variable as a predictor;\nWithout using the modification to the model fitting code, i.e. deleting the line that includes glmerControl(optimizer = \"bobyqa\");\nMaking the last most complex model more complex by adding further random effects of subjects or items on the slopes of both main effects and the interaction.\n\n\n\n\nMy advice, then, is to consider whether random effects should be included in a model based on:\n\nTheoretical reasons, in terms of what your understanding of a study design allows and requires, with respect to random differences between groups (classes, participants, stimuli etc.) or stimuli;\nModel convergence, as when models do or do not converge;\nOver a series of model comparisons, an evaluation of whether model fit is improved by the inclusion of the random effect.\n\nThis sounds like it involves work, judgment, and a process. It also sounds like people may disagree on the judgment or the process so that you shall have to share data and code, to enable others to check if the results vary depending on different decisions or different approaches. And it sounds like you will need to not only figure out what to do but also justify the approach you take when you report the results.\nI think all these things are true.\nThis is why Lotte Meteyard and I advise that researchers need to explain their approach, and share their data and code, when they report their analyses. Our best practice guidance for reporting mixed-effects models includes, among other things, the advice that in reports …\n\nRandom effects are explicitly specified according to sampling units (e.g., participants, items), the data structure (e.g., repeated measures) and anticipated interactions between fixed effects and sampling units (e.g., intercepts only or intercepts and slopes). Fixed effects and covariates are specified from explicitly stated research questions and/or hypotheses.\nReport the size of the sample analysed in terms of total number of data points and of sampling units (e.g., number of participants, number of items, number of other groups specified as random effects, such as classes of children).\nA clear statement of the methods by which models are compared/selected; e.g., simple to complex, covariates first, random effects first, fixed effects first etc.\nReport comparison method (LRT, AIC, BIC) and justify the choice.\nA complete report of all models compared (e.g., in appendices/supplementary data/analysis scripts) with model equations and the result of comparisons.\nIf models fail to converge, the approach taken to manage this should be comprehensively reported. This should include the formula for each model that did or did not converge and a rationale for a) the simplification method used and b) the final model reported. This may be most easily presented in an analysis script.\n\n\n\n\n\n\n\nTip\n\n\n\nThis looks like a lot of work.\n\nWhy bother?\n\n\n\nI think it is always worth asking this question.\nThe first answer is that it is all relative. In my own experience, a lot of the effort spent in the research workflow used to be occupied by face-to-face data collection: weeks or months of testing; now all data get collected online, and it gets finished overnight. Considerable time and effort (as Hadley Wickham’s joke runs, 80% of analysis effort) was spent on tidying the data before analysis; I still do this work but it is now much faster and less effort, thanks to {tidyverse}. A lot of effort used to be spent by me or colleagues on the literature review, the power analysis, or the stimulus preparation: that still happens. And a lot of effort used to be spent on doing the analysis and figuring out what the results mean: that, too, still happens. It is up to you if you want to spend ten months on data collection and five minutes on data analysis (as another joke has it, a million bucks on the data and a nickel on the statistics).\nI think we do need to work at understanding the most appropriate analysis for our data, based on both our theoretical expectations and a data-driven evaluation. No-one is going to help us unsee multilevel structure in the data, or save us from the obligation to take into account random effects. Matuschek et al. (2017) (2017; p.312) argue that:\n\nThe goal of model selection is not to obtain a significant p-value; the goal is to identify the most parsimonious model that can be assumed to have generated the data.\n\nThis makes sense to me. And their analyses show that determining a parsimonious model with a standard model selection criterion is a defensible choice, a way to take into account random effects, while controlling for both the risk of false positives, and the risk of false negatives.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 19. Conceptual introduction to Generalized Linear Mixed-effects Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/04-glmm.html#sec-glmm-reporting-results",
    "href": "PSYC412/part2/04-glmm.html#sec-glmm-reporting-results",
    "title": "Week 19. Conceptual introduction to Generalized Linear Mixed-effects Models",
    "section": "",
    "text": "We have discussed how to report the results of mixed-effects models previously. The same conceptual structure, and similar language, can be used to report the results of Generalized Linear Mixed-effects Models (GLMMs).\nI think you need to think about reporting the analysis as a task in which you first prepare the reader, explaining the motivation for using GLMMs, then present the analysis you did (the process, in outline), then present the results you shall later discuss.\n\nStart by explaining the study design: outline the fixed effects that have to be estimated.\nExplain how random effects structure was selected – be prepared to present a short version of the story in the main part of the report – sharing your code, in an appendix, to illustrate the steps in full.\n\nLotte Meteyard and I recommend that results reporting should:\n\nProvide equation(s) that transparently define the reported model(s). An elegant way to do this is providing the model equation with the table that reports the model output.\nAnd that final model(s) should be reported in a table that includes all parameter estimates for fixed effects (coefficients, standard errors and/or confidence intervals, associated test statistics and p-values if used), random effects (standard deviation and/or variance for each random effect, correlations/covariances if modelled) and some measure of model fit (e.g. R-squared, correlation between fitted values and data).\nWhile researchers should be able to share the coding script used to complete the analysis and, wherever possible, share data that generated the reported results.\n\nFor the word learning study we have been working through, the Results section for the report would include the following elements:\n\nExplain approach\n\n\nWe used mixed-effects models to analyse data because this approach permits modelling of both participant- and item-level variability simultaneously, unlike more traditional approaches such as ANOVA. In this study, multiple participants responded to multiple items, meaning that both participants and items were sources of nonindependence in our data (i.e. responses from the same participant are likely to be correlated, as are responses to the same item). Compared to ANOVA, mixed-effects models offer a more flexible approach, and are better able to handle missing data without significant loss of statistical power (Baayen, Davidson, & Bates, 2008).\n\n\nExplain how you get from the study design to the model you use to test or estimate key effects\n\n\nWe took a hypothesis driven approach, estimating the fixed effects of time (Time 1 versus Time 2), Orthography (absent versus present), Instructions (incidental versus explicit) and consistency (standardized H), as well as the interaction between orthography and instructions and the interaction between orthography and consistency. Different levels of the three binary fixed effects were sum coded… Consistency H, as a numeric predictor variable, was standardized to z scores before entry to models as a predictor. scores before entry to models as predictors. –&gt;\n\n\nOutline the model comparison or model selection work\n\n\nThe models were initially fitted specifying just random effects to account for variation by participants and stimuli in accuracy (random intercepts) plus terms to estimate the fixed effects of the experimental conditions ([name them]), and the interactions [name them]. Following the recommendations of Barr, Levy, Scheepers, and Tily (2013; see also Baayen, 2008; Matuschek et al., 2017), we fitted further models adding both random intercepts and random slopes for the random effects. Likelihood ratio test comparison of models showed that a model with both random intercepts and slopes … fit the data better than a model with just random intercepts \\((\\chi^2(df) = ..., p = ...)\\).\n\n\nUse appendices or supplementary materials\n\n\nTo give the reader full information on models fit, model comparisons.\nTo Help the reader with a concise summary of estimates.\n\nAs I have advised for reporting linear models, I included a tabled summary of coefficient estimates, presenting fixed and random effects (see e.g. Davies et al., 2013; Monaghan et al., 2015)\n\nShow and tell\n\nUse figures – model prediction plots, as seen – to help the reader to see what the fixed effects estimates imply.\n\n\n\n\n\n\nTip\n\n\n\nWhich model do we report?\n\nNote that given the model comparison results we have seen, I would probably report the estimates from long.orth.3.glmer. The model appears to include the most comprehensive account of random effects while still being capable of converging.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 19. Conceptual introduction to Generalized Linear Mixed-effects Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/04-glmm.html#sec-glmm-summary",
    "href": "PSYC412/part2/04-glmm.html#sec-glmm-summary",
    "title": "Week 19. Conceptual introduction to Generalized Linear Mixed-effects Models",
    "section": "",
    "text": "We focused on the need to use Generalized Linear Mixed-effects Models (GLMMs). We identified the kind of outcome data (like response accuracy) that requires analysis using GLMMs. Alternative methods, and their limitations, were discussed.\nWe examined a study that incorporates repeated measures (participants respond to multiple stimuli), a 2 x 2 factorial design, and a longitudinal aspect (participants tested at two time points), the word learning study (Ricketts et al., 2021).\nWe discussed the need to use effect coding for categorical predictor variables (factors). We work through example code to set factor level coding as required.\nWe worked through a random intercepts GLMM, and identified the critical elements of the model code, and of the results summary, including hypothesis test p-values. We examined how to present visualizations of fixed effects estimates (model predictions) using different libraries.\nWe then moved on to considering the question of what random effects we should include in the model. We considered the study design in some depth, and explored what random effects we could, in theory, expect to require. We then worked through a model comparison approach. We looked at some warning signs, what they indicate, and how to deal with them.\nWe considered how to report the model selection (or comparison, or building) process, and how to report the model for presentation of results.\n\n\nWe used two functions to fit and evaluate mixed-effects models.\n\nWe used glmer() to fit a mixed-effects model\nWe used anova() to compare two or more models using AIC, BIC and the Likelihood Ratio Test",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 19. Conceptual introduction to Generalized Linear Mixed-effects Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/04-glmm.html#sec-glmm-recommended-reading",
    "href": "PSYC412/part2/04-glmm.html#sec-glmm-recommended-reading",
    "title": "Week 19. Conceptual introduction to Generalized Linear Mixed-effects Models",
    "section": "",
    "text": "The example studies referred to in this chapter are published in (Monaghan et al., 2015; Ricketts et al., 2021).\nBen Bolker provides a very readable introduction to Generalized Linear Mixed-effects Models (Bolker et al., 2009; see also Jaeger, 2008).\n(Baayen et al., 2008; see also Barr et al., 2013) discuss mixed-effects models with crossed random effects.\nThe issue of model comparison or model selection, and the appropriate choice of random effects structure is discussed helpfully by Baayen et al. (2008), Bates et al. (2015), Barr et al. (2013), Eager & Roy (n.d.) and Matuschek et al. (2017).\nI wrote a tutorial article on mixed-effects models with Lotte Meteyard (Meteyard & Davies, 2020). We discuss how important the approach now is for psychological science, what researchers worry about when they use it, and what they should do and report when they use the method.\nAccessible ook length introductions are provided by Snijders & Bosker (2004) and Gelman & Hill (2007b).\n\n\nCan be found here.\nOther helpful online advice by Ben Bolker (besides his numerous helpful interventions on StackOverflow) can be found here and here.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 19. Conceptual introduction to Generalized Linear Mixed-effects Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/04-glmm.html#sec-glmm-appendix",
    "href": "PSYC412/part2/04-glmm.html#sec-glmm-appendix",
    "title": "Week 19. Conceptual introduction to Generalized Linear Mixed-effects Models",
    "section": "",
    "text": "Further information about the variables in the long.orth_2020-08-11.csv data-set is listed following.\n\nParticipant\n\nCell values comprise character strings coding for participant. Participant identity codes were used to anonymise participation. Children included in studies 1 and 2 – participants in the longitudinal data collection – were coded EOF[number]. Children included in Study 2 only (i.e., the older, additional, sample) were coded ND[number].\n\nTime\n\nTest time was coded 1 (time 1) or 2 (time 2). For the Study 1 longitudinal data, it can be seen that each participant identity code is associated with observations taken at test times 1 and 2.\n\nStudy\n\nObservations taken for children included in studies 1 and 2 – participants in the longitudinal daa collection – were coded Study1&2. Children included in Study 2 only (i.e., the older, additional, sample) were coded Study2.\n\nInstructions – Variable coding for whether participants undertook training in the explicit} or incidental} conditions.\nVersion – Experiment administration coding\nWord – Letter string values show the words presented as stimuli to children.\nConsistency-H – Calculated orthography-to-phonology value for each word.\nOrthography – Variable coding for whether participants had seen a word in training in the orthography absent} or present} conditions.\nMeasure – Variable coding for the post-test measure: Sem_all: if the semantic post-test;Orth_sp: if the orthographic post-test.\nScore – Variable coding for response category. For the semantic (sequential or dynamic) post-test, responses were scored as corresponding to:\n3 – correct response in the definition task\n2 – correct response in the cued definition task\n1 – correct response in the recognition task\n0 – if the item wasn’t correctly defined or recognised\nFor the orthographic post-test, responses were scored as:\n1 – correct, if the target spelling was produced in full\n0 – incorrect\nWASImRS Raw score – Matrix Reasoning subtest of the Wechsler Abbreviated Scale of Intelligence\nTOWREsweRS Raw score – Sight Word Efficiency (SWE) subtest of the Test of Word Reading Efficiency; number of words read correctly in 45 seconds.\nTOWREpdeRS Raw score – Phonemic Decoding Efficiency (PDE) subtest of the Test of Word Reading Efficiency; number of nonwords read correctly in 45 seconds.\nCC2regRS Raw score – Castles and Coltheart Test 2; number of regular words read correctly\nCC2irregRS Raw score – Castles and Coltheart Test 2; number of irregular words read correctly\nCC2nwRS Raw score – Castles and Coltheart Test 2; number of nonwords read correctly\nWASIvRS Raw score – vocabulary knowledge indexed by the Vocabulary subtest of the WASI-II\nBPVSRS Raw score – vocabulary knowledge indexed by the British Picture Vocabulary Scale – Third Edition\nSpelling.transcription Transcription of the spelling response produced by children in the orthographic post-test\nLevenshtein.Score Children were asked to spell each word to dictation and spelling productions were transcribed for scoring. Responses were scored using a Levenshtein distance measure, using the `stringdist: library (van der Loo, 2019). This score indexes the number of letter deletions, insertions and substitutions that distinguish between the target and child’s response. For example, the response ‘epegram’ for target ‘epigram’ attracts a Levenshtein score of 1 (one substitution). Thus, this score gives credit for partially correct responses, as well as entirely correct responses. The maximum score is 0, with higher scores indicating less accurate responses.\nzTOWREsweRS We standardized TOWREsweRS values, calculating the z score as \\(z = \\frac{x - \\bar{x}}{sd_x}\\), over all observations in the longitudinal} (Study 1) or concurrent} (Study 2) data-set, using the scale() function in R.\nzTOWREpdeRS Standardized TOWREpdeRS scores\nzCC2regRS] Standardized CC2regRS scores\nzCC2irregRSStandardized CC2irregRS scores\nzCC2nwRS Standardized CC2nwRS scores\nzWASIvRS Standardized WASIvRS scores\nzBPVSRSStandardized BPVSRS scores\n\n\n\n\n\n\n\nAgresti, A. (2002). Categorical data analysis. Wiley Series in Probability and Statistics. https://doi.org/10.1002/0471249688\n\n\nBaayen, R. H., Davidson, D. J., & Bates, D. M. (2008). Mixed-effects modeling with crossed random effects for subjects and items. Journal of Memory and Language, 59(4), 390–412. https://doi.org/10.1016/j.jml.2007.12.005\n\n\nBaguley, T. (2012). Modeling discrete outcomes (pp. 667–723). Macmillan Education UK. https://doi.org/10.1007/978-0-230-36355-7_17\n\n\nBarr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of Memory and Language, 68, 255–278.\n\n\nBates, D., Kliegl, R., Vasishth, S., & Baayen, H. (2015). Parsimonious mixed models. arXiv Preprint arXiv:1506.04967.\n\n\nBolker, B. M., Brooks, M. E., Clark, C. J., Geange, S. W., Poulsen, J. R., Stevens, M. H. H., & White, J.-S. S. (2009). Generalized linear mixed models: a practical guide for ecology and evolution. Trends in Ecology & Evolution, 24(3), 127–135. https://doi.org/10.1016/j.tree.2008.10.008\n\n\nColenbrander, D., Miles, K. P., & Ricketts, J. (2019). To See or Not to See: How Does Seeing Spellings Support Vocabulary Learning? Language, Speech, and Hearing Services in Schools, 50(4), 609–628. https://doi.org/10.1044/2019_lshss-voia-18-0135\n\n\nEager, C., & Roy, J. (n.d.). Mixed effects models are sometimes terrible. https://doi.org/10.48550/arXiv.1701.04858\n\n\nGelman, A., & Hill, J. (2007b). Data analysis using regression and multilevel/hierarchical models. Cambridge University Press.\n\n\nGelman, A., & Hill, J. (2007a). Data analysis using regression and multilevel/hierarchical models. Cambridge University Press.\n\n\nJaeger, T. F. (2008). Categorical data analysis: Away from ANOVAs (transformation or not) and towards logit mixed models. Journal of Memory and Language, 59(4), 434–446. https://doi.org/10.1016/j.jml.2007.11.007\n\n\nLoo, M. van der, Laan, J. van der, R Core Team, Logan, N., Muir, C., Gruber, J., & Ripley, B. (2022). Stringdist: Approximate string matching, fuzzy text search, and string distance functions. https://CRAN.R-project.org/package=stringdist\n\n\nMatuschek, H., Kliegl, R., Vasishth, S., Baayen, H., & Bates, D. (2017). Balancing type i error and power in linear mixed models. Journal of Memory and Language, 94, 305–315. https://doi.org/10.1016/j.jml.2017.01.001\n\n\nMeteyard, L., & Davies, R. A. I. (2020). Best practice guidance for linear mixed-effects models in psychological science. Journal of Memory and Language, 112, 104092. https://doi.org/10.1016/j.jml.2020.104092\n\n\nMonaghan, P., Mattock, K., Davies, R. A. I., & Smith, A. C. (2015). Gavagai is as gavagai does: Learning nouns and verbs from cross-situational statistics. Cognitive Science, 39(5), 1099–1112. https://doi.org/10.1111/cogs.12186\n\n\nMousikou, P., Sadat, J., Lucas, R., & Rastle, K. (2017). Moving beyond the monosyllable in models of skilled reading: Mega-study of disyllabic nonword reading. Journal of Memory and Language, 93, 169–192. https://doi.org/10.1016/j.jml.2016.09.003\n\n\nRicketts, J., Dawson, N., & Davies, R. (2021). The hidden depths of new word knowledge: Using graded measures of orthographic and semantic learning to measure vocabulary acquisition. Learning and Instruction, 74, 101468. https://doi.org/10.1016/j.learninstruc.2021.101468\n\n\nSnijders, T. A. B., & Bosker, R. J. (2004). Multilevel analysis: An introduction to basic and advanced multilevel modeling. Sage Publications Ltd.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 19. Conceptual introduction to Generalized Linear Mixed-effects Models"
    ]
  },
  {
    "objectID": "PSYC412/part2/01-multilevel.html",
    "href": "PSYC412/part2/01-multilevel.html",
    "title": "Week 16. Conceptual introduction to multilevel data",
    "section": "",
    "text": "In this chapter, we shall start to develop skills in using a method or approach that is essential in modern data analysis: multilevel modeling. We are going to invest four weeks in working on this approach. This investment is designed to give you a specific, important, advantage in your work as a psychologist, or as someone who produces or consumes psychological research.\n\n\n\n\n\n\nNote\n\n\n\nMultilevel models: why do we need to do this? - Four weeks is a lot of time to spend on one method.\n\n\nIt is now clear that someone who works in psychological research has to know about multilevel or hierarchically structured data, and has to know how to apply multilevel models (or mixed-effects models). Growth in the popularity of these kinds of analysis has been very very rapid, as can be seen in Figure 1. It is now, effectively, the standard or default method for professional data analysis in most areas of psychological and other social or clinical sciences (or it soon will be). There are good reasons for this (Baayen et al., 2008).\n\n\n\n\n\n\nFigure 1: Number of Pubmed citations for ‘Linear Mixed Models’ by year. Generated using the tool available at http://dan.corlan.net/medline-trend.html, entering “Linear Mixed Models” as the phrase search term and using data from 2000 to 2018, from Meteyard and Davies (2020) – used without permission\n\n\n\nWe continue to teach ANOVA and multiple regression (linear models) in our courses because the research literature is full of the results of analyses done using these methods and because many psychologists continue to use these methods in their research. However, there is increasingly wide-spread recognition that these classical methods have serious problems when applied to data with hierarchical structure. Because most psychological data (not all) will have hierarchical structure, this makes learning about multilevel or mixed-effects methods a key learning objective.\nBut, because it is relatively new, many professional psychologists struggle to understand why or how to use these methods effectively. This means that students who acquire the skill graduate with a clear employability advantage. It also means that we have to take seriously the challenge of learning about these methods. This is why we will spend a bit of time on them. In my experience, in over a decade of teaching multilevel models, in working with both students and professionals, we shall need to develop understanding and skills gradually. We will work patiently, so that we can secure understanding by building our learning through a series of practical examples, increasing the scope of our practical skills, and developing the sophistication of our understanding, step-by-step, as we go.\n\n\nMultilevel models are also known as hierarchical models or linear mixed-effects models or random effects models. People use these terms interchangeably. They also use the abbreviations LMMs or LMEs. Sorry about that: humans make methods, and the names we use for things do vary.\nI will only use the terms multilevel or linear mixed-effects models.\n\n\n\n\n\n\nWarning\n\n\n\n\nIn this chapter, we emphasize the multilevel perspective but, to anticipate future development, we will come to think in terms of mixed-effects models.\n\n\n\n\n\n\n\nThe key challenges for learning should be explained at the start so that we know what we shall have to do to overcome them.\n\nEven though most psychological data has some sort of multilevel or hierarchical structure, we are not used to recognizing it. This is because the structure has often been hidden or ignored in the education and practice of traditional research methods in Psychology.\n\nIn time, you will come to see multilevel structure everywhere (Kreft & Leeuw, 1998). But first you will need to get some practice so that you can become familiar with the idea and learn to recognize what it looks like when data have a multilevel or hierarchical structure. This is why we will examine multilevel structured data across a range of different kinds of experiments or surveys, over a series of weeks.\nWe will learn to identify and understand hierarchical structure in psychological data by just looking at datasets, by producing visualizations, by doing analyses, and by trying to explain to ourselves and each other what we think we see.\n\nThe ideas that support an understanding of why and how we use multilevel models can be intimidating when we first encounter them. The mathematics behind how the models work is both profound and sophisticated. But the good news is that we can practice the application of the analysis method while talking and thinking about the critical ideas using just words or plots.\n\nWe cannot or should not avoid engaging with the ideas – because we have to be able to explain what we are doing – but there are many routes to an effective understanding. For those who want to develop a more mathematically-based perspective, I will provide references to important texts in the literature on multilevel models (see Section 1.11).\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nMultilevel models are a general form of linear model.\n\n\nAnother way of saying this is: linear models are a special form of multilevel models.\nThis is because linear models assume that observations are independent. We often cannot make this assumption, as we shall see. More generally, then, we do not assume that observations are independent and so we use multilevel models.\n\n\n\nWe are not going to take a mathematical approach to learning about multilevel models. We do not have to. The approach we are going to take is:\n\nverbal – We will talk about the main ideas in words. Sometimes, we will present formulas but that is just to save having to use too many words.\nvisual – We will show ourselves and each other what multilevel structure in data looks like, and what that structure means for our analyses of behaviour.\npractical – We will use R to complete analyses, so we will learn about coding models in practice. Fortunately, through coding we can get a clear idea of what we want the models to do.\n\n\n\n\nOur learning objectives include the development of key concepts and skills.\n\nconcepts – how data can have multilevel structures and what this requires in models\nskills – where skills comprise the capacity to:\n\n\nuse visualization to examine observations within groups\nrun linear models over all data and within each class\nuse the lmer() function to fit models of multilevel data\n\nWe are just getting started. Our plan will be to build depth and breadth in understanding as we progress over the next few weeks.\n\n\n\nI have provided a collection of materials you can use. Here, I explain what they are and how I suggest you use them.\n1. Video recordings of lectures\n1.1. I have recorded a lecture in three parts. The lectures should be accessible by anyone who has the link.\n\nPart 1 – about 16 minutes\nPart 2 – about 13 minutes\nPart 3 – about 13 minutes\n\n1.2. I suggest you watch the recordings then read the rest of this chapter.\n\nThe lectures provide a summary of the main points.\n\n1.3. You can download the lecture slides in three different versions:\n\n402-week-17-LME-1.pdf: high resolution .pdf, exactly as delivered [4 MB];\n402-week-17-LME-1_1pp.pdf: low resolution .pdf, printable version, one-slide-per-page [359 KB];\n402-week-17-LME-1_6pp.pdf: low resolution .pdf, printable version, six-slides-per-page [359 KB].\n\nThe high resolution version is the version delivered for the lecture recordings. Because the images are produced to be high resolution, the file size is quite big (4 MB) so, to make the slides easier to download, I produced low resolution versions: 1pp and 6pp. These should be easier to download and print out if that is what you want to do.\n2. Chapter: 01-multilevel\n2.1. I have written this chapter to discuss the main ideas and set out the practical steps you can follow to start to develop the skills required to analyse multilevel structured data.\n2.2. The practical elements include data tidying, visualization and analysis steps.\n2.3. You can read the chapter, see in the chapter how to run the code, and do the exercises.\n\nRead in the example data BAFACALO_DATASET.RData and identify how the data are structured at multiple levels.\nUse visualizations to explore the impact of the structure.\nRun analyses using linear models (revision) and linear mixed-effects models (extension) code.\nReview the recommended readings (Section 1.11).\n\n3. Practical materials\n3.1 In the following sections, I describe the practical steps, and associated practical materials (exercise workbooks and data), you can use for your learning.\n\n\n\nIn this chapter, we will be working with data taken from a study on education outcomes in Brazilian children, reported by Golino & Gomes (2014). First, we will progress through the steps required to download and prepare the files for analysis in R.\nThe BAFACALO_DATASET.RData data were collected and shared online by Golino & Gomes (2014). Information about the background motivating the study, the methods of data collection, along with the dataset itself, can be found here.\nGolino & Gomes (2014) collected school end-of-year subject grades for a sample of 292 children recruited from multiple classes in a school in Brazil. Here, each subject is a theme or course that children studied in school, e.g., Physics or English language, and for which each child was awarded a grade at the end of the school year. So, we have information on different children, and information about their subject grades. Children were taught in different classes but the classes appear to be units of the school organization (the information is not quite clear) not subject or course groupings. Thus, we also have information on which classes children were in when data about them were collected.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nGet the data: get the data file and the .R script you can use to do the exercises that will support your learning.\n\nYou can download the files folder for this chapter by clicking on the link 01-multilevel.zip.\n\n\n\nThe practical materials folder includes:\n\nBAFACALO_DATASET.RData\n402-01-multilevel-workbook.R the workbook you will need to do the practical exercises.\n\nThe data file is in the .RData format: .RData is R’s own file format so the code you use to load and access the data for analysis is a bit simpler than you are used to, see Section 1.7.2 next. (You will have used read.csv() or read_csv() previously.)\nIn my own practice, I prefer to keep files in formats like .csv which can be opened and read in other software applications (like Excel), so using an .RData is an exception in these materials.\nYou can also access the data from the link associated with the Golino & Gomes (2014) article here.\n\n\n\nDuring practical sessions, each week, you can use the workbook to prompt your code construction and your thinking, with support.\nAfter practical sessions, you will be able to download an answers version of the workbook .R so check back here after the session.\n\n\n\n\n\n\n\nImportant\n\n\n\nGet the answers: get the data file and the .R script with answers.\n\n\n\nThe link to this folder will not be provided until after a session has finished.\nWhen the link is live, you will be able to download a folder including:\n\n402-01-multilevel-workbook-with-answers.R with answers to questions and example code for doing the exercises.\n\n\n\n\n\nYou can read the BAFACALO_DATASET.RData file into the R workspace or environment using the following code.\n\nload(\"BAFACALO_DATASET.RData\")\n\n\n\n\nThe dataset consists of rows and columns. Take a look. If you have successfully loaded the dataset into the R environment, then you should be able to view it.\nYou could look at the dataset by using the View() function.\n\nView(BAFACALO_DATASET)\n\nOr you can use the head() function to see the top few rows of the dataset.\n\nhead(BAFACALO_DATASET)\n\nYou can check for yourself that each row holds data about a child, including their participant identity code, as well as information about their parents, household, gender, age, school class, and their grades on end-of-year subject attainment (e.g., how well they did in English language).\nThere are many more variables in the dataset than we need for our exercises, and a summary would fill pages. You can see for yourself if you inspect the dataset using\n\nsummary(BAFACALO_DATASET)\n\n\n\n\n\nWhen you inspect the file, you will see that it includes a large number of variables, but we only really care about those we will use in our exercises:\n\nparticipant_id gives the participant identity code for each child;\nclass_number gives the class identity code for the school class for each child;\nand values in portuguese, english, math, and physics columns give the score for each child in subject class attainment measures.\n\nYou can see that I am not explaining the variables in great depth. For our aims, we do not need more detailed information but please do read the data article if you wish to find out more.\nWe need to tidy the data before we can get to the analysis parts of this chapter. We are going to:\n\nSelect the variables we want to work with;\nFilter out missing values, if any are present;\nAnd make sure R knows how we want each variable to be identified; what type the variable should have.\n\nWe shall need the tidyverse library of functions.\n\nlibrary(tidyverse)\n\n\n\nWe can start by selecting the variables we want, which include those named here, ignoring all the rest. We take the BAFACALO_DATASET. We then use the select() function to select the variables we want.\n\nbrazil &lt;- BAFACALO_DATASET %&gt;%\n                    select(\n                      class_number, participant_id,\n                      portuguese, english, math, physics\n                      )\n\n\n\n\n\n\n\nTip\n\n\n\nNotice the code is written to create the new selected dataset and, at the same time, gave it a more usable (shorter) name, with brazil &lt;- BAFACALO_DATASET ...\n\n\nInspect the data to see what you have.\n\nsummary(brazil)\n\n  class_number participant_id     portuguese     english         math    \n M11    : 21   Min.   :  1.00   60     : 10   100    : 17   60     : 13  \n M15    : 20   1st Qu.: 73.75   76     :  9   79     : 11   69     :  9  \n M14    : 19   Median :146.50   65     :  8   96     :  8   70     :  9  \n M36    : 19   Mean   :146.50   73     :  8   81     :  7   62     :  8  \n M18    : 18   3rd Qu.:219.25   74     :  7   89     :  7   71     :  7  \n (Other):133   Max.   :292.00   (Other):188   (Other):180   (Other):184  \n NA's   : 62                    NA's   : 62   NA's   : 62   NA's   : 62  \n    physics   \n 60     : 16  \n 60.1   :  5  \n 0      :  4  \n 61.8   :  4  \n 66     :  4  \n (Other):197  \n NA's   : 62  \n\n\n\n\n\nIf you look at the results of the selection, you can see that there are missing values, written as e.g. NA's 62 at the bottom of each summary of each variable (if a variable column includes missing values).\n\n\n\n\n\n\nTip\n\n\n\nRemember that in R NA means “not available” i.e. missing.\n\n\nWe will need to get rid of the missing values. It is simpler to do this at the start rather than wait for an error message, later, when some arithmetic function tells us it cannot give us a result because there are NAs present.\nWe can get rid of the missing values using na.omit()\n\nbrazil &lt;- na.omit(brazil)\n\nIf you then look at a summary of the data again then you will see that the NAs are gone.\n\nsummary(brazil)\n\n  class_number participant_id    portuguese     english         math    \n M11    : 21   Min.   :  3.0   60     : 10   100    : 17   60     : 13  \n M15    : 20   1st Qu.: 77.5   76     :  9   79     : 11   69     :  9  \n M14    : 19   Median :144.5   65     :  8   96     :  8   70     :  9  \n M36    : 19   Mean   :146.6   73     :  8   81     :  7   62     :  8  \n M18    : 18   3rd Qu.:222.8   74     :  7   89     :  7   71     :  7  \n M21    : 17   Max.   :291.0   82     :  7   86     :  6   66     :  6  \n (Other):116                   (Other):181   (Other):174   (Other):178  \n    physics   \n 60     : 16  \n 60.1   :  5  \n 0      :  4  \n 61.8   :  4  \n 66     :  4  \n 74.2   :  4  \n (Other):193  \n\n\n\n\n\nBut if you look closely at the output from summary(brazil) you will see that the portuguese and english variables are summarized in the way that R summarizes factors.\nWhen you ask R to summarize factors, R gives you a count of the number of observations associated with each factor level, that is, each category in each variable. Here, it is treating a grade score like \\(100\\) in english as a category (like you might treat \\(dog\\) as a category of pets), and you can see that the count shows you that 17 children were recorded as having scored 100 in their class. We do not want numeric variables like subject grades (e.g. children’s grades in English) treated as categorical variables, factors.\nYou can also see that R gives you a numeric summary of the recorded values in the participant_id variable. This makes no sense because the identity code numbers are (presumably) assigned at random so identity numbers provide no useful numeric information for us. We do not want this either.\nWe want R to treat the educational attainment scores as numbers. We can do this using the as.numeric() function. We want R to treat the class and participant identity numbers as factors (categorical variables). We can do this using the as.factor() function.\nWe could do this one variable at a time.\n\nbrazil$portuguese &lt;- as.numeric(brazil$portuguese)\nbrazil$english &lt;- as.numeric(brazil$english)\nbrazil$math &lt;- as.numeric(brazil$math)\nbrazil$physics &lt;- as.numeric(brazil$physics)\n\nbrazil$class_number &lt;- as.factor(brazil$class_number)\nbrazil$participant_id &lt;- as.factor(brazil$participant_id)\n\nBut it is simpler and more efficient in tidyverse style. (You can see a discussion here that helped me to figure this out.)\n\nbrazil &lt;- brazil %&gt;%\n  mutate(across(c(portuguese, english, math, physics), as.integer),\n         across(c(class_number, participant_id), as.factor)) \n\nIf you now look at the summary of the data, you can see that R will give you mean etc. for the subject class score variables e.g. english, showing that it is now treating them as numeric variables. In comparison, R gives you counts of the numbers of observations for each level (category) of categorical or nominal variables like participant_id.\n\nsummary(brazil)\n\n  class_number participant_id   portuguese       english           math       \n M11    : 21   3      :  1    Min.   : 1.00   Min.   : 1.00   Min.   :  1.00  \n M15    : 20   4      :  1    1st Qu.:26.25   1st Qu.:29.00   1st Qu.: 39.00  \n M14    : 19   5      :  1    Median :42.00   Median :55.00   Median : 58.00  \n M36    : 19   6      :  1    Mean   :43.55   Mean   :50.25   Mean   : 58.17  \n M18    : 18   7      :  1    3rd Qu.:58.75   3rd Qu.:73.00   3rd Qu.: 79.75  \n M21    : 17   9      :  1    Max.   :83.00   Max.   :93.00   Max.   :104.00  \n (Other):116   (Other):224                                                    \n    physics      \n Min.   :  1.00  \n 1st Qu.: 34.00  \n Median : 71.50  \n Mean   : 71.57  \n 3rd Qu.:107.75  \n Max.   :148.00  \n                 \n\n\n\n\nR treats things like variables as vectors. A vector can be understood to be a set or list of elements: things like numbers of words.\nR gives each vector a type (factor, numeric etc.) which helps to inform different functions how to handle that vector. Usually, R assigns type correctly but sometimes it does not. We can use what is called coercion to force R to assign the correct type to a variable.\nIt is more efficient to do this at the start of an analysis workflow.\nNormally, I would use read_csv() from tidyverse and assign type to variable using col_types() specification. (See here for more information and an example.) But it is useful to learn what you need to do if you need to change the way a variable is treated after you have got the data into the R environment.\n\n\n\nIn R, there are a family of functions that work together. You can test whether a variable (a vector, technically) is or is not a certain type using the is.[something] function. For example:\n\nis.factor(brazil$english)\nis.numeric(brazil$english)\nis.character(brazil$english)\n\nAnd you can coerce variables so that they are treated as having certain types.\n\nbrazil$english &lt;- as.factor(brazil$english)\nbrazil$english &lt;- as.numeric(brazil$english)\nbrazil$english &lt;- as.character(brazil$english)\n\nNow try it out.\n\nTest out type for different variables using is...() for some of the variables.\nTest out coercion – and its results – using as...() for some of the variables.\nLook at the results using summary().\n\n\n\n\n\n\n\n\nWe (Psychologists) often adopt Repeated Measures or Clustered designs in our studies, and these designs yield data that have a multilevel structure. Examples of research which results in data with a multilevel structure include:\n\nStudies where we test the same people multiple times, maybe in developmental or longitudinal investigations;\nIntervention, learning or treatment studies where we need to make pre- and post-treatment comparisons;\nStudies where we present multiple stimuli and everyone sees the same stimuli;\nStudies that involve multi-stage sampling e.g. selecting a sample of classes or schools then testing a sample of children within each classes or within each school.\n\nThe key insight to keep in mind when considering the analysis of such data is that observations are clustered and are not independent: they are correlated. What is correlated with what?\nImagine testing a number of subjects by giving them all the same test. In that test, you might present them all with the same stimuli over a series of trials, so that everybody sees the same set of stimuli. Do you think an observed response recorded for any one individual will be uncorrelated i.e. independent of that person’s other responses?\nPeople are different and one usually finds that a slow or inaccurate subject is slow or inaccurate for most of their responses. That means that if you have information about one of their responses you can predict, in part, what the time or accuracy of one of their other responses would be. That capacity to predict one response from another is what we mean when we talk about a lack of independence.\nAlternatively, imagine going to test children in a school. Will the children in one class be more like each other than they are like children in other classes? In other words, is there an effect of class – maybe due to the approach of the teacher, the effect of the class environment etc. – so that outcomes for children in a class are correlated with each other?\n\n\n\n\n\n\nImportant\n\n\n\nWe are talking, here, about a really quite general property of data collected in certain, very widely used, designs in Psychology: the clustering or hierarchical ordering of data.\n\n\nThis week, we will see that we must deal with the dependence of observations within a class, where the observed responses were made about the different pupils tested in a class, for a number of different classes.\n\n\n\nThe utility of multilevel models to analyze multilevel data or hierarchically structured data is well established in education. In educational research, we often need to think about effects of interventions or correlations in the context of observing children in classes, schools or districts, perhaps over time or at different time points. This means that many of the critical textbooks present examples that are based on educational research data (Goldstein, 1995; Kreft & Leeuw, 1998; Raudenbush & Bryk, 2002; Snijders & Bosker, 2004).\nMultilevel models are growing in popularity in Psychology as well as in Education because they can be used to account for systematic and random sources of variance in observed outcomes when data are hierarchically structured. A hierarchical structure is present in data when a researcher: tests participants who belong to different groups like classes, clinics or schools; presents a sample of stimuli to each member of a sample of participants; or makes repeated observations for each participant over a series of test occasions.\nIn these circumstances, the application of traditional analytic methods has typically required the researcher to aggregate their data (e.g., averaging the responses made by a participant to different stimuli) or to ignore the hierarchical structure in their data, (e.g., analyzing the responses made by some pupils while ignoring the fact that the pupils were tested in different classes). But the application of traditional analysis approaches (e.g., regression, ANOVA) to multilevel structured data extracts scientific costs (Baayen et al., 2008; Barr et al., 2013).\nIgnoring structure by ignoring or averaging over sources of variability like differences between classes, participants, or stimulus items can mean that analyses are less sensitive because they fail to fully account for error variance. Where differences between classes, participants or stimuli include variation in the impact of experimental variables, e.g., individual differences in response to an experimental manipulation, the application of traditional methods can be associated with an increased risk of false positives in discovery. Yet these costs need no longer be suffered because the capacity to perform multilevel modeling is now readily accessible.\n\n\n\nIf a researcher tests participants belonging to different groups, e.g., records the educational attainment of different children recruited from different classes in a school, the test scores for the participants are observations that occupy the lowest level of a hierarchy (see Figure 2). In multilevel modeling, those observations are understood to be nested within higher-level sampling units, here, the classes. We can say that the children are sampled from the population of children. And the classes are sampled from the population of classes. Critically, we recognize that the children’s test scores are nested within the classes. This multi-stage sampling has important consequences, as we shall see.\n\n\n\n\n\n\n\n\nD\n\n\n\nA\n\nA\n\n\n\nB1\n\nB1\n\n\n\nA-&gt;B1\n\n\n\n\n\nB2\n\nB2\n\n\n\nA-&gt;B2\n\n\n\n\n\nC1\n\nC1\n\n\n\nB1-&gt;C1\n\n\n\n\n\nC2\n\nC2\n\n\n\nB1-&gt;C2\n\n\n\n\n\nC3\n\nC3\n\n\n\nB1-&gt;C3\n\n\n\n\n\nC4\n\nC4\n\n\n\nB2-&gt;C4\n\n\n\n\n\nC5\n\nC5\n\n\n\nB2-&gt;C5\n\n\n\n\n\nC6\n\nC6\n\n\n\nB2-&gt;C6\n\n\n\n\n\n\n\n\nFigure 2: Multilevel or hierarchically structured data\n\n\n\n\n\nWe are going to be working with the school data collected by Golino & Gomes (2014) in Brazil, so let’s take another look at that dataset. The data extract, following, presents the first 25 rows of the selected-variables brazil dataset. (I have arranged the rows by class number ID, and you may need to make sure the browser window is wide enough to see the data properly.) If you examine the extract, you can see that there are multiple rows of data for each class_number, one row for each child, with multiple children (you can see different participant_id numbers). This presentation of the dataset illustrates in practical terms – what you can actually see when you look at your data – what multilevel structured data can look like.\n\nbrazil %&gt;%\n       arrange(desc(class_number)) %&gt;%\n                 head(n = 25)\n\n   class_number participant_id portuguese english math physics\n1           M36             11         80      55   60      81\n2           M36             30         48      55   43      57\n3           M36             93         83      75   55      80\n4           M36             95         80      66   55      66\n5           M36            111         80      57   99     129\n6           M36            117         27      76   83      89\n7           M36            153          2      66   89     141\n8           M36            163         68      88   74     137\n9           M36            176         80      75   60     127\n10          M36            216         75      79   64     124\n11          M36            223         81      56   58      78\n12          M36            234         81      75   49      74\n13          M36            242         81      67   45      77\n14          M36            254         76      51   33     103\n15          M36            260         81      59   39      82\n16          M36            271         64      69   82     138\n17          M36            278         80      57   56     107\n18          M36            280         83      64   56      84\n19          M36            286         80      91   64      98\n20          M35             20         42      61   97      92\n21          M35            118         33      59   33      80\n22          M35            140         28      40   33      58\n23          M35            179         38      69   56     145\n24          M35            247         65      51   64      97\n25          M33             15         76      70   73      85\n\n\n\n\n\nRecognizing that the children’s scores data are observed within classes means that, when we examine the factors that influence variance in observed outcomes, we need to take into account the fact that the children can be grouped by (or under) the class they were in when their grades were recorded. We can develop an understanding of what this means by moving through a series of steps.\nTo illustrate the understanding we need to develop, we analyze the end-of-year school subject grades for the sample of 292 children studied by Golino & Gomes (2014). With these data, we can examine whether differences between children in their Portuguese language grades predicts differences in their English language grades. (We do not have a theoretical reason to make this prediction though it does not seem unreasonable.) Let’s make this our research question.\n\n\n\n\n\n\nNote\n\n\n\n\nResearch question: Does Portuguese grade predict English grade?\n\n\n\nNow, the first step we take in our development of understanding will be to first ignore the importance of the potential differences between classes.\nWe can begin our analysis in order to address the research question by plotting the possible association between Portuguese and English grades. We shall create a scatterplot to do this, and we create the plot by running the following code and ignoring group (class) membership. In the following chunk of code, I pipe the brazil data using %&gt;% to ggplot() and then create the plot step-by-step, with each ggplot() step separated by a + except for the last step.\n\nbrazil %&gt;%\nggplot(aes(x = portuguese, y = english)) +\n  geom_point(colour = \"black\", size = 3, alpha = .5) +\n  geom_smooth(method = \"lm\", size = 2, se = FALSE, colour = \"red\") +\n  xlab(\"Portuguese\") + ylab(\"English\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 3: Portuguese compared to English grades: each point represents the scores for one child\n\n\n\n\n\nRemember that each child ID is associated with a pair of grades: their grade in English and their grade in Portuguese. Each point in Figure 3 represents the paired grade data for one child.\nBecause I say that we are interested in predicting English grades then, by convention, we map English grades to the height of the points (i.e. English grade differences are shown as differences on the y-axis). Because we are using Portuguese grades to do the predicting, by convention, we map Portuguese grades to the horizontal position of the points (i.e. Portuguese grade differences are shown as differences on the x-axis).\nI have added a line using geom_smooth() in red to indicate the trend of the potential association between variation in Portuguese grades and variation in English grades.\nFigure 3 suggests that children with higher grades in Portuguese tend, on average, to also have higher grades in English.\n\n\nNotice that when we write the code to produce the plot, we add arguments to the geom_point() and geom_smooth() function calls to adjust the appearance of the points and the smoother line. Notice, also, that we adjust the labels for the x-axis and y-axis and, finally, that we determine the overall appearance of the plot using the theme_bw() function call.\nDo the following exercises to practice your ggplot() skills\n\nChange the x and y variables to math and physics\nChange the theme from theme_bw() to something different\nChange the appearance of the points, try different colours, shapes or sizes.\n\nFurther information to help you try out coding options can be found here, on scatterplots, and here, on themes.\n\n\n\n\nOur plot indicates the relationship between English and Portuguese language grades, in Figure 3, ignoring the fact that children in the sample belonged to different classes when they were tested. The plot shows us only information about the children and grades, with each point representing the observed English and Portuguese grades for each \\(i\\) child.\nCan we predict variation in English grades given information about child Portuguese grades? Are English grades related to Portuguese grades? We can estimate the relationship between English and Portuguese grades using a linear model in which the English grades variable is the outcome variable (or the dependent variable) and the Portuguese grades variable is the predictor or independent variable:\n\\[\ny_i = \\beta_0 + \\beta_1X_i + e_i\n\\]\n\nwhere \\(y_i\\) represents the English grade for each \\(i\\) child;\n\\(\\beta_0\\) represents the intercept, the outcome value obtained if values of the explanatory variable are zero;\n\\(\\beta_1\\) represents the effect of variation in \\(X_i\\) the Portuguese grade for each child, with the effect of that variation estimated as the the rate of change in English grade for unit change in Portuguese grade;\n\\(e_i\\) represents differences, for each child, between the observed English grade and the English grade predicted by the relationship with Portuguese grades.\n\nAs you have seen, the code for running a linear model corresponds transparently to the statistical model given in the formula.\n\nsummary(lm(english ~ portuguese, data = brazil))\n\n\nCall:\nlm(formula = english ~ portuguese, data = brazil)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-64.909 -17.573   2.782  20.042  53.292 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 29.77780    3.81426   7.807 2.11e-13 ***\nportuguese   0.47001    0.07897   5.952 9.91e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 25.02 on 228 degrees of freedom\nMultiple R-squared:  0.1345,    Adjusted R-squared:  0.1307 \nF-statistic: 35.43 on 1 and 228 DF,  p-value: 9.906e-09\n\n\nThe linear model yields the estimate that for unit increase in Portuguese grade there is an associated increase of about .47 in English grade, on average (for the model, \\(F(1, 228) = 35, p &lt; .001; adj. R^2 = .13\\)). So, we have a preliminary answer to our research question.\n\n\n\n\n\n\nNote\n\n\n\n\nResearch question: Does Portuguese grade predict English grade?\nResult: Our analysis shows that children who score one grade higher in their Portuguese class (e.g., 61 compared to 60) tended to score .47 of a grade higher in their English class.\n\n\n\nWe shall see that we will need to revise this estimate when we do an analysis that does take school class differences into account.\n\n\n\n\n\n\nTip\n\n\n\nNotice: here, I have dispensed with the creation of a model object by name. The model is estimated anyway, and I have embedded the lm() function call within a summary() function so that I am asking R to do two things:\n\nlm() fit a linear model\nsummary() print out a summary of the fitted model object\n\nI do this to give an example of the way in which code can be varied. Also, to show you how one function can be embedded inside another.\n\n\n\n\nDo the following exercises to practice your lm() skills\n\nChange the outcome and predictor variables to math and physics\nWhat do the results tell you about the relationship between maths and physics ability?\n\n\n\n\n\nIn the following discussion, I will present some model formulas as equations. I am not doing that because the discussion is going to consider the modeling in terms of the underlying mathematics. I am doing it with the aim of clarifying how quantities – observed, estimated or predicted – add up, in terms of the linear and then the linear mixed-effects model.\n\nI am going to refer to the dependent or outcome variable (e.g., school grade) as \\(y\\),\nand to explanatory or experimental or independent variables (e.g., language skill) as \\(X\\).\nModels shall be fitted to estimate the coefficients, written \\(\\beta\\), of the effects of the explanatory variables on the outcome variable\nTo distinguish the different coefficients of the different effects, I am going to number the coefficients so …\n\n\n\\(\\beta_0\\) is the coefficient of the intercept;\n\\(\\beta_1\\) will be the coefficient of the effect of a first explanatory variable: in the Brazilian schools example, the coefficient of the effect of variation in Portuguese language skill on variation in English language scores.\n\nTo make it clear that each observation can be understood as part of a complex multilevel or crossed random effects structure, I am going to use indices as subscripts for variables.\n\nI will, here, index individual participants (children) using \\(i\\);\nI will index index classes in which children were tested using \\(j\\).\n\nThus, in the Brazilian schools example, we shall see that we are concerned with observations about children’s school grades, where children are sampled as individuals nested in samples of classes. We will examine how an outcome variable (English language grade) is related to a predictor variable (Portuguese language grade) such that:\n\n\\(y_{ij}\\) is the outcome English language grade recorded for each child \\(i\\) in each class \\(j\\);\nwhile \\(X_{ij}\\) represents the explanatory variable, see following, the Portuguese language grade.\n\n\\(X_{ij}\\) is subscripted \\(_{ij}\\) because values of the variable depend upon child identity, and children are identified as child \\(i\\) in class \\(j\\) to represent the multilevel structure of the data\n\n\n\nThe linear model ignores the higher-level structure, the distinction between classes: does this matter?\nWe can see the answer to that question if we inspect Figure 4. We create the plot using the following chunk of code; we discuss that later, first reflecting on what the plot shows us.\n\nbrazil %&gt;%\nggplot(aes(x = portuguese, y = english)) +\ngeom_point(colour = \"darkgrey\") +\n  geom_smooth(method = \"lm\", se = FALSE, colour = \"black\") +\n  facet_wrap(~ class_number) +\n  xlab(\"Portuguese\") + ylab(\"English\") +\n  theme_bw() +\n  scale_x_continuous(breaks=c(25,50,75)) + scale_y_continuous(breaks=c(0,50,100))\n\n\n\n\n\n\n\nFigure 4: Plot of child grades, comparing English with Portuguese grades, shown separately for each school class\n\n\n\n\n\nFigure 4 presents a grid of scatterplots, with a different scatterplot to show the relationship between children’s Portuguese and English grades for the children in each different class. We can see that the relationship between Portuguese and English grades is (roughly) similar across classes: in general, children with higher Portuguese grades also tend to have higher English grades. However, Figure 4 makes it obvious that there are important differences between classes.\nWe can see that the slope of the best fit line (shown in black) varies between classes. And we can see that the intercept (where the line meets the y-axis) also varies between classes. Further, we can see that in some classes, there is no relationship or a negative relationship between Portuguese and English grades.\nCritically, we can see that classes differ in how much data we have for each. For some classes, we have many observations (e.g., M11) and for other classes we have few or one observation (e.g., M22, with one child). We know, in advance, that variation in sample size will be associated with variation in the uncertainty we have about the estimated relationship between Portuguese and English grades. You will remember that the Central Limit Theorem allows us to calculate the standard error of an estimate like the mean, given the sample size, and that the standard error is an index of our uncertainty of the estimate.\n\n\n\n\n\n\nTip\n\n\n\nFacetting – notice that, in the plotting code, the key expression is facet_wrap(~ class_number) This means:\n\nThe class_number variable is a factor: we ask R to check the summary for the dataframe, and that factor codes what class a child is in.\nThe facet_wrap(...) function then asks R to produce separate plots for each facet for the data – the word facet means face or aspect.\nWe use the formula facet_wrap(~ ...) to ask R to split the data up by using the classification information in the named variable, here class_number.\n\n\n\nThe production of a grid or lattice of plots is a useful method for comparing patterns between data sub-sets or groups. You can see more information about facet_wrap() here\n\n\n\n\n\n\nTip\n\n\n\nAdjusting scales – notice that in these plots I modified the axes to show x-axis and y-axis ticks at specific locations using scale functions. The tick is the notch or short line where we show the numbers on the axes.\n\nscale_x_continuous(breaks=c(25,50,75)) means: set the x-axis ticks at 25, 50 and 75, defined using a vector c() of values\nscale_y_continuous(breaks=c(0,50,100)) means: set the y-axis ticks at 0, 50, 100, defined using a vector of values.\n\n\n\nIt can be useful to adjust the ticks on axes, where other plot production factors cause crowding of labels.\nYou can see more information on scales here.\n\n\nDo the following exercises to practice your facet_wrap() skills\n\nChange the x and y variables to math and physics.\nExperiment with showing the differences between classes in a different way: instead of using facet_wrap() in aes() add colour = class_number: what happens?\n\n\n\n\n\nFigure 4 shows that there is variation between classes in both the average English grade, shown as differences in the y-axis intercept, and the ‘effect’ of Portuguese language skill, shown as differences in the slope of the best fit line for the predicted relationship between Portuguese and English grades.\nWe put ‘effect’ in quotes to signal the fact that we do not wish to assert a causal relation. The interpretation of the results of the linear model assumes those results are valid provided that the observations are independent, among other things. We can see that we cannot make the assumption of independence because individuals in classes with high average English grades are more likely to have higher English grades.\nWe can represent in our analysis the information we have about hierarchical structure in the data (child within class) by allowing the regression coefficients to vary between groups. We therefore modify the subscripting to take into account the fact that we must distinguish which child \\(i\\) and which class \\(j\\) we are examining, adapting our model to:\n\\[\ny_{ij} = \\beta_{0j} + \\beta_{1j}X_{ij} + e_{ij}\n\\]\n\nwhere \\(y_{ij}\\) represents the outcome measure, the English grade for each \\(i\\) child in each \\(j\\) class;\n\\(\\beta_{0j}\\) represents the average grade, different in different classes;\n\\(\\beta_{1j}X_{ij}\\) represents the variation in \\(X\\) the Portuguese grade for each \\(i\\) child in \\(j\\) class, with the effect of that variation estimated as the coefficient \\(\\beta_{1j}\\), different in different classes;\nand \\(e_{ij}\\) represents differences between observed and predicted English grades for each \\(i\\) child in \\(j\\) class.\n\n\n\n\nIn practice, we could capture the variation between classes by performing a two-step analysis.\n\nFirst, we estimate the coefficient of the ‘Portuguese’ effect for each class separately. We do multiple (per-class) analyses. In each of these analyses, we estimate the coefficient looking only at the data for one class.\nSecond, we can take those per-class coefficients as the outcome variable in a ‘slopes-as-outcomes analysis’ to examine if the per-class estimates of the experimental effect are reliably different from zero or, more usefully, if the per-class estimates vary in relation to some explanatory variable like teacher skill.\n\nThe problem with the approach is apparent in Figure 5: the figure shows the estimated intercept and coefficient of the slope of the ‘Portuguese’ effect for each class, when we have analyzed the data for each class in a separate linear model.\nThe estimate for each class is shown as a black dot. The standard error of the estimate is shown as a black vertical line, shown above and below a point.\nYou can say that where there is a longer line there we have more uncertainty about the location of the estimate. Notice that the standard errors vary a lot between classes. In some classes, the standard error is small (the black line is short) so we can maybe have more certainty over the estimated intercept or slope for those classes. In other classes, the standard error is large (the black line is long) so we can maybe have less certainty over the estimated intercept or slope for those classes.\n\n\n\n\n\n\nImportant\n\n\n\nThe key idea here is that standard errors vary widely between classes but the two-step modeling approach, while it can take into account the between-class differences in estimates, cannot account for the variation in the standard errors about those estimates.\n\n\n\n\n\n\n\n\n\n\nFigure 5: Plot showing the estimated intercept and coefficient of the slope of the Portuguese effect for each class analysed separately\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNotice that the code I used fits a separate model for each class, and then plots the per-class estimates of intercepts and slopes of the english ~ portuguese relationship.\n\n\n(In case you want to see how this plot is produced, I work through the code for drawing Figure 5 in the workbook .R.)\n\n\nThe ‘slopes-as-outcomes’ approach is quite common and can be found in a number of papers in the psychological and educational research literatures. An influential example can be found in the report by Balota et al. (2004) of their analysis of psycholinguistic effects in older and younger adults. Balota et al. (2004) wanted to examine if or how effects of variables like word frequency, on reading response latencies, were different in different age groups. To do this, they first estimated the effect of the (word-level) psycholinguistic variables in separate linear model (multple regressions) for each adult. They then took the estimated coefficients as the dependent variable (slopes-as-outcomes) for a second analysis in which they tested the effect of age group on variation in the estimated coefficients (see Lorch & Myers (1990) for a discussion of the approach).\nYou may be asked to do this. I would advise you against it because there is a better way, as we see next.\n\n\n\n\nMultilevel models incorporate estimates of the intercept and the effect of independent (experimental or correlational) variables plus estimates of the random variation between classes in intercepts and slopes. Multilevel models are also known as mixed-effects models because they involve the estimation of fixed effects (effects due to independent variables) and random effects (effects due to random differences between groups).\nWe model the intercept (varying between classes) as:\n\\[\n\\beta_{0j} = \\gamma_0 + U_{0j}\n\\]\n\nwhere \\(\\beta_{0j}\\) values equal \\(\\gamma_0\\) the overall average intercept,\nplus \\(U_{0j}\\) the differences, for each class, between that average intercept and the intercept for that class.\n\nWe model the coefficient of the (Portuguese language) skill effect as:\n\\[\n\\beta_{1j} = \\gamma_1 + U_{1j}\n\\]\n\nwhere \\(\\beta_{1j}\\) values equal \\(\\gamma_1\\) the average slope,\nplus \\(U_{1j}\\) the differences, for each class, between that average slope and the slope for that class.\n\nThese models can then be combined:\n\\[\ny_{ij} = \\gamma_0 + \\gamma_1X_{ij} + U_{0j} + U_{1j}X_{ij} + e_{ij}\n\\]\nsuch that the English grade observed for each child \\(y_{ij}\\) can be predicted given:\n\nthe average grade overall \\(\\gamma_0\\) plus\nthe average relationship between English and Portuguese language skills \\(\\gamma_1\\) plus\nadjustments to capture\n\n\nthe difference between the average grade overall and the average grade for the child’s class \\(U_{0j}\\),\nas well as the difference between the average slope of the Portuguese effect and the slope of that effect for their class \\(U_{1j}X_{ij}\\),\nplus any residual differences between the observed and the model predicted English grade \\(e_{ij}\\).\n\n\n\n\nIf you have had some experience analyzing psychological data, then there is a potential approach to thinking about the effect of the differences between classes that would seem natural. This approach has, in fact, been proposed (see e.g. Lorch & Myers, 1990) and applied in the literature. I would not recommend using it but thinking about it helps to develop our understanding of what we are doing with multilevel models\nWe could seek to estimate (1.) the relationship of interest – here, the association between English and Portuguese grades, while (2.) also estimating the relationship between (outcome) English grades and the impact made by what class a child is in.\nIn this approach, we would construct a model in which we have English grades as the outcome variable, Portuguese grades as one predictor variable, and then add a variable to code for class identity. The ‘effect’ of the class variable would then capture the differences in intercepts between classes. You could add a further variable to code for differences between classes in the slope of the relationship between English and Portuguese grades. This would allow for the fact that the relationship is positive or negative, stronger or weaker, in different classes.\nIf we added that further variable to code for differences between classes in the English-Portuguese relationship, then we would be estimating those differences as interactions between (1.) the predictive ‘effect’ of Portuguese grades on English grades and (2.) the class effect. An interaction effect is what we have when the effect of one variable (the predictive ‘effect’ of Portuguese grades) is different for different levels of the other variable (the predictive ‘effect’ of Portuguese grades is different for different classes).\nThinking about this approach helps us to think about what we are doing when we are working with multilevel structured data. But the problem with the approach is that it does not allow us to generalize beyond the sample we have. The estimates we would have, given a model in which we code directly for class, would tell us only about the classes in our sample.\nMost of the time, we would prefer to do analyses whose results could be generalized: to other children, to other classes, etc.. For this reason, it makes more sense to suppose that \\(U_{0j} + U_{1j}\\), the class-level deviations, are unexplained differences drawn at random from a population of potential class differences.\nWhat does this mean? Think back to your understanding of the linear model. You have learnt that when we fit a linear model like \\(y_i = \\beta_0 + \\beta_1X_i + e_i\\) we include a term \\(e_i\\) to represents the differences, for each child, between the observed (outcome) English grade and the English grade predicted by the relationship with Portuguese grades. Those differences between observed and predicted outcomes are called residuals. We assume that the direction and size of any one residual, for any one child, is randomly determined because we typically have no idea why there might be a big difference between the predicted and observed grade for one child but a small residual difference for another.\nNow, we can imagine that there will be many classes in many schools, and we can surely expect that there will be differences between the classes. These differences will result from plenty of factors we do not measure or cannot explain. Indeed, we have seen that there are differences between the average (outcome) English grade i.e. the predicted intercept and the observed intercept for each class, and we have seen that there are differences between the average slope of the English-Portuguese grades relationship i.e. the predicted slope and the slope for each class. These differences would, in effect, be random differences. And we can see how the variation in these differences are, for us, just a kind of random error variance, which we can see as class-level residuals.\nWe suppose, technically, that the differences between classes, controlling for the effect of the explanatory variable, are exchangeable (it does not matter which class is which), with classes varying at random. Multilevel models incorporate estimation of the explanatory variables effects \\(\\gamma_0 + \\gamma_1\\) accounting for group-level error variance \\(U_{0j} + U_{1j}\\).\nThe difference between the two-step approach and the multilevel modelling approach is this:\n\nIn the two-step approach, seen earlier, we estimate – separately, for each class – the intercept and the slope of the Portuguese effect, using just the data for that class and ignoring the data for other classes.\nIn the multilevel model, in contrast, we use all the observations, estimating the average intercept and the average slope of the Portuguese effect plus the variance due to the difference for each class (1.) between the average intercept and the class intercept or (2.) between the average slope and the class slope.\nWe can understand these differences between the average (intercept or class) and the class differences as class-level residuals \\(U_{0j} + U_{1j}\\) in addition to the child-level residuals \\(e_{ij}\\).\n\nThis leads us to a conclusion that represents a critical way to understand multilevel models.\n\n\n\n\n\n\nImportant\n\n\n\nThe multilevel model is a linear model but with multiple random effects terms to take into account the hierarchical structure in the data.\n\n\n\n\n\nWe can fit a multilevel model of the english ~ portuguese relationship, taking into account the fact that the pupils tested in the study were recruited from different classes, using the convenient and powerful lmer() function.\nHappily, the code we use to define a model in lmer() is much like the code we use to define a model in lm() with one important difference which I shall explain. Let’s try it out.\nTo use the lmer() function you need to make the lme4 library available.\n\nlibrary(lme4)\n\nThe model you are going to code will correspond to the statistical model that we have been discussing:\n\\[\ny_{ij} = \\gamma_0 + \\gamma_1X_{ij} + U_{0j} + U_{1j}X_{ij} + e_{ij}\n\\]\nAnd the code is written as follows.\n\nporto.lmer1 &lt;- lmer(english ~ portuguese +\n\n                              (portuguese + 1|class_number),\n\n                              data = brazil)\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.0391843 (tol = 0.002, component 1)\n\n\nYou can see that the lmer() function call is closely similar to the lm() function call, with one critical exception, as I explain next.\nFirst, we have a chunk of code mostly similar to what we do when we do a regression analysis.\n\nporto.lmer1 &lt;- lmer(...) creates a linear mixed-effects model object using the lmer() function.\nenglish ~ portuguese is a formula expressing the model in which we estimate the fixed effect on the outcome or dependent variable english (English grades) predicted \\(\\sim\\) by the independent or predictor variable portuguese (Portuguese grades).\nIf there were more terms in the model, the terms would be added in series separated as variable names separated by + sum symbols.\n...(..., data = brazil) specifies the dataset in which you can find the variables named in the model fitting code.\nsummary(porto.lmer1) gets a summary of the fitted model object.\n\nSecond, we have a bit that is specific to multilevel or mixed-effects models.\n\nCritically, we add (...|class_number) to tell R about the random effects corresponding to random differences between sample groups (classes) coded by the class_number variable.\n(...1 |class_number) says that we want to estimate random differences between sample groups (classes) in intercepts coded 1.\n(portuguese... |class_number) adds random differences between sample groups (classes) in slopes of the portuguese effect coded by using the portuguese variable name.\n\nIf you run the model code as written – see the .R workbook file for an example of the code – and it works then the code will be shown in the console window in R-Studio. To show the model results, you need to get a summary of the model, using the model name.\n\nsummary(porto.lmer1)\n\n\n\nMixed-effects model code is hard to get used to at first. A bit of practice helps to show you which bits of code are important, and which bits you will change for your own analyses\n\nChange the outcome (from english) and the predictor (from portuguese): this is about changing the fixed effect part of the model.\nVary the random effects part of the model.\n\n\nChange it from (portuguese + 1 | class_number) to (1 | class_number) what you are doing is asking R to ignore the differences in the slope of the effect of Portuguese grades.\nChange it from (portuguese + 1 | class_number) to (portuguese + 0 | class_number what you are doing is asking R to ignore the differences in the intercept.\n\nTry out these variations and look carefully at the different results.\n\n\n\n\nNow let’s take a look at the results.\n\nsummary(porto.lmer1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: english ~ portuguese + (portuguese + 1 | class_number)\n   Data: brazil\n\nREML criterion at convergence: 2104.3\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.81240 -0.59545  0.04338  0.59975  2.23667 \n\nRandom effects:\n Groups       Name        Variance Std.Dev. Corr \n class_number (Intercept) 338.9057 18.4094       \n              portuguese    0.3278  0.5725  -0.98\n Residual                 493.2866 22.2101       \nNumber of obs: 230, groups:  class_number, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  25.2838     6.2522   4.044\nportuguese    0.6589     0.1726   3.817\n\nCorrelation of Fixed Effects:\n           (Intr)\nportuguese -0.943\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.0391843 (tol = 0.002, component 1)\n\n\nNotice that the output has a number of elements.\n\nFirst, we see information about the function used to fit the model, and the model object created by the lmer() function call.\nThen, we see the model formula english ~ portuguese + (portuguese + 1 | class_number).\nThen, we see `REML criterion at convergence: about the model fitting process, which we can usually ignore.\nThen, we see information about the distribution of the model residuals.\nThen, we see information about the error terms estimated (technically, predicted) by the model.\n\n\nResiduals, just like a linear model plus error terms specific to multilevel or mixed-effects models, group-level residuals:\ndifferences between the average intercept and, here, the intercept (average `english: score) per class;\nand differences between the average slope capturing the \\(english \\sim portuguese\\) relationship and the slope (of the effect) per class.\n\n\nThen, just as for linear models, we see estimates of the coefficients (of the slopes) of the fixed effects, the intercept and the slope of the \\(english \\sim portuguese\\) relationship.\n\nNote that we see coefficient estimates like in a linear model summary but no p-values. We will come back to p-values later but note that their absence is not a bug. Note also that we do not see an \\(R^2\\) estimate. We will come back to that too.\n\n\nThere is no convention, yet, on how to report the results of these models. Lotte Meteyard and I argue for a set of conventions that will help researchers to understand each others’ results better.\n\nIn this exercise, go read the bit where we advise Psychologists how to write about the results, in our paper (Meteyard & Davies, 2020).\n\n\n\n\n\nYou will have noticed the reference to fixed and random effects in the discussion in this chapter, and the use of fixed and random effects as titles for sections of the model output. The terms fixed effects and random effects are not used consistently in the statistical literature (Gelman & Hill, 2007). I am going to use these terms because they are helpful, at first, and because they are widely used, not least in the results of our analyses in R.\nIt is common in the psychological literature to refer to the effects on outcomes of experimental manipulations (e.g., the effect on outcomes of differences between experimental conditions) or to the effects of correlated variables of theoretical or explanatory interest (e.g., the effect of differences in Portuguese language skill) as fixed effects. Typically, we are aiming to get estimates of the coefficients of these effects. And, much like we would do when we use linear models, we expect that these coefficients represent an estimate of the effects of these variables, on average, across the population. (Hence, some analysts prefer to talk about these effects as population average effects).\nIn comparison, when we are thinking about the effects on outcomes of what we understand to be the random differences between sampled children (e.g., the child-level residuals) or the random differences between sampled classes (e.g., the class-level residuals) then we refer to these effects as random effects. As we have seen, we usually estimate random effects as variances and can estimate them as random error variances. While we may care to estimate how differences in, say, Portuguese language score, is associated with English grade, we typically do not care about the impact of the specific difference between any two classes in English grade.\nHowever, if you take your education in this area further, you will find that the way that fixed and random effects are talked about in the statistical literature can be, at best, inconsistent. And, ultimately, you might ask yourself if there are principled distinctions between fixed and random effects. We can leave these problems aside, here, because they do not influence how we shall learn and practice using multilevel models in the early to medium term in our development of skills and understanding.\n\n\n\nRecall that the linear model yields the estimate that for unit increase in Portuguese grade there is an associated .47 increase in English grade, on average Section 1.9.5. We can see that the estimate of the effect for the mixed-effects model is \\(\\beta = .659\\) which is somewhat different from the effect estimate we got from the linear model.\n\n\n\n\n\n\nNote\n\n\n\n\nResearch question: Does Portuguese grade predict English grade?\nResult: Our analysis shows that children who score one grade higher in their Portuguese class (e.g., 61 compared to 60) tended to score about \\(.66\\) of a grade higher in their English class.\n\n\n\n\n\n\n\nPsychological studies frequently result in hierarchically structured data. The structure can be understood in terms of the grouping of observations, as when there are multiple observations per group, participant or stimulus. The existence of such structure must be taken into account in analyses. Multilevel or mixed-effects models can be specified by the researcher to include random effects parameters that capture unexplained differences between participants or other sampling units in the intercepts or the slopes of explanatory variables. Where a sample of participants is asked to respond to a sample of stimuli, structure relating both to participants and to stimuli can be incorporated.\nMany researchers will be aware of concerns over the non-replication of published effects in psychological science (see the discussion here). As Gelman (2014) discusses, non-replication of results may arise if effects vary between contexts groups while traditional analytic methods assume that effects are constant. Psychological researchers can expect average outcomes and the effects of independent variables to vary among sampling units, whether their investigations involve multiple observations per child or stimulus, or children sampled within classes, clinics or schools. However, traditional analytic methods often require us to ignore this variation by averaging, say, responses over multiple trials for each child, to collapse an inherently multilevel data structure into a single level of observations that can be analysed using regression or ANOVA. By using multilevel models, researchers will, instead, be able to properly estimate the effects of theoretical interest, and better understand how those effects vary.\n\n\n\nWe outlined the features of a multilevel structure dataset.\nWe then discussed visualizing and modeling the relationship between outcome and predictor variables when you ignore the multilevel structure.\nWe examined why ignoring the structure is a bad idea.\nWe considered how slopes-as-outcomes analyses are an approximate method to take the structure into account.\nWe dmonstrated how multilevel modeling is a better method to estimate the relationship between outcome and predictor variables\n\n\n\n\nWe used functions to read-in the libraries of functions we needed.\n\nlibrary(tidyverse)\nlibrary(lme4)\n\nWe used some new functions, or focused on functions we have seen before but not discussed.\n\nWe used load() to load an .RData file into R workspace.\nFor visualization, we used facet_wrap() to show plots of the relationship between English and Portuguese scores in each class separately.\nWe used lmer() to fit a multilevel model.\n\nWe used the summary() function to get model results for both linear models and for the multilevel or linear mixed-effects model.\n\n\n\n\nSnijders & Bosker (2004) present a helpful overview of multilevel modelling. Readers familiar with the book will see that I rely on it to construct the formal presentation of the models.\nGelman & Hill (2007) present a more in-depth treatment of multilevel models.\n\n\n\n\nBaayen, R. H., Davidson, D. J., & Bates, D. M. (2008). Mixed-effects modeling with crossed random effects for subjects and items. Journal of Memory and Language, 59(4), 390–412. https://doi.org/10.1016/j.jml.2007.12.005\n\n\nBalota, D. A., Cortese, M. J., Sergent-Marshall, S. D., Spieler, D. H., & Yap, M. J. (2004). Visual Word Recognition of Single-Syllable Words. Journal of Experimental Psychology: General, 133(2), 283–316. https://doi.org/10.1037/0096-3445.133.2.283\n\n\nBarr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of Memory and Language, 68(3), 255–278. https://doi.org/10.1016/j.jml.2012.11.001\n\n\nGelman, A. (2014). The Connection Between Varying Treatment Effects and the Crisis of Unreplicable Research. Journal of Management, 41(2), 632–643. https://doi.org/10.1177/0149206314525208\n\n\nGelman, A., & Hill, J. (2007). Data analysis using regression and multilevel/hierarchical models. Cambridge University Press.\n\n\nGoldstein, H. (1995). Multilevel statistical models. Edward Arnold.\n\n\nGolino, H., & Gomes, C. (2014). Psychology data from the “BAFACALO project: The Brazilian Intelligence Battery based on two state-of-the-art models  Carroll’s Model and the CHC model”. Journal of Open Psychology Data, 2(1), e6. https://doi.org/10.5334/jopd.af\n\n\nKreft, I., & Leeuw, J. de. (1998). Introducing multilevel modeling (D. Wright, Ed.). Sage Publications.\n\n\nLorch, R. F., & Myers, J. L. (1990). Regression analyses of repeated measures data in cognitive research. Journal of Experimental Psychology: Learning, Memory and Cognition, 16(1), 149–157.\n\n\nMeteyard, L., & Davies, R. A. I. (2020). Best practice guidance for linear mixed-effects models in psychological science. Journal of Memory and Language, 112, 104092. https://doi.org/10.1016/j.jml.2020.104092\n\n\nRaudenbush, S. W., & Bryk, A. S. (2002). Hierarchical linear models: Applications and data analysis methods (Vol. 1). sage.\n\n\nSnijders, T. A. B., & Bosker, R. J. (2004). Multilevel analysis: An introduction to basic and advanced multilevel modeling. Sage Publications Ltd.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 16. Conceptual introduction to multilevel data"
    ]
  },
  {
    "objectID": "PSYC412/part2/01-multilevel.html#sec-multi-motivations",
    "href": "PSYC412/part2/01-multilevel.html#sec-multi-motivations",
    "title": "Week 16. Conceptual introduction to multilevel data",
    "section": "",
    "text": "In this chapter, we shall start to develop skills in using a method or approach that is essential in modern data analysis: multilevel modeling. We are going to invest four weeks in working on this approach. This investment is designed to give you a specific, important, advantage in your work as a psychologist, or as someone who produces or consumes psychological research.\n\n\n\n\n\n\nNote\n\n\n\nMultilevel models: why do we need to do this? - Four weeks is a lot of time to spend on one method.\n\n\nIt is now clear that someone who works in psychological research has to know about multilevel or hierarchically structured data, and has to know how to apply multilevel models (or mixed-effects models). Growth in the popularity of these kinds of analysis has been very very rapid, as can be seen in Figure 1. It is now, effectively, the standard or default method for professional data analysis in most areas of psychological and other social or clinical sciences (or it soon will be). There are good reasons for this (Baayen et al., 2008).\n\n\n\n\n\n\nFigure 1: Number of Pubmed citations for ‘Linear Mixed Models’ by year. Generated using the tool available at http://dan.corlan.net/medline-trend.html, entering “Linear Mixed Models” as the phrase search term and using data from 2000 to 2018, from Meteyard and Davies (2020) – used without permission\n\n\n\nWe continue to teach ANOVA and multiple regression (linear models) in our courses because the research literature is full of the results of analyses done using these methods and because many psychologists continue to use these methods in their research. However, there is increasingly wide-spread recognition that these classical methods have serious problems when applied to data with hierarchical structure. Because most psychological data (not all) will have hierarchical structure, this makes learning about multilevel or mixed-effects methods a key learning objective.\nBut, because it is relatively new, many professional psychologists struggle to understand why or how to use these methods effectively. This means that students who acquire the skill graduate with a clear employability advantage. It also means that we have to take seriously the challenge of learning about these methods. This is why we will spend a bit of time on them. In my experience, in over a decade of teaching multilevel models, in working with both students and professionals, we shall need to develop understanding and skills gradually. We will work patiently, so that we can secure understanding by building our learning through a series of practical examples, increasing the scope of our practical skills, and developing the sophistication of our understanding, step-by-step, as we go.\n\n\nMultilevel models are also known as hierarchical models or linear mixed-effects models or random effects models. People use these terms interchangeably. They also use the abbreviations LMMs or LMEs. Sorry about that: humans make methods, and the names we use for things do vary.\nI will only use the terms multilevel or linear mixed-effects models.\n\n\n\n\n\n\nWarning\n\n\n\n\nIn this chapter, we emphasize the multilevel perspective but, to anticipate future development, we will come to think in terms of mixed-effects models.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 16. Conceptual introduction to multilevel data"
    ]
  },
  {
    "objectID": "PSYC412/part2/01-multilevel.html#sec-multi-challenges",
    "href": "PSYC412/part2/01-multilevel.html#sec-multi-challenges",
    "title": "Week 16. Conceptual introduction to multilevel data",
    "section": "",
    "text": "The key challenges for learning should be explained at the start so that we know what we shall have to do to overcome them.\n\nEven though most psychological data has some sort of multilevel or hierarchical structure, we are not used to recognizing it. This is because the structure has often been hidden or ignored in the education and practice of traditional research methods in Psychology.\n\nIn time, you will come to see multilevel structure everywhere (Kreft & Leeuw, 1998). But first you will need to get some practice so that you can become familiar with the idea and learn to recognize what it looks like when data have a multilevel or hierarchical structure. This is why we will examine multilevel structured data across a range of different kinds of experiments or surveys, over a series of weeks.\nWe will learn to identify and understand hierarchical structure in psychological data by just looking at datasets, by producing visualizations, by doing analyses, and by trying to explain to ourselves and each other what we think we see.\n\nThe ideas that support an understanding of why and how we use multilevel models can be intimidating when we first encounter them. The mathematics behind how the models work is both profound and sophisticated. But the good news is that we can practice the application of the analysis method while talking and thinking about the critical ideas using just words or plots.\n\nWe cannot or should not avoid engaging with the ideas – because we have to be able to explain what we are doing – but there are many routes to an effective understanding. For those who want to develop a more mathematically-based perspective, I will provide references to important texts in the literature on multilevel models (see Section 1.11).",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 16. Conceptual introduction to multilevel data"
    ]
  },
  {
    "objectID": "PSYC412/part2/01-multilevel.html#the-key-idea-to-get-us-started",
    "href": "PSYC412/part2/01-multilevel.html#the-key-idea-to-get-us-started",
    "title": "Week 16. Conceptual introduction to multilevel data",
    "section": "",
    "text": "Important\n\n\n\nMultilevel models are a general form of linear model.\n\n\nAnother way of saying this is: linear models are a special form of multilevel models.\nThis is because linear models assume that observations are independent. We often cannot make this assumption, as we shall see. More generally, then, we do not assume that observations are independent and so we use multilevel models.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 16. Conceptual introduction to multilevel data"
    ]
  },
  {
    "objectID": "PSYC412/part2/01-multilevel.html#sec-multi-approach",
    "href": "PSYC412/part2/01-multilevel.html#sec-multi-approach",
    "title": "Week 16. Conceptual introduction to multilevel data",
    "section": "",
    "text": "We are not going to take a mathematical approach to learning about multilevel models. We do not have to. The approach we are going to take is:\n\nverbal – We will talk about the main ideas in words. Sometimes, we will present formulas but that is just to save having to use too many words.\nvisual – We will show ourselves and each other what multilevel structure in data looks like, and what that structure means for our analyses of behaviour.\npractical – We will use R to complete analyses, so we will learn about coding models in practice. Fortunately, through coding we can get a clear idea of what we want the models to do.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 16. Conceptual introduction to multilevel data"
    ]
  },
  {
    "objectID": "PSYC412/part2/01-multilevel.html#sec-multi-targets",
    "href": "PSYC412/part2/01-multilevel.html#sec-multi-targets",
    "title": "Week 16. Conceptual introduction to multilevel data",
    "section": "",
    "text": "Our learning objectives include the development of key concepts and skills.\n\nconcepts – how data can have multilevel structures and what this requires in models\nskills – where skills comprise the capacity to:\n\n\nuse visualization to examine observations within groups\nrun linear models over all data and within each class\nuse the lmer() function to fit models of multilevel data\n\nWe are just getting started. Our plan will be to build depth and breadth in understanding as we progress over the next few weeks.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 16. Conceptual introduction to multilevel data"
    ]
  },
  {
    "objectID": "PSYC412/part2/01-multilevel.html#sec-multi-guide",
    "href": "PSYC412/part2/01-multilevel.html#sec-multi-guide",
    "title": "Week 16. Conceptual introduction to multilevel data",
    "section": "",
    "text": "I have provided a collection of materials you can use. Here, I explain what they are and how I suggest you use them.\n1. Video recordings of lectures\n1.1. I have recorded a lecture in three parts. The lectures should be accessible by anyone who has the link.\n\nPart 1 – about 16 minutes\nPart 2 – about 13 minutes\nPart 3 – about 13 minutes\n\n1.2. I suggest you watch the recordings then read the rest of this chapter.\n\nThe lectures provide a summary of the main points.\n\n1.3. You can download the lecture slides in three different versions:\n\n402-week-17-LME-1.pdf: high resolution .pdf, exactly as delivered [4 MB];\n402-week-17-LME-1_1pp.pdf: low resolution .pdf, printable version, one-slide-per-page [359 KB];\n402-week-17-LME-1_6pp.pdf: low resolution .pdf, printable version, six-slides-per-page [359 KB].\n\nThe high resolution version is the version delivered for the lecture recordings. Because the images are produced to be high resolution, the file size is quite big (4 MB) so, to make the slides easier to download, I produced low resolution versions: 1pp and 6pp. These should be easier to download and print out if that is what you want to do.\n2. Chapter: 01-multilevel\n2.1. I have written this chapter to discuss the main ideas and set out the practical steps you can follow to start to develop the skills required to analyse multilevel structured data.\n2.2. The practical elements include data tidying, visualization and analysis steps.\n2.3. You can read the chapter, see in the chapter how to run the code, and do the exercises.\n\nRead in the example data BAFACALO_DATASET.RData and identify how the data are structured at multiple levels.\nUse visualizations to explore the impact of the structure.\nRun analyses using linear models (revision) and linear mixed-effects models (extension) code.\nReview the recommended readings (Section 1.11).\n\n3. Practical materials\n3.1 In the following sections, I describe the practical steps, and associated practical materials (exercise workbooks and data), you can use for your learning.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 16. Conceptual introduction to multilevel data"
    ]
  },
  {
    "objectID": "PSYC412/part2/01-multilevel.html#sec-multi-data-intro",
    "href": "PSYC412/part2/01-multilevel.html#sec-multi-data-intro",
    "title": "Week 16. Conceptual introduction to multilevel data",
    "section": "",
    "text": "In this chapter, we will be working with data taken from a study on education outcomes in Brazilian children, reported by Golino & Gomes (2014). First, we will progress through the steps required to download and prepare the files for analysis in R.\nThe BAFACALO_DATASET.RData data were collected and shared online by Golino & Gomes (2014). Information about the background motivating the study, the methods of data collection, along with the dataset itself, can be found here.\nGolino & Gomes (2014) collected school end-of-year subject grades for a sample of 292 children recruited from multiple classes in a school in Brazil. Here, each subject is a theme or course that children studied in school, e.g., Physics or English language, and for which each child was awarded a grade at the end of the school year. So, we have information on different children, and information about their subject grades. Children were taught in different classes but the classes appear to be units of the school organization (the information is not quite clear) not subject or course groupings. Thus, we also have information on which classes children were in when data about them were collected.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nGet the data: get the data file and the .R script you can use to do the exercises that will support your learning.\n\nYou can download the files folder for this chapter by clicking on the link 01-multilevel.zip.\n\n\n\nThe practical materials folder includes:\n\nBAFACALO_DATASET.RData\n402-01-multilevel-workbook.R the workbook you will need to do the practical exercises.\n\nThe data file is in the .RData format: .RData is R’s own file format so the code you use to load and access the data for analysis is a bit simpler than you are used to, see Section 1.7.2 next. (You will have used read.csv() or read_csv() previously.)\nIn my own practice, I prefer to keep files in formats like .csv which can be opened and read in other software applications (like Excel), so using an .RData is an exception in these materials.\nYou can also access the data from the link associated with the Golino & Gomes (2014) article here.\n\n\n\nDuring practical sessions, each week, you can use the workbook to prompt your code construction and your thinking, with support.\nAfter practical sessions, you will be able to download an answers version of the workbook .R so check back here after the session.\n\n\n\n\n\n\n\nImportant\n\n\n\nGet the answers: get the data file and the .R script with answers.\n\n\n\nThe link to this folder will not be provided until after a session has finished.\nWhen the link is live, you will be able to download a folder including:\n\n402-01-multilevel-workbook-with-answers.R with answers to questions and example code for doing the exercises.\n\n\n\n\n\nYou can read the BAFACALO_DATASET.RData file into the R workspace or environment using the following code.\n\nload(\"BAFACALO_DATASET.RData\")\n\n\n\n\nThe dataset consists of rows and columns. Take a look. If you have successfully loaded the dataset into the R environment, then you should be able to view it.\nYou could look at the dataset by using the View() function.\n\nView(BAFACALO_DATASET)\n\nOr you can use the head() function to see the top few rows of the dataset.\n\nhead(BAFACALO_DATASET)\n\nYou can check for yourself that each row holds data about a child, including their participant identity code, as well as information about their parents, household, gender, age, school class, and their grades on end-of-year subject attainment (e.g., how well they did in English language).\nThere are many more variables in the dataset than we need for our exercises, and a summary would fill pages. You can see for yourself if you inspect the dataset using\n\nsummary(BAFACALO_DATASET)",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 16. Conceptual introduction to multilevel data"
    ]
  },
  {
    "objectID": "PSYC412/part2/01-multilevel.html#sec-multi-data-tidy",
    "href": "PSYC412/part2/01-multilevel.html#sec-multi-data-tidy",
    "title": "Week 16. Conceptual introduction to multilevel data",
    "section": "",
    "text": "When you inspect the file, you will see that it includes a large number of variables, but we only really care about those we will use in our exercises:\n\nparticipant_id gives the participant identity code for each child;\nclass_number gives the class identity code for the school class for each child;\nand values in portuguese, english, math, and physics columns give the score for each child in subject class attainment measures.\n\nYou can see that I am not explaining the variables in great depth. For our aims, we do not need more detailed information but please do read the data article if you wish to find out more.\nWe need to tidy the data before we can get to the analysis parts of this chapter. We are going to:\n\nSelect the variables we want to work with;\nFilter out missing values, if any are present;\nAnd make sure R knows how we want each variable to be identified; what type the variable should have.\n\nWe shall need the tidyverse library of functions.\n\nlibrary(tidyverse)\n\n\n\nWe can start by selecting the variables we want, which include those named here, ignoring all the rest. We take the BAFACALO_DATASET. We then use the select() function to select the variables we want.\n\nbrazil &lt;- BAFACALO_DATASET %&gt;%\n                    select(\n                      class_number, participant_id,\n                      portuguese, english, math, physics\n                      )\n\n\n\n\n\n\n\nTip\n\n\n\nNotice the code is written to create the new selected dataset and, at the same time, gave it a more usable (shorter) name, with brazil &lt;- BAFACALO_DATASET ...\n\n\nInspect the data to see what you have.\n\nsummary(brazil)\n\n  class_number participant_id     portuguese     english         math    \n M11    : 21   Min.   :  1.00   60     : 10   100    : 17   60     : 13  \n M15    : 20   1st Qu.: 73.75   76     :  9   79     : 11   69     :  9  \n M14    : 19   Median :146.50   65     :  8   96     :  8   70     :  9  \n M36    : 19   Mean   :146.50   73     :  8   81     :  7   62     :  8  \n M18    : 18   3rd Qu.:219.25   74     :  7   89     :  7   71     :  7  \n (Other):133   Max.   :292.00   (Other):188   (Other):180   (Other):184  \n NA's   : 62                    NA's   : 62   NA's   : 62   NA's   : 62  \n    physics   \n 60     : 16  \n 60.1   :  5  \n 0      :  4  \n 61.8   :  4  \n 66     :  4  \n (Other):197  \n NA's   : 62  \n\n\n\n\n\nIf you look at the results of the selection, you can see that there are missing values, written as e.g. NA's 62 at the bottom of each summary of each variable (if a variable column includes missing values).\n\n\n\n\n\n\nTip\n\n\n\nRemember that in R NA means “not available” i.e. missing.\n\n\nWe will need to get rid of the missing values. It is simpler to do this at the start rather than wait for an error message, later, when some arithmetic function tells us it cannot give us a result because there are NAs present.\nWe can get rid of the missing values using na.omit()\n\nbrazil &lt;- na.omit(brazil)\n\nIf you then look at a summary of the data again then you will see that the NAs are gone.\n\nsummary(brazil)\n\n  class_number participant_id    portuguese     english         math    \n M11    : 21   Min.   :  3.0   60     : 10   100    : 17   60     : 13  \n M15    : 20   1st Qu.: 77.5   76     :  9   79     : 11   69     :  9  \n M14    : 19   Median :144.5   65     :  8   96     :  8   70     :  9  \n M36    : 19   Mean   :146.6   73     :  8   81     :  7   62     :  8  \n M18    : 18   3rd Qu.:222.8   74     :  7   89     :  7   71     :  7  \n M21    : 17   Max.   :291.0   82     :  7   86     :  6   66     :  6  \n (Other):116                   (Other):181   (Other):174   (Other):178  \n    physics   \n 60     : 16  \n 60.1   :  5  \n 0      :  4  \n 61.8   :  4  \n 66     :  4  \n 74.2   :  4  \n (Other):193  \n\n\n\n\n\nBut if you look closely at the output from summary(brazil) you will see that the portuguese and english variables are summarized in the way that R summarizes factors.\nWhen you ask R to summarize factors, R gives you a count of the number of observations associated with each factor level, that is, each category in each variable. Here, it is treating a grade score like \\(100\\) in english as a category (like you might treat \\(dog\\) as a category of pets), and you can see that the count shows you that 17 children were recorded as having scored 100 in their class. We do not want numeric variables like subject grades (e.g. children’s grades in English) treated as categorical variables, factors.\nYou can also see that R gives you a numeric summary of the recorded values in the participant_id variable. This makes no sense because the identity code numbers are (presumably) assigned at random so identity numbers provide no useful numeric information for us. We do not want this either.\nWe want R to treat the educational attainment scores as numbers. We can do this using the as.numeric() function. We want R to treat the class and participant identity numbers as factors (categorical variables). We can do this using the as.factor() function.\nWe could do this one variable at a time.\n\nbrazil$portuguese &lt;- as.numeric(brazil$portuguese)\nbrazil$english &lt;- as.numeric(brazil$english)\nbrazil$math &lt;- as.numeric(brazil$math)\nbrazil$physics &lt;- as.numeric(brazil$physics)\n\nbrazil$class_number &lt;- as.factor(brazil$class_number)\nbrazil$participant_id &lt;- as.factor(brazil$participant_id)\n\nBut it is simpler and more efficient in tidyverse style. (You can see a discussion here that helped me to figure this out.)\n\nbrazil &lt;- brazil %&gt;%\n  mutate(across(c(portuguese, english, math, physics), as.integer),\n         across(c(class_number, participant_id), as.factor)) \n\nIf you now look at the summary of the data, you can see that R will give you mean etc. for the subject class score variables e.g. english, showing that it is now treating them as numeric variables. In comparison, R gives you counts of the numbers of observations for each level (category) of categorical or nominal variables like participant_id.\n\nsummary(brazil)\n\n  class_number participant_id   portuguese       english           math       \n M11    : 21   3      :  1    Min.   : 1.00   Min.   : 1.00   Min.   :  1.00  \n M15    : 20   4      :  1    1st Qu.:26.25   1st Qu.:29.00   1st Qu.: 39.00  \n M14    : 19   5      :  1    Median :42.00   Median :55.00   Median : 58.00  \n M36    : 19   6      :  1    Mean   :43.55   Mean   :50.25   Mean   : 58.17  \n M18    : 18   7      :  1    3rd Qu.:58.75   3rd Qu.:73.00   3rd Qu.: 79.75  \n M21    : 17   9      :  1    Max.   :83.00   Max.   :93.00   Max.   :104.00  \n (Other):116   (Other):224                                                    \n    physics      \n Min.   :  1.00  \n 1st Qu.: 34.00  \n Median : 71.50  \n Mean   : 71.57  \n 3rd Qu.:107.75  \n Max.   :148.00  \n                 \n\n\n\n\nR treats things like variables as vectors. A vector can be understood to be a set or list of elements: things like numbers of words.\nR gives each vector a type (factor, numeric etc.) which helps to inform different functions how to handle that vector. Usually, R assigns type correctly but sometimes it does not. We can use what is called coercion to force R to assign the correct type to a variable.\nIt is more efficient to do this at the start of an analysis workflow.\nNormally, I would use read_csv() from tidyverse and assign type to variable using col_types() specification. (See here for more information and an example.) But it is useful to learn what you need to do if you need to change the way a variable is treated after you have got the data into the R environment.\n\n\n\nIn R, there are a family of functions that work together. You can test whether a variable (a vector, technically) is or is not a certain type using the is.[something] function. For example:\n\nis.factor(brazil$english)\nis.numeric(brazil$english)\nis.character(brazil$english)\n\nAnd you can coerce variables so that they are treated as having certain types.\n\nbrazil$english &lt;- as.factor(brazil$english)\nbrazil$english &lt;- as.numeric(brazil$english)\nbrazil$english &lt;- as.character(brazil$english)\n\nNow try it out.\n\nTest out type for different variables using is...() for some of the variables.\nTest out coercion – and its results – using as...() for some of the variables.\nLook at the results using summary().",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 16. Conceptual introduction to multilevel data"
    ]
  },
  {
    "objectID": "PSYC412/part2/01-multilevel.html#sec-multilevel-ideas-intro",
    "href": "PSYC412/part2/01-multilevel.html#sec-multilevel-ideas-intro",
    "title": "Week 16. Conceptual introduction to multilevel data",
    "section": "",
    "text": "We (Psychologists) often adopt Repeated Measures or Clustered designs in our studies, and these designs yield data that have a multilevel structure. Examples of research which results in data with a multilevel structure include:\n\nStudies where we test the same people multiple times, maybe in developmental or longitudinal investigations;\nIntervention, learning or treatment studies where we need to make pre- and post-treatment comparisons;\nStudies where we present multiple stimuli and everyone sees the same stimuli;\nStudies that involve multi-stage sampling e.g. selecting a sample of classes or schools then testing a sample of children within each classes or within each school.\n\nThe key insight to keep in mind when considering the analysis of such data is that observations are clustered and are not independent: they are correlated. What is correlated with what?\nImagine testing a number of subjects by giving them all the same test. In that test, you might present them all with the same stimuli over a series of trials, so that everybody sees the same set of stimuli. Do you think an observed response recorded for any one individual will be uncorrelated i.e. independent of that person’s other responses?\nPeople are different and one usually finds that a slow or inaccurate subject is slow or inaccurate for most of their responses. That means that if you have information about one of their responses you can predict, in part, what the time or accuracy of one of their other responses would be. That capacity to predict one response from another is what we mean when we talk about a lack of independence.\nAlternatively, imagine going to test children in a school. Will the children in one class be more like each other than they are like children in other classes? In other words, is there an effect of class – maybe due to the approach of the teacher, the effect of the class environment etc. – so that outcomes for children in a class are correlated with each other?\n\n\n\n\n\n\nImportant\n\n\n\nWe are talking, here, about a really quite general property of data collected in certain, very widely used, designs in Psychology: the clustering or hierarchical ordering of data.\n\n\nThis week, we will see that we must deal with the dependence of observations within a class, where the observed responses were made about the different pupils tested in a class, for a number of different classes.\n\n\n\nThe utility of multilevel models to analyze multilevel data or hierarchically structured data is well established in education. In educational research, we often need to think about effects of interventions or correlations in the context of observing children in classes, schools or districts, perhaps over time or at different time points. This means that many of the critical textbooks present examples that are based on educational research data (Goldstein, 1995; Kreft & Leeuw, 1998; Raudenbush & Bryk, 2002; Snijders & Bosker, 2004).\nMultilevel models are growing in popularity in Psychology as well as in Education because they can be used to account for systematic and random sources of variance in observed outcomes when data are hierarchically structured. A hierarchical structure is present in data when a researcher: tests participants who belong to different groups like classes, clinics or schools; presents a sample of stimuli to each member of a sample of participants; or makes repeated observations for each participant over a series of test occasions.\nIn these circumstances, the application of traditional analytic methods has typically required the researcher to aggregate their data (e.g., averaging the responses made by a participant to different stimuli) or to ignore the hierarchical structure in their data, (e.g., analyzing the responses made by some pupils while ignoring the fact that the pupils were tested in different classes). But the application of traditional analysis approaches (e.g., regression, ANOVA) to multilevel structured data extracts scientific costs (Baayen et al., 2008; Barr et al., 2013).\nIgnoring structure by ignoring or averaging over sources of variability like differences between classes, participants, or stimulus items can mean that analyses are less sensitive because they fail to fully account for error variance. Where differences between classes, participants or stimuli include variation in the impact of experimental variables, e.g., individual differences in response to an experimental manipulation, the application of traditional methods can be associated with an increased risk of false positives in discovery. Yet these costs need no longer be suffered because the capacity to perform multilevel modeling is now readily accessible.\n\n\n\nIf a researcher tests participants belonging to different groups, e.g., records the educational attainment of different children recruited from different classes in a school, the test scores for the participants are observations that occupy the lowest level of a hierarchy (see Figure 2). In multilevel modeling, those observations are understood to be nested within higher-level sampling units, here, the classes. We can say that the children are sampled from the population of children. And the classes are sampled from the population of classes. Critically, we recognize that the children’s test scores are nested within the classes. This multi-stage sampling has important consequences, as we shall see.\n\n\n\n\n\n\n\n\nD\n\n\n\nA\n\nA\n\n\n\nB1\n\nB1\n\n\n\nA-&gt;B1\n\n\n\n\n\nB2\n\nB2\n\n\n\nA-&gt;B2\n\n\n\n\n\nC1\n\nC1\n\n\n\nB1-&gt;C1\n\n\n\n\n\nC2\n\nC2\n\n\n\nB1-&gt;C2\n\n\n\n\n\nC3\n\nC3\n\n\n\nB1-&gt;C3\n\n\n\n\n\nC4\n\nC4\n\n\n\nB2-&gt;C4\n\n\n\n\n\nC5\n\nC5\n\n\n\nB2-&gt;C5\n\n\n\n\n\nC6\n\nC6\n\n\n\nB2-&gt;C6\n\n\n\n\n\n\n\n\nFigure 2: Multilevel or hierarchically structured data\n\n\n\n\n\nWe are going to be working with the school data collected by Golino & Gomes (2014) in Brazil, so let’s take another look at that dataset. The data extract, following, presents the first 25 rows of the selected-variables brazil dataset. (I have arranged the rows by class number ID, and you may need to make sure the browser window is wide enough to see the data properly.) If you examine the extract, you can see that there are multiple rows of data for each class_number, one row for each child, with multiple children (you can see different participant_id numbers). This presentation of the dataset illustrates in practical terms – what you can actually see when you look at your data – what multilevel structured data can look like.\n\nbrazil %&gt;%\n       arrange(desc(class_number)) %&gt;%\n                 head(n = 25)\n\n   class_number participant_id portuguese english math physics\n1           M36             11         80      55   60      81\n2           M36             30         48      55   43      57\n3           M36             93         83      75   55      80\n4           M36             95         80      66   55      66\n5           M36            111         80      57   99     129\n6           M36            117         27      76   83      89\n7           M36            153          2      66   89     141\n8           M36            163         68      88   74     137\n9           M36            176         80      75   60     127\n10          M36            216         75      79   64     124\n11          M36            223         81      56   58      78\n12          M36            234         81      75   49      74\n13          M36            242         81      67   45      77\n14          M36            254         76      51   33     103\n15          M36            260         81      59   39      82\n16          M36            271         64      69   82     138\n17          M36            278         80      57   56     107\n18          M36            280         83      64   56      84\n19          M36            286         80      91   64      98\n20          M35             20         42      61   97      92\n21          M35            118         33      59   33      80\n22          M35            140         28      40   33      58\n23          M35            179         38      69   56     145\n24          M35            247         65      51   64      97\n25          M33             15         76      70   73      85\n\n\n\n\n\nRecognizing that the children’s scores data are observed within classes means that, when we examine the factors that influence variance in observed outcomes, we need to take into account the fact that the children can be grouped by (or under) the class they were in when their grades were recorded. We can develop an understanding of what this means by moving through a series of steps.\nTo illustrate the understanding we need to develop, we analyze the end-of-year school subject grades for the sample of 292 children studied by Golino & Gomes (2014). With these data, we can examine whether differences between children in their Portuguese language grades predicts differences in their English language grades. (We do not have a theoretical reason to make this prediction though it does not seem unreasonable.) Let’s make this our research question.\n\n\n\n\n\n\nNote\n\n\n\n\nResearch question: Does Portuguese grade predict English grade?\n\n\n\nNow, the first step we take in our development of understanding will be to first ignore the importance of the potential differences between classes.\nWe can begin our analysis in order to address the research question by plotting the possible association between Portuguese and English grades. We shall create a scatterplot to do this, and we create the plot by running the following code and ignoring group (class) membership. In the following chunk of code, I pipe the brazil data using %&gt;% to ggplot() and then create the plot step-by-step, with each ggplot() step separated by a + except for the last step.\n\nbrazil %&gt;%\nggplot(aes(x = portuguese, y = english)) +\n  geom_point(colour = \"black\", size = 3, alpha = .5) +\n  geom_smooth(method = \"lm\", size = 2, se = FALSE, colour = \"red\") +\n  xlab(\"Portuguese\") + ylab(\"English\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 3: Portuguese compared to English grades: each point represents the scores for one child\n\n\n\n\n\nRemember that each child ID is associated with a pair of grades: their grade in English and their grade in Portuguese. Each point in Figure 3 represents the paired grade data for one child.\nBecause I say that we are interested in predicting English grades then, by convention, we map English grades to the height of the points (i.e. English grade differences are shown as differences on the y-axis). Because we are using Portuguese grades to do the predicting, by convention, we map Portuguese grades to the horizontal position of the points (i.e. Portuguese grade differences are shown as differences on the x-axis).\nI have added a line using geom_smooth() in red to indicate the trend of the potential association between variation in Portuguese grades and variation in English grades.\nFigure 3 suggests that children with higher grades in Portuguese tend, on average, to also have higher grades in English.\n\n\nNotice that when we write the code to produce the plot, we add arguments to the geom_point() and geom_smooth() function calls to adjust the appearance of the points and the smoother line. Notice, also, that we adjust the labels for the x-axis and y-axis and, finally, that we determine the overall appearance of the plot using the theme_bw() function call.\nDo the following exercises to practice your ggplot() skills\n\nChange the x and y variables to math and physics\nChange the theme from theme_bw() to something different\nChange the appearance of the points, try different colours, shapes or sizes.\n\nFurther information to help you try out coding options can be found here, on scatterplots, and here, on themes.\n\n\n\n\nOur plot indicates the relationship between English and Portuguese language grades, in Figure 3, ignoring the fact that children in the sample belonged to different classes when they were tested. The plot shows us only information about the children and grades, with each point representing the observed English and Portuguese grades for each \\(i\\) child.\nCan we predict variation in English grades given information about child Portuguese grades? Are English grades related to Portuguese grades? We can estimate the relationship between English and Portuguese grades using a linear model in which the English grades variable is the outcome variable (or the dependent variable) and the Portuguese grades variable is the predictor or independent variable:\n\\[\ny_i = \\beta_0 + \\beta_1X_i + e_i\n\\]\n\nwhere \\(y_i\\) represents the English grade for each \\(i\\) child;\n\\(\\beta_0\\) represents the intercept, the outcome value obtained if values of the explanatory variable are zero;\n\\(\\beta_1\\) represents the effect of variation in \\(X_i\\) the Portuguese grade for each child, with the effect of that variation estimated as the the rate of change in English grade for unit change in Portuguese grade;\n\\(e_i\\) represents differences, for each child, between the observed English grade and the English grade predicted by the relationship with Portuguese grades.\n\nAs you have seen, the code for running a linear model corresponds transparently to the statistical model given in the formula.\n\nsummary(lm(english ~ portuguese, data = brazil))\n\n\nCall:\nlm(formula = english ~ portuguese, data = brazil)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-64.909 -17.573   2.782  20.042  53.292 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 29.77780    3.81426   7.807 2.11e-13 ***\nportuguese   0.47001    0.07897   5.952 9.91e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 25.02 on 228 degrees of freedom\nMultiple R-squared:  0.1345,    Adjusted R-squared:  0.1307 \nF-statistic: 35.43 on 1 and 228 DF,  p-value: 9.906e-09\n\n\nThe linear model yields the estimate that for unit increase in Portuguese grade there is an associated increase of about .47 in English grade, on average (for the model, \\(F(1, 228) = 35, p &lt; .001; adj. R^2 = .13\\)). So, we have a preliminary answer to our research question.\n\n\n\n\n\n\nNote\n\n\n\n\nResearch question: Does Portuguese grade predict English grade?\nResult: Our analysis shows that children who score one grade higher in their Portuguese class (e.g., 61 compared to 60) tended to score .47 of a grade higher in their English class.\n\n\n\nWe shall see that we will need to revise this estimate when we do an analysis that does take school class differences into account.\n\n\n\n\n\n\nTip\n\n\n\nNotice: here, I have dispensed with the creation of a model object by name. The model is estimated anyway, and I have embedded the lm() function call within a summary() function so that I am asking R to do two things:\n\nlm() fit a linear model\nsummary() print out a summary of the fitted model object\n\nI do this to give an example of the way in which code can be varied. Also, to show you how one function can be embedded inside another.\n\n\n\n\nDo the following exercises to practice your lm() skills\n\nChange the outcome and predictor variables to math and physics\nWhat do the results tell you about the relationship between maths and physics ability?\n\n\n\n\n\nIn the following discussion, I will present some model formulas as equations. I am not doing that because the discussion is going to consider the modeling in terms of the underlying mathematics. I am doing it with the aim of clarifying how quantities – observed, estimated or predicted – add up, in terms of the linear and then the linear mixed-effects model.\n\nI am going to refer to the dependent or outcome variable (e.g., school grade) as \\(y\\),\nand to explanatory or experimental or independent variables (e.g., language skill) as \\(X\\).\nModels shall be fitted to estimate the coefficients, written \\(\\beta\\), of the effects of the explanatory variables on the outcome variable\nTo distinguish the different coefficients of the different effects, I am going to number the coefficients so …\n\n\n\\(\\beta_0\\) is the coefficient of the intercept;\n\\(\\beta_1\\) will be the coefficient of the effect of a first explanatory variable: in the Brazilian schools example, the coefficient of the effect of variation in Portuguese language skill on variation in English language scores.\n\nTo make it clear that each observation can be understood as part of a complex multilevel or crossed random effects structure, I am going to use indices as subscripts for variables.\n\nI will, here, index individual participants (children) using \\(i\\);\nI will index index classes in which children were tested using \\(j\\).\n\nThus, in the Brazilian schools example, we shall see that we are concerned with observations about children’s school grades, where children are sampled as individuals nested in samples of classes. We will examine how an outcome variable (English language grade) is related to a predictor variable (Portuguese language grade) such that:\n\n\\(y_{ij}\\) is the outcome English language grade recorded for each child \\(i\\) in each class \\(j\\);\nwhile \\(X_{ij}\\) represents the explanatory variable, see following, the Portuguese language grade.\n\n\\(X_{ij}\\) is subscripted \\(_{ij}\\) because values of the variable depend upon child identity, and children are identified as child \\(i\\) in class \\(j\\) to represent the multilevel structure of the data\n\n\n\nThe linear model ignores the higher-level structure, the distinction between classes: does this matter?\nWe can see the answer to that question if we inspect Figure 4. We create the plot using the following chunk of code; we discuss that later, first reflecting on what the plot shows us.\n\nbrazil %&gt;%\nggplot(aes(x = portuguese, y = english)) +\ngeom_point(colour = \"darkgrey\") +\n  geom_smooth(method = \"lm\", se = FALSE, colour = \"black\") +\n  facet_wrap(~ class_number) +\n  xlab(\"Portuguese\") + ylab(\"English\") +\n  theme_bw() +\n  scale_x_continuous(breaks=c(25,50,75)) + scale_y_continuous(breaks=c(0,50,100))\n\n\n\n\n\n\n\nFigure 4: Plot of child grades, comparing English with Portuguese grades, shown separately for each school class\n\n\n\n\n\nFigure 4 presents a grid of scatterplots, with a different scatterplot to show the relationship between children’s Portuguese and English grades for the children in each different class. We can see that the relationship between Portuguese and English grades is (roughly) similar across classes: in general, children with higher Portuguese grades also tend to have higher English grades. However, Figure 4 makes it obvious that there are important differences between classes.\nWe can see that the slope of the best fit line (shown in black) varies between classes. And we can see that the intercept (where the line meets the y-axis) also varies between classes. Further, we can see that in some classes, there is no relationship or a negative relationship between Portuguese and English grades.\nCritically, we can see that classes differ in how much data we have for each. For some classes, we have many observations (e.g., M11) and for other classes we have few or one observation (e.g., M22, with one child). We know, in advance, that variation in sample size will be associated with variation in the uncertainty we have about the estimated relationship between Portuguese and English grades. You will remember that the Central Limit Theorem allows us to calculate the standard error of an estimate like the mean, given the sample size, and that the standard error is an index of our uncertainty of the estimate.\n\n\n\n\n\n\nTip\n\n\n\nFacetting – notice that, in the plotting code, the key expression is facet_wrap(~ class_number) This means:\n\nThe class_number variable is a factor: we ask R to check the summary for the dataframe, and that factor codes what class a child is in.\nThe facet_wrap(...) function then asks R to produce separate plots for each facet for the data – the word facet means face or aspect.\nWe use the formula facet_wrap(~ ...) to ask R to split the data up by using the classification information in the named variable, here class_number.\n\n\n\nThe production of a grid or lattice of plots is a useful method for comparing patterns between data sub-sets or groups. You can see more information about facet_wrap() here\n\n\n\n\n\n\nTip\n\n\n\nAdjusting scales – notice that in these plots I modified the axes to show x-axis and y-axis ticks at specific locations using scale functions. The tick is the notch or short line where we show the numbers on the axes.\n\nscale_x_continuous(breaks=c(25,50,75)) means: set the x-axis ticks at 25, 50 and 75, defined using a vector c() of values\nscale_y_continuous(breaks=c(0,50,100)) means: set the y-axis ticks at 0, 50, 100, defined using a vector of values.\n\n\n\nIt can be useful to adjust the ticks on axes, where other plot production factors cause crowding of labels.\nYou can see more information on scales here.\n\n\nDo the following exercises to practice your facet_wrap() skills\n\nChange the x and y variables to math and physics.\nExperiment with showing the differences between classes in a different way: instead of using facet_wrap() in aes() add colour = class_number: what happens?\n\n\n\n\n\nFigure 4 shows that there is variation between classes in both the average English grade, shown as differences in the y-axis intercept, and the ‘effect’ of Portuguese language skill, shown as differences in the slope of the best fit line for the predicted relationship between Portuguese and English grades.\nWe put ‘effect’ in quotes to signal the fact that we do not wish to assert a causal relation. The interpretation of the results of the linear model assumes those results are valid provided that the observations are independent, among other things. We can see that we cannot make the assumption of independence because individuals in classes with high average English grades are more likely to have higher English grades.\nWe can represent in our analysis the information we have about hierarchical structure in the data (child within class) by allowing the regression coefficients to vary between groups. We therefore modify the subscripting to take into account the fact that we must distinguish which child \\(i\\) and which class \\(j\\) we are examining, adapting our model to:\n\\[\ny_{ij} = \\beta_{0j} + \\beta_{1j}X_{ij} + e_{ij}\n\\]\n\nwhere \\(y_{ij}\\) represents the outcome measure, the English grade for each \\(i\\) child in each \\(j\\) class;\n\\(\\beta_{0j}\\) represents the average grade, different in different classes;\n\\(\\beta_{1j}X_{ij}\\) represents the variation in \\(X\\) the Portuguese grade for each \\(i\\) child in \\(j\\) class, with the effect of that variation estimated as the coefficient \\(\\beta_{1j}\\), different in different classes;\nand \\(e_{ij}\\) represents differences between observed and predicted English grades for each \\(i\\) child in \\(j\\) class.\n\n\n\n\nIn practice, we could capture the variation between classes by performing a two-step analysis.\n\nFirst, we estimate the coefficient of the ‘Portuguese’ effect for each class separately. We do multiple (per-class) analyses. In each of these analyses, we estimate the coefficient looking only at the data for one class.\nSecond, we can take those per-class coefficients as the outcome variable in a ‘slopes-as-outcomes analysis’ to examine if the per-class estimates of the experimental effect are reliably different from zero or, more usefully, if the per-class estimates vary in relation to some explanatory variable like teacher skill.\n\nThe problem with the approach is apparent in Figure 5: the figure shows the estimated intercept and coefficient of the slope of the ‘Portuguese’ effect for each class, when we have analyzed the data for each class in a separate linear model.\nThe estimate for each class is shown as a black dot. The standard error of the estimate is shown as a black vertical line, shown above and below a point.\nYou can say that where there is a longer line there we have more uncertainty about the location of the estimate. Notice that the standard errors vary a lot between classes. In some classes, the standard error is small (the black line is short) so we can maybe have more certainty over the estimated intercept or slope for those classes. In other classes, the standard error is large (the black line is long) so we can maybe have less certainty over the estimated intercept or slope for those classes.\n\n\n\n\n\n\nImportant\n\n\n\nThe key idea here is that standard errors vary widely between classes but the two-step modeling approach, while it can take into account the between-class differences in estimates, cannot account for the variation in the standard errors about those estimates.\n\n\n\n\n\n\n\n\n\n\nFigure 5: Plot showing the estimated intercept and coefficient of the slope of the Portuguese effect for each class analysed separately\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNotice that the code I used fits a separate model for each class, and then plots the per-class estimates of intercepts and slopes of the english ~ portuguese relationship.\n\n\n(In case you want to see how this plot is produced, I work through the code for drawing Figure 5 in the workbook .R.)\n\n\nThe ‘slopes-as-outcomes’ approach is quite common and can be found in a number of papers in the psychological and educational research literatures. An influential example can be found in the report by Balota et al. (2004) of their analysis of psycholinguistic effects in older and younger adults. Balota et al. (2004) wanted to examine if or how effects of variables like word frequency, on reading response latencies, were different in different age groups. To do this, they first estimated the effect of the (word-level) psycholinguistic variables in separate linear model (multple regressions) for each adult. They then took the estimated coefficients as the dependent variable (slopes-as-outcomes) for a second analysis in which they tested the effect of age group on variation in the estimated coefficients (see Lorch & Myers (1990) for a discussion of the approach).\nYou may be asked to do this. I would advise you against it because there is a better way, as we see next.\n\n\n\n\nMultilevel models incorporate estimates of the intercept and the effect of independent (experimental or correlational) variables plus estimates of the random variation between classes in intercepts and slopes. Multilevel models are also known as mixed-effects models because they involve the estimation of fixed effects (effects due to independent variables) and random effects (effects due to random differences between groups).\nWe model the intercept (varying between classes) as:\n\\[\n\\beta_{0j} = \\gamma_0 + U_{0j}\n\\]\n\nwhere \\(\\beta_{0j}\\) values equal \\(\\gamma_0\\) the overall average intercept,\nplus \\(U_{0j}\\) the differences, for each class, between that average intercept and the intercept for that class.\n\nWe model the coefficient of the (Portuguese language) skill effect as:\n\\[\n\\beta_{1j} = \\gamma_1 + U_{1j}\n\\]\n\nwhere \\(\\beta_{1j}\\) values equal \\(\\gamma_1\\) the average slope,\nplus \\(U_{1j}\\) the differences, for each class, between that average slope and the slope for that class.\n\nThese models can then be combined:\n\\[\ny_{ij} = \\gamma_0 + \\gamma_1X_{ij} + U_{0j} + U_{1j}X_{ij} + e_{ij}\n\\]\nsuch that the English grade observed for each child \\(y_{ij}\\) can be predicted given:\n\nthe average grade overall \\(\\gamma_0\\) plus\nthe average relationship between English and Portuguese language skills \\(\\gamma_1\\) plus\nadjustments to capture\n\n\nthe difference between the average grade overall and the average grade for the child’s class \\(U_{0j}\\),\nas well as the difference between the average slope of the Portuguese effect and the slope of that effect for their class \\(U_{1j}X_{ij}\\),\nplus any residual differences between the observed and the model predicted English grade \\(e_{ij}\\).\n\n\n\n\nIf you have had some experience analyzing psychological data, then there is a potential approach to thinking about the effect of the differences between classes that would seem natural. This approach has, in fact, been proposed (see e.g. Lorch & Myers, 1990) and applied in the literature. I would not recommend using it but thinking about it helps to develop our understanding of what we are doing with multilevel models\nWe could seek to estimate (1.) the relationship of interest – here, the association between English and Portuguese grades, while (2.) also estimating the relationship between (outcome) English grades and the impact made by what class a child is in.\nIn this approach, we would construct a model in which we have English grades as the outcome variable, Portuguese grades as one predictor variable, and then add a variable to code for class identity. The ‘effect’ of the class variable would then capture the differences in intercepts between classes. You could add a further variable to code for differences between classes in the slope of the relationship between English and Portuguese grades. This would allow for the fact that the relationship is positive or negative, stronger or weaker, in different classes.\nIf we added that further variable to code for differences between classes in the English-Portuguese relationship, then we would be estimating those differences as interactions between (1.) the predictive ‘effect’ of Portuguese grades on English grades and (2.) the class effect. An interaction effect is what we have when the effect of one variable (the predictive ‘effect’ of Portuguese grades) is different for different levels of the other variable (the predictive ‘effect’ of Portuguese grades is different for different classes).\nThinking about this approach helps us to think about what we are doing when we are working with multilevel structured data. But the problem with the approach is that it does not allow us to generalize beyond the sample we have. The estimates we would have, given a model in which we code directly for class, would tell us only about the classes in our sample.\nMost of the time, we would prefer to do analyses whose results could be generalized: to other children, to other classes, etc.. For this reason, it makes more sense to suppose that \\(U_{0j} + U_{1j}\\), the class-level deviations, are unexplained differences drawn at random from a population of potential class differences.\nWhat does this mean? Think back to your understanding of the linear model. You have learnt that when we fit a linear model like \\(y_i = \\beta_0 + \\beta_1X_i + e_i\\) we include a term \\(e_i\\) to represents the differences, for each child, between the observed (outcome) English grade and the English grade predicted by the relationship with Portuguese grades. Those differences between observed and predicted outcomes are called residuals. We assume that the direction and size of any one residual, for any one child, is randomly determined because we typically have no idea why there might be a big difference between the predicted and observed grade for one child but a small residual difference for another.\nNow, we can imagine that there will be many classes in many schools, and we can surely expect that there will be differences between the classes. These differences will result from plenty of factors we do not measure or cannot explain. Indeed, we have seen that there are differences between the average (outcome) English grade i.e. the predicted intercept and the observed intercept for each class, and we have seen that there are differences between the average slope of the English-Portuguese grades relationship i.e. the predicted slope and the slope for each class. These differences would, in effect, be random differences. And we can see how the variation in these differences are, for us, just a kind of random error variance, which we can see as class-level residuals.\nWe suppose, technically, that the differences between classes, controlling for the effect of the explanatory variable, are exchangeable (it does not matter which class is which), with classes varying at random. Multilevel models incorporate estimation of the explanatory variables effects \\(\\gamma_0 + \\gamma_1\\) accounting for group-level error variance \\(U_{0j} + U_{1j}\\).\nThe difference between the two-step approach and the multilevel modelling approach is this:\n\nIn the two-step approach, seen earlier, we estimate – separately, for each class – the intercept and the slope of the Portuguese effect, using just the data for that class and ignoring the data for other classes.\nIn the multilevel model, in contrast, we use all the observations, estimating the average intercept and the average slope of the Portuguese effect plus the variance due to the difference for each class (1.) between the average intercept and the class intercept or (2.) between the average slope and the class slope.\nWe can understand these differences between the average (intercept or class) and the class differences as class-level residuals \\(U_{0j} + U_{1j}\\) in addition to the child-level residuals \\(e_{ij}\\).\n\nThis leads us to a conclusion that represents a critical way to understand multilevel models.\n\n\n\n\n\n\nImportant\n\n\n\nThe multilevel model is a linear model but with multiple random effects terms to take into account the hierarchical structure in the data.\n\n\n\n\n\nWe can fit a multilevel model of the english ~ portuguese relationship, taking into account the fact that the pupils tested in the study were recruited from different classes, using the convenient and powerful lmer() function.\nHappily, the code we use to define a model in lmer() is much like the code we use to define a model in lm() with one important difference which I shall explain. Let’s try it out.\nTo use the lmer() function you need to make the lme4 library available.\n\nlibrary(lme4)\n\nThe model you are going to code will correspond to the statistical model that we have been discussing:\n\\[\ny_{ij} = \\gamma_0 + \\gamma_1X_{ij} + U_{0j} + U_{1j}X_{ij} + e_{ij}\n\\]\nAnd the code is written as follows.\n\nporto.lmer1 &lt;- lmer(english ~ portuguese +\n\n                              (portuguese + 1|class_number),\n\n                              data = brazil)\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.0391843 (tol = 0.002, component 1)\n\n\nYou can see that the lmer() function call is closely similar to the lm() function call, with one critical exception, as I explain next.\nFirst, we have a chunk of code mostly similar to what we do when we do a regression analysis.\n\nporto.lmer1 &lt;- lmer(...) creates a linear mixed-effects model object using the lmer() function.\nenglish ~ portuguese is a formula expressing the model in which we estimate the fixed effect on the outcome or dependent variable english (English grades) predicted \\(\\sim\\) by the independent or predictor variable portuguese (Portuguese grades).\nIf there were more terms in the model, the terms would be added in series separated as variable names separated by + sum symbols.\n...(..., data = brazil) specifies the dataset in which you can find the variables named in the model fitting code.\nsummary(porto.lmer1) gets a summary of the fitted model object.\n\nSecond, we have a bit that is specific to multilevel or mixed-effects models.\n\nCritically, we add (...|class_number) to tell R about the random effects corresponding to random differences between sample groups (classes) coded by the class_number variable.\n(...1 |class_number) says that we want to estimate random differences between sample groups (classes) in intercepts coded 1.\n(portuguese... |class_number) adds random differences between sample groups (classes) in slopes of the portuguese effect coded by using the portuguese variable name.\n\nIf you run the model code as written – see the .R workbook file for an example of the code – and it works then the code will be shown in the console window in R-Studio. To show the model results, you need to get a summary of the model, using the model name.\n\nsummary(porto.lmer1)\n\n\n\nMixed-effects model code is hard to get used to at first. A bit of practice helps to show you which bits of code are important, and which bits you will change for your own analyses\n\nChange the outcome (from english) and the predictor (from portuguese): this is about changing the fixed effect part of the model.\nVary the random effects part of the model.\n\n\nChange it from (portuguese + 1 | class_number) to (1 | class_number) what you are doing is asking R to ignore the differences in the slope of the effect of Portuguese grades.\nChange it from (portuguese + 1 | class_number) to (portuguese + 0 | class_number what you are doing is asking R to ignore the differences in the intercept.\n\nTry out these variations and look carefully at the different results.\n\n\n\n\nNow let’s take a look at the results.\n\nsummary(porto.lmer1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: english ~ portuguese + (portuguese + 1 | class_number)\n   Data: brazil\n\nREML criterion at convergence: 2104.3\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.81240 -0.59545  0.04338  0.59975  2.23667 \n\nRandom effects:\n Groups       Name        Variance Std.Dev. Corr \n class_number (Intercept) 338.9057 18.4094       \n              portuguese    0.3278  0.5725  -0.98\n Residual                 493.2866 22.2101       \nNumber of obs: 230, groups:  class_number, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  25.2838     6.2522   4.044\nportuguese    0.6589     0.1726   3.817\n\nCorrelation of Fixed Effects:\n           (Intr)\nportuguese -0.943\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.0391843 (tol = 0.002, component 1)\n\n\nNotice that the output has a number of elements.\n\nFirst, we see information about the function used to fit the model, and the model object created by the lmer() function call.\nThen, we see the model formula english ~ portuguese + (portuguese + 1 | class_number).\nThen, we see `REML criterion at convergence: about the model fitting process, which we can usually ignore.\nThen, we see information about the distribution of the model residuals.\nThen, we see information about the error terms estimated (technically, predicted) by the model.\n\n\nResiduals, just like a linear model plus error terms specific to multilevel or mixed-effects models, group-level residuals:\ndifferences between the average intercept and, here, the intercept (average `english: score) per class;\nand differences between the average slope capturing the \\(english \\sim portuguese\\) relationship and the slope (of the effect) per class.\n\n\nThen, just as for linear models, we see estimates of the coefficients (of the slopes) of the fixed effects, the intercept and the slope of the \\(english \\sim portuguese\\) relationship.\n\nNote that we see coefficient estimates like in a linear model summary but no p-values. We will come back to p-values later but note that their absence is not a bug. Note also that we do not see an \\(R^2\\) estimate. We will come back to that too.\n\n\nThere is no convention, yet, on how to report the results of these models. Lotte Meteyard and I argue for a set of conventions that will help researchers to understand each others’ results better.\n\nIn this exercise, go read the bit where we advise Psychologists how to write about the results, in our paper (Meteyard & Davies, 2020).\n\n\n\n\n\nYou will have noticed the reference to fixed and random effects in the discussion in this chapter, and the use of fixed and random effects as titles for sections of the model output. The terms fixed effects and random effects are not used consistently in the statistical literature (Gelman & Hill, 2007). I am going to use these terms because they are helpful, at first, and because they are widely used, not least in the results of our analyses in R.\nIt is common in the psychological literature to refer to the effects on outcomes of experimental manipulations (e.g., the effect on outcomes of differences between experimental conditions) or to the effects of correlated variables of theoretical or explanatory interest (e.g., the effect of differences in Portuguese language skill) as fixed effects. Typically, we are aiming to get estimates of the coefficients of these effects. And, much like we would do when we use linear models, we expect that these coefficients represent an estimate of the effects of these variables, on average, across the population. (Hence, some analysts prefer to talk about these effects as population average effects).\nIn comparison, when we are thinking about the effects on outcomes of what we understand to be the random differences between sampled children (e.g., the child-level residuals) or the random differences between sampled classes (e.g., the class-level residuals) then we refer to these effects as random effects. As we have seen, we usually estimate random effects as variances and can estimate them as random error variances. While we may care to estimate how differences in, say, Portuguese language score, is associated with English grade, we typically do not care about the impact of the specific difference between any two classes in English grade.\nHowever, if you take your education in this area further, you will find that the way that fixed and random effects are talked about in the statistical literature can be, at best, inconsistent. And, ultimately, you might ask yourself if there are principled distinctions between fixed and random effects. We can leave these problems aside, here, because they do not influence how we shall learn and practice using multilevel models in the early to medium term in our development of skills and understanding.\n\n\n\nRecall that the linear model yields the estimate that for unit increase in Portuguese grade there is an associated .47 increase in English grade, on average Section 1.9.5. We can see that the estimate of the effect for the mixed-effects model is \\(\\beta = .659\\) which is somewhat different from the effect estimate we got from the linear model.\n\n\n\n\n\n\nNote\n\n\n\n\nResearch question: Does Portuguese grade predict English grade?\nResult: Our analysis shows that children who score one grade higher in their Portuguese class (e.g., 61 compared to 60) tended to score about \\(.66\\) of a grade higher in their English class.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 16. Conceptual introduction to multilevel data"
    ]
  },
  {
    "objectID": "PSYC412/part2/01-multilevel.html#sec-multi-conclusions",
    "href": "PSYC412/part2/01-multilevel.html#sec-multi-conclusions",
    "title": "Week 16. Conceptual introduction to multilevel data",
    "section": "",
    "text": "Psychological studies frequently result in hierarchically structured data. The structure can be understood in terms of the grouping of observations, as when there are multiple observations per group, participant or stimulus. The existence of such structure must be taken into account in analyses. Multilevel or mixed-effects models can be specified by the researcher to include random effects parameters that capture unexplained differences between participants or other sampling units in the intercepts or the slopes of explanatory variables. Where a sample of participants is asked to respond to a sample of stimuli, structure relating both to participants and to stimuli can be incorporated.\nMany researchers will be aware of concerns over the non-replication of published effects in psychological science (see the discussion here). As Gelman (2014) discusses, non-replication of results may arise if effects vary between contexts groups while traditional analytic methods assume that effects are constant. Psychological researchers can expect average outcomes and the effects of independent variables to vary among sampling units, whether their investigations involve multiple observations per child or stimulus, or children sampled within classes, clinics or schools. However, traditional analytic methods often require us to ignore this variation by averaging, say, responses over multiple trials for each child, to collapse an inherently multilevel data structure into a single level of observations that can be analysed using regression or ANOVA. By using multilevel models, researchers will, instead, be able to properly estimate the effects of theoretical interest, and better understand how those effects vary.\n\n\n\nWe outlined the features of a multilevel structure dataset.\nWe then discussed visualizing and modeling the relationship between outcome and predictor variables when you ignore the multilevel structure.\nWe examined why ignoring the structure is a bad idea.\nWe considered how slopes-as-outcomes analyses are an approximate method to take the structure into account.\nWe dmonstrated how multilevel modeling is a better method to estimate the relationship between outcome and predictor variables\n\n\n\n\nWe used functions to read-in the libraries of functions we needed.\n\nlibrary(tidyverse)\nlibrary(lme4)\n\nWe used some new functions, or focused on functions we have seen before but not discussed.\n\nWe used load() to load an .RData file into R workspace.\nFor visualization, we used facet_wrap() to show plots of the relationship between English and Portuguese scores in each class separately.\nWe used lmer() to fit a multilevel model.\n\nWe used the summary() function to get model results for both linear models and for the multilevel or linear mixed-effects model.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 16. Conceptual introduction to multilevel data"
    ]
  },
  {
    "objectID": "PSYC412/part2/01-multilevel.html#sec-multi-reading",
    "href": "PSYC412/part2/01-multilevel.html#sec-multi-reading",
    "title": "Week 16. Conceptual introduction to multilevel data",
    "section": "",
    "text": "Snijders & Bosker (2004) present a helpful overview of multilevel modelling. Readers familiar with the book will see that I rely on it to construct the formal presentation of the models.\nGelman & Hill (2007) present a more in-depth treatment of multilevel models.\n\n\n\n\nBaayen, R. H., Davidson, D. J., & Bates, D. M. (2008). Mixed-effects modeling with crossed random effects for subjects and items. Journal of Memory and Language, 59(4), 390–412. https://doi.org/10.1016/j.jml.2007.12.005\n\n\nBalota, D. A., Cortese, M. J., Sergent-Marshall, S. D., Spieler, D. H., & Yap, M. J. (2004). Visual Word Recognition of Single-Syllable Words. Journal of Experimental Psychology: General, 133(2), 283–316. https://doi.org/10.1037/0096-3445.133.2.283\n\n\nBarr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of Memory and Language, 68(3), 255–278. https://doi.org/10.1016/j.jml.2012.11.001\n\n\nGelman, A. (2014). The Connection Between Varying Treatment Effects and the Crisis of Unreplicable Research. Journal of Management, 41(2), 632–643. https://doi.org/10.1177/0149206314525208\n\n\nGelman, A., & Hill, J. (2007). Data analysis using regression and multilevel/hierarchical models. Cambridge University Press.\n\n\nGoldstein, H. (1995). Multilevel statistical models. Edward Arnold.\n\n\nGolino, H., & Gomes, C. (2014). Psychology data from the “BAFACALO project: The Brazilian Intelligence Battery based on two state-of-the-art models  Carroll’s Model and the CHC model”. Journal of Open Psychology Data, 2(1), e6. https://doi.org/10.5334/jopd.af\n\n\nKreft, I., & Leeuw, J. de. (1998). Introducing multilevel modeling (D. Wright, Ed.). Sage Publications.\n\n\nLorch, R. F., & Myers, J. L. (1990). Regression analyses of repeated measures data in cognitive research. Journal of Experimental Psychology: Learning, Memory and Cognition, 16(1), 149–157.\n\n\nMeteyard, L., & Davies, R. A. I. (2020). Best practice guidance for linear mixed-effects models in psychological science. Journal of Memory and Language, 112, 104092. https://doi.org/10.1016/j.jml.2020.104092\n\n\nRaudenbush, S. W., & Bryk, A. S. (2002). Hierarchical linear models: Applications and data analysis methods (Vol. 1). sage.\n\n\nSnijders, T. A. B., & Bosker, R. J. (2004). Multilevel analysis: An introduction to basic and advanced multilevel modeling. Sage Publications Ltd.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 16. Conceptual introduction to multilevel data"
    ]
  },
  {
    "objectID": "PSYC412/part2/00-quarto.html",
    "href": "PSYC412/part2/00-quarto.html",
    "title": "Week 00. Writing reproducible reports using Quarto",
    "section": "",
    "text": "I am sharing materials here to support the development of skills in producing reproducible reports using Quarto.\nThese materials should provide a quick entry to the what, why and how.\nYou can write in Quarto, in R-Studio (or any code editor), to produce:\n\nreports or manuscripts;\npresentations;\nbooks;\nwebpages.\n\nHere, we are going to focus on reports that will be output as .docx.\n\n\n\n\n\n\nTip\n\n\n\nWhat we are engaged in, here, is writing a combination of text and (here, R) code to generate a document.\nThis idea is old — see: e.g., Donald Knuth (who developed LaTex): https://www-cs-faculty.stanford.edu/~knuth/lp.html\nBut Quarto is a modern, efficient, way to do a powerful thing easily and efficiently.\n\n\n\n\nQuarto was developed by Posit (formerly: R-Studio).\n\n\n\n\n\n\nTip\n\n\n\nYou can find the Posit guide to Quarto here:\nhttps://quarto.org/docs/get-started/hello/rstudio.html\n\n\n\n\n\n\nBefore you get started, you will need a local installation of R and R-Studio.\n\n\nInstall R first, following the instructions here:\n\nhttps://cran.r-project.org/\n\nThen install RStudio, see:\n\nhttps://posit.co/download/rstudio-desktop/\nYou need to do the installation in that order because R-Studio is the interface (an Interactive Development Editor (IDE)) but R does the work.\n\nIt is probably worth your time having a skim of the “hello world” quick start guide:\n\nhttps://quarto.org/docs/get-started/hello/rstudio.html\n\n\n\n\nThere are at least the following justifications for investing time in this (in no particular order):\n\nReproducible manuscripts: create documents that incorporate text, computation or plotting code, and bibliographic information in one place. By doing so you avoid the risk of losing track of what data underlie which calculations or plots or table summaries. Copy and pasting statistical results will inevitably lead to errors.\nAutomate the boring stuff: figure, table or section cross-referencing; producing documents in different formats; generating bibliographies.\nMake manuscripts or presentations or notes share-able: Quarto is free so removes barrier to entry presented by licensed software like MS Word.\nMake nice things: plots and tables will look better.\n\n\n\n\nOpen R-Studio.\nClick on the menu buttons:\n\nFile \\(\\rightarrow\\) New File \\(\\rightarrow\\) Quarto Document...\nA menu box will open: click on the Word radio button, and the Create button.\n\n\n\n\nNew Create Document menu box\n\n\nIf you do that, you will see this file open:\n\n\n\nA new .qmd file\n\n\n\nSee that Render button at the top of the window? Click on it.\n\n\nYou will be asked to give the file a name. Name it.\n\nThis action will create a Quarto script file, a .qmd and will generate a Word .docx.\nThe whole script looks like this:\n\n\n\nA new .qmd file: dummy file\n\n\nAnd the .docx looks like this:\n\n\n\nA new .qmd file: dummy file output\n\n\n\n\nThis example shows you the main parts of a Quarto file. Let’s identify these parts before we move on:\n\nThe yaml at the top is where you set document options:\n\n---\ntitle: \"Untitled\"\nformat: docx\neditor: visual\n---\nUsually, this will be where you specify what output formats you want, whether you want a ToC, and (for APA 7 documents) it will be where you add author, title and abstract information, see more information here:\nhttps://quarto.org/docs/output-formats/ms-word.html\n\nYou have section titles:\n\n## Quarto\nNotice the ## – the more hash signs you add, the lower the section title in the section hierarchy.\n\nYou have plain text:\n\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see &lt;https://quarto.org&gt;.\n\nAnd you have computing code — chunks:\n\n\n```{r}\n#| label: example\n1 + 1\n```\n\n[1] 2\n\n\nThe tick marks are at the top and bottom of the chunks tell R to read the code and work with it.\nNotice that you can specify the identity (for cross-referencing) and the behaviour or appearance of the code at the top of the chunk.\n\n\n\n\nIn writing reports, we are often going to want to do tasks like:\n\nGet sample data and show:\n\n\nDistributions or other preliminary information to characterize samples.\n\n\nFit a model.\n\n\nPresent a table summary of the model estimates.\nPresent a plot showing model predictions.\n\n\nCross-reference figures, tables and sections.\n\nWe look at how to do these things next.\n\n\nWe will be working with an example data-set: click on the link to download the example data file.\n\nstudy-one-general-participants.csv\n\nThe file has the structure you can see in the extract below.\n\n\n\n\n\nparticipant_ID\nmean.acc\nmean.self\nstudy\nAGE\nSHIPLEY\nHLVA\nFACTOR3\nQRITOTAL\nGENDER\nEDUCATION\nETHNICITY\n\n\n\n\nstudytwo.1\n0.4107143\n6.071429\nstudytwo\n26\n27\n6\n50\n9\nFemale\nHigher\nAsian\n\n\nstudytwo.10\n0.6071429\n8.500000\nstudytwo\n38\n24\n9\n58\n15\nFemale\nSecondary\nWhite\n\n\nstudytwo.100\n0.8750000\n8.928571\nstudytwo\n66\n40\n13\n60\n20\nFemale\nHigher\nWhite\n\n\nstudytwo.101\n0.9642857\n8.500000\nstudytwo\n21\n31\n11\n59\n14\nFemale\nHigher\nWhite\n\n\n\n\n\n\n\nYou can use the scroll bar at the bottom of the data window to view different columns.\nYou can see the columns:\n\nparticipant_ID participant code;\nmean.acc average accuracy of response to questions testing understanding of health guidance (varies between 0-1);\nmean.self average self-rated accuracy of understanding of health guidance (varies between 1-9);\nstudy variable coding for what study the data were collected in\nAGE age in years;\nHLVA health literacy test score (varies between 1-16);\nSHIPLEY vocabulary knowledge test score (varies between 0-40);\nFACTOR3 reading strategy survey score (varies between 0-80);\nGENDER gender code;\nEDUCATION education level code;\nETHNICITY ethnicity (Office National Statistics categories) code.\n\n\n\nDownload and read the data into the R environment.\n\n```{r}\n#| label: data-read-in\nstudy.two.gen &lt;- read_csv(\"study-one-general-participants.csv\")\n```\n\nRows: 169 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): participant_ID, study, GENDER, EDUCATION, ETHNICITY\ndbl (7): mean.acc, mean.self, AGE, SHIPLEY, HLVA, FACTOR3, QRITOTAL\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\nLet’s take a look at the distribution of ages in this sample.\nYou can do that by examining a histogram, see Figure 1.\nYou can embed the code that does that work, together with your text, in a chunk like this:\n\n```{r}\n#| label: fig-example-histogram\nstudy.two.gen %&gt;%\n  ggplot(aes(x = AGE)) +\n  geom_histogram() +\n  labs(title = \"Sample characteristics\", x = \"Age (years\") +\n  theme_bw() \n```\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\nIn practice, unless you are teaching, or sharing your workings as part of your documentation, you are going to want to embed a chunk of code to produce a plot so that the plot is presented in your output document while the code chunk that does the work is invisible (at output).\nYou may also want to add a figure caption and alt-text, and you will want to manipulate figure dimensions.\nWe can learn how to do that stuff while producing a scatterplot, next.\nLet’s do something a little fancy, as in Figure 2: a scatterplot with marginal histograms.\nTo produce the plot, you will need to have installed the {ggExtra} library.\nLet’s go through the control elements first.\nTake a look at the chunk of code:\n\n```{r}\n#| label: fig-ggextra-demo-non-eval\n#| fig-cap: \"Scatterplot showing the potential association between accuracy of comprehension and health literacy\"\n#| fig-alt: \"The figure presents a grid of scatterplots indicating the association between variables mean accuracy (on y-axis) and health literacy (x-axis) scores. The points are shown in black, and clustered such that higher health literacy scores tend to be associated with higher accuracy scores. The trend is indicated by a thick red line. Marginal histograms indicates the distributio of data on each variable.\"\n#| warning: false\n#| eval: false\n#| fig-width: 4.5\n#| fig-height: 4.5\n# -- note that can use gridExtra\n# -- to show marginal distributions in scatterplots\n# https://github.com/daattali/ggExtra\nplot &lt;- study.two.gen %&gt;%\n  ggplot(aes(x = HLVA, y = mean.acc)) +\n  geom_point(size = 1.75, alpha = .5) +\n  geom_smooth(size = 1.5, colour = \"red\", method = \"lm\", se = FALSE) +\n  xlim(0, 15) +\n  ylim(0, 1.1)+\n  theme_bw() +\n  theme(\n    axis.text = element_text(size = rel(1.15)),\n    axis.title = element_text(size = rel(1.5))\n  ) +\n  labs(x = 'Health literacy (HLVA)', y = 'Mean accuracy')\nggExtra::ggMarginal(plot, type = \"histogram\", colour = \"lightgrey\", fill = \"lightgrey\")\n```\n\nNotice the bits of text at the top of the chunk:\n\n#| label: fig-ggextra-demo-non-eval labels the chunk. You need this for figure referencing.\n#| fig-cap: \"Scatterplot showing ...\" adds the caption that will be shown next to the plot.\n#| fig-alt: \"The figure presents ...\" adds alt-text describing the plot for people who use screen readers.\n#| warning: false stops R from producing the plot with warnings.\n#| echo: false stops R from showing both the code and the plot.\n#| eval: false here stops R from actually running the code.\n#| fig-width: 4.5 adjusts figure width.\n#| fig-height: 4.5 adjusts figure height.\n\nYou can see that I have added comments: # -- note that can use gridExtra to make the chunk self-documenting.\nNow show the plot: Figure 2.\n\n# -- note that can use gridExtra\n# -- to show marginal distributions in scatterplots\n# https://github.com/daattali/ggExtra\nplot &lt;- study.two.gen %&gt;%\n  ggplot(aes(x = HLVA, y = mean.acc)) +\n  geom_point(size = 1.75, alpha = .5) +\n  geom_smooth(size = 1.5, colour = \"red\", method = \"lm\", se = FALSE) +\n  xlim(0, 15) +\n  ylim(0, 1.1)+\n  theme_bw() +\n  theme(\n    axis.text = element_text(size = rel(1.15)),\n    axis.title = element_text(size = rel(1.5))\n  ) +\n  labs(x = 'Health literacy (HLVA)', y = 'Mean accuracy')\nggExtra::ggMarginal(plot, type = \"histogram\", colour = \"lightgrey\", fill = \"lightgrey\")\n\n\n\n\n\n\n\nFigure 2: Scatterplot showing the potential association between accuracy of comprehension and health literacy\n\n\n\n\n\nYou can read more about chunk options here:\nhttps://quarto.org/docs/computations/r.html#chunk-options\n\n\n\n\n\n\nTip\n\n\n\nNote that the figure reference is computed by Quarto:\n\nIf the code chunk has a label like #| label: fig-ggextra-demo-eval (notice the grammar fig-...)\nand the text outside the chunk has the reference code @fig-ggextra-demo-eval then R will link the two objects, and write the reference in the rendered document.\nFigure reference numbers will be automated tallied by R.\n\n\n\nRead more about alt-text here:\nhttps://mine-cetinkaya-rundel.github.io/quarto-tip-a-day/posts/28-fig-alt/\n\n\n\nIt is a hard habit to break but you never have to calculate things and enter values by hand again e.g.\n\nWhat is the number of observations in the example data? It is: 169.\n\nThis is the bit of code that does the calculation in the sentence:\n\nWhat is the number of observations in the example data? It is: r length(study.two.gen$participant_ID)`.\n\nFor some statistical tests (t, cor, anovas, etc) there is a handy package and function, apa::apa that will render the statistical result in the correct apa format:\nFor example, you can wrap the apa() function around this cor.test() command to get automatically formatted statistical reports.\n\n\n\nNow let’s fit a model and get some results.\nLet’s assume we want to model the effect of health literacy on mean accuracy of understanding of health information.\nLet’s say that our model assumes:\n\\[\ny_i \\sim \\beta_0 + \\beta_1X_1 + \\epsilon_i\n\\]\nwhere:\n\nOutcome mean accuracy of response \\(y_i\\) is expected to vary, on average\nIn relation to \\(\\beta_0 + \\beta_1X_1\\)\n\nwhere:\n\\(\\beta_1X_1\\) the coefficient of the effect of variation in health literacy.\nThis is how you write math in Quarto (notice the dollar signs):\n\n$y_i \\sim \\beta_0 + \\beta_1X_1 + \\epsilon_i$\n\n\n\n\n\n\n\nTip\n\n\n\nI’m not writing the math because this is a serious model but to show off the equation writing capacities of Quarto.\nhttps://quarto.org/docs/visual-editor/technical.html\nIt is much, much more user friendly, and accurate, than MS Word equation builders.\nYou can read more about writing equations in:\nhttps://en.wikibooks.org/wiki/LaTeX/Mathematics#Operators\n\n\nSo fit the model and get a summary:\n\nmodel &lt;- lm(mean.acc ~ HLVA, data = study.two.gen)\nsummary(model)\n\n\nCall:\nlm(formula = mean.acc ~ HLVA, data = study.two.gen)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.40848 -0.05304  0.01880  0.07608  0.19968 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.61399    0.03387  18.128  &lt; 2e-16 ***\nHLVA         0.02272    0.00369   6.158 5.31e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1068 on 167 degrees of freedom\nMultiple R-squared:  0.1851,    Adjusted R-squared:  0.1802 \nF-statistic: 37.92 on 1 and 167 DF,  p-value: 5.307e-09\n\n\nObviously, the summary is not formatted for presentation. We deal with that next.\n\n\n\n\n\n\nTip\n\n\n\nWe absolutely do not want to copy statistics from our model outputs because:\n\nwe will make copy-paste errors;\nwe will lose track of which model underlies the statistics;\nit is slow and boring.\n\nWe want to get R to do the work for us.\n\n\n\n\n\nThere are a variety of ways to produce tables.\nTable 1 illustrates one method.\n\n# -- tidy model information\nmodel.tidy &lt;- tidy(model) %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  mutate(term = as.factor(term)) %&gt;%\n  mutate(term = fct_recode(term,\n\n    \"Health literacy (HLVA)\" = \"HLVA\",\n    \n  )) %&gt;%\n  rename(\n    \"Predictor\" = \"term\",\n    \"Estimate\" = \"estimate\",\n    \"Standard error\" = \"std.error\",\n    \"t\" = \"statistic\"\n  )\n\n# -- present table of model information\nkable(model.tidy, digits = 4) %&gt;%\n  kable_styling(bootstrap_options = \"striped\")\n\n\n\nTable 1: Table summary of model\n\n\n\n\n\n\nPredictor\nEstimate\nStandard error\nt\np.value\n\n\n\n\nHealth literacy (HLVA)\n0.0227\n0.0037\n6.1581\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nI can probably fix the p-value presentation but I’ll leave that there for now. The general principle is what counts:\n\nYou can programmatically fit a model \\(\\rightarrow\\) get the statistics \\(\\rightarrow\\) produce a table.\n\n\n\nBut see also:\n\nhttps://vincentarelbundock.github.io/tinytable/#tutorial\nhttps://modelsummary.com/index.html\nhttps://www.danieldsjoberg.com/gtsummary/\nhttps://broom.tidymodels.org/articles/broom.html\n\nThere is, however, a persuasive argument that model information (estimates, etc.) is best communicated visually, see the discussion in (e.g., Kastellec & Leoni, 2007)\nhttps://www.cambridge.org/core/journals/perspectives-on-politics/article/abs/using-graphs-instead-of-tables-in-political-science/9FD63E9EE686AF046732191EE8A68034\n\n\n\nI prefer to general marginal or conditional effects plots, see:\nhttps://marginaleffects.com/\n\n\n\n\nAs you generate a full manuscript you’ll begin to wish there was an easy way to make it compatible with APA format, enabling easy submission to a journal. Thankfully there is a wonderful “extension” for doing just that: https://wjschne.github.io/apaquarto/. As the creator of this extension notes, “I wrote apaquarto so that I would never have to worry about APA style ever again.”. It is easy to intall and works well, plus the creator is extremely responsive to requests and bug reports on github.\nDetailed instructions for installing and using the extension are given on the main github page:\nhttps://github.com/wjschne/apaquarto\nYou can either convert an old article or you can create a new one using the extension.\nSee here for a list of the options you can control in this extension: https://wjschne.github.io/apaquarto/options.html\n\n\n\nAdding references to a quarto document is easy. In the visual editor you can click Insert -&gt; Citation. You can also just type “@” which will bring up a list of references contained in any “.bib” file you have in the current folder. bib files are common methods for storing bibliographic information about publications. You can export references in bib format from common referencing management systems such as Endnote, Mendeley, Zotero etc.\nOnce you have an exported bib file in your working directory, the citations will appear when you type “@”. Each item in the bib file has a “key”, which is typically the first author name, followed by a year. If you want the reference to appear in parentheses in your generated document, put the key in square parentheses in your document. Separate multiple references in the usual way with a semicolon.\nEven more useful is if you link RStudio to a reference manager. Zotero is a popular and open source reference manager and plays well with quarto manuscripts. If you have Zotero installed, then the latest version of RStudio will detect it and import the library. This saves you creating the bib file, as RStudio will automatically add any references you type into a new bib file.\nReferences are then assembled in apa format in a final reference list at the end of the document.\nMore info on this process here: https://posit.co/blog/rstudio-1-4-preview-citations/\n\n\n\n\ncross-referencing\nbibliographies\ninline computing\nformat to multiple outputs\n\n\n\nThere a bunch of resources to get APA formatted manuscripts:\nhttps://github.com/wjschne/apaquarto\nhttps://wjschne.github.io/apaquarto/writing.html\n\n\n\nYou can cross-reference:\n\nfigures\ntables\nequations\nsections\nfootnotes\n\nIn output documents, the cross references appear as numbered hyperlinks, which is nice\nFrom the writer’s perspective, the main benefit is that you no longer have to try to keep track of object numbering. If you make an edit, the tallying will change automatically, e.g., remove or move a figure, and all the figure reference numbers will be re-calculated automatically.\n\n\n\nIt’s easy to do citations and build bibliographies:\nhttps://quarto.org/docs/visual-editor/technical.html#citations\nAnd you can have the references styled using any style with a .csl (Citation Style Language):\nhttps://www.zotero.org/styles?q=psychological%20bulletin\n\n\n\n\n\n\nTip\n\n\n\nFor people contributing to this tutorial, this:\nhttps://bookdown.org/yihui/rmarkdown-cookbook/verbatim-code-chunks.html\nis handy.\n\n\n\n\n\nYou can generate webpages, books, .pdf or .docx outputs from Quarto source files.\n\n\n\n\n\n\nKastellec, J. P., & Leoni, E. L. (2007). Using Graphs Instead of Tables in Political Science. Perspectives on Politics, 5(4), 755–771. https://doi.org/10.1017/S1537592707072209",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 00. Writing reproducible reports using Quarto"
    ]
  },
  {
    "objectID": "PSYC412/part2/00-quarto.html#sec-quarto-overview-what",
    "href": "PSYC412/part2/00-quarto.html#sec-quarto-overview-what",
    "title": "Week 00. Writing reproducible reports using Quarto",
    "section": "",
    "text": "I am sharing materials here to support the development of skills in producing reproducible reports using Quarto.\nThese materials should provide a quick entry to the what, why and how.\nYou can write in Quarto, in R-Studio (or any code editor), to produce:\n\nreports or manuscripts;\npresentations;\nbooks;\nwebpages.\n\nHere, we are going to focus on reports that will be output as .docx.\n\n\n\n\n\n\nTip\n\n\n\nWhat we are engaged in, here, is writing a combination of text and (here, R) code to generate a document.\nThis idea is old — see: e.g., Donald Knuth (who developed LaTex): https://www-cs-faculty.stanford.edu/~knuth/lp.html\nBut Quarto is a modern, efficient, way to do a powerful thing easily and efficiently.\n\n\n\n\nQuarto was developed by Posit (formerly: R-Studio).\n\n\n\n\n\n\nTip\n\n\n\nYou can find the Posit guide to Quarto here:\nhttps://quarto.org/docs/get-started/hello/rstudio.html\n\n\n\n\n\n\nBefore you get started, you will need a local installation of R and R-Studio.\n\n\nInstall R first, following the instructions here:\n\nhttps://cran.r-project.org/\n\nThen install RStudio, see:\n\nhttps://posit.co/download/rstudio-desktop/\nYou need to do the installation in that order because R-Studio is the interface (an Interactive Development Editor (IDE)) but R does the work.\n\nIt is probably worth your time having a skim of the “hello world” quick start guide:\n\nhttps://quarto.org/docs/get-started/hello/rstudio.html",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 00. Writing reproducible reports using Quarto"
    ]
  },
  {
    "objectID": "PSYC412/part2/00-quarto.html#sec-quarto-overview-why",
    "href": "PSYC412/part2/00-quarto.html#sec-quarto-overview-why",
    "title": "Week 00. Writing reproducible reports using Quarto",
    "section": "",
    "text": "There are at least the following justifications for investing time in this (in no particular order):\n\nReproducible manuscripts: create documents that incorporate text, computation or plotting code, and bibliographic information in one place. By doing so you avoid the risk of losing track of what data underlie which calculations or plots or table summaries. Copy and pasting statistical results will inevitably lead to errors.\nAutomate the boring stuff: figure, table or section cross-referencing; producing documents in different formats; generating bibliographies.\nMake manuscripts or presentations or notes share-able: Quarto is free so removes barrier to entry presented by licensed software like MS Word.\nMake nice things: plots and tables will look better.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 00. Writing reproducible reports using Quarto"
    ]
  },
  {
    "objectID": "PSYC412/part2/00-quarto.html#sec-quarto-quick-start",
    "href": "PSYC412/part2/00-quarto.html#sec-quarto-quick-start",
    "title": "Week 00. Writing reproducible reports using Quarto",
    "section": "",
    "text": "Open R-Studio.\nClick on the menu buttons:\n\nFile \\(\\rightarrow\\) New File \\(\\rightarrow\\) Quarto Document...\nA menu box will open: click on the Word radio button, and the Create button.\n\n\n\n\nNew Create Document menu box\n\n\nIf you do that, you will see this file open:\n\n\n\nA new .qmd file\n\n\n\nSee that Render button at the top of the window? Click on it.\n\n\nYou will be asked to give the file a name. Name it.\n\nThis action will create a Quarto script file, a .qmd and will generate a Word .docx.\nThe whole script looks like this:\n\n\n\nA new .qmd file: dummy file\n\n\nAnd the .docx looks like this:\n\n\n\nA new .qmd file: dummy file output\n\n\n\n\nThis example shows you the main parts of a Quarto file. Let’s identify these parts before we move on:\n\nThe yaml at the top is where you set document options:\n\n---\ntitle: \"Untitled\"\nformat: docx\neditor: visual\n---\nUsually, this will be where you specify what output formats you want, whether you want a ToC, and (for APA 7 documents) it will be where you add author, title and abstract information, see more information here:\nhttps://quarto.org/docs/output-formats/ms-word.html\n\nYou have section titles:\n\n## Quarto\nNotice the ## – the more hash signs you add, the lower the section title in the section hierarchy.\n\nYou have plain text:\n\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see &lt;https://quarto.org&gt;.\n\nAnd you have computing code — chunks:\n\n\n```{r}\n#| label: example\n1 + 1\n```\n\n[1] 2\n\n\nThe tick marks are at the top and bottom of the chunks tell R to read the code and work with it.\nNotice that you can specify the identity (for cross-referencing) and the behaviour or appearance of the code at the top of the chunk.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 00. Writing reproducible reports using Quarto"
    ]
  },
  {
    "objectID": "PSYC412/part2/00-quarto.html#sec-quarto-report-first",
    "href": "PSYC412/part2/00-quarto.html#sec-quarto-report-first",
    "title": "Week 00. Writing reproducible reports using Quarto",
    "section": "",
    "text": "In writing reports, we are often going to want to do tasks like:\n\nGet sample data and show:\n\n\nDistributions or other preliminary information to characterize samples.\n\n\nFit a model.\n\n\nPresent a table summary of the model estimates.\nPresent a plot showing model predictions.\n\n\nCross-reference figures, tables and sections.\n\nWe look at how to do these things next.\n\n\nWe will be working with an example data-set: click on the link to download the example data file.\n\nstudy-one-general-participants.csv\n\nThe file has the structure you can see in the extract below.\n\n\n\n\n\nparticipant_ID\nmean.acc\nmean.self\nstudy\nAGE\nSHIPLEY\nHLVA\nFACTOR3\nQRITOTAL\nGENDER\nEDUCATION\nETHNICITY\n\n\n\n\nstudytwo.1\n0.4107143\n6.071429\nstudytwo\n26\n27\n6\n50\n9\nFemale\nHigher\nAsian\n\n\nstudytwo.10\n0.6071429\n8.500000\nstudytwo\n38\n24\n9\n58\n15\nFemale\nSecondary\nWhite\n\n\nstudytwo.100\n0.8750000\n8.928571\nstudytwo\n66\n40\n13\n60\n20\nFemale\nHigher\nWhite\n\n\nstudytwo.101\n0.9642857\n8.500000\nstudytwo\n21\n31\n11\n59\n14\nFemale\nHigher\nWhite\n\n\n\n\n\n\n\nYou can use the scroll bar at the bottom of the data window to view different columns.\nYou can see the columns:\n\nparticipant_ID participant code;\nmean.acc average accuracy of response to questions testing understanding of health guidance (varies between 0-1);\nmean.self average self-rated accuracy of understanding of health guidance (varies between 1-9);\nstudy variable coding for what study the data were collected in\nAGE age in years;\nHLVA health literacy test score (varies between 1-16);\nSHIPLEY vocabulary knowledge test score (varies between 0-40);\nFACTOR3 reading strategy survey score (varies between 0-80);\nGENDER gender code;\nEDUCATION education level code;\nETHNICITY ethnicity (Office National Statistics categories) code.\n\n\n\nDownload and read the data into the R environment.\n\n```{r}\n#| label: data-read-in\nstudy.two.gen &lt;- read_csv(\"study-one-general-participants.csv\")\n```\n\nRows: 169 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): participant_ID, study, GENDER, EDUCATION, ETHNICITY\ndbl (7): mean.acc, mean.self, AGE, SHIPLEY, HLVA, FACTOR3, QRITOTAL\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\nLet’s take a look at the distribution of ages in this sample.\nYou can do that by examining a histogram, see Figure 1.\nYou can embed the code that does that work, together with your text, in a chunk like this:\n\n```{r}\n#| label: fig-example-histogram\nstudy.two.gen %&gt;%\n  ggplot(aes(x = AGE)) +\n  geom_histogram() +\n  labs(title = \"Sample characteristics\", x = \"Age (years\") +\n  theme_bw() \n```\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\nIn practice, unless you are teaching, or sharing your workings as part of your documentation, you are going to want to embed a chunk of code to produce a plot so that the plot is presented in your output document while the code chunk that does the work is invisible (at output).\nYou may also want to add a figure caption and alt-text, and you will want to manipulate figure dimensions.\nWe can learn how to do that stuff while producing a scatterplot, next.\nLet’s do something a little fancy, as in Figure 2: a scatterplot with marginal histograms.\nTo produce the plot, you will need to have installed the {ggExtra} library.\nLet’s go through the control elements first.\nTake a look at the chunk of code:\n\n```{r}\n#| label: fig-ggextra-demo-non-eval\n#| fig-cap: \"Scatterplot showing the potential association between accuracy of comprehension and health literacy\"\n#| fig-alt: \"The figure presents a grid of scatterplots indicating the association between variables mean accuracy (on y-axis) and health literacy (x-axis) scores. The points are shown in black, and clustered such that higher health literacy scores tend to be associated with higher accuracy scores. The trend is indicated by a thick red line. Marginal histograms indicates the distributio of data on each variable.\"\n#| warning: false\n#| eval: false\n#| fig-width: 4.5\n#| fig-height: 4.5\n# -- note that can use gridExtra\n# -- to show marginal distributions in scatterplots\n# https://github.com/daattali/ggExtra\nplot &lt;- study.two.gen %&gt;%\n  ggplot(aes(x = HLVA, y = mean.acc)) +\n  geom_point(size = 1.75, alpha = .5) +\n  geom_smooth(size = 1.5, colour = \"red\", method = \"lm\", se = FALSE) +\n  xlim(0, 15) +\n  ylim(0, 1.1)+\n  theme_bw() +\n  theme(\n    axis.text = element_text(size = rel(1.15)),\n    axis.title = element_text(size = rel(1.5))\n  ) +\n  labs(x = 'Health literacy (HLVA)', y = 'Mean accuracy')\nggExtra::ggMarginal(plot, type = \"histogram\", colour = \"lightgrey\", fill = \"lightgrey\")\n```\n\nNotice the bits of text at the top of the chunk:\n\n#| label: fig-ggextra-demo-non-eval labels the chunk. You need this for figure referencing.\n#| fig-cap: \"Scatterplot showing ...\" adds the caption that will be shown next to the plot.\n#| fig-alt: \"The figure presents ...\" adds alt-text describing the plot for people who use screen readers.\n#| warning: false stops R from producing the plot with warnings.\n#| echo: false stops R from showing both the code and the plot.\n#| eval: false here stops R from actually running the code.\n#| fig-width: 4.5 adjusts figure width.\n#| fig-height: 4.5 adjusts figure height.\n\nYou can see that I have added comments: # -- note that can use gridExtra to make the chunk self-documenting.\nNow show the plot: Figure 2.\n\n# -- note that can use gridExtra\n# -- to show marginal distributions in scatterplots\n# https://github.com/daattali/ggExtra\nplot &lt;- study.two.gen %&gt;%\n  ggplot(aes(x = HLVA, y = mean.acc)) +\n  geom_point(size = 1.75, alpha = .5) +\n  geom_smooth(size = 1.5, colour = \"red\", method = \"lm\", se = FALSE) +\n  xlim(0, 15) +\n  ylim(0, 1.1)+\n  theme_bw() +\n  theme(\n    axis.text = element_text(size = rel(1.15)),\n    axis.title = element_text(size = rel(1.5))\n  ) +\n  labs(x = 'Health literacy (HLVA)', y = 'Mean accuracy')\nggExtra::ggMarginal(plot, type = \"histogram\", colour = \"lightgrey\", fill = \"lightgrey\")\n\n\n\n\n\n\n\nFigure 2: Scatterplot showing the potential association between accuracy of comprehension and health literacy\n\n\n\n\n\nYou can read more about chunk options here:\nhttps://quarto.org/docs/computations/r.html#chunk-options\n\n\n\n\n\n\nTip\n\n\n\nNote that the figure reference is computed by Quarto:\n\nIf the code chunk has a label like #| label: fig-ggextra-demo-eval (notice the grammar fig-...)\nand the text outside the chunk has the reference code @fig-ggextra-demo-eval then R will link the two objects, and write the reference in the rendered document.\nFigure reference numbers will be automated tallied by R.\n\n\n\nRead more about alt-text here:\nhttps://mine-cetinkaya-rundel.github.io/quarto-tip-a-day/posts/28-fig-alt/\n\n\n\nIt is a hard habit to break but you never have to calculate things and enter values by hand again e.g.\n\nWhat is the number of observations in the example data? It is: 169.\n\nThis is the bit of code that does the calculation in the sentence:\n\nWhat is the number of observations in the example data? It is: r length(study.two.gen$participant_ID)`.\n\nFor some statistical tests (t, cor, anovas, etc) there is a handy package and function, apa::apa that will render the statistical result in the correct apa format:\nFor example, you can wrap the apa() function around this cor.test() command to get automatically formatted statistical reports.\n\n\n\nNow let’s fit a model and get some results.\nLet’s assume we want to model the effect of health literacy on mean accuracy of understanding of health information.\nLet’s say that our model assumes:\n\\[\ny_i \\sim \\beta_0 + \\beta_1X_1 + \\epsilon_i\n\\]\nwhere:\n\nOutcome mean accuracy of response \\(y_i\\) is expected to vary, on average\nIn relation to \\(\\beta_0 + \\beta_1X_1\\)\n\nwhere:\n\\(\\beta_1X_1\\) the coefficient of the effect of variation in health literacy.\nThis is how you write math in Quarto (notice the dollar signs):\n\n$y_i \\sim \\beta_0 + \\beta_1X_1 + \\epsilon_i$\n\n\n\n\n\n\n\nTip\n\n\n\nI’m not writing the math because this is a serious model but to show off the equation writing capacities of Quarto.\nhttps://quarto.org/docs/visual-editor/technical.html\nIt is much, much more user friendly, and accurate, than MS Word equation builders.\nYou can read more about writing equations in:\nhttps://en.wikibooks.org/wiki/LaTeX/Mathematics#Operators\n\n\nSo fit the model and get a summary:\n\nmodel &lt;- lm(mean.acc ~ HLVA, data = study.two.gen)\nsummary(model)\n\n\nCall:\nlm(formula = mean.acc ~ HLVA, data = study.two.gen)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.40848 -0.05304  0.01880  0.07608  0.19968 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.61399    0.03387  18.128  &lt; 2e-16 ***\nHLVA         0.02272    0.00369   6.158 5.31e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1068 on 167 degrees of freedom\nMultiple R-squared:  0.1851,    Adjusted R-squared:  0.1802 \nF-statistic: 37.92 on 1 and 167 DF,  p-value: 5.307e-09\n\n\nObviously, the summary is not formatted for presentation. We deal with that next.\n\n\n\n\n\n\nTip\n\n\n\nWe absolutely do not want to copy statistics from our model outputs because:\n\nwe will make copy-paste errors;\nwe will lose track of which model underlies the statistics;\nit is slow and boring.\n\nWe want to get R to do the work for us.\n\n\n\n\n\nThere are a variety of ways to produce tables.\nTable 1 illustrates one method.\n\n# -- tidy model information\nmodel.tidy &lt;- tidy(model) %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  mutate(term = as.factor(term)) %&gt;%\n  mutate(term = fct_recode(term,\n\n    \"Health literacy (HLVA)\" = \"HLVA\",\n    \n  )) %&gt;%\n  rename(\n    \"Predictor\" = \"term\",\n    \"Estimate\" = \"estimate\",\n    \"Standard error\" = \"std.error\",\n    \"t\" = \"statistic\"\n  )\n\n# -- present table of model information\nkable(model.tidy, digits = 4) %&gt;%\n  kable_styling(bootstrap_options = \"striped\")\n\n\n\nTable 1: Table summary of model\n\n\n\n\n\n\nPredictor\nEstimate\nStandard error\nt\np.value\n\n\n\n\nHealth literacy (HLVA)\n0.0227\n0.0037\n6.1581\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nI can probably fix the p-value presentation but I’ll leave that there for now. The general principle is what counts:\n\nYou can programmatically fit a model \\(\\rightarrow\\) get the statistics \\(\\rightarrow\\) produce a table.\n\n\n\nBut see also:\n\nhttps://vincentarelbundock.github.io/tinytable/#tutorial\nhttps://modelsummary.com/index.html\nhttps://www.danieldsjoberg.com/gtsummary/\nhttps://broom.tidymodels.org/articles/broom.html\n\nThere is, however, a persuasive argument that model information (estimates, etc.) is best communicated visually, see the discussion in (e.g., Kastellec & Leoni, 2007)\nhttps://www.cambridge.org/core/journals/perspectives-on-politics/article/abs/using-graphs-instead-of-tables-in-political-science/9FD63E9EE686AF046732191EE8A68034\n\n\n\nI prefer to general marginal or conditional effects plots, see:\nhttps://marginaleffects.com/",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 00. Writing reproducible reports using Quarto"
    ]
  },
  {
    "objectID": "PSYC412/part2/00-quarto.html#apa-formatted-manuscripts",
    "href": "PSYC412/part2/00-quarto.html#apa-formatted-manuscripts",
    "title": "Week 00. Writing reproducible reports using Quarto",
    "section": "",
    "text": "As you generate a full manuscript you’ll begin to wish there was an easy way to make it compatible with APA format, enabling easy submission to a journal. Thankfully there is a wonderful “extension” for doing just that: https://wjschne.github.io/apaquarto/. As the creator of this extension notes, “I wrote apaquarto so that I would never have to worry about APA style ever again.”. It is easy to intall and works well, plus the creator is extremely responsive to requests and bug reports on github.\nDetailed instructions for installing and using the extension are given on the main github page:\nhttps://github.com/wjschne/apaquarto\nYou can either convert an old article or you can create a new one using the extension.\nSee here for a list of the options you can control in this extension: https://wjschne.github.io/apaquarto/options.html",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 00. Writing reproducible reports using Quarto"
    ]
  },
  {
    "objectID": "PSYC412/part2/00-quarto.html#adding-references",
    "href": "PSYC412/part2/00-quarto.html#adding-references",
    "title": "Week 00. Writing reproducible reports using Quarto",
    "section": "",
    "text": "Adding references to a quarto document is easy. In the visual editor you can click Insert -&gt; Citation. You can also just type “@” which will bring up a list of references contained in any “.bib” file you have in the current folder. bib files are common methods for storing bibliographic information about publications. You can export references in bib format from common referencing management systems such as Endnote, Mendeley, Zotero etc.\nOnce you have an exported bib file in your working directory, the citations will appear when you type “@”. Each item in the bib file has a “key”, which is typically the first author name, followed by a year. If you want the reference to appear in parentheses in your generated document, put the key in square parentheses in your document. Separate multiple references in the usual way with a semicolon.\nEven more useful is if you link RStudio to a reference manager. Zotero is a popular and open source reference manager and plays well with quarto manuscripts. If you have Zotero installed, then the latest version of RStudio will detect it and import the library. This saves you creating the bib file, as RStudio will automatically add any references you type into a new bib file.\nReferences are then assembled in apa format in a final reference list at the end of the document.\nMore info on this process here: https://posit.co/blog/rstudio-1-4-preview-citations/",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 00. Writing reproducible reports using Quarto"
    ]
  },
  {
    "objectID": "PSYC412/part2/00-quarto.html#todo-list",
    "href": "PSYC412/part2/00-quarto.html#todo-list",
    "title": "Week 00. Writing reproducible reports using Quarto",
    "section": "",
    "text": "cross-referencing\nbibliographies\ninline computing\nformat to multiple outputs\n\n\n\nThere a bunch of resources to get APA formatted manuscripts:\nhttps://github.com/wjschne/apaquarto\nhttps://wjschne.github.io/apaquarto/writing.html\n\n\n\nYou can cross-reference:\n\nfigures\ntables\nequations\nsections\nfootnotes\n\nIn output documents, the cross references appear as numbered hyperlinks, which is nice\nFrom the writer’s perspective, the main benefit is that you no longer have to try to keep track of object numbering. If you make an edit, the tallying will change automatically, e.g., remove or move a figure, and all the figure reference numbers will be re-calculated automatically.\n\n\n\nIt’s easy to do citations and build bibliographies:\nhttps://quarto.org/docs/visual-editor/technical.html#citations\nAnd you can have the references styled using any style with a .csl (Citation Style Language):\nhttps://www.zotero.org/styles?q=psychological%20bulletin\n\n\n\n\n\n\nTip\n\n\n\nFor people contributing to this tutorial, this:\nhttps://bookdown.org/yihui/rmarkdown-cookbook/verbatim-code-chunks.html\nis handy.\n\n\n\n\n\nYou can generate webpages, books, .pdf or .docx outputs from Quarto source files.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 00. Writing reproducible reports using Quarto"
    ]
  },
  {
    "objectID": "PSYC412/part2/00-quarto.html#references",
    "href": "PSYC412/part2/00-quarto.html#references",
    "title": "Week 00. Writing reproducible reports using Quarto",
    "section": "",
    "text": "Kastellec, J. P., & Leoni, E. L. (2007). Using Graphs Instead of Tables in Political Science. Perspectives on Politics, 5(4), 755–771. https://doi.org/10.1017/S1537592707072209",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 00. Writing reproducible reports using Quarto"
    ]
  },
  {
    "objectID": "PSYC412/index.html",
    "href": "PSYC412/index.html",
    "title": "Analysing and Interpreting Psychological Data II",
    "section": "",
    "text": "Welcome\nWelcome to PSYC412!\nThis module builds on the knowledge and skills acquired in Statistics for Psychologists 1 (PSYC411). You will continue to practise data handling, data processing and data visualisation, using R and R Studio. In addition, you will extend your knowledge of the general linear model and learn how to adapt it when working with different types of data. Of course you’ll also learn how to implement those methods in R and R Studio.\nThis page gives you access to all the materials that you will need. You will have timetabled lab classes during which you are expected to work through a series of exercises to practise that week’s material. Before you come to your lab session, you should watch the lectures for that week, read the relevant book chapter and complete the pre-lab activities.\n\n\nAsking for help\nWe have carefully prepared and refined the lab materials in this course over several years, and we feel that the pace of the materials is just right for our students. Some students will complete them more quickly, and others more slowly - both of these scenarios are absolutely fine. You should work at the pace that suits you best, making sure you understand the materials before you move forward.\nIt is fairly inevitable that you will get stuck on the lab materials in this module at some point. This might be in Week 11, Week 12, or later. When you do, it’s important you reach out for help:\n\nAsk your friends on your table. We’ve designed this teaching space to help collaborative work. You are encouraged to work with other students. Make sure you ask others to explain how they’ve solved an exercise. Make sure you help out others where you can. Always make sure you understand the code and the exercise; don’t simply be satisfied that you’ve got the right answer.\nAsk a GTA or Lecturer. Our teaching staff is there to help you. There are no “stupid questions” in statistics, so just ask the GTA or the lecturer any question about what you’re doing.\nAsk on the Discussion Forum. On the PSYC402 moodle page you will find a Discussion Forum. This is a great way to ask a question outside of the lab sessions. It might seem scary to ask a question in the forum, but please don’t be afraid to do this. If you have a question, you can bet other students also have the same question! So by asking the question on the forum, you help out many more people on the module. A friendly GTA or Lecturer will be along to answer the question as soon as possible (we aim for within 48 hours during the working week).\nAsk on the module Q & A session. Each week we hold a “Q&A” online session where we will try and resolve any general queries and problems. It’s an ideal time to discuss things that students are struggling with or confused about, and can share ideas and answers. You can ask on the discussion forum above and then we might be able to pick up the issues and discuss them, but also you can ask in the session itself.\n\n\n\nCourse Contacts\n\n\n\n\nEmail Address\n\n\n\n\nMargriet Groen\nm.groen at lancaster dot ac dot uk\n\n\nRob Davies\nr.davies1 at lancaster dot ac dot uk\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "PSYC412/part1/Week13.html",
    "href": "PSYC412/part1/Week13.html",
    "title": "Week 13. More on interactions",
    "section": "",
    "text": "Interactions are ubiquitous in psychological science, which is why we’ll spend some more time building models that include interaction terms. Last week we modelled well-being as a function of screen-time (a continuous predictor) and biological sex (a categorical predictor) and their interaction. This week we’ll look at interactions between continuous predictors.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 13. More on interactions"
    ]
  },
  {
    "objectID": "PSYC412/part1/Week13.html#sec-wk13-lectures",
    "href": "PSYC412/part1/Week13.html#sec-wk13-lectures",
    "title": "Week 13. More on interactions",
    "section": "Lectures",
    "text": "Lectures\nThe lecture material for this week follows the recommended chapters in Winter (2020) – see under ‘Reading’ below – and is presented below:\n\n\n\n\n\n\nTip\n\n\n\nYou’ll find links to the slides and transcripts underneath each video. You can also download them all at once here\n\n\nMore on interactions (~18 min)\n\nSlides Transcript\nIf you need to remind yourself about the why’s and how’s of centering and standardising, please revisit this video. We’ll be using this in the lab activity.\nCentering and standardising (~5 min)\n\nSlides Transcript",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 13. More on interactions"
    ]
  },
  {
    "objectID": "PSYC412/part1/Week13.html#sec-wk13-reading",
    "href": "PSYC412/part1/Week13.html#sec-wk13-reading",
    "title": "Week 13. More on interactions",
    "section": "Reading",
    "text": "Reading\n\nWinter (2020)\nLink\nChapter 8 explains what interactions are and how to model and interpret them.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 13. More on interactions"
    ]
  },
  {
    "objectID": "PSYC412/part1/Week13.html#sec-wk13-pre-lab-activities",
    "href": "PSYC412/part1/Week13.html#sec-wk13-pre-lab-activities",
    "title": "Week 13. More on interactions",
    "section": "Pre-lab activities",
    "text": "Pre-lab activities\nAfter having watched the lectures and read the textbook chapters you’ll be in a good position to try these activities. Completing them before you attend your lab session will help you to consolidate your learning and help move through the lab activities more smoothly.\n\nPre-lab activity 1: Cheatsheets\nIn the first week, we had a look at some of the ‘recipes’ that Posit (the company behind RStudio) provides. If you need a reminder, an overview of all their ‘recipes’ can be found here.\nAnother great resource they provide are the ‘cheatsheets’: one-page overviews of the most important functions in a particular R package/library. There are many and some have been translated into different languages. You can access them online, or print them as a pdf to have on hand.\n\nTASK Explore the cheatsheets linked to below. The ones for RStudio IDE, dplyr, tidyr and ggplot2 are most relevant to the work we’ve been doing.\n\n\nOverview of all cheatsheets\nCheatsheet for RStudio IDE\nCheatsheet for dplyr\nCheatsheet for tidyr\nCheatsheet for ggplot2\n\n\n\nPre-lab activity 2: Getting ready for the lab class\n\nGet your files ready\nDownload the 412_week13_lab.zip file and upload it into a new folder in RStudio Server.\n\n\nRemind yourself of how to access and work with the RStudio Server.\n\nSign in to the RStudio Server. Note that when you are not on campus you need to log into the VPN first (look on the portal if you need more information about that).\nCreate a new folder for this week’s work.\nUpload the zip-file to the folder you have created on the RStudio server. Note you can either upload a single file or a zip-file.\n\n\n\n\n\n\n\nIf you have difficulty uploading files to the server\n\n\n\nIf you get error messages when attempting to upload a file or a folder with files to the server, you can try the following steps:\n\nClose the R Studio server, close your browser and start afresh.\nOpen the R Studio server in a different browser.\nFollow a work around where you use code to directly download the file to the server. The code to do that will be available at the start of the lab activity where you need that particular file. The code to download the file you need to complete the quiz is below.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 13. More on interactions"
    ]
  },
  {
    "objectID": "PSYC412/part1/Week13.html#sec-wk13-lab-activities",
    "href": "PSYC412/part1/Week13.html#sec-wk13-lab-activities",
    "title": "Week 13. More on interactions",
    "section": "Lab activities",
    "text": "Lab activities\nIn this lab, you’ll gain understanding of and practice with:\n\nwhen and why to apply multiple regression to answer questions in psychological science\nconducting multiple regression in R including interaction between continuous predictors\ninterpreting the R output of multiple linear regression (when including an interaction between continuous predictors)\nreporting results for multiple linear regression (when including an interaction between continuous predictors), following APA guidelines\n\n\nBackground\nToday, we’ll be working with a dataset from the following paper: Hamermesh, D. S. and Parker, A. (2005). Beauty in the classroom: instructors’ pulchritude and putative pedagogical productivity. Economics of Education Review, 24(4), 369 – 376.\nThe abstract of their paper is below or see here for the paper itself.\nAbstract: Adjusted for many other determinants, beauty affects earnings; but does it lead directly to the differences in productivity that we believe generate earnings differences? We take a large sample of student instructional ratings for a group of university teachers and acquire six independent measures of their beauty, and a number of other descriptors of them and their classes. Instructors who are viewed as better looking receive higher instructional ratings, with the impact of a move from the 10th to the 90th percentile of beauty being substantial. This impact exists within university departments and even within particular courses, and is larger for male than for female instructors. Disentangling whether this outcome represents productivity or discrimination is, as with the issue generally, probably impossible.\nOur research question: Do professors’ beauty score and age predict how students evaluate their teaching?\nTo complete this lab activity, you can use the R-script (412_wk13_labAct1_template.R) that you downloaded as part of the ‘Pre-lab activities’ as a template. Work through the activity below, adding relevant bits of code to your script as you go along.\n\n\nStep 1: Set up\n\n\n\n\n\n\nSet your working directory\n\n\n\nThe folder you were asked to download under ‘Pre-lab activity 2: Getting ready for the lab class’ contains the data files we’ll need. Make sure you have set your working directory to this folder by right-clicking on it and selecting ‘Set as working directory’.\n\n\n\n\n\n\n\n\nEmpty the R environment\n\n\n\nBefore you do anything else, when starting a new analysis, it is a good idea to empty the R environment. This prevents objects and variables from previous analyses interfering with the current one. To do this, you can click on the little broom icon in the top right of the Environment pane, or you can use rm(list=ls()).\n\n\nBefore we can get started we need to tell R which libraries to use. For this analysis we’ll need broom, car, lsr and tidyverse.\n\nTASK: Load the relevant libraries. If you are unsure how to do that, you can look at the ‘Hint’ below for a clue by expanding it. After that, if you are still unsure, you can view the code by expanding the ‘Code’ section below.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the library() function.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nThe code to do this is below.\nlibrary(broom)\nlibrary(car)\nlibrary(lsr)\nlibrary(tidyverse)\n\n\n\n\n\n\n\n\n\nIf you couldn’t upload files to the server, do this:\n\n\n\nIf you experienced difficulties with uploading a folder or a file to the server, you can use the code below to directly download the file you need in this lab activity to the server (instead of first downloading it to you computer and then uploading it to the server). Remember that you can copy the code to your clipboard by clicking on the ‘clipboard’ in the top right corner.\n\n\n\ndownload.file(\"https://github.com/mg78/2324_PSYC402/blob/main/data/week13/beauty.csv?raw=true\", destfile = \"beauty.csv\")\n\n\nTASK: Finally, read in the data file (beauty.csv), and familiarise yourself with its structure.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the read_csv() function and the head() function.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nThe code to do this is below.\nbeauty &lt;- read_csv(\"beauty.csv\")    \nhead(beauty)\n\n\n\nQUESTION 1: Do you notice anything about the name of one of the variables and the name of the data table?\nThe table contains, amongst other things, the following characteristics of the professors\n\n‘beauty’ - beauty score per professor\n‘eval’ - teaching evaluation score per professor\n‘age’ - age of the professor\n\nQUESTION 2: Go back to the research question (see under ‘Background’ above), which of these three variables is the outcome variable? Which ones are the predictors?\n\n\nStep 2: Descriptive statistics and distributions\n\nTASK: Calculate some descriptive statistics for the variables of interest (eval, beauty and age).\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou can use summarise() to calculate the mean, sd, min and max values.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\ndescriptives &lt;- beauty %&gt;% \n  summarise(mean_age = mean(age, na.rm = TRUE),\n            sd_age = sd(age, na.rm = TRUE),\n            min_age = min(age, na.rm = TRUE),\n            max_age = max(age, na.rm = TRUE),\n            mean_eval = mean(eval, na.rm = TRUE),\n            sd_eval = sd(eval, na.rm = TRUE),\n            min_eval = min(eval, na.rm = TRUE),\n            max_eval = max(eval, na.rm = TRUE),\n            mean_beauty = mean(beauty, na.rm = TRUE),\n            sd_beauty = sd(beauty, na.rm = TRUE),\n            min_beauty = min(beauty, na.rm = TRUE),\n            max_beauty = max(beauty, na.rm = TRUE))\n\n\n\nNow that we have the descriptive statistics, let’s get further information about the distribution of the variables by plotting histograms.\n\nTASK: Visualise the distributions of the variables of interest in histograms.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse ggplot() and geom_historgram()\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nggplot(data = beauty, aes(beauty)) +\n  geom_histogram()\n\nggplot(data = beauty, aes(eval)) +\n  geom_histogram()\n\nggplot(data = beauty, aes(age)) +\n  geom_histogram()\n\n\n\n\n\nStep 3: Center and standardise\nAs mentioned before, it will make it easier to interpret regression models with multiple predictors if we center and standardise our predictors. Before we go any further, we’ll do that.\n\nTASK: Center and standardise the predictor variables.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nCentering involves subtracting the mean; standardising involves dividing by the standard deviation.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nbeauty_z &lt;- beauty %&gt;%\n  mutate(age_z = (age - mean(age, na.rm = TRUE)) / sd(age),\n         beauty_z = (beauty - mean(beauty, na.rm = TRUE)) / sd(beauty))\n\n\n\n\n\nStep 4: Scatterplots\nNow let’s have a look at the relationships between variables using scatterplots. To remind yourself of what centering and standardising does, do this for both the raw data and the centered and standardised data.\n\nTASK: Visualise the relationships between the variables of interest in scatterplots.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nCreate six different scatterplots using ggplot() with geom_point() and geom_smooth().\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nggplot(beauty, aes(x = beauty, y = age)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  theme_bw()\n\nggplot(beauty, aes(x = beauty, y = eval)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  theme_bw()\n\nggplot(beauty, aes(x = age, y = eval)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  theme_bw()\n\nggplot(beauty_z, aes(x = beauty_z, y = age_z)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  theme_bw()\n\nggplot(beauty_z, aes(x = beauty_z, y = eval)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  theme_bw()\n\nggplot(beauty_z, aes(x = age_z, y = eval)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  theme_bw()\n\n\n\nQUESTION 3: Can you write an interpretation of the above plots in plain English?\nQUESTION 4: What is the difference between the scatterplots plotting the raw data and the ones plotting the centered and standardised data?\nIt is useful to have a quick look at the bivariate correlations between the variables of interest, before you run a regression model. We can easily generate a correlation matrix for these variables.\n\nTASK: Add the code below to your script and check you understand what each line does.\n\n\nbeauty_matrix &lt;- beauty_z %&gt;%\n  select(eval, age_z, beauty_z) %&gt;% # only keep relevant variables\n  as.data.frame() # tell R it is a specific type of data frame (needed for the correlate() function)\n\npairs(beauty_matrix) # make multiple scatterplots in one go\n\nintercor_results &lt;- correlate(x = beauty_matrix, # our data\n                              test = TRUE, # compute p-values\n                              corr.method = \"pearson\", # run a spearman test \n                              p.adjust.method = \"bonferroni\") # use the bonferroni correction\nintercor_results\n\nAfter you’ve run this code, look at the output in the console. It creates three tables, one with correlation coefficients, one with p-values for these coefficients and one with sample sizes.\n\n\nStep 5: The regression model\nWe’ve looked at descriptive statistics and distributions of variables and also at relations between variables. This has given us a good idea of what the data look like. Now we’ll construct the regression model to predict ‘evaluation score’ as a function of ‘age’ and ‘beauty score’. We’ll do this in two stages. First we’ll construct a model without an interaction term. Then we’ll construct a model that includes an interaction term betweeen the two predictor variables. Don’t forget to use the standardised data for all this.\n\nTASK: Construct a regression model without an interaction term.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the following formula, lm(y ~ x1 + x2, data); go back to the research question for your outcome and predictor variables.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nmod &lt;- lm(eval ~ age_z + beauty_z, data = beauty_z)\n\n\n\n\nTASK: Call and save the summary of your model; then have a look at it.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the summary() function.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nmod_int_summary &lt;- summary(mod_int)\nmod_int_summary\n\n\n\nQUESTION 5: Is the overall model significant?\nQUESTION 6: Are the predictors significant? What does this mean?\n\nTASK: Now create a model that includes an interaction term for the two predictors. Again, use the centered and standardised data.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the following formula, lm(y ~ x1 + x2 + x1:x2, data); go back to the research question for your outcome and predictor variables.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nmod_int &lt;- lm(eval ~ age_z + beauty_z + age_z:beauty_z, data = beauty_z)\nmod_int_summary &lt;- summary(mod_int)\nmod_int_summary\n\n\n\nQUESTION 7: Is the overall model significant?\nQUESTION 8: Have a good look at the coefficients. Can you interpret each one of them in turn and then formulate an overall interpretation?\n\n\n\n\n\n\nHint\n\n\n\n\n\nRemember that after centering and standardising, the meaning of 0 has changed for both predictor variables.\n\n\n\nInterpretation of coefficients in a multiple regression can be facilitated by ‘added variable’ plots.\n\nTASK: Use the function avPlots() to create ‘added variable’ plots.\n\nCreating a scatterplot with our outcome variable on the y-axis and the significant predictor on the x-axis and then plotting our third variable (age) using different colours gives some information. Do you see how high age scores (light blue + 2 SD) seem to be more frequent in the bottom left corner?\n\nTASK: Use the code below to create the plot.\n\n\nggplot(data = beauty_z, aes(x = beauty_z, y = eval, colour = age_z)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE, colour = 'black') +\n  theme_bw() +\n  labs(x = \"Beauty score\", y = \"Teaching evaluation score\")\n\nBut it might be more useful to plot different regression lines for different values of age. We can do this be transforming age into a categorical variable for plotting purposes. We want to create three categories, based on eye-balling the histogram for age:\n\nyoungest (40 and younger)\naverage (between 41 and 53)\noldest (54 and older).\n\nWe can do this by using the mutate() function, in combination with the cut() function. There is a ‘recipe’ for this on the Posit website:\n\ncut() - group continuous variable into categories - recipe\n\n\nTASK: Read through the recipe mentioned above. Working through the example will be particularly helpful. Write the code to create the categories mentioned above.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nFill in the relevant bits in the code template below:\n  mutate(NEW VARIABLE = cut(\n    CONTINUOUS VARIABLE,\n    breaks = c(DEFINE BREAK POINTS),\n    labels = c(ADD LABELS FOR BREAK POINTS)\n    )\n  )\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nbeauty_z_ageCat &lt;- beauty_z %&gt;%\n  mutate(ageCat = cut(\n    age,\n    breaks = c(0, 40, 54, Inf),\n    labels = c(\"youngest\",\"average\",\"oldest\")\n    )\n  )\n\n\n\nNow let’s create a single plot with three different lines, one for each of the age groups created above.\n\nTASK: Copy the code below to your script and make sure you understand what it does.\n\n\nggplot(data = beauty_z_ageCat, aes(x = beauty_z, y = eval, colour = ageCat)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  theme_bw() +\n  labs(x = \"Beauty score\", y = \"Teaching evaluation score\")\n\nThe line for the oldest participants seems much steeper than for the other two groups, suggesting that the interaction between age and beauty is mostly driven by older participants who have received more extreme beauty scores.\n\n\nStep 6: Checking assumptions\nNow that we’ve fitted a model, let’s check whether it meets the assumptions of linearity, normality and homoscedasticity.\nLinearity Unlike when we did simple regression we can’t use crPlots() to test for linearity when there is an interaction, but we know from looking at the grouped scatterplot that this assumption has been met.\nNormality Normally we would test for normality with a qq-plot and a Shapiro-Wilk test. However, because this dataset is so large, the Shapiro-Wilk is not appropriate (if you try to run the test it will produce a warning telling you that the sample size must be between 3 and 5000). This is because with extremely large sample sizes the Shapiro-Wilk test will find that any deviation from normality is significant. Therefore we should judge normality based upon the qq-plot.\n\nTASK: Create a qq-plot to check the residuals are normally distributed.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the qqPlot() function; mind the capital P.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nqqPlot(mod_int$residuals)\n\n\n\nQUESTION 9: What do you conclude from the qq-plot?\nHomoscedasticity Here we have the same problem as with testing for normality: with such a large sample the ncvTest() will produce a significant result for any deviation from homoscedasticity. So we need to rely on plots again.\nTo check for homoscedasticity we can use plot() from Base R that will produce a bunch of helpful plots (more information [here] (https://www.r-bloggers.com/2016/01/how-to-detect-heteroscedasticity-and-rectify-it/)).\n\nTASK: Copy the code below to your script and run it to create the plots\n\n\npar(mfrow=c(2,2))                 # 4 charts in 1 panel\nplot(mod_int)                     # this may take a few seconds to run\n\nQUESTION 10: What do you conclude from the residuals vs leverage plot?\nMulti-collinearity Now let’s check for multi-collinearity using the vif() function. Essentially, this function estimates how much the variance of a coefficient is “inflated” because of linear dependence with other predictors, i.e., that a predictor isn’t actually adding any unique variance to the model, it’s just really strongly related to other predictors. Thankfully, the vif() function is not affected by large samples like the other tests. There are various rules of thumb, but most converge on a VIF of above 2 - 2.5 for any one predictor being problematic.\n\nTASK: Use the vif() function to test for multi-collinearity.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nvif(mod_int)   \n\n\n\nQUESTION 11: Do any of the predictors show evidence of multi-collinearity?\nFinally, we need to write up the results.\nQUESTION 12: Can you write up the results of the regression analysis following APA guidelines?\n\n\n\n\n\n\nHint\n\n\n\n\n\nDon’t forget to mention and interpret the interaction effect.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 13. More on interactions"
    ]
  },
  {
    "objectID": "PSYC412/part1/Week13.html#sec-wk13-answers",
    "href": "PSYC412/part1/Week13.html#sec-wk13-answers",
    "title": "Week 13. More on interactions",
    "section": "Answers",
    "text": "Answers\nWhen you have completed all of the lab content, you may want to check your answers with our completed version of the script for this week. Remember, looking at this script (studying/revising it) does not replace the process of working through the lab activities, trying them out for yourself, getting stuck, asking questions, finding solutions, adding your own comments, etc. Actively engaging with the material is the way to learn these analysis skills, not by looking at someone else’s completed code…\n\nYou can download the R-script that includes the relevant code here: 412_wk13_labAct1_withAnswers.R.\n\nDo you notice anything about the name of one of the variables and the name of the data table? Both the data table and one of the variables are called ‘beauty’. The code works, but it is not good practice. It would be better to have different names for the data table and the variable, to you or R getting confused.\nGo back to the research question (see under ‘Background’ above), which of these three variables is the outcome variable? Which ones are the predictors? The research question is ‘Do professors’ beauty score and age predict how students evaluate their teaching?’ From this we can deduct that the outcome variable is teaching evaluation score and that the predictors are age and beauty score.\nCan you write an interpretation of the above plots in plain English? A moderate negative association seems present between beauty score and age: with increasing age, beauty score decreases. A moderate positive association seems present between beauty score and teaching evaluation: professors with higher beauty scores also receive higher teaching evaluations. Not much of a association seems present between age and teaching evaluation (the line is pretty horizontal).\nWhat is the difference between the scatterplots plotting the raw data and the ones plotting the centered and standardised data? The units of the x-axis have changed from years (for age) and scores (for beauty) to standard units, with zero in the middle.\nIs the overall model significant? Yes, F(2, 460) = 8.53, p = .0002\nAre the predictors significant? What does this mean? The beauty score significantly predicts teaching evaluation score, but age does not. Professors with higher beauty scores, received better teaching evaluations.\nIs the overall model significant? Yes, F(3, 459) = 9.32, p = 5.451e-06\nHave a good look at the coefficients. Can you interpret each one of them in turn and then formulate an overall interpretation? HINT: Remember that after centering and standardising, the meaning of 0 has changed for both predictor variables. The intercept is predicted teaching evaluation score for a professor with average age and average beauty score. The slope of ‘age’ is positive; this means that for higher age, teaching evaluation scores were better. However the coefficient is not significant, therefore has little predictive power.The slope of ‘beauty’ is positive; this means that with higher beauty score, professors receive higher teaching evaluations. This predictor is significant. The slope for the interaction is also positive. This can be read as follows: When age and beauty both increase, teaching evaluation score also increases. The interaction is significant.\nWhat do you conclude from the qq-plot? The residuals are mostly normally distributed. At the top right (quantile + 3), there are some values that don’t quite look normally distributed, this is probably due to fewer data points being available in the highest age bracket. Have a look at a histogram for age. There are a few individuals well above the retirement age, but clearly a lot fewer than in younger age brackets. This basically means that the model does not do a particularly good job for predicting evaluation score at high values of age. As retirement age is quite a natural point to limit the data, you could run the model again, only including people below retirement age, this should give you better behaving residuals. Ideally, you’d pre-register a decision such as this. If you didn’t do this prior to data collection, you could still limit the age range included in the final model, but you would need to be transparent in your reporting.\nWhat do you conclude from the residuals vs leverage plot? The residuals vs leverage plot shows a flat red line so, whilst it isn’t perfect, we can assume that with regression is still an appropriate analysis.\nDo any of the predictors show evidence of multi-collinearity? No\nCan you write up the results of the regression analysis following APA guidelines? The results of the regression indicated that the model significantly predicted teaching evaluation scores (F(3, 459) = 9.316, p &lt; .001, adjusted R^2 = 0.05), accounting for 5% of the variance. A professor’s beauty score was a significant positive predictor of teaching evaluation score (\\(\\beta\\) = 0.12, p &lt; .001). This effect was moderated by a significant positive interaction between beauty score and age (\\(\\beta\\) = 0.08, p &lt; .001), suggesting that when age and beauty score both increased, teaching evaluation score also increased.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 13. More on interactions"
    ]
  },
  {
    "objectID": "PSYC412/part1/Week14.html",
    "href": "PSYC412/part1/Week14.html",
    "title": "Week 14. Logistic regression",
    "section": "",
    "text": "All of the models considered up to this point dealt with continuous response/dependent/outcome variables. Previously we looked at categorical predictors, but what if the response itself is categorical? For instance, whether the participant has made an accurate or inaccurate selection or whether a job candidate gets hired or not. Another common type of data is count data, where values are also discrete. Often with count data, the number of opportunities for something to occur is not well-defined. For instance, the number of speech error in a corpus, the number of turn shifts between speakers in a conversation or the number of visits to the doctor. Logistic regression allows us to model a categorical response variable.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 14. Logistic regression"
    ]
  },
  {
    "objectID": "PSYC412/part1/Week14.html#sec-wk14-lectures",
    "href": "PSYC412/part1/Week14.html#sec-wk14-lectures",
    "title": "Week 14. Logistic regression",
    "section": "Lectures",
    "text": "Lectures\nThe lecture material for this week follows the recommended chapters in Winter (2020) – see under ‘Reading’ below – and is presented below:\nLogistic regression (~38 min)\n\nSlides Transcript",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 14. Logistic regression"
    ]
  },
  {
    "objectID": "PSYC412/part1/Week14.html#sec-wk14-reading",
    "href": "PSYC412/part1/Week14.html#sec-wk14-reading",
    "title": "Week 14. Logistic regression",
    "section": "Reading",
    "text": "Reading\n\nBarr (2020)\nLink\nThis online textbook provides a useful overview of logistic regression. It does talk about modelling multi-level data and random effects. Don’t worry about that for now, those will be covered in the second half of 412. This week we’ll focus on ‘single-level’ data.\n\n\nWinter (2020)\nLink\nChapter 12 provides a comprehensive introduction to logistic regression and its implementation in R.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 14. Logistic regression"
    ]
  },
  {
    "objectID": "PSYC412/part1/Week14.html#sec-wk14-pre-lab-activities",
    "href": "PSYC412/part1/Week14.html#sec-wk14-pre-lab-activities",
    "title": "Week 14. Logistic regression",
    "section": "Pre-lab activities",
    "text": "Pre-lab activities\nAfter having watched the lectures and read the textbook chapters you’ll be in a good position to try these activities. Completing them before you attend your lab session will help you to consolidate your learning and help move through the lab activities more smoothly.\n\nPre-lab activity 1: Getting ready\n\nGet your files ready\nDownload the 412_week14_forStudents.zip file and upload it into a new folder in RStudio Server.\n\n\nRemind yourself of how to access and work with the RStudio Server.\n\nSign in to the RStudio Server. Note that when you are not on campus you need to log into the VPN first (look on the portal if you need more information about that).\nCreate a new folder for this week’s work.\nUpload the zip-file to the folder you have created on the RStudio server. Note you can either upload a single file or a zip-file.\n\n\n\n\n\n\n\nIf you have difficulty uploading files to the server\n\n\n\nIf you get error messages when attempting to upload a file or a folder with files to the server, you can try the following steps:\n\nClose the R Studio server, close your browser and start afresh.\nOpen the R Studio server in a different browser.\nFollow a work around where you use code to directly download the file to the server. The code to do that will be available at the start of the lab activity where you need that particular file. The code to download the file you need to complete the quiz is below.\n\n\n\n\n\n\nPre-lab activity 2: Rainy days\nTry running the code mentioned in the online textbook by Barr. If you find it easier, use the rainy_days.R script (in the ‘412_week14_forStudents folder you were asked to download in ’Pre-lab activity 1’). It illustrates the point that for discrete data, the variance is often not independent from the mean. In addition, it introduces some very useful R functions: What do the rep() function, the c() function and the facet_wrap() function do? Remember, you can type ?function name() (e.g., ?rep()) in the Console to get more information about a function. Finally, can you add a graph for rainy days in Lancaster?\n\n\nPre-lab activity 3: Gesture perception\nPlease go through the example described in section 12.6 of the chapter on logistic regression in Bodo Winter’s book (link under ‘Reading’). Read the section and (simultaneously) work through the script (chapter12_6.R; in the ‘412_week14_forStudents folder you were asked to download in ’Pre-lab activity 1’). We’ll be working more with this dataset during the lab, so it is helpful if you get a feel for it now.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 14. Logistic regression"
    ]
  },
  {
    "objectID": "PSYC412/part1/Week14.html#sec-wk14-lab-activities",
    "href": "PSYC412/part1/Week14.html#sec-wk14-lab-activities",
    "title": "Week 14. Logistic regression",
    "section": "Lab activities",
    "text": "Lab activities\nIn this lab, you’ll gain understanding of and practice with:\n\nwhen and why to apply logistic regression to answer questions in psychological science\nconducting logistic regression in R\ninterpreting the R output of logistic regression\nreporting results for logistic regression following APA guidelines\n\nMore info will be uploaded soon.\n\nLab activity 1: More work on gesture perception\n\nBackground\nThe dataset we’ll be working with is described in section 12.6 of the chapter on logistic regression in Bodo Winter’s book (link under ‘Reading’). In the pre-lab activity, we explored the dataset and fitted a first logistic regression model assessing whether participants’ perception of a gesture (expressed as a categorical decision between a ‘shape’ vs. a ‘height’ interpretation of the gesture) was affected by the extent of ‘pinkie curl’. In this lab activity, we’ll be building on that analysis by: 1) Repeating the analysis with a centered pinkie curl variable, and 2) by adding a second predictor: index_curve.\nOur research question: Is gesture perception associated with different aspects of hand shape?\n\n\nStep 1: Set up\n\n\n\n\n\n\nSet your working directory\n\n\n\nThe folder you were asked to download under ‘Pre-lab activity 1: Getting ready for the lab class’ contains the data files we’ll need. Make sure you have set your working directory to this folder by right-clicking on it and selecting ‘Set as working directory’.\n\n\n\n\n\n\n\n\nEmpty the R environment\n\n\n\nBefore you do anything else, when starting a new analysis, it is a good idea to empty the R environment. This prevents objects and variables from previous analyses interfering with the current one. To do this, you can click on the little broom icon in the top right of the Environment pane, or you can use rm(list=ls()).\n\n\nBefore we can get started we need to tell R which libraries to use. For this analysis we’ll need broom and tidyverse.\n\nTASK: Load the relevant libraries. If you are unsure how to do that, you can look at the ‘Hint’ below for a clue by expanding it. After that, if you are still unsure, you can view the code by expanding the ‘Code’ section below.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the library() function.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nThe code to do this is below.\nlibrary(broom)\nlibrary(tidyverse)\n\n\n\n\n\n\n\n\n\nIf you couldn’t upload files to the server, do this:\n\n\n\nIf you experienced difficulties with uploading a folder or a file to the server, you can use the code below to directly download the file you need in this lab activity to the server (instead of first downloading it to you computer and then uploading it to the server). Remember that you can copy the code to your clipboard by clicking on the ‘clipboard’ in the top right corner.\n\n\n\ndownload.file(\"https://github.com/mg78/2324_PSYC402/blob/main/data/week14/hassemer_winter_2016_gesture.csv?raw=true\", destfile = \"hassemer_winter_2016_gesture.csv\")\n\n\nTASK: Finally, read in the data file (hassemer_winter_2016_gesture.csv), and familiarise yourself with its structure.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the read_csv() function and the head() function.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nThe code to do this is below.\nges &lt;- read_csv(\"hassemer_winter_2016_gesture.csv\")\nhead(ges)\n\n\n\n\n\nStep 2: Descriptive statistics and distributions\nAs part of the pre-lab activities, you’ve already familiarised yourself with the dataset by looking at:\n\nhow participants were distributed across the pinkie curl conditions;\nwhich response option was chosen more frequently in total and across pinkie curl conditions;\nproportions of response options across pinkie curl conditions\n\n\nTASK: Please remind yourself of these stages if necessary by revisiting the chaper12_6.R script.\n\n\n\nStep 3: Data wrangling\nFor logistic regression, the outcome variable needs to either be coded as 0 or 1, or it needs to be coded as a factor.\n\nTASK: Write the code to convert the outcome variable to a factor.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse mutate() and factor().\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nges &lt;- mutate(ges, choice = factor(choice))\n\n\n\nAs with linear regression, centering your predictor makes it easier to interpret the output of a logistic regression.\n\nTASK: Write the code to center the pinkie curl predictor.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nCentering involves subtracting the mean.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nges &lt;- mutate(ges,\n    pinkie_c = pinkie_curl - mean(pinkie_curl))\n\n\n\n\n\nStep 5: Fit the regression model with the centered variable\nNow let’s re-fit choice (height vs. shape) as a function of pinkie curl, but using the centered variable.\n\nTASK: Write the code to fit the model and check the coefficients.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the glm() function and remember you need to specify the ‘family’ argument. To check the coefficients, you can use the tidy() function.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nges_mdl_c &lt;- glm(choice ~ pinkie_c,\n               data = ges, family = 'binomial')\ntidy(ges_mdl_c)\n\n\n\nQUESTION 1: How does the coefficient table differ from the model fitted earlier where you used the pinkie curl predictor before you centered it?\nQUESTION 2: How should you now (using the model with the centered pinkie curl predictor) interpret the intercept?\nThe average pinkie curl is step 5 on the 9 step continuum. From our previous analysis (pre-lab activity), we know that the predicted log odds of ‘shape’ at that step was 0.38 and the probability was 0.59. So if we extract the intercept from the coefficient table of the model using the centered predictor, we should get those values. Use the code below to do so and compare the values to the ones from the pre-lab activity.\n\n# Extracting the estimate for the intercept and storing it in an object called intercept:\nintercept_logOdds &lt;- tidy(ges_mdl_c)$estimate[1]\n\n# Getting the predicted probability by applying the logistic:\nintercept_prob &lt;- plogis(intercept_logOdds)\n\nintercept_logOdds\nintercept_prob\n\nThe values are roughly the same!\n\n\nStep 6: Incorporating a second predictor\nIn addition to the ‘pinkie curl’, the extent to which the index finger is curved may also affect gesture perception. This is quantified in the ‘index_curve’ variable. Before we fit the model, we need to center both predictors.\n\nTASK: Write the code to center the second predictor.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nCentering involves subtracting the mean.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nges &lt;- mutate(ges,\n              index_c = index_curve - mean(index_curve))\n\n\n\n\nTASK: Write the code to fit model with both pinkie curl and index curve and check the coefficients.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the glm() function and remember you need to specify the ‘family’ argument. To check the coefficients, you can use the tidy() function.\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nboth_mdl &lt;- glm(choice ~ pinkie_c + index_c,\n    data = ges, family = 'binomial')\ntidy(both_mdl)\n\n\n\nQUESTION 3: How does the index_curve predictor affect the proportion of shape responses?\nWe can see this more easily if we compare the descriptive proportions:\n\nindex_tab &lt;- with(ges, table(index_curve, choice))\nindex_tab\n\nprop.table(index_tab, 1)\n\nThe “1” in here stands for row-wise proportions.”2” would compute column-wise proportions.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 14. Logistic regression"
    ]
  },
  {
    "objectID": "PSYC412/part1/Week14.html#sec-wk14-answers",
    "href": "PSYC412/part1/Week14.html#sec-wk14-answers",
    "title": "Week 14. Logistic regression",
    "section": "Answers",
    "text": "Answers\nWhen you have completed all of the lab content, you may want to check your answers with our completed version of the script for this week. Remember, looking at this script (studying/revising it) does not replace the process of working through the lab activities, trying them out for yourself, getting stuck, asking questions, finding solutions, adding your own comments, etc. Actively engaging with the material is the way to learn these analysis skills, not by looking at someone else’s completed code…\n\nYou can download the R-script that includes the relevant code here: 412_wk14_labAct1_withAnswers.R.\n\nHow does the coefficient table differ from the model fitted earlier where you used the pinkie curl predictor before you centered it? The intercept has changed from 1.065 to 0.397.\nHow should you now (using the model with the centered pinkie curl predictor) interpret the intercept? The intercept is now the predicted log odds of ‘shape’ for a gesture with average pinkie curl.\nHow does the index_curve predictor affect the proportion of shape responses? The slope is positive, so more index-curved fingers results in more shape responses.",
    "crumbs": [
      "Home",
      "PSYC412",
      "Week 14. Logistic regression"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Welcome\n\nWelcome to analysing and interpreting data for Masters students. For Term 1, please click on PSYC411 button below. For Term 2, please click on the PSYC412 button.\n\n\n  \n    \n\n    \n      PSYC411\n    \n    \n      PSYC412"
  },
  {
    "objectID": "PSYC411/part2/intro.html",
    "href": "PSYC411/part2/intro.html",
    "title": "Week 6. Why we are asking you to do this",
    "section": "",
    "text": "The PSYC411 Research Report assignment requires students to locate, access, analyse and report previously collected data.\n\n\n\n\n\n\nTip\n\n\n\nHere, we answer the question:\n\nWhy: what will you learn about, what is our motivation?\n\n\n\nI hope you will agree that the discussion that follows is worth your time in reading it. It will help you to understand why we are asking you to do the assignment, and why we are looking for what we are looking for. It will help you to understand how this work will aid your development.\nYou may not be interested in the discussion in this chapter.\nWe also provide a summary guide to What you have to write.\nIf you want more in-depth guidance or support, we provide detailed step-by-step notes on the work you can plan to do in a chapter we have written to support you: How you can do the analysis work.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 6. Why we are asking you to do this"
    ]
  },
  {
    "objectID": "PSYC411/part2/intro.html#sec-intro-why",
    "href": "PSYC411/part2/intro.html#sec-intro-why",
    "title": "Week 6. Why we are asking you to do this",
    "section": "",
    "text": "The PSYC411 Research Report assignment requires students to locate, access, analyse and report previously collected data.\n\n\n\n\n\n\nTip\n\n\n\nHere, we answer the question:\n\nWhy: what will you learn about, what is our motivation?\n\n\n\nI hope you will agree that the discussion that follows is worth your time in reading it. It will help you to understand why we are asking you to do the assignment, and why we are looking for what we are looking for. It will help you to understand how this work will aid your development.\nYou may not be interested in the discussion in this chapter.\nWe also provide a summary guide to What you have to write.\nIf you want more in-depth guidance or support, we provide detailed step-by-step notes on the work you can plan to do in a chapter we have written to support you: How you can do the analysis work.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 6. Why we are asking you to do this"
    ]
  },
  {
    "objectID": "PSYC411/part2/intro.html#sec-why-key-ideas",
    "href": "PSYC411/part2/intro.html#sec-why-key-ideas",
    "title": "Week 6. Why we are asking you to do this",
    "section": "The key ideas",
    "text": "The key ideas\nThere are two ideas motivating our approach. It will be helpful to you if I sketch them out early, here. We can demonstrate the usefulness of these ideas as we progress through our work.\nThe first key idea is expressed clearly in sociological discussions of science. This is that there is a difference between science “…being done, science in the making, and science already done, a finished product …” [Bourdieu (2004); p.2]. The awareness we want to develop is that there are two things: there is the story that may be presented in a textbook or in a lecture about scientific work or scientific claims; and there is the work we do in practice, as we develop graduate skills, and as we exercise those skills professionally in the workplace.\nThe second key idea connects to the first. This idea is that reported analyses are not necessary or sufficient to the data or the question. What does this mean? It means that the same data can reasonably be analysed in different ways. There is no necessary way to analyse some data though there may be conventions or normal practices (Kuhn, 1970). It means that it is unlikely that any one analysis will do all the work that could be done (a sufficiency) to get you from your data to useful or reasonable answers to your questions.\nThese ideas may be unsettling but they are realistic. Stating them will better prepare you for professional work. In the workplace, the accuracy of these ideas will emerge when you see how a team in any sector (health, marketing …) gets from its data to its product. If we talk about the ideas now, we can get you ready for dealing with the practical and the ethical concerns you will confront when that happens.\nWe will begin by discussing psychological research, and research about psychological research, to answer the question: Why: what is the motivation for the assignment?",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 6. Why we are asking you to do this"
    ]
  },
  {
    "objectID": "PSYC411/part2/intro.html#sec-motivation-for-assignment",
    "href": "PSYC411/part2/intro.html#sec-motivation-for-assignment",
    "title": "Week 6. Why we are asking you to do this",
    "section": "Why: what is the motivation for the assignment?",
    "text": "Why: what is the motivation for the assignment?\n\nThe wider context: crisis and revolution\nWe are here because we are interested in humans and human behaviour, and because we are interested in scientific methods of making sense of these things. Some of us are aware that science (including psychological science) has undergone a rolling series of crises: the replicability or replication crisis (Pashler & Harris, 2012; Pashler & Wagenmakers, 2012); the statistical crisis (A. Gelman & Loken, 2014b); and the generalizability crisis (Yarkoni, 2022). And that science is undergoing a response to these crises, evidenced in the advocacy of pre-registration (Nosek et al., 2018, 2019), and of registered reports (Nosek & Lakens, 2014), the use of open science badges (e.g., for the journal Psychological Science), the completion of large-scale replication studies (Aarts et al., 2015), and the identification of open science principles (Munafò et al., 2017). We may usefully refer, collectively, to the crises and the responses, as the credibility revolution (Vazire, 2018)\nWe could teach a course on this (in Lancaster, we do) but, here, I invite you to follow the references if you are interested. Before going on, I want to call your attention to the fact that important elements of the hard work in trying to make science work better has been led by PhD students and by junior researchers (e.g., Herndon et al., 2014). Graduate students may, at first, assume that the fact that a research article has been published in a journal means the findings that are reported must be true. Most of the time, some educated skepticism is more appropriate. An important driver of the realization that there are problems evident in the literature, and that there are changes we can make to improve practice, comes from independent post-publication review work exposing the problems in published work (see, e.g., this account by Andrew Gelman)\n\n\n\n\n\n\nTip\n\n\n\n\nAllow yourself to feel skeptical about the reports you read then work with the motivation this feeling provides.\n\n\n\nIn brief, then, most practicing scientists now understand or should understand that many of the claims we encounter in the published scientific literature are unlikely to be supported by the evidence (Ioannidis, 2005), whether we are looking at the evidence of the results in the reports themselves, or evidence in later attempts to find the same results (e.g., Aarts et al., 2015). We suspect that this may result from a number of causes. We understand that researchers may engage in questionable research practices (John et al., 2012). We understand that researchers may exploit the potential for flexibility in doing and reporting analyses (Simmons et al., 2011a). We understand that there are problems in how psychologists use or talk about the measurement of psychological constructs (Flake & Fried, 2020). We understand that there are problems in how psychologists sample people for their studies, both in where we recruit (Bornstein et al., 2013; Henrich et al., 2010; Wild et al., 2022), and in how many we recruit (Button et al., 2013; Cohen, 1962; Sedlmeier & Gigerenzer, 1989; Vankov et al., 2014). We understand that there are problems in how psychologists specify or think about their hypotheses or predictions (Meehl, 1967; Scheel, 2022). And we understand that there are problems in how scientists do, or rather do not, comply with good practice recommendations designed to fix these problems (discussed further in the following).\nThis discussion could (again) be unsettling. This list of problems could make you angry or sad. I, like others, think it is exciting. It is exciting because these problems have probably existed for a long time (e.g., Cohen, 1962; Meehl, 1967) but now, having identified the problems, we can hope to do something about it. It is exciting because if you care about people, the study of people, or the applications in clinical, education and other domains of the results of the study of people, then you might hope to see better, more useful, science in the future (Vazire, 2018).\nAs someone who teaches graduate and undergraduate students, I want to help you to be the change you want to see in the world 1. We cannot solve every problem but we can try to do better those things that are within our reach. I am going to end this introduction with a brief discussion of some ideas we can use to guide our better practices.\n\n\nThe specific context: what we need to look at, conceptually and practically\nIn this course, for this assignment, we are going to focus on:\n\nmultiverse analyses\nkinds of reproducibility\nthe current state of the match between open science ideas and practices\n\nIn the classes on the linear model, we will discuss:\n\nthe links between theory, prediction and analysis\npsychological measurement\nsamples\nvariation in results\n\n\n\nMultiverse analyses: multi- what?\n\nA first useful metaphor: the pipeline\nI am going to link this discussion to a metaphor (see Figure Figure 1) or a description you will find useful: the data analysis pipeline or workflow.\n\n\n\n\n\n\n\n\nQ\n\n\ncluster_R\n\n\n\n\nnd_1\n\nGet raw data\n\n\n\nnd_2\n\nTidy data\n\n\n\nnd_1-&gt;nd_2\n\n\n\n\n\nnd_3_l\n\nVisualize\n\n\n\nnd_2-&gt;nd_3_l\n\n\n\n\n\nnd_3\n\nanalyse\n\n\n\nnd_2-&gt;nd_3\n\n\n\n\n\nnd_3_r\n\nExplore\n\n\n\nnd_2-&gt;nd_3_r\n\n\n\n\n\nnd_3_a\n\nAssumptions\n\n\n\nnd_3_a-&gt;nd_3_l\n\n\n\n\n\nnd_3_a-&gt;nd_3\n\n\n\n\n\nnd_3_a-&gt;nd_3_r\n\n\n\n\n\nnd_3_l-&gt;nd_3\n\n\n\n\nnd_4\n\nPresent\n\n\n\nnd_3_l-&gt;nd_4\n\n\n\n\n\nnd_3-&gt;nd_4\n\n\n\n\n\n\n\n\nFigure 1: The data analysis pipeline or workflow\n\n\n\n\n\nThis metaphor or way of thinking is very common (take a look at the diagram in Wickham and Grolemund’s 2017 book “R for Data Science) and you may see the words “data pipeline” used in job descriptions, or you may benefit from saying, in a job application, something like: I am skilled in designing and implementing each stage of the quantitative data analysis pipeline, from data tidying to results presentation. I say this because scientists I have mentored got their jobs because they can do these things – and successfully explained that they can do these things – in sectors like educational testing, behavioural analysis, or public policy research.\nThe reason this metaphor is useful is that it helps us to organize our thinking, and to manage what we do when we do data analysis, we:\n\nget some data;\nprocess or tidy the data;\nexplore, visualize, and analyse the data;\npresent or report our findings.\n\nWe introduce the idea that your analysis work will flow through the stages of a pipeline from getting the data to presenting your findings because, next, we will examine how pipelines can multiply.\n\n\n\n\n\n\nTip\n\n\n\n\nAs you practice your data analysis work, try to identify the elements and the order of your work, as the parts of a workflow.\n\n\n\n\n\nA second useful metaphor: the garden of forking paths\nWhat researchers have come to realize: because we started looking … The open secret that has been well kept (Bourdieu, 2004): because everybody who does science knows about it, yet we may not teach it; and because we do not write textbooks revealing it … Is that at each stage in the analysis workflow, we can and do make choices where multiple alternative choices are possible. A. Gelman & Loken (2014a) capture this insight as the “garden of forking paths”2 (see Figure 2).\nThe general idea is that it is possible to have multiple potential different paths from the data to the results. The results will vary, depending on the path we take. In an analysis, we could take multiple different paths simply because at point A we decide to do B1, B2 or B3, maybe we choose B1, and then at point B1, we may decide to do C1, C2 or C3. Here, maybe we have our raw data at point A. Maybe we could do one of two different things when we tidy the data: action B1 or B2. Then, when we have our tidy data, maybe we can choose to do our analysis in one of six ways. Where we are at each step depends on the choices we made at the previous steps.\n\n\n\n\n\n\n\n\nD\n\n\n\nA\n\nA\n\n\n\nB1\n\nB1\n\n\n\nA-&gt;B1\n\n\n\n\n\nB2\n\nB2\n\n\n\nA-&gt;B2\n\n\n\n\n\nC1\n\nC1\n\n\n\nB1-&gt;C1\n\n\n\n\n\nC2\n\nC2\n\n\n\nB1-&gt;C2\n\n\n\n\n\nC3\n\nC3\n\n\n\nB1-&gt;C3\n\n\n\n\n\nC4\n\nC4\n\n\n\nB2-&gt;C4\n\n\n\n\n\nC5\n\nC5\n\n\n\nB2-&gt;C5\n\n\n\n\n\nC6\n\nC6\n\n\n\nB2-&gt;C6\n\n\n\n\n\n\n\n\nFigure 2: Forking paths in data analysis\n\n\n\n\n\nIn the end, it may appear to us that we took one path or that only one path was possible. When we report our analysis, in a dissertation or in a published journal article, we may report the analysis as if only one analysis path had been considered. But, critically, our findings may depend on the choices we made and this variation in results may be hidden from view.\nI am talking about forking paths because the multiplicity of paths has consequences, and we discuss these next.\n\n\n\n\n\n\nTip\n\n\n\n\nIt is about here, I hope, that you can start to see why it would makes sense to access data from a published study and to examine if you can get the same results as the study authors.\n\n\n\n\n\n\nMultiverse analyses\nI am going to discuss, now, what are commonly called multiverse analyses. Psychologists use this term, having been introduced to it in an influential paper by Steegen et al. (2016a), but it comes from theoretical physics (take a look at wikipedia).\nI explain this because I do not want you to worry. The ideas themselves are within your grasp whatever your background in psychology or elsewhere. It is the implications for our data analysis practices that are challenging. They are challenging because what we discuss should increase your skepticism about the results you encounter in published papers. And they are challenging because they reveal your freedom to question whether published authors could have done their analysis in a different way.\nWe are going to look at:\n\ndata-set construction\nanalysis choices\n\n\nThe link between the credibility revolution and the multiverse\nIn first discussing the wider context (of crisis and revolution), then discussing the specific context (of multiverses and, in the following, of reproducibility), I should be clear about the link between the two things. The finding that some results may not be supported by the evidence is probably due to a mix of causes. But one of those causes will be the combination of uncertainty over data processing or the uncertainty over analysis methods revealed in multiverse analyses, as we see next, combined with the limitations of data and code sharing, and the incompleteness of results reporting (as we see later).\n\n\nThe data multiverse\nWhen you collect or access data for a research study, the complete raw data-set you receive is almost never the complete data-set you analyse or whose analysis you report. This is not a story about deliberately cheating. It is a story about the normal practice of science (Kuhn, 1970).\nPicture some common scenarios. You did a survey, you got responses from a 100 participants on 10 questions, and you asked people to report their education, ethnicity and gender. You did an experimental study, you tested two groups of 50 people each in 100 trials (imagine a common task like the Stroop test), and you observed the accuracy and the timing of their responses. You tested 100 children, 20 children in each of five different schools, on a range of educational ability measures.\nIn these scenarios, the psychologist or the analyst of behavioural data must process their data. In doing so, you will ask yourself a series of questions like:\n\nhow do we code for gender, ethnicity, education?\nwhat do we about reaction times that are very short, e.g., \\(RT &lt; 200ms\\) or very long, e.g., \\(RT &gt; 1500ms\\))?\nif we present multiple questions measuring broadly the same thing (e.g. how confident are you that you understand what you have read? how easy did you find what you read?) how do we summarize the scores on those questions? do we combine scores?\nwhat do we do about people who may not appear to have understood the task instructions?\n\nTypically, the answers to these questions will be given to you by your supervisor, a colleague or a textbook example. For example, we might say:\n\n“We excluded all reaction times greater than 1500ms before analysis.”\n\nTypically, the explanation for these answers are rarely explained. We might say:\n\n“Consistent with common practice in this field, we excluded all reaction times greater than 1500ms before analysis.”\n\nBut the reader of a journal article typically will not see an explanation for why, as in the example, we exclude reaction times greater than 1500ms and not 2000ms or 3000ms, etc. We typically do not see an explanation for why we exclude all reaction times greater than 1500ms but other researchers exclude all reaction times greater than 2000ms. (I do not pick this example at random: there are serious concerns about the impact on analyses of exclusions like this (Ulrich & Miller, 1994).)\nWhat Steegen et al. (2016a) showed is that a data-set can be processed for analysis in multiple different ways, with a number of reasonable alternate choices that can be applied, for each choice point: construction choices about classifying people or about excluding participants given their responses. If a different data-set is constructed for each combination of alternatives then many different data-sets can be produced, all starting from the same raw data. (For their example study, Steegen et al. (2016a) found they could construct 120 or 210 different data-sets, based on the choice combinations.) Critically, for us, Steegen et al. (2016a) showed that if we apply the same analysis method to the different data-sets then our results will vary.\nLet me spell this out, bit by bit:\n\nwe approach our study with the same research question, and the same verbal prediction;\nwe begin with the exact same data;\nwe then construct different data-sets depending on different but equally reasonable processing choices;\nwe then apply the same analysis analysis, to test the same prediction, using each different data-set;\nwe will see different results for the analyses of the different data-sets.\n\nAlternate constructions of the same data may cause variation in the results of statistical tests. Some kinds of data processing choices may be more influential on results than others. It seems unlikely that we can identify, in advance, which choices matter more.\nSteegen et al. (2016a) suggest that we can deflate (shrink) the multiverse in different ways. I want to state their suggestions, here, because we will come back to these ideas in the classes on the linear model.\n\nDevelop better theories and improved measurement of the constructs of interest.\nDevelop more complete and precise theory for why some processing options are better than others.\n\nBut you will be asking yourself: What do I need to think about, for the research report assignment?\n\n\n\n\n\n\nTip\n\n\n\n\nWhen you read a psychological research report, identify where the researchers talk about how they process their data: classification, coding, exclusion, transformation, etc.\nIf you can access the raw data, ask yourself: could different choices change the results of the same analysis?\n\n\n\n\n\nAnalysis multiverses\nEven if we begin with the same research question and, critically, the same data-set, the results of a series of studies show that different researchers will often (reasonably) make different choices about the analysis they do to answer the research question. We often call these studies (analysis or model) multiverse studies. In these studies, we see variation in analysis and this variation is also associated with variation in results.\nAn influential example, in psychology, is reported by Silberzahn and colleagues (Silberzahn et al., 2017; Silberzahn & Uhlmann, 2015) who asked 29 teams of researchers to answer the same question (“Are (soccer) referees more likely to give red cards to players with dark skin than to players with light skin?”) with the same data-set (data about referee decisions in football league games). The teams made their own decisions about how to answer the question in doing the analysis. The teams shared their plans, and commented on each others’ ideas. The discussion did not lead to a consensus about what analysis approach is best. In the end, the different teams did different analyses and, critically, the different analyses had different results. The results varied in whether the test of the effect of players skin colour (on whether red cards were given) was significant or not, and on the strength of the estimated association between the darkness of skin colour (lighter to darker) and the chances (low to high) of getting a red card.\nThere have now been a series of multiverse or multi-analyst studies which demonstrate that, under certain conditions, different researchers may adopt different analysis approaches – which will have different results – in answering the same research question with the same data. This demonstration has been repeated in studies in health, medicine, psychology, neuoscience, and sociology, among other research fields (e.g., Parsons (n.d.); Breznau et al. (2022); Klau et al. (n.d.); Klau et al. (2021); Wessel et al. (2020); Poline et al. (2006); Maier-Hein et al. (2017); Starns et al. (2019); Fillard et al. (2011); Dutilh et al. (2019); Salganik et al. (2020); Bastiaansen et al. (2020); Botvinik-Nezer et al. (2020); Schweinsberg et al. (2021); Patel et al. (2015); see, for reviews, and some helpful guidance, Aczel et al. (2021); Del Giudice & Gangestad (2021); Hoffmann et al. (n.d.); Wagenmakers et al. (2022)).\nIn these studies, we typically see variation in how psychological constructs are operationalized (e.g., how do we measure or code for social status?), how data are processed or data-sets constructed (as in Steegen et al. (2016b)), plus variation in what statistical techniques are used, and in how those techniques are used. This variation can be understood to reflect kinds of uncertainty (Klau et al., n.d.; Klau et al., 2021): uncertainty about how to process data, and uncertainty about the model or methods we should use to test or estimate effects. Further research makes it clear that we should be aware, if we are not already, of the variation in results that can be expected because different researchers may choose to design studies, and construct stimulus materials, in different ways given the same research hypothesis information (Landy et al., 2020).\nBut you will be asking yourself: What do I need to think about, for the research report assignment?\n\n\n\n\n\n\nTip\n\n\n\n\nWhen you read a psychological research report, identify where the researchers talk about how they analyse their data: the hypothesis or prediction they test; the method; their assumptions; the variables they include; the checks or the alternate analyses they did or did not do.\nIf you can access the data and analysis code, ask yourself: could different methods change the results of the same analysis?\n\n\n\n\n\nWhat can we conclude – the story so far?\nThis is a good place to look at what we have discussed, and present an evaluation of the story so far.\nThis is not a story where everybody or nobody is right or where everything or nothing is true 3. Instead, we can be guided by the advice (Meehl, 1967; Scheel, 2022; Steegen et al., 2016a) that we should:\n\nseek better and more complete theorizing about the constructs of interest and how we measure them, and\nseek more complete and more precise theory so that some options are theoretically superior than others, and should be preferred, when constructing data-sets or specifying analysis methods.\n\nNot all research questions and not all hypothesis information will allow an equally wide variety of potential reasonable approaches to the analysis. As Paul Meehl argued a long time ago (Meehl, 1967, 1978), and researchers like Anne Scheel (Scheel et al., 2021; Scheel, 2022) argued more recently, the complexity of the thing we study – people, and what they do – and the still early development of our understanding of this thing, mean that what we want but what we do not see, in psychology, are scientifically productive tests of falsifiable theories. (See, consistent with this perspective, discussions by Auspurg & Brüderl (2021) and by Del Giudice & Gangestad (2021) about the range of analysis possibilities that may or may not be allowed, in multiverse analyses, by more or less clear research questions or well-developed causal theories.) Our concern should not so much be with being able to do statistical analysis, or with finding significant or not significant results. It would be more useful to do analyses to test concrete, inflexible, precise predictions that can be wrong.\nNor is this a story, I think, about the potential for cheating. While we may refer to subjective choices or to researcher flexibility, the differences that we see do not resemble the researcher degrees of freedom (Simmons et al., 2011b) some may exploit, consciously or unconsciously, to change results to suit their aims. Instead, the multiverse results show us the impact of the reasonable differences in approach that different researchers may sensibly choose to take when they try to answer a research question with data.\nNot all alternates, at a given point of choosing, in the data analysis workflow, will have equal impact. Work by Young (Young, 2018; Young & Holsteen, 2017) indicates that if we deliberately examine the impact of method or model uncertainty, over different sets of possible choices — about what variables or what observations we include in an analysis, for example — we may find that some results are robust to an array of different options, while other results are highly susceptible to different choices. This work suggests another way in which uncertainty about methods or variation in results can be turned into progress in understanding the phenomena that interest us: through systematic, informed, interrogation of the ways that results can vary.\nIn general, in science, the acceptance of research findings must always be negotiated (Bourdieu, 2004). Here, we see that the grounds of negotiation should often include an analysis of the impact on the value of evidence of the different analysis approaches that researchers can or do apply to the data that underlie that evidence.\nBut you will be asking yourself: what do I need to think about, for the assignment?\n\n\n\n\n\n\nTip\n\n\n\n\nThe results of multiverse analyses show us that if we see one analysis reported in a paper, or one workflow, that does not mean that only one analysis can reasonably be applied.\nIf you read the methods or results section of a paper, you should reflect: what other analysis methods could be used here? How could variation in analysis method — in what or how you do the analysis — influence the results?\n\n\n\nMaking you aware of the potential for analysis choices is useful because developing researchers, including graduate students, are often not aware of the room for choice in the data analysis workflow. Developing researchers — you — may be instructed that “this is how we do things” or “you should follow what researchers did previously”. Following convention is not necessarily a bad thing: it is a feature of the normal practice of science (Kuhn, 1970). However, you can now see, perhaps, that there likely will be alternative ways to process or to analyse data than the approach a supervisor, lab or field normally adopts.\nThis understanding or awareness has three implications for practice, it means:\n\nWhen we talk about the analysis we do, we should explain our choices.\nWe should check, or enable others to check, what impact making different choices would have on our results.\nMost importantly: we can allow ourselves the freedom to critically evaluate the choices researchers make, even the choices researchers make in published articles.\n\n\n\n\nFrom the multiverse to kinds of reproducibility\nMultiverse analyses and post-publication analyses, in general, show that we can and should question or critically evaluate the analyses we encounter in the literature. This work can usefully detect problems in original published analyses (e.g., A. Gelman & Weakliem, 2009; Herndon et al., 2014; Wagenmakers et al., 2011). It can demonstrate where original published claims are or are not robust to variation of analysis method or approach.\nGiven these lessons, and the implications we have identified, we should expect or hope to see open science practices (Munafò et al., 2017; Nosek et al., 2022):\n\nshare data and code;\npublish research reports in ways that enable others to check or query analyses.\n\nAs we discuss, following, these practices are now common but the quality of practice can sometimes be questioned. This matters for you because it makes it more challenging – in specific identifiable locations – to locate, access, analyse and report previously collected data.\nThe discussion of current practices identifies where or how the assignment may be more challenging, but also identifies some of the exact places where the assignment provides a real opportunity to do original research work.\nFirst, I am going to introduce some ideas that will help you to think about what you are doing when you do this work. We focus on the concept of reproducibility.\nGilmore et al. (2017; following Goodman et al., 2016) present three kinds of reproducibility:\n\nmethods reproducibility\nresults reproducibility\ninferential reproducibility\n\nIn looking at reproducibility, here, we are considering how much, or in what ways, the results or the claims that are made in a published study can be found or repeated by someone else.\n\nMethods reproducibility\nAs Gilmore et al. (2017) discuss, methods reproducibility means that another researcher should be able to get the same results if they use the same tools and analysis methods to analyse the same data-set [some researchers also refer to analytic reproducibility or computational reproducibility; see, e.g., Crüwell et al. (n.d.); Hardwicke et al. (2018); Hardwicke et al. (n.d.); Laurinavichyute et al. (2022); Minocher et al. (n.d.)].\nIn neuroimaging, the multiplicity of possible implementations of the data analysis pipeline (Carp, 2012a), and the fact that important elements or information about the pipeline deployed by researchers may be missing from published reports (Carp, 2012b), can make it challenging to identify how results can be reproduced.\nIn psychological science, in evaluating reports of results from analyses of behavioural data collected through survey or experimental work, in principle, we should expect to be able to access the data collected by the study authors, follow the description of their analysis method, and reproduce the results they report.\n\n\n\n\n\n\nTip\n\n\n\n\nFor an assignment in which we ask students to locate, access, analyse and report previously collected data, we are directly concerned with methods reproducibility.\n\n\n\n\n\nResults reproducibility\nResults reproducibility means that if another researcher completes a new study with new data they are able to get the same results as the results reported following an original study: this often referred to as replication. The replication studies that have been reported (e.g., Aarts et al., 2015), and continue to be reported (see, for example, the studies discussed by Nosek et al. (2022)), in the last several years, present attempts to examine the results reproducibility of published findings.\nIn the classes on the linear model, we will examine if similar or different results are observed in a series of studies using the same procedure and the same materials. We shall discuss, in those classes, in more depth, what results reproducibility (or study replication) can or cannot tell us about the behaviours that interest us.\n\n\nInferential reproducibility\nInferential reproducibility means that if a researcher repeats a study (aiming for results reproducibility) or re-analyses an original data-set (aiming for methods reproducibility) then they can come to the same or similar conclusions as the authors of the report of an original study.\nHow is inferential reproducibility not methods or results reproducibility? Goodman et al. (2016) explain that researchers can make the same conclusions from different sets of results and can reach different conclusions from the same set of results.\nHow is it possible to reach different conclusions from the same results? We can imagine two scenarios.\nFirst, we have to think about the wider research field, the research context, within which we consider a set of results. It may be that two different researchers will come to look at the same results with different expectations about what the results could tell us (in Bayesian terms, with different prior expectations). Given different expectations, it is easy to imagine different researchers looking at the same results and, for example, one researcher being more skeptical than another about what conclusion can be taken from those results. (In the class on graduate writing skills, I discuss in some depth the importance of reviewing a research literature in order to get an understanding of the assumptions, conventions or expectations that may be shared by the researchers working in the field.)\nSecond, imagine two different researchers looking at the same results — picture the original authors of a published study, and someone doing a post-publication re-analysis of their data — you can expect that the re-analysis or the reproducibility analysis could identify reasons to value the evidence differently, or to reach more skeptical conclusions, through critical evaluation of:\n\ndata processing choices;\nthe choice of the method used to do analysis;\nchoices in how the analysis method is used.\n\n… where that critical evaluation involves an analysis of the choices the original researchers made, perhaps involving an analysis of other choices they could have made, perhaps reflecting on how effectively the analyses address a given research question or test a given prediction.\n\n\n\n\n\n\nTip\n\n\n\n\nWe can think about the work we do, when we analyse previously reported data, in terms of the need to identify the reproducibility of results, methods and inferences.\nIn psychological science, determining that someone can get the same results, by analysing the same data, or will reach the same conclusions from the same results, are important – potentially, original – research contributions.\n\n\n\n\n\n\nThe current state of the match between open science ideas and practices\nI have said that we should expect or hope to see open science practices (Munafò et al., 2017; Nosek et al., 2022) where researchers:\n\nshare data and code;\npublish research reports in ways that enable others to check or query analyses.\n\nThis raises an important question: What exactly do we see, when we look at current practices? The question is important because answering it helps to identify where the challenges are located when you complete your work to locate, access, analyse and report previously collected data.\nI break the discussion of what we see into two parts. Firstly, I look at the results of audits of data and code sharing: are data shared and can we access the data? Secondly, I discuss analyses of methods reproducibility, and shared data and code usability: can others reproduce the results reported in published articles, given shared data? can others access and run shared analysis code? can others use the shared code to reproduce the reported results? Again, I need to be brief so I reference sources that you can follow-up.\n\nThe link between the credibility revolution and the reproducibility of results\nI should be clear, before we go on, about the link between the credibility revolution in science, and the effort to examine reproducibility of results. Many elements of the credibility revolution emerged out of the observation that it has often been difficult to repeat the results of published studies when we conduct new studies (replication studies or results reproducibility; e.g., Aarts et al. (2015)). However, it is clearly difficult to know what to replicate or reproduce if we cannot reproduce the results presented in a study report (methods reproducibility), given the study data (Artner et al., 2021; Laurinavichyute et al., 2022; Minocher et al., n.d.).\n\n\nData and code sharing\nResearch on data and code sharing practices suggest that practices have improved, from earlier low levels.\nIn an important early report, Wicherts et al. (2006) observed that it was very difficult to obtain data reported in psychological research articles from the authors of the articles. They asked for data from the lead authors of 141 articles published in four leading psychology journals, for about 25% of the studies. This low response rate was found despite the fact that authors in these journals must agree to the principle that data can be shared with others wishing to verify claims.\nPractice has changed: how?\nOne change to practice has involved the use of open science badges. In journals like Psychological Science authors of articles may be awarded badges — Open Data, Open Materials, Preregistration badges — by the editorial team. Authors can apply for and earn the badges by providing information about open practices, and journal articles are published with the badges displayed near the front of the articles.\nIn theory, initiatives like encouraging authors to earn open science badges should mean that data sharing practices improve, enabling access to data and code for those, like you, who would like to re-analyse previously published data. In theory, all you should need to do — to locate and access data — is just search articles in the journal Psychological Science for studies with open data badges, and follow links from the published articles to then access study data at an open repository like the Open Science Framework (OSF) What do we see in practice?\nAnalyses reported by Kidwell et al. (2016) as well as analyses reviewed by Nosek et al. (2022) indicate that more articles have claimed to make data available in the time since badges were introduced. When they did their analysis, Kidwell et al. (2016) found that a substantial proportion, but not all, of the articles in Psychological Science can be found to actually provide access to shared data. However, critically, many but not all the articles with open data badges provide access to data available through an open repository, data that are correct, complete and usable (Kidwell et al., 2016). In their later report, the analyses reviewed by Nosek et al. (2022) suggest that the use of repositories like OSF for data sharing may be accelerating but that, over the last few years, the rate at which open science practices like sharing data, overall, appears to be substantial but not yet reported or observed in a majority of the work of researchers.\nMany journals now require the authors of articles to include a Data Availability Statement to locate their data. Analyses by Federer (2022) indicate that Data Availability Statements for articles published in the open access 4 journal PLOS ONE often, helpfully, include Digital Object Identifiers (DOIs) or Universal resource locators (URLs) enabling direct access to shared data (i.e., without having to contact authors). Of those DOIs or URLs, most appeared to be associated with resources that could successfully be retrieved. In contrast, analyses reported by Gabelica et al. (2022) indicate that where article authors state that “data sets are available on reasonable request” (the most common availability statement), most of the time, the authors did not respond or declined to share the data (see similar findings, across fields, by Tedersoo et al., 2021). Clearly, in the analyses of open science practices we have seen so far, data sharing is more effective where sharing does not have to work through authors.\n\n\n\n\n\n\nTip\n\n\n\n\nWhen you are looking for a study in order to get data that you can then re-analyse, it makes sense to look, first, for studies focusing on research questions that interest you.\nWhen you are looking for published reports where the authors share data, look for articles with open science badges or where you can see a Data Availability Statement.\nChoose articles where the authors provide a direct link to their data, where the data are located on an open repository like the Open Science Framework (there are other repositories).\n\n\n\n\n\nEnabling others to check or query analyses\nResearch on data and code sharing practices suggest that practices have improved but that there are concerns about the quality of the sharing. Here, the critical concern relates to the word enable in the objective: that we should publish research reports in ways that enable others to check or query analyses.\nJohn Towse and colleagues (Towse et al., 2021) at Lancaster University examined the quality of open data-sets to assess their quality in terms of their completeness and reusability (see also Roche et al., 2015).\n\ncompleteness: are all the data and the data descriptors supporting a study’s findings publicly available?\nreusability: how readily can the data be accessed and understood by others?\n\nFor a sample of data-sets, they found that about half were incomplete, and about two-thirds were shared in a way that made them difficult to use. Practices tended to be slightly better in more recent publications. (Broadly similar results are reported by (Hardwicke et al., 2018).)\nWhere data were found to be incomplete, this appeared to be, in part, because participants were excluded in the processing of the data for analysis but this information was not in the report, or because data were shared without a guide or “readme” file or data dictionary (or codebook) explaining the structure, coding or composition of the shared data.\nPotentially important for future open science practices, Towse et al. (2021; see, also, Roche et al., 2015) found that sharing data as Supplementary materials may appear to carry risks that, in the long term, mean that data may become inaccessible.\n\n\n\n\n\n\nTip\n\n\n\n\nWhen you locate open data you can access, look for a guide, “readme” file, codebook or data dictionary explaining the data: you need to be able to understand what the variables are, what the observations relate to (observations per person, per trial?) and how variables are coded.\nLocate and examine carefully the parts of the published report, or the data guide, where the authors explain how they processed their data.\n\n\n\nA number of studies have been conducted to examine whether shared data and analysis code can be reused by others to reproduce the results reported in papers (e.g., Artner et al., 2021; Crüwell et al., n.d.; Hardwicke et al., n.d.; Hardwicke et al., 2018; Laurinavichyute et al., 2022; Minocher et al., n.d.; Obels et al., 2020; see Artner et al., 2021 for a review of reproducibility studies). In critical respects, the researchers doing this work are doing work similar to the work we are helping students to do, locating, accessing, and analysing previously collected data. In these studies, typically, the researchers progressed through a series of steps.\n\nSearched the articles published in a journal (e.g., Cognition, the Journal of Memory and Language, Psychological Science), published in a topic area across multiple journals (e.g., social learning, psychological research), or associated with a specific practice (e.g., registered reports.\nSelected a subset of articles where it was identified that data could be accessed.\nIdentify a target result or outcome to reproduce, for each article. In their analyses, Hardwicke and colleagues (Hardwicke et al., n.d.; Hardwicke et al., 2018) focused on attempting to reproduce primary or straightforward and substantive outcomes: substantive – if emphasized in the abstract, or presented in a table or figure; straightforward – if the outcome could be calculated using the kind of test one would learn in an introductory psychology course (e.g., t-test, correlation).\nAttempted to reproduce the results reported in the article, using the description of the data analysis presented in the article, and the analysis code (if provided), in some cases asking for information from the original study authors, in other cases working independently of original authors.\n\nWhat the reproducibility studies appear to show is that, for many published reports, if data are shared and if the shared data are accessible and reusable then, most of the time, the researchers could reproduce the results presented by the original study authors (Hardwicke et al., n.d.; Hardwicke et al., 2018; Laurinavichyute et al., 2022; Minocher et al., n.d.; Obels et al., 2020; but see Crüwell et al., n.d.). This is great. But what is interesting, for us, is where the reproducibility researchers encountered challenges. You may encounter the same or similar challenges.\nI list some challenges that the researchers describe, following. Before you look at the list, I want to assure you: you will not find all these challenges present for any one article you look at. Most likely, you will find one or two challenges. Obviously, some challenges will be more difficult than others.\n\n\n\n\n\n\nTip\n\n\n\n\nWhen you find a study you are interested in, with open data and maybe open analysis code, your main challenge will often be to identify exactly what analysis the original study authors did to answer their research question.\nLocate and examine carefully the parts of the published report where the authors explain how they did the analysis that gave them their key result. Usually that key result should be identified in the abstract or in the conclusion.\n\n\n\n\nData challenges\n\nData Availability Statements or open science badges indicate data are shared but data are not directly accessible through a link to an open repository.\nThe data are shared and accessible but there is missing or incorrect information about the data. The documentation, codebook or data dictionary is missing or incomplete. There is unclear or missing information about the variables or the observations, or about the coding of variable values, responses.\nOriginal study authors may share raw and processed data or just processed or just raw data. It may not be clear how raw data were processed to construct the data analysed for the report. It may not be clear how variables were transformed or calculated or processed.\nThere may be mismatches between the variables referred to in the report and the variables named in the data file. It may be unclear how a data file corresponds to a study described in a report, where there are multiple studies and multiple data files.\n\n\n\nAnalysis challenges\n\nThe original report includes a description of the analysis but the description of the analysis procedure is incomplete or ambiguous.\nThere may be a mismatch, in the report, between a hypothesis, and the analysis specified to test the hypothesis (maybe in the Methods section), compared to a long sequence of results reported in the Results section. This makes it difficult to identify the key analysis.\nIt is easier to reproduce results if both data and code are shared because the presentation of the analysis code usually (not always) makes clear what analysis was done to get the results presented in the report.\nSometimes, analysis code is shared but it is difficult to use because it requires proprietary software (e.g., SPSS) or because it requires function libraries that are no longer publicly available.\nSometimes, there are errors in the analysis. Sometimes, there are errors in the presentation of the results, where results have been incorrectly copied into reports from analysis outputs.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 6. Why we are asking you to do this"
    ]
  },
  {
    "objectID": "PSYC411/part2/intro.html#this-is-why",
    "href": "PSYC411/part2/intro.html#this-is-why",
    "title": "Week 6. Why we are asking you to do this",
    "section": "This is why",
    "text": "This is why\nThe research report assignment requires students to locate, access, analyse and report previously collected data. At the start of the introduction, I said I would explain the answer to the question:\n\nWhy: what is the motivation for the assignment?\n\nI summarize, following, the main points of the answer I have given. When you review these points, I want you to think about two things, returning to the ideas of Bourdieu (2004) and Kuhn (1970) I sketched at the start.\nOften what we do in science is guided by convention, the assumptions and habits of normal practice (Kuhn, 1970). These conventions can work in our minds so that if we encounter an anomaly or discrepancy between what we expect and what we find, in our work, we may usually blame ourselves: it was something wrong that we did or failed to do. It can cause us anxiety if we do not reproduce a result we think we should be able to reproduce (Lubega et al., n.d.). But I want you to understand, from the start, that sometimes, if you think you have found an error or a problem in a published analysis or a shared data-set, you may be right.\nIf there is anything we have learned, through the findings of replication studies, multiverse analyses, and reproducibility audits it is that people make mistakes, different choices are often reasonable, and we always need to check the evidence.\n\nSummary: this is why\n\nWe are in the middle of a credibility revolution. The lessons we have learned so far oblige us to think about and to teach good open science practices that safeguard the value of evidence in psychology.\nThis matters, even if we do not care about scientific methods, because if we care about the translation into policy or practice – in clinical psychology, in education, health, marketing and other fields – what we do will depend on the value of the research evidence that informs policy ideas or practice guides.\nFocusing on data analysis, it is useful to think about the whole data pipeline in analysis, the workflow that takes us from data collection to raw data to data processing to analysis to the presentation of results.\nAt every stage of the data pipeline, there are choices about what to do. There are not always reasons why we make one choice instead of another. Sometimes, we are guided by convention, example or instruction.\nThe existence of choices means the path we take, when we do data analysis, can be one path among multiple different forking paths.\nFor some parts of the pipeline – data-set construction, data analysis choices – reasonable people might make different decisions to sensibly answer the same research question, given the same data. This variation between pathways can be more or less important in influencing the results we see.\nIf results tend to stay similar across different ways of doing analysis, we might conclude that the results are reasonably robust across contexts, choices, or other variation in methods.\nTo enable others to see what we did (versus what we could have done), to see how we got to our results from our data, it is important to share our data and code.\nEveryone makes mistakes and we should make it easy for others, and ourselves, to find those mistakes by sharing our data and code in accessible, clear, usable ways.\nWe need to teach and learn how to share effectively the data and the code that we used to answer our research questions.\n\nIn constructing the assignment – in asking and supporting students to locate, access, analyse and report previously collected data – we are presenting an opportunity to really investigate and evaluate existing practices.\nYou may find that this work is challenging, in some of the places that reproducibility research has identified there can be challenges. Where the challenges cannot be fixed – if you have found an interesting study but the study data are inaccessible or unusable – we will advise you to move on to another study. Where the challenges can be fixed – if data require processing, or if analysis information requires clarification – we will provide you with help or enabling information so that you fix the problems yourself.\n\n\n\n\n\n\nTip\n\n\n\n\nMaybe the main lesson from this exercise is a reminder of the Golden rule: treat others as you would like to be treated.\nIf it is frustrating when it is difficult to understand information about an analysis or about data, or when it is difficult to access and reuse shared data and code.\nWhen it is your turn — do better — reflecting on what frustrated you.\n\n\n\nOne last question: why not just do less demanding or challenging tasks? Because this is part of what makes graduate degree valuable, what will make you more skilled in the workplace. Most of the time, we work in teams, we inherit problems or data analysis tasks, or are given results with partial information. The lessons you learn here will help you to effectively navigate those situations.\n\n\nReferences\n\n\nAarts, E., Dolan, C. V., Verhage, M., & Van der Sluis, S. (2015). Multilevel analysis quantifies variation in the experimental effect while optimizing power and preventing false positives. BMC Neuroscience, 16(1), 1–15. https://doi.org/10.1186/s12868-015-0228-5\n\n\nAczel, B., Szaszi, B., Nilsonne, G., Akker, O. R. van den, Albers, C. J., Assen, M. A. van, Bastiaansen, J. A., Benjamin, D., Boehm, U., Botvinik-Nezer, R., Bringmann, L. F., Busch, N. A., Caruyer, E., Cataldo, A. M., Cowan, N., Delios, A., Dongen, N. N. van, Donkin, C., Doorn, J. B. van, … Wagenmakers, E.-J. (2021). Consensus-based guidance for conducting and reporting multi-analyst studies. eLife, 10, e72185. https://doi.org/10.7554/eLife.72185\n\n\nArtner, R., Verliefde, T., Steegen, S., Gomes, S., Traets, F., Tuerlinckx, F., & Vanpaemel, W. (2021). The reproducibility of statistical results in psychological research: An investigation using unpublished raw data. Psychological Methods, 26(5), 527–546. https://doi.org/10.1037/met0000365\n\n\nAuspurg, K., & Brüderl, J. (2021). Has the Credibility of the Social Sciences Been Credibly Destroyed? Reanalyzing the “Many Analysts, One Data Set” Project. Socius, 7, 23780231211024421. https://doi.org/10.1177/23780231211024421\n\n\nBastiaansen, J. A., Kunkels, Y. K., Blaauw, F. J., Boker, S. M., Ceulemans, E., Chen, M., Chow, S.-M., Jonge, P. de, Emerencia, A. C., Epskamp, S., Fisher, A. J., Hamaker, E. L., Kuppens, P., Lutz, W., Meyer, M. J., Moulder, R., Oravecz, Z., Riese, H., Rubel, J., … Bringmann, L. F. (2020). Time to get personal? The impact of researchers choices on the selection of treatment targets using the experience sampling methodology. Journal of Psychosomatic Research, 137, 110211. https://doi.org/10.1016/j.jpsychores.2020.110211\n\n\nBornstein, M. H., Jager, J., & Putnick, D. L. (2013). Sampling in developmental science: Situations, shortcomings, solutions, and standards. Developmental Review, 33(4), 357–370. https://doi.org/10.1016/j.dr.2013.08.003\n\n\nBotvinik-Nezer, R., Holzmeister, F., Camerer, C. F., Dreber, A., Huber, J., Johannesson, M., Kirchler, M., Iwanir, R., Mumford, J. A., Adcock, R. A., Avesani, P., Baczkowski, B. M., Bajracharya, A., Bakst, L., Ball, S., Barilari, M., Bault, N., Beaton, D., Beitner, J., … Schonberg, T. (2020). Variability in the analysis of a single neuroimaging dataset by many teams. Nature, 582(7810), 84–88. https://doi.org/10.1038/s41586-020-2314-9\n\n\nBourdieu, P. (2004). Science of Science and Reflexivity. Polity.\n\n\nBreznau, N., Rinke, E. M., Wuttke, A., Nguyen, H. H. V., Adem, M., Adriaans, J., Alvarez-Benjumea, A., Andersen, H. K., Auer, D., Azevedo, F., Bahnsen, O., Balzer, D., Bauer, G., Bauer, P. C., Baumann, M., Baute, S., Benoit, V., Bernauer, J., Berning, C., … Żółtak, T. (2022). Observing many researchers using the same data and hypothesis reveals a hidden universe of uncertainty. Proceedings of the National Academy of Sciences, 119(44), e2203150119. https://doi.org/10.1073/pnas.2203150119\n\n\nButton, K. S., Ioannidis, J. P. A., Mokrysz, C., Nosek, B. A., Flint, J., Robinson, E. S. J., & Munafò, M. R. (2013). Power failure: Why small sample size undermines the reliability of neuroscience. Nature Reviews Neuroscience, 14(5), 365–376.\n\n\nCarp, J. (2012a). On the plurality of (methodological) worlds: Estimating the analytic flexibility of FMRI experiments. Frontiers in Neuroscience, 6, 149.\n\n\nCarp, J. (2012b). The secret lives of experiments: Methods reporting in the fMRI literature. Neuroimage, 63(1), 289–300.\n\n\nCohen, J. (1962). The statistical power of abnormal-social psychological research: A review. Journal of Abnormal and Social Psychology, 65(3), 145–153. https://doi.org/10.1037/h0045186\n\n\nCrüwell, S., Apthorp, D., Baker, B. J., Colling, L., Elson, M., Geiger, S. J., Lobentanzer, S., Monéger, J., Patterson, A., Schwarzkopf, D. S., Zaneva, M., & Brown, N. J. L. (n.d.). What’s in a badge? A computational reproducibility investigation of the open data badge policy in one issue of psychological science. https://doi.org/10.31234/osf.io/729qt\n\n\nDel Giudice, M., & Gangestad, S. W. (2021). A Traveler’s Guide to the Multiverse: Promises, Pitfalls, and a Framework for the Evaluation of Analytic Decisions. Advances in Methods and Practices in Psychological Science, 4(1), 2515245920954925. https://doi.org/10.1177/2515245920954925\n\n\nDutilh, G., Annis, J., Brown, S. D., Cassey, P., Evans, N. J., Grasman, R. P. P. P., Hawkins, G. E., Heathcote, A., Holmes, W. R., Krypotos, A.-M., Kupitz, C. N., Leite, F. P., Lerche, V., Lin, Y.-S., Logan, G. D., Palmeri, T. J., Starns, J. J., Trueblood, J. S., Maanen, L. van, … Donkin, C. (2019). The Quality of Response Time Data Inference: A Blinded, Collaborative Assessment of the Validity of Cognitive Models. Psychonomic Bulletin & Review, 26(4), 1051–1069. https://doi.org/10.3758/s13423-017-1417-2\n\n\nFederer, L. M. (2022). Long-term availability of data associated with articles in PLOS ONE. PLOS ONE, 17(8), e0272845. https://doi.org/10.1371/journal.pone.0272845\n\n\nFillard, P., Descoteaux, M., Goh, A., Gouttard, S., Jeurissen, B., Malcolm, J., Ramirez-Manzanares, A., Reisert, M., Sakaie, K., Tensaouti, F., Yo, T., Mangin, J.-F., & Poupon, C. (2011). Quantitative evaluation of 10 tractography algorithms on a realistic diffusion MR phantom. NeuroImage, 56(1), 220–234. https://doi.org/10.1016/j.neuroimage.2011.01.032\n\n\nFlake, J. K., & Fried, E. I. (2020). Measurement Schmeasurement: Questionable Measurement Practices and How to Avoid Them. Advances in Methods and Practices in Psychological Science, 3(4), 456–465. https://doi.org/10.1177/2515245920952393\n\n\nGabelica, M., Bojčić, R., & Puljak, L. (2022). Many researchers were not compliant with their published data sharing statement: a mixed-methods study. Journal of Clinical Epidemiology, 150, 33–41. https://doi.org/10.1016/j.jclinepi.2022.05.019\n\n\nGelman, a. (2015). The connection between varying treatment effects and the crisis of unreplicable research: A bayesian perspective. Journal of Management, 41(2), 632–643. https://doi.org/10.1177/0149206314525208\n\n\nGelman, A., & Loken, E. (2014a). The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time. Psychological Bulletin, 140(5), 1272–1280.\n\n\nGelman, A., & Loken, E. (2014b). The statistical crisis in science. American Scientist, 102(6), 460–465. https://doi.org/10.1511/2014.111.460\n\n\nGelman, A., & Weakliem, D. (2009). Of beauty, sex and power. American Scientist, 97(4), 310–316. https://doi.org/10.1511/2009.79.310\n\n\nGilmore, R. O., Diaz, M. T., Wyble, B. A., & Yarkoni, T. (2017). Progress toward openness, transparency, and reproducibility in cognitive neuroscience. Annals of the New York Academy of Sciences, 1396, 5–18. https://doi.org/10.1111/nyas.13325\n\n\nGoodman, S. N., Fanelli, D., & Ioannidis, J. P. A. (2016). What does research reproducibility mean? Science Translational Medicine, 8(341).\n\n\nHardwicke, T. E., Bohn, M., MacDonald, K., Hembacher, E., Nuijten, M. B., Peloquin, B. N., deMayo, B. E., Long, B., Yoon, E. J., & Frank, M. C. (n.d.). Analytic reproducibility in articles receiving open data badges at the journal psychological science: An observational study. Royal Society Open Science, 8(1), 201494. https://doi.org/10.1098/rsos.201494\n\n\nHardwicke, T. E., Mathur, M. B., MacDonald, K., Nilsonne, G., Banks, G. C., Kidwell, M. C., Hofelich Mohr, A., Clayton, E., Yoon, E. J., Henry Tessler, M., Lenne, R. L., Altman, S., Long, B., & Frank, M. C. (2018). Data availability, reusability, and analytic reproducibility: evaluating the impact of a mandatory open data policy at the journal Cognition. Royal Society Open Science, 5(8), 180448. https://doi.org/10.1098/rsos.180448\n\n\nHenrich, J., Heine, S. J., & Norenzayan, A. (2010). The weirdest people in the world? The Behavioral and Brain Sciences, 33(2-3). https://doi.org/10.1017/S0140525X0999152X\n\n\nHerndon, T., Ash, M., & Pollin, R. (2014). Does high public debt consistently stifle economic growth? A critique of Reinhart and Rogoff. Cambridge Journal of Economics, 38(2), 257–279. https://doi.org/10.1093/cje/bet075\n\n\nHoffmann, S., Schönbrodt, F., Elsas, R., Wilson, R., Strasser, U., & Boulesteix, A.-L. (n.d.). The multiplicity of analysis strategies jeopardizes replicability: Lessons learned across disciplines. Royal Society Open Science, 8(4), 201925. https://doi.org/10.1098/rsos.201925\n\n\nIoannidis, J. P. a. (2005). Why most published research findings are false. PLoS Medicine, 2(8), 0696–0701. https://doi.org/10.1371/journal.pmed.0020124\n\n\nJohn, L. K., Loewenstein, G., & Prelec, D. (2012). Measuring the prevalence of questionable research practices with incentives for truth telling. Psychological Science, 23(5), 524–532. https://doi.org/10.1177/0956797611430953\n\n\nKidwell, M. C., Lazarević, L. B., Baranski, E., Hardwicke, T. E., Piechowski, S., Falkenberg, L. S., Kennett, C., Slowik, A., Sonnleitner, C., Hess-Holden, C., Errington, T. M., Fiedler, S., & Nosek, B. A. (2016). Badges to acknowledge open practices: A simple, low-cost, effective method for increasing transparency. PLoS Biology, 14(5), 1–15. https://doi.org/10.1371/journal.pbio.1002456\n\n\nKlau, S., Hoffmann, S., Patel, C. J., Ioannidis, J. P., & Boulesteix, A.-L. (2021). Examining the robustness of observational associations to model, measurement and sampling uncertainty with the vibration of effects framework. International Journal of Epidemiology, 50(1), 266–278. https://doi.org/10.1093/ije/dyaa164\n\n\nKlau, S., Schönbrodt, F., Patel, C. J., Ioannidis, J., Boulesteix, A.-L., & Hoffmann, S. (n.d.). Comparing the vibration of effects due to model, data pre-processing and sampling uncertainty on a large data set in personality psychology. https://doi.org/10.31234/osf.io/c7v8b\n\n\nKuhn, T. S. (1970). The structure of scientific revolutions ([2d ed., enl). University of Chicago Press.\n\n\nLandy, J. F., Jia, M. L., Ding, I. L., Viganola, D., Tierney, W., Dreber, A., Johannesson, M., Pfeiffer, T., Ebersole, C. R., Gronau, Q. F., Ly, A., Bergh, D. van den, Marsman, M., Derks, K., Wagenmakers, E.-J., Proctor, A., Bartels, D. M., Bauman, C. W., Brady, W. J., … Uhlmann, E. L. (2020). Crowdsourcing hypothesis tests: Making transparent how design choices shape research results. Psychological Bulletin, 146(5), 451–479. https://doi.org/10.1037/bul0000220\n\n\nLaurinavichyute, A., Yadav, H., & Vasishth, S. (2022). Share the code, not just the data: A case study of the reproducibility of articles published in the Journal of Memory and Language under the open data policy. Journal of Memory and Language, 125, 104332. https://doi.org/10.1016/j.jml.2022.104332\n\n\nLubega, N., Anderson, A., & Nelson, N. (n.d.). Experience of irreproducibility as a risk factor for poor mental health in biomedical science doctoral students: A survey and interview-based study. https://doi.org/10.31222/osf.io/h37kw\n\n\nMaier-Hein, K. H., Neher, P. F., Houde, J.-C., Côté, M.-A., Garyfallidis, E., Zhong, J., Chamberland, M., Yeh, F.-C., Lin, Y.-C., Ji, Q., Reddick, W. E., Glass, J. O., Chen, D. Q., Feng, Y., Gao, C., Wu, Y., Ma, J., He, R., Li, Q., … Descoteaux, M. (2017). The challenge of mapping the human connectome based on diffusion tractography. Nature Communications, 8(1), 1349. https://doi.org/10.1038/s41467-017-01285-x\n\n\nMeehl, P. E. (1967). Theory-testing in psychology and physics: A methodological paradox. Philosophy of Science, 34(2), 103–115.\n\n\nMeehl, P. E. (1978). Theoretical risks and tabular asterisks: Sir karl, sir ronald, and the slow progress of soft psychology. 46(September 1976), 806–834.\n\n\nMinocher, R., Atmaca, S., Bavero, C., McElreath, R., & Beheim, B. (n.d.). Estimating the reproducibility of social learning research published between 1955 and 2018. Royal Society Open Science, 8(9), 210450. https://doi.org/10.1098/rsos.210450\n\n\nMunafò, M. R., Nosek, B. A., Bishop, D. V. M., Button, K. S., Chambers, C. D., Percie Du Sert, N., Simonsohn, U., Wagenmakers, E. J., Ware, J. J., & Ioannidis, J. P. A. (2017). A manifesto for reproducible science. Nature Human Behaviour, 1(1), 1–9. https://doi.org/10.1038/s41562-016-0021\n\n\nNosek, B. A., Beck, E. D., Campbell, L., Flake, J. K., Hardwicke, T. E., Mellor, D. T., van?t Veer, A. E., & Vazire, S. (2019). Preregistration is hard, and worthwhile. Trends in Cognitive Sciences, 23(10), 815–818.\n\n\nNosek, B. A., Ebersole, C. R., DeHaven, A. C., & Mellor, D. T. (2018). The preregistration revolution. Proceedings of the National Academy of Sciences, 115(11), 2600–2606.\n\n\nNosek, B. A., Hardwicke, T. E., Moshontz, H., Allard, A., Corker, K. S., Dreber, A., Fidler, F., Hilgard, J., Kline Struhl, M., Nuijten, M. B., Rohrer, J. M., Romero, F., Scheel, A. M., Scherer, L. D., Schönbrodt, F. D., & Vazire, S. (2022). Replicability, Robustness, and Reproducibility in Psychological Science. Annual Review of Psychology, 73, 719–748. https://doi.org/10.1146/annurev-psych-020821-114157\n\n\nNosek, B. A., & Lakens, D. (2014). Registered reports: A method to increase the credibility of published results. Social Psychology, 45(3), 137–141. https://doi.org/10.1027/1864-9335/a000192\n\n\nObels, P., Lakens, D., Coles, N. A., Gottfried, J., & Green, S. A. (2020). Analysis of open data and computational reproducibility in registered reports in psychology. Advances in Methods and Practices in Psychological Science, 3(2), 229–237. https://doi.org/10.1177/2515245920918872\n\n\nParsons, S. (n.d.). Exploring reliability heterogeneity with multiverse analyses: Data processing decisions unpredictably influence measurement reliability. https://doi.org/10.31234/osf.io/y6tcz\n\n\nPashler, H., & Harris, C. (2012). Is the replicability crisis overblown? Three arguments examined. Perspectives on Psychological Science, 7(6), 531–536. https://doi.org/10.1177/1745691612463401\n\n\nPashler, H., & Wagenmakers, E. J. (2012). Editors’ introduction to the special section on replicability in psychological science: A crisis of confidence? Perspectives on Psychological Science, 7(6), 528–530. https://doi.org/10.1177/1745691612465253\n\n\nPatel, C. J., Burford, B., & Ioannidis, J. P. A. (2015). Assessment of vibration of effects due to model specification can demonstrate the instability of observational associations. Journal of Clinical Epidemiology, 68(9), 1046–1058. https://doi.org/10.1016/j.jclinepi.2015.05.029\n\n\nPoline, J.-B., Strother, S. C., Dehaene-Lambertz, G., Egan, G. F., & Lancaster, J. L. (2006). Motivation and synthesis of the FIAC experiment: Reproducibility of fMRI results across expert analyses. Human Brain Mapping, 27(5), 351–359. https://doi.org/10.1002/hbm.20268\n\n\nRoche, D. G., Kruuk, L. E. B., Lanfear, R., & Binning, S. A. (2015). Public data archiving in ecology and evolution: How well are we doing? PLoS Biology, 13(11), 1–12. https://doi.org/10.1371/journal.pbio.1002295\n\n\nSalganik, M. J., Lundberg, I., Kindel, A. T., Ahearn, C. E., Al-Ghoneim, K., Almaatouq, A., Altschul, D. M., Brand, J. E., Carnegie, N. B., Compton, R. J., Datta, D., Davidson, T., Filippova, A., Gilroy, C., Goode, B. J., Jahani, E., Kashyap, R., Kirchner, A., McKay, S., … McLanahan, S. (2020). Measuring the predictability of life outcomes with a scientific mass collaboration. Proceedings of the National Academy of Sciences, 117(15), 8398–8403. https://doi.org/10.1073/pnas.1915006117\n\n\nScheel, A. M. (2022). Why most psychological research findings are not even wrong. Infant and Child Development, 31(1), e2295. https://doi.org/10.1002/icd.2295\n\n\nScheel, A. M., Tiokhin, L., Isager, P. M., & Lakens, D. (2021). Why Hypothesis Testers Should Spend Less Time Testing Hypotheses. Perspectives on Psychological Science, 16(4), 744–755. https://doi.org/10.1177/1745691620966795\n\n\nSchweinsberg, M., Feldman, M., Staub, N., Akker, O. R. van den, Aert, R. C. M. van, Assen, M. A. L. M. van, Liu, Y., Althoff, T., Heer, J., Kale, A., Mohamed, Z., Amireh, H., Venkatesh Prasad, V., Bernstein, A., Robinson, E., Snellman, K., Amy Sommer, S., Otner, S. M. G., Robinson, D., … Luis Uhlmann, E. (2021). Same data, different conclusions: Radical dispersion in empirical results when independent analysts operationalize and test the same hypothesis. Organizational Behavior and Human Decision Processes, 165, 228–249. https://doi.org/10.1016/j.obhdp.2021.02.003\n\n\nSedlmeier, P., & Gigerenzer, G. (1989). Statistical power studies. Psychological Bulletin, 105(2), 309–316.\n\n\nSilberzahn, R., & Uhlmann, E. L. (2015). Crowdsourced research: Many hands make tight work. Nature, 526(7572), 189–191. https://doi.org/10.1038/526189a\n\n\nSilberzahn, R., Uhlmann, E. L., Martin, D. P., Anselmi, P., Aust, F., Awtrey, E., Bahník, Š., Bai, F., Bannard, C., Bonnier, E., Carlsson, R., Cheung, F., Christensen, G., Clay, R., Craig, M., Dalla Rosa, A., Dam, L., Evans, M., Flores Cervantes, I., … Nosek, B. (2017). Many analysts, one dataset: Making transparent how variations in analytical choices affect results. Advances in Methods and Practices in Psychological Science. https://doi.org/10.31234/osf.io/qkwst\n\n\nSimmons, J. P., Nelson, L. D., & Simonsohn, U. (2011b). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632\n\n\nSimmons, J. P., Nelson, L. D., & Simonsohn, U. (2011a). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632\n\n\nStarns, J. J., Cataldo, A. M., Rotello, C. M., Annis, J., Aschenbrenner, A., Bröder, A., Cox, G., Criss, A., Curl, R. A., Dobbins, I. G., Dunn, J., Enam, T., Evans, N. J., Farrell, S., Fraundorf, S. H., Gronlund, S. D., Heathcote, A., Heck, D. W., Hicks, J. L., … Wilson, J. (2019). Assessing Theoretical Conclusions With Blinded Inference to Investigate a Potential Inference Crisis. Advances in Methods and Practices in Psychological Science, 2(4), 335–349. https://doi.org/10.1177/2515245919869583\n\n\nSteegen, S., Tuerlinckx, F., Gelman, A., & Vanpaemel, W. (2016a). Increasing transparency through a multiverse analysis. Perspectives on Psychological Science, 11(5), 702–712.\n\n\nSteegen, S., Tuerlinckx, F., Gelman, A., & Vanpaemel, W. (2016b). Increasing transparency through a multiverse analysis. Perspectives on Psychological Science, 11(5), 702–712.\n\n\nTedersoo, L., Küngas, R., Oras, E., Köster, K., Eenmaa, H., Leijen, Ä., Pedaste, M., Raju, M., Astapova, A., Lukner, H., Kogermann, K., & Sepp, T. (2021). Data sharing practices and data availability upon request differ across scientific disciplines. Scientific Data, 8(1), 192. https://doi.org/10.1038/s41597-021-00981-0\n\n\nTowse, J. N., Ellis, D. A., & Towse, A. S. (2021). Opening Pandora’s Box: Peeking inside Psychology’s data sharing practices, and seven recommendations for change. Behavior Research Methods, 53(4), 1455–1468. https://doi.org/10.3758/s13428-020-01486-1\n\n\nUlrich, R., & Miller, J. (1994). Effects of truncation on reaction time analysis. Journal of Experimental Psychology: General, 123, 34–80.\n\n\nVankov, I., Bowers, J., & Munafò, M. R. (2014). On the persistence of low power in psychological science. Quarterly Journal of Experimental Psychology, 67(5), 1037–1040. https://doi.org/10.1080/17470218.2014.885986\n\n\nVasishth, S., & Gelman, A. (2021). How to embrace variation and accept uncertainty in linguistic and psycholinguistic data analysis. Linguistics, 59(5), 1311–1342. https://doi.org/10.1515/ling-2019-0051\n\n\nVazire, S. (2018). Implications of the Credibility Revolution for Productivity, Creativity, and Progress. Perspectives on Psychological Science, 13(4), 411–417. https://doi.org/10.1177/1745691617751884\n\n\nWagenmakers, E.-J., Sarafoglou, A., & Aczel, B. (2022). One statistical analysis must not rule them all. Nature, 605(7910), 423–425. https://doi.org/10.1038/d41586-022-01332-8\n\n\nWagenmakers, E.-J., Wetzels, R., Borsboom, D., & Maas, H. L. J. van der. (2011). Why psychologists must change the way they analyze their data: The case of psi: Comment on bem (2011). Journal of Personality and Social Psychology, 100(3), 426–432. https://doi.org/10.1037/a0022790\n\n\nWessel, I., Albers, C., Zandstra, A. R. E., & Heininga, V. E. (2020). A multiverse analysis of early attempts to replicate memory suppression with the think/no-think task.\n\n\nWicherts, J. M., Borsboom, D., Kats, J., & Molenaar, D. (2006). The poor availability of psychological research data for reanalysis. American Psychologist, 61(7), 726–728. https://doi.org/10.1037/0003-066X.61.7.726\n\n\nWild, H., Kyröläinen, A.-J., & Kuperman, V. (2022). How representative are student convenience samples? A study of literacy and numeracy skills in 32 countries. PLOS ONE, 17(7), e0271191. https://doi.org/10.1371/journal.pone.0271191\n\n\nYarkoni, T. (2022). The generalizability crisis. Behavioral and Brain Sciences, 45, e1. https://doi.org/10.1017/S0140525X20001685\n\n\nYoung, C. (2018). Model uncertainty and the crisis in science. Socius, 4, 2378023117737206.\n\n\nYoung, C., & Holsteen, K. (2017). Model Uncertainty and Robustness: A Computational Framework for Multimodel Analysis. Sociological Methods & Research, 46(1), 3–40. https://doi.org/10.1177/0049124115610347",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 6. Why we are asking you to do this"
    ]
  },
  {
    "objectID": "PSYC411/part2/intro.html#footnotes",
    "href": "PSYC411/part2/intro.html#footnotes",
    "title": "Week 6. Why we are asking you to do this",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis encouragement is often attributed to Gandhi but is attributed ((here)) to a Brooklyn school teacher, Ms Arleen Lorrance, who led a transformative school project in the 1970s.↩︎\nThe term is taken from the name of a short story by Jorge Luis Borges, “El jardin de senderos que se bifurcan” (translated as ‘The garden of forking paths’).↩︎\nThere could be a story where the hero (us) ultimately learns to reject binary (present, absent; significant, non-significant) choices, and embrace variation, or embrace uncertainty (a. Gelman, 2015; Vasishth & Gelman, 2021).↩︎\nOpen access journals publish articles that are free to read or download.↩︎",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 6. Why we are asking you to do this"
    ]
  },
  {
    "objectID": "PSYC411/part2/lm-dev.html",
    "href": "PSYC411/part2/lm-dev.html",
    "title": "Week 10. Developing the linear model",
    "section": "",
    "text": "Welcome to your overview of the work we will do together in Week 10.\nThis week, we focus on strengthening your ability to apply the linear model approach to a wider range of research questions.\nIn the context of the Clearly understood project, we frame our analysis concerns and methods in relation to example research questions, including the question:\n\nWhat person attributes predict success in understanding?\n\nThis is to help you to learn to think critically about what it is you want to do with linear models when you use them, or when you read about their results in research reports.\nIt will be seen that, to answer research questions like this example question, we will need to think about how we analyze data when multiple different predictor variables could be included in our model.\nMost of the time, in your future professional work, when you use linear models you will be trying to predict outcomes (behaviours, person attributes) given information from multiple different predictor variables at once.\nYou will be able to do this work using what you learn this week.\nAs students, now, learning how to move from analyses involving one outcome and one predictor variable, to analyses involving one outcome and multiple outcome variables unlocks a much wider range of contexts in which you can apply the skills and understanding you develop here to address research problems and questions of your own.\n\n\n\n\n\n\nTip\n\n\n\nWe encounter linear models with multiple predictors, often, as multiple regression or regression analyses. The difference in names in different fields disguises the fact that we are working with the same technique: multiple regressions are linear models.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 10. Developing the linear model"
    ]
  },
  {
    "objectID": "PSYC411/part2/lm-dev.html#sec-lm-dev-overview",
    "href": "PSYC411/part2/lm-dev.html#sec-lm-dev-overview",
    "title": "Week 10. Developing the linear model",
    "section": "",
    "text": "Welcome to your overview of the work we will do together in Week 10.\nThis week, we focus on strengthening your ability to apply the linear model approach to a wider range of research questions.\nIn the context of the Clearly understood project, we frame our analysis concerns and methods in relation to example research questions, including the question:\n\nWhat person attributes predict success in understanding?\n\nThis is to help you to learn to think critically about what it is you want to do with linear models when you use them, or when you read about their results in research reports.\nIt will be seen that, to answer research questions like this example question, we will need to think about how we analyze data when multiple different predictor variables could be included in our model.\nMost of the time, in your future professional work, when you use linear models you will be trying to predict outcomes (behaviours, person attributes) given information from multiple different predictor variables at once.\nYou will be able to do this work using what you learn this week.\nAs students, now, learning how to move from analyses involving one outcome and one predictor variable, to analyses involving one outcome and multiple outcome variables unlocks a much wider range of contexts in which you can apply the skills and understanding you develop here to address research problems and questions of your own.\n\n\n\n\n\n\nTip\n\n\n\nWe encounter linear models with multiple predictors, often, as multiple regression or regression analyses. The difference in names in different fields disguises the fact that we are working with the same technique: multiple regressions are linear models.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 10. Developing the linear model"
    ]
  },
  {
    "objectID": "PSYC411/part2/lm-dev.html#sec-lm-dev-goals",
    "href": "PSYC411/part2/lm-dev.html#sec-lm-dev-goals",
    "title": "Week 10. Developing the linear model",
    "section": "10.2 Our learning goals",
    "text": "10.2 Our learning goals\nWe will learn how to:\n\nSkills – extend our capacity to code models so that we can incorporate multiple predictors;\nConcepts – develop the critical thinking processes required to make decisions about what predictors to include in your model;\nConcepts and skills – learn how to critically evaluate results, given variation between samples.\n\nWe will revise how to:\n\nSkills – identify and interpret model statistics;\nConcepts and skills – critically evaluate the results;\nConcepts and skills – communicate the results.\n\nAs we progress, we will continue to strengthen your skills in building professional visualizations. This week, we will learn how to exploit professional tools to automatically generate and plot model predictions, when previously we produced model predictions by hand.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 10. Developing the linear model"
    ]
  },
  {
    "objectID": "PSYC411/part2/lm-dev.html#sec-lm-dev-resources",
    "href": "PSYC411/part2/lm-dev.html#sec-lm-dev-resources",
    "title": "Week 10. Developing the linear model",
    "section": "10.3 Learning resources",
    "text": "10.3 Learning resources\nYou will see, next, the lectures we share to explain the concepts you will learn about, and the practical data analysis skills you will develop. Then you will see information about the practical materials you can use to build and practise your skills.\nEvery week, you will learn best if you first watch the lectures then do the practical exercises.\n\n\n\n\n\n\nLinked resources\n\n\n\n\nIn the chapter for week 7, we share materials to support your development of critical thinking about associations, and your development of practical skills in working with correlation-based analyses.\nIn the chapter for week 8, we introduce you to the main ideas and practical steps involved in conducting linear model analyses.\n\n\n\n\n10.3.1 Lectures\nThe lecture materials for this week are presented in four short parts.\nClick on a link and your browser should open a tab showing the Panopto video for the lecture part.\n\nPart 1 (20 minutes) Developing the linear model: The concepts and skills we will learn about in week 10: our aims, the research questions we can answer with linear models, making the move to working with linear models with multiple predictors, why the main challenge is not the coding but the choices over which predictors to include in a model.\n\n\n\nPart 2 (13 minutes): Coding, thinking about, and reporting linear models with multiple predictors.\n\n\n\nPart 3 (21 minutes): Critically evaluating the results of analyses involving linear models.\n\n\n\nPart 4 (19 minutes): The linear model is very flexible, powerful and general.\n\n\n\n\n10.3.2 Lecture slides\n\n\n\n\n\n\nDownload the lecture slides\n\n\n\nThe slides presented in the videos can be downloaded here:\n\nThe slides exactly as presented (11 MB).\n\nYou can download the web page .html file and click on it to open it in any browser (e.g., Chrome, Edge or Safari). The slide images are high quality so the file is quite big and may take a few seconds to download.\n\n\nWe are going to work through some practical exercises, next, to develop your critical thinking and practical skills for working with linear models.\n\n\n10.4 Practical materials: data and R-Studio\nWe will work with one data file which you can download by clicking on its name (below):\n\n2022-12-08_all-studies-subject-scores.csv.\n\nOnce you have downloaded the file, you will need to upload it to the R-Studio server to access it so that you can do the practical exercises.\n\n\n\n\n\n\nImportant\n\n\n\nHere is a link to the sign-in page for R-Studio Server\n\n\n\n\n10.4.1 Practical materials guide\nAs usual, you will find that the practical exercises are simpler to do if you follow these steps in order.\n\nThe data — We will take a quick look at what is inside the data files so you know what everything means.\nThe how-to guide — We will go through the practical analysis and visualization coding steps, showing all the code required for each step.\nThe practical exercises — We will set out the tasks, questions and challenges that you should complete to learn the practical skills we target this week.\n\nThis week — Week 10 — we aim to further develop skills in working with the linear model, and in visualizing and testing the associations between variables in psychological data.\n\nWhile we build on everything you have learned so far, the skills you learn in this class unlock your capacity to analyse most kinds of data you will encounter in most situations: a big expansion in the scope of your capacities.\n\n\n\n\n\n\n\nWeek 10 parts\n\n\n\n\nSet-up\nLoad the data\nRevision: using a linear model to answer research questions – one predictor.\nNew: using a linear model to answer research questions – multiple predictors.\nNew: plot predictions from linear models with multiple predictors.\nNew: estimate the effects of factors as well as numeric variables.\nNew: examine associations comparing data from different samples.\n\n\n\nWe learn these skills so that we can answer research questions like:\n\nWhat person attributes predict success in understanding?\n\nQuestions like this are often answered by analyzing psychological data using some form of linear model.\nAs usual, when we do these analyses, we need to think about how we report the results, so part of the learning you will complete will enable you to:\n\nreport information about the kind of model you specify;\nreport the nature of the associations (or effects) estimated in your model;\nevaluate the results, making decisions about (i.) the significance of effects (ii.) whether estimates of effects suggest a positive or negative relationship between outcome and predictor (iii.) whether estimates of effects suggest a strong or a weak relationship.\n\nWe will really strengthen your ability to produce professional visualizations by learning how to translate model results into plots that help you and your audience to translate what your model tells you into accurate understanding and plain language.\n\n10.4.1.1 The data files\nEach of the data files we have worked with has had a similar structure. This week, that continuity remains. But, this week, we move on to working with a big data-set similar to the data you may encounter in real-world situations.\n\nWhat is new about this data-set is that it holds data from multiple studies in which the same methods were used – these are replication studies – enabling us to look the questions about results reproducibility across studies that you have been hearing about.\n\nHere are what the first few rows in the data file all.studies.subjects looks like:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResponseId\nmean.acc\nmean.self\nAGE\nGENDER\nEDUCATION\nETHNICITY\nSHIPLEY\nHLVA\nFACTOR3\nNATIVE.LANGUAGE\nstudy\n\n\n\n\nR_1lcaBAGJNNI2kju\n1.00\n7.6\n18\nFemale\nFurther\nWhite\n31\n10\n48\nEnglish\nPSYC122\n\n\nR_AG4jiTm8oxmuOOZ\n0.90\n7.6\n18\nFemale\nFurther\nWhite\n35\n10\n40\nEnglish\nPSYC122\n\n\nR_2Ckb6YXLPGwYSvg\n0.95\n7.2\n18\nMale\nFurther\nAsian\n35\n9\n47\nOther\nPSYC122\n\n\nR_27JY5xHHcMs7jGi\n0.90\n6.8\n18\nFemale\nFurther\nWhite\n35\n8\n52\nEnglish\nPSYC122\n\n\nR_1DtJ4mrOXmxre01\n0.85\n6.4\n19\nFemale\nFurther\nWhite\n33\n9\n41\nEnglish\nPSYC122\n\n\nR_PRFQFInzSS6T8e5\n0.90\n6.2\n19\nFemale\nFurther\nMixed\n36\n5\n52\nEnglish\nPSYC122\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe webpage has a slider under the data table window, so you can scroll across the columns: move your cursor over the window to show the slider.\n\n\nWhen you look at the data table, you can see the columns:\n\nResponseId participant code\nmean.acc average accuracy of response to questions testing understanding of health guidance\nmean.self average self-rated accuracy of understanding of health guidance\nstudy variable coding for what study the data were collected in\nAGE age in years\nHLVA health literacy test score\nSHIPLEY vocabulary knowledge test score\nFACTOR3 reading strategy survey score\nGENDER gender code\nEDUCATION education level code\nETHNICITY ethnicity (Office National Statistics categories) code\n\nYou can now see a new column:\n\nNATIVE.LANGUAGE which codes for what language study participants grew up speaking (English, Other)\n\n\n\n10.4.2 The how-to guide\nWe will take things step-by-step.\nMake sure you complete each part, task and question, in order, before you move on to the next one.\n\n\n10.4.2.1 How-to Part 1: Set-up\nTo begin, we set up our environment in R.\n\nHow-to Task 1 – Run code to empty the R environment\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nrm(list=ls())                            \n\n\n\n\n\n\nHow-to Task 2 – Run code to load libraries\nLoad libraries using library().\n\nNotice that we are working with libraries that may be new to you here.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nlibrary(\"ggeffects\")\nlibrary(\"patchwork\")\nlibrary(\"psych\")\nlibrary(\"tidyverse\")\n\n\n\n\n\n\n\n10.4.2.2 How-to Part 2: Load the data\n\nHow-to Task 3 – Read in the data file we will be using\nWe will be working with the data file:\n\n2022-12-08_all-studies-subject-scores.csv.\n\nRead in the file – using read_csv().\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nall.studies.subjects &lt;- read_csv(\"2022-12-08_all-studies-subject-scores.csv\")\n\nRows: 615 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): ResponseId, GENDER, EDUCATION, ETHNICITY, NATIVE.LANGUAGE, study\ndbl (6): mean.acc, mean.self, AGE, SHIPLEY, HLVA, FACTOR3\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\nHow-to Task 4 – Inspect the data file\nIn previous classes, we have used the summary() function to inspect the data.\nNow, let’s do something new.\nWe use the describe() function from the {psych} library to produce descriptive statistics for the variables in the data-set.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\ndescribe(all.studies.subjects)\n\n                 vars   n   mean     sd median trimmed    mad   min max  range\nResponseId*         1 615 308.00 177.68  308.0  308.00 228.32  1.00 615 614.00\nmean.acc            2 561   0.73   0.21    0.8    0.76   0.22  0.15   1   0.85\nmean.self           3 561   6.53   1.72    6.8    6.69   1.78  1.00   9   8.00\nAGE                 4 615  28.34  13.56   23.0   25.59   5.93 18.00 100  82.00\nGENDER*             5 615   1.32   0.54    1.0    1.25   0.00  1.00   5   4.00\nEDUCATION*          6 615   1.73   0.64    2.0    1.67   0.00  1.00   3   2.00\nETHNICITY*          7 615   2.75   1.95    1.0    2.69   0.00  1.00   5   4.00\nSHIPLEY             8 615  28.98   9.52   32.0   30.48   7.41  0.00  40  40.00\nHLVA                9 615   7.22   3.09    8.0    7.49   2.97  0.00  13  13.00\nFACTOR3            10 615  44.03  14.67   48.0   46.69   8.90  0.00  63  63.00\nNATIVE.LANGUAGE*   11 615   1.54   0.50    2.0    1.55   0.00  1.00   2   1.00\nstudy*             12 615   4.04   2.69    4.0    3.92   2.97  1.00   8   7.00\n                  skew kurtosis   se\nResponseId*       0.00    -1.21 7.16\nmean.acc         -0.83    -0.31 0.01\nmean.self        -0.86     0.49 0.07\nAGE               2.05     4.70 0.55\nGENDER*           2.15     8.21 0.02\nEDUCATION*        0.30    -0.70 0.03\nETHNICITY*        0.25    -1.90 0.08\nSHIPLEY          -1.47     1.96 0.38\nHLVA             -0.69     0.00 0.12\nFACTOR3          -1.85     3.25 0.59\nNATIVE.LANGUAGE* -0.15    -1.98 0.02\nstudy*            0.29    -1.46 0.11\n\n\n\n\n\nIn the reports you write – whether as a student or, later, in your professional work – you may often be required to present table summaries of the variables in your data, incorporating descriptive statistics.\n\nYou can use the describe() function to get R to do the work for you.\n\nWe want to get just the descriptive statistics we want.\nWe are going to do this in two steps:\n\nwe select the variables we care about;\nwe get just the descriptive statistics we want for those variables\n\nWe will do this using two different but equivalent methods.\nFirst, we are going to do this using the %&gt;% pipes that you saw previously (in week 5):\n\nall.studies.subjects %&gt;%\n  select(mean.acc, mean.self, AGE, SHIPLEY, HLVA, FACTOR3) %&gt;%\n  describe(skew = FALSE)\n\n          vars   n  mean    sd median   min max range   se\nmean.acc     1 561  0.73  0.21    0.8  0.15   1  0.85 0.01\nmean.self    2 561  6.53  1.72    6.8  1.00   9  8.00 0.07\nAGE          3 615 28.34 13.56   23.0 18.00 100 82.00 0.55\nSHIPLEY      4 615 28.98  9.52   32.0  0.00  40 40.00 0.38\nHLVA         5 615  7.22  3.09    8.0  0.00  13 13.00 0.12\nFACTOR3      6 615 44.03 14.67   48.0  0.00  63 63.00 0.59\n\n\nThese are the code elements and what they do:\n\nall.studies.subjects %&gt;% – you tell R to use the all.studies.subjects dataset, then use %&gt;% to ask R to take those data to the next step.\nselect(mean.acc, mean.self, AGE, SHIPLEY, HLVA, FACTOR3) %&gt;% – you tell R to select just those variables in all.studies.subjects that you name and %&gt;% pipe them to the next step.\ndescribe(...) – you tell R to give you descriptive statistics for the variables you have selected.\ndescribe(skew = FALSE) – critically, you add the argument skew = FALSE to turn off the option in describe() to report skew, kurtosis: because we do not typically see these statistics reported in psychology.\n\n\n\n\n\n\n\nTip\n\n\n\nModern R coding often uses the pipe. When you see it, you see it used in a process involving a series of steps.\n\nDo this thing, then send the results %&gt; to the next step, do the next thing then send the results %&gt;% to the next step…\nWhen you look at ggplot() code, the + at the end of each line of plotting code is doing a similar job to the pipe.\n\n\n\nNot everyone is comfortable coding this way so next we will do the same thing in an old style.\n\nall.studies.subjects.select &lt;- select(all.studies.subjects, \n                           mean.acc, mean.self, AGE, SHIPLEY, HLVA, FACTOR3)\ndescribe(all.studies.subjects.select, skew = FALSE)\n\n          vars   n  mean    sd median   min max range   se\nmean.acc     1 561  0.73  0.21    0.8  0.15   1  0.85 0.01\nmean.self    2 561  6.53  1.72    6.8  1.00   9  8.00 0.07\nAGE          3 615 28.34 13.56   23.0 18.00 100 82.00 0.55\nSHIPLEY      4 615 28.98  9.52   32.0  0.00  40 40.00 0.38\nHLVA         5 615  7.22  3.09    8.0  0.00  13 13.00 0.12\nFACTOR3      6 615 44.03 14.67   48.0  0.00  63 63.00 0.59\n\n\nThese are the code elements and what they do:\n\nall.studies.subjects.select &lt;- ...(all.studies.subjects, ...) – you tell R to create a new dataset all.studies.subjects.select from the original all.studies.subjects – the new dataset will include just the variables we select from the original.\nselect(all.studies.subjects, ...) – you tell R to select the variables you want using the select(...) function: telling R to select variables from all.studies.subjects.\n... select(..., mean.acc, mean.self, AGE, SHIPLEY, HLVA, FACTOR3) – you tell R to select the variables you want by entering the variable names, separated by commas, after you have named the dataset.\ndescribe(...) – you tell R to give you descriptive statistics.\ndescribe(all.studies.subjects.select, ...) – you name the dataset with the selection of variables you have created, telling the describe() function what data to work with.\ndescribe(skew = FALSE) – critically, you add the argument skew = FALSE to turn off the option to report skew, kurtosis to keep the descriptive statistics short.\n\nWhat are we learning here?\nFirst, we can do exactly the same thing in two different but related ways:\n\nUse the way that (1.) works and (2.) you prefer.\nWhich should you prefer? You may reflect on how easy the code is to write, read, understand and use\n\nSecond, we modify how the function describe() works by adding an argument: describe(skew = FALSE).\n\nWe have been doing this kind of move, already, by adding arguments to, e.g., specify point colour in ggplot() code.\n\nAs your skills advance, so your preferences on how you want R to work for you will become more specific.\n\nYou can modify the outputs from functions so that you get exactly what you want in the way that you want it.\n\nThe information on the options available to you for any argument can be found in different kinds of places, see the further information box, below.\n\n\n\n\n\n\nFurther information you can explore\n\n\n\n\nHere is an explanation for how to use pipes %&gt;% when you code and why it may be helpful to do so:\n\nhttps://r4ds.had.co.nz/pipes.html\nNote that we do not require the use of pipes in PSYC411: we are showing you this to expand your knowledge.\n\nYou can get a guide to the {psych} library here:\n\nhttp://personality-project.org/r/psych/vignettes/intro.pdf\nEvery “official” R library has a technical manual on the central R resource CRAN, and the manual for ‘psych’ can be found here:\nhttps://cran.r-project.org/web/packages/psych/psych.pdf\n\nwhere you can see information on the functions the library provides, and how you can use each function.\n\nThis is how you ask for help in R for a function, e.g., just type: ?describe or help(describe).\nThe {psych} library is written by William Revelle who provides a lot of useful resources here:\nhttp://personality-project.org/r/psych/\n\n\n\n\n\n10.4.2.3 How-to Part 3: Using a linear model to answer research questions – one predictor\n\nHow-to Task 5 – Examine the relation between outcome mean accuracy (mean.acc) and health literacy (HLVA)\nOne of our research questions is:\n\nWhat person attributes predict success in understanding?\n\nHere, we can address the question directly because we have information on an outcome, mean accuracy (mean.acc), and information on a predictor, health literacy (HLVA).\n\nGiven the theoretical account outlined in the chapter for week 7, our hypothesis is that people who possess a higher level of background knowledge, measured here using the health literacy (HLVA) test, will be more likely to accurately understand health information when they read it, with understanding tested to calculate the mean accuracy (mean.acc).\nThis hypothesis translates into a statistical prediction: if people in our sample have higher HLVA scores, we predict that, on average, they will also have higher mean.acc scores.\n\nTo address the research question, we test the statistical prediction using the linear model:\n\nmodel &lt;- lm(mean.acc ~ HLVA, data = all.studies.subjects)\nsummary(model)\n\n\nCall:\nlm(formula = mean.acc ~ HLVA, data = all.studies.subjects)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.59375 -0.11731  0.02981  0.13269  0.37100 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.414250   0.025912   15.99   &lt;2e-16 ***\nHLVA        0.041188   0.003182   12.94   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1853 on 559 degrees of freedom\n  (54 observations deleted due to missingness)\nMultiple R-squared:  0.2306,    Adjusted R-squared:  0.2292 \nF-statistic: 167.5 on 1 and 559 DF,  p-value: &lt; 2.2e-16\n\n\nIf you look at the model summary you can answer the following questions\n\nQ.1. What is the estimate for the coefficient of the effect of the predictor, HLVA?\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nA.1. 0.041188\n\n\n\n\n\nQ.2. Is the effect significant?\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nA.2. It is significant because p &lt; .05\n\n\n\n\n\nQ.3. What are the values for t and p for the significance test for the coefficient?\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nA.3. t = 12.94, p &lt;2e-16\n\n\n\n\n\nQ.4. What do you conclude is the answer to the research question, given the linear model results?\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nA.4. The model slope estimate suggests that as HLVA scores increase so also do mean.acc scores.\n\n\n\n\n\nQ.5. What is the F-statistic for the regression? Report F, DF and the p-value.\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nA.5. F-statistic: 167.5 on 1 and 559 DF, p-value: &lt; 2.2e-16\n\n\n\n\n\nQ.6. Is the regression significant?\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nA.6. Yes: the regression is significant.\n\n\n\n\n\nQ.7. What is the Adjusted R-squared?\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nA.7. Adjusted R-squared: 0.2292\n\n\n\n\n\nQ.8. Explain in words what this R-squared value indicates?\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nA.8. The R-squared suggests that 23% of outcome variance can be explained # by the model\n\n\n\n\nWhat are we learning here?\nIt is worth your time reflecting, here, on how statistical analyses are incorporated in research work.\n\nWe have ideas about a psychological process: e.g., reading comprehension.\nWe have assumptions of a theoretical account of that process: e.g., we use background knowledge to understand what we read.\nGiven those assumptions, we derive hypotheses that we translate into statistical predictions: e.g., higher health literacy predicts higher accuracy of understanding.\nWe test a statistical prediction using a method like lm(), specifying a model that aligns with the prediction terms: the outcome, the predictor(s).\n\n\n\n\n\n\n\nTip\n\n\n\nThinking ahead to your dissertation project, can you reflect on the research question you will hope to answer, how you will address that question through measurement, then data analysis to test a statistical prediction?\n\n\n\n\n\n10.4.2.4 How-to Part 4: Using a linear model to answer research questions – multiple predictors\n\nHow-to Task 6 –Examine the relation between outcome mean accuracy (mean.acc) and multiple predictors including: health literacy (HLVA); vocabulary (SHIPLEY); AGE; reading strategy (FACTOR3)\nWe saw that our research question is:\n\nWhat person attributes predict success in understanding?\n\nIn the previous task, we tried to model success in understanding using just one predictor variable but it is unrealistic that a person’s capacity to understand health information will be predicted by just one thing, their background knowledge. It is more likely that variation on multiple dimensions will predict understanding.\nWhat could those variables be?\nWe can examine the relation between outcome mean accuracy (mean.acc) and multiple predictors simultaneously. Here, the candidate predictor variables include: health literacy (HLVA); vocabulary (SHIPLEY); AGE; reading strategy (FACTOR3).\nWe can incorporate all predictors together in a linear model: we use lm(), as before, but now specify each variable listed here by variable name.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nmodel &lt;- lm(mean.acc ~ HLVA + SHIPLEY + FACTOR3 + AGE, data = all.studies.subjects)\nsummary(model)\n\n\nCall:\nlm(formula = mean.acc ~ HLVA + SHIPLEY + FACTOR3 + AGE, data = all.studies.subjects)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.57543 -0.08115  0.02303  0.11169  0.38810 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.0996243  0.0468313   2.127   0.0338 *  \nHLVA         0.0274954  0.0032461   8.470  &lt; 2e-16 ***\nSHIPLEY      0.0083457  0.0011430   7.302 9.88e-13 ***\nFACTOR3      0.0050332  0.0009246   5.444 7.84e-08 ***\nAGE         -0.0025503  0.0005074  -5.026 6.75e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1668 on 556 degrees of freedom\n  (54 observations deleted due to missingness)\nMultiple R-squared:  0.3799,    Adjusted R-squared:  0.3755 \nF-statistic: 85.17 on 4 and 556 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nIf we look closely at this code example, we can first identify the working elements in the linear model code, but then we can identify some general properties that it will be useful to you to learn about.\nFirst, let’s work through the elements of the linear model code so we can see what everything does:\n\nmodel &lt;- lm(...) – tell R you want to fit the model using lm(...), and give the model a name: here, we call it “model”.\n...lm(mean.acc ~ HLVA...) – tell R you want a model of the outcome mean.acc predicted ~ by the predictors: HLVA, SHIPLEY, FACTOR3, AGE.\n\n\nNote that we use the variable names as they appear in the dataset, and that each predictor variable is separated from the next by a plus sign.\n\n\n...data = all.studies.subjects) – tell R that the variables you name in the formula are in the all.studies.subjects data-set.\nsummary(model) – ask R for a summary of the model you called “model”: this is how you get the results.\n\n\n\n\n\n\n\nTip\n\n\n\nNotice that R has a general formula syntax: outcome ~ predictor or y ~ x\n\nand uses the same format across a number of different functions;\neach time, on the left of the tilde symbol ~ you identify the output or outcome variable;\nhere, what is new is that you see on the right of the tilde ~ multiple predictor variables: y ~ x1 + x2 + ...\n\n\n\nIf you look at the model summary you can answer the following questions.\n(We have hidden the answers – so that you can test yourself – but you can reveal each answer by clicking on the box.)\n\nQ.7. What is the estimate for the coefficient of the effect of the predictor HLVA in this model?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA.7. 0.0274954\n\n\n\n\n\nQ.8. Is the effect significant?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA.8. It is significant because p &lt; .05.\n\n\n\n\n\nQ.9. What are the values for t and p for the significance test for the coefficient?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA.9. t = 8.470, p &lt; 2e-16\n\n\n\n\n\nQ.10. What do you conclude is the answer to the research question, given the linear model results?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA.10. The model slope estimate 0.0274954 suggests that as HLVA scores # increase so also do mean.acc scores\n\n\n\n\n\nQ.11. How is the coefficient estimate for the HLVA slope similar or different, comparing this model with multiple predictors to the previous model with one predictor?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA.11. It can be seen that the HLVA estimate in the two models is different in that it is a bit smaller in the model with multiple predictors compared to the model with one predictor. The HLVA estimate is similar in that it remains positive, and it is about the same size.\n\n\n\n\nWhat are we learning here?\nThe estimate of the coefficient of any one predictor can be expected to vary depending on the presence of other predictors.\n\nThis is one reason why we need to be transparent about why we choose to use the predictors we include in our model.\nThe lecture for this week discusses this concern in relation to the motivation for good open science practices.\n\n\nQ.12. Can you report the estimated effect of SHIPLEY (the measure of vocabulary)?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA.12. The effect of vocabulary knowledge (SHIPLEY) on mean accuracy of understanding is significant (estimate = 0.01, t = 7.30, p &lt; .001), indicating that increasing skill is associated with increasing accuracy.\n\n\n\n\n\nQ.13. Can you report the model and the model fit statistics?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA.13. We fitted a linear model with mean comprehension accuracy as the outcome and health literacy (HLVA), reading strategy (FACTOR3), vocabulary (SHIPLEY) and AGE (in years) as predictors. The model is significant overall, with F(4, 556) = 85.17, p &lt; .001, and explains 38% of variance (adjusted R2 = 0.38).\n\n\n\n\n\n\n\n10.4.2.5 How-to Part 5: Plot predictions from linear models with multiple predictors\n\nHow-to Task 7 – Produce and plot model results using R functions that make the process easier\nWe can begin by plotting linear model predictions for one of the predictors in the multiple predictors model we have been working with.\n\nPreviously, we used geom_abline(), specifying intercept and slope estimates, to produce and plot model predictions.\nHere, we use functions that are very helpful when we need to plot model predictions for a predictor, for models where we have multiple predictors.\n\nWe do this in three steps:\n\nWe first fit a linear model of the outcome, given our predictors, and we save information about the model.\nWe use the ggpredict() function from the {ggeffects} library to take the information about the model and create a set of predictions we can use for plotting.\nWe plot the model predictions, producing what are also called marginal effects plots.\n\nWe write the code to do the work as follows.\n\nWe first fit a linear model of the outcome, given our predictors:\n\n\nmodel &lt;- lm(mean.acc ~ HLVA + SHIPLEY + FACTOR3 + AGE, \n                         data = all.studies.subjects)\n\n\nmodel &lt;- lm(...) – we fit the model using lm(...), giving the model a name: “model”.\n...lm(mean.acc ~ HLVA + ... + ...) – this model estimates how the outcome mean.acc changes, is predicted ~ given information about the predictors HLVA, SHIPLEY, FACTOR3, and AGE.\nNotice: when we use lm() to fit the model, R creates a set of information about the model, including estimates.\nWe give that set of information a name (“model”), and we use that name, next, to call on and use the model information to make the plot.\n\n\nWe use the ggpredict() function from the {ggeffects} library to take the information about the model and create a set of predictions we can use for plotting:\n\n\ndat &lt;- ggpredict(model, \"HLVA\")\n\n\ndat &lt;- ggpredict(...) – we ask R to create a set of predictions, and we give that set of predictions a name dat.\n... ggpredict(model, \"HLVA\") – we tell R what model information it should use (from “model”), and which predictor variable we need predictions for: HLVA.\n\n\nWe plot the model predictions (marginal effects plots):\n\n\nplot(dat)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFurther information you can explore\n\n\n\nIn producing prediction plots, we are using functions from the {ggeffects} library. Can you locate online information about working with the library functions?\n\nTry doing a search with the key words: in r ggeffects.\n\nIf you do that, you will see links to the website:\nhttps://strengejacke.github.io/ggeffects/\nIn the {ggeffects} online information, you can see links to practical examples. Can you use the information under Practical examples to adjust the appearance of the prediction plots?\n\n\n\n\nHow-to Task 8 – Optional – Edit the appearance of the marginal effect (prediction) plot as you can with any ggplot object\nOnce we have created the model predictions plot, we can edit it like we would edit any other ggplot object.\n\nThis is clearly advanced but it helps you to see just how powerful your capabilities can be if you aim to develop skills in this kind of work.\n\n\np.model &lt;- plot(dat)\np.model +\n  geom_point(data = all.studies.subjects, \n             aes(x = HLVA, y = mean.acc), size = 1.5, alpha = .75, colour = \"lightgreen\") +\n  geom_line(linewidth = 1.5) +\n  ylim(0, 1.1) + xlim(0, 16) +\n  theme_bw() +\n  theme(\n    axis.text = element_text(size = rel(1.15)),\n    axis.title = element_text(size = rel(1.25)),\n    plot.title = element_text(size = rel(1.4))\n  ) +\n  xlab(\"Health literacy (HLVA)\") + ylab(\"Mean accuracy\") +\n  ggtitle(\"Effect of health literacy on mean comprehension accuracy\")\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n\n\nWarning: Removed 54 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nWe do this in stages, as we have done for other kinds of plots:\n\np.model &lt;- plot(dat) – we create a plot object, which we call ‘p.model’.\np.model + – we then set up the first line of a series of code lines, starting with the name of the plot, p.model and a + to show we are going to add some edits.\ngeom_point(data = all.studies.subjects, aes(x = HLVA, y = mean.acc) ...) – we first add the raw data points showing the observed HLVA and mean.acc for each person in our sample.\ngeom_point(... size = 1.5, alpha = .75, colour = \"lightgreen\") + – we modify the appearance of the points, then\ngeom_line(size = 1.5) + – we add the prediction line, using the predictions created earlier, then\nylim(0, 1.1) + xlim(0, 16) + – we set axis limits to show the full potential range of variation in each variable, then\ntheme_bw() – we set the theme to black and white, then\ntheme(axis.text = element_text(size = rel(1.15)), ...) – we modify the relative size of x-axis, y-axis and plot title label font, then\nxlab(\"Health literacy (HLVA)\") + ylab(\"Mean accuracy\") + – we edit labels to make them easier to understand, then\nggtitle(\"Effect of health literacy on mean comprehension accuracy\") – we give the plot a title\n\nNotice that we are constructing a complex plot from two datasets:\n\nWe use plot(dat) to construct a plot from (1.) the set of predictions, showing the prediction line using geom_line(size = 1.5) +\nWe then add a layer to the plot to show (2.) the raw sample data, using geom_point(data = all.studies.subjects, ...) – to tell R to work with the sample data-set all.studies.subjects, and to show points representing the (aesthetic mappings) values, given that sample data, for (x = HLVA, y = mean.acc).\n\nWhat are we learning here?\nThis complex plot is a good example of:\n\nHow we can plot summary of prediction data plus observed outcomes, and this connects to the classes on visualization.\nHow we can add layers of edits, or geometric objects, together to create complex plots that can show our audience messages like, here, that there is an average trend (shown by the line) but that individual outcomes vary quite a bit from the average.\n\n\n\n\n\n\n\nTip\n\n\n\nNo model will be perfect.\n\nIt is often useful to compare model estimates – here, the slopes – with the raw data observations.\nVisualizing the difference between what we observe and what we predict helps to make sense of how our prediction model could be improved.\n\n\n\n\n\nHow-to Task 9 – Optional – Now produce plots that show the predictions for all the predictor variables in the model\nThis is optional but we offer the task as an opportunity for practice with these notes.\n\nThe code may get pretty lengthy.\nYou will need to adjust axis labels so for each plot we see the correct predictor as the x-axis label\nYou will give the plot titles as letters a-e so that, if you put this plot in a report, you can refer to each plot by letter in comments in a report.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nWe will use the {Patchwork} library:\n\nlibrary(patchwork)\n\nFirst, fit the model and get the model information:\n\nmodel &lt;- lm(mean.acc ~ HLVA + SHIPLEY + FACTOR3 + AGE, \n                         data = all.studies.subjects)\n\nSecond, get the predictions, given the model information, for two different predictor variables.\n\nNotice that I use different names for different sets of predictors.\n\n\ndat.HLVA &lt;- ggpredict(model, \"HLVA\")\ndat.SHIPLEY &lt;- ggpredict(model, \"SHIPLEY\")\n\nThird, produce plots of the predictions, giving each plot object a different plot name.\n\nNotice that I use different names for different sets of predictors.\n\n\nplot.HLVA &lt;- plot(dat.HLVA)\nplot.SHIPLEY &lt;- plot(dat.SHIPLEY)\n\nLast, put the two plots together by calling their names.\n\nplot.HLVA + plot.SHIPLEY\n\n\n\n\n\n\n\n\nThe {Patchwork} library functions are really powerful and the documentation information is really helpful:\nhttps://patchwork.data-imaginist.com/articles/patchwork.html\n\n\n\nWhat are we learning here?\nIf you follow the code example, and learn how to:\n\ncomplete a linear model analysis with multiple predictors and\npresent the model results, for all effects (estimates of the impact of all predictors) simultaneously\n\n\nthen this is a big deal, as a skill development learning achievement\nbecause knowing about this process, and knowing how to do it, unlocks an important advanced analysis and visualization skill.\n\n\n\n\n\n\n\nTip\n\n\n\nKnowing how to follow this model \\(\\rightarrow\\) visualization workflow gives you a competitive advantage in future professional settings (job applications, interviews, workplace) because not many Psychology graduates can do it, yet.\n\n\n\n\n\n10.4.2.6 How-to Part 6: New: estimate the effects of factors as well as numeric variables\nAs we have seen, one of our research questions is:\n\nWhat person attributes predict success in understanding?\n\nWe have not yet included any categorical or nominal variables as predictors but we can, and should: lm() can cope with any kind of variable as a predictor.\n\n\n\n\n\n\nTip\n\n\n\nIn Part 4 of the Week 10 lectures, I explain that fundamentally every kind of analysis you have learnt before (t-test, ANOVA, etc.) can be understood as a kind of linear model. This part shows how that works.\n\nIn a (two groups) t-test, you are comparing the average outcome between two different groups.\nTo do that analysis, you need an outcome variable (like mean.acc) and a categorical variable or factor (like, say, experimental condition: condition 1 versus condition 2).\nHere, we see how you can do the same kind of group comparison using a linear model.\n\n\n\nThere are different ways to do this, here we look at one. Extra information about the second can be found in the drop-down box, following.\n\nHow-to Task 10 – Fit a linear model including both numeric variables and categorical variables as predictors model including both numeric variables and categorical variables as predictors\nWe begin by inspecting the data to check what variables are categorical or nominal variables – factors – using summary().\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nsummary(all.studies.subjects)\n\n  ResponseId           mean.acc       mean.self          AGE        \n Length:615         Min.   :0.150   Min.   :1.000   Min.   : 18.00  \n Class :character   1st Qu.:0.600   1st Qu.:5.600   1st Qu.: 20.00  \n Mode  :character   Median :0.800   Median :6.800   Median : 23.00  \n                    Mean   :0.734   Mean   :6.529   Mean   : 28.34  \n                    3rd Qu.:0.900   3rd Qu.:7.800   3rd Qu.: 30.50  \n                    Max.   :1.000   Max.   :9.000   Max.   :100.00  \n                    NA's   :54      NA's   :54                      \n    GENDER           EDUCATION          ETHNICITY            SHIPLEY     \n Length:615         Length:615         Length:615         Min.   : 0.00  \n Class :character   Class :character   Class :character   1st Qu.:25.00  \n Mode  :character   Mode  :character   Mode  :character   Median :32.00  \n                                                          Mean   :28.98  \n                                                          3rd Qu.:36.00  \n                                                          Max.   :40.00  \n                                                                         \n      HLVA           FACTOR3      NATIVE.LANGUAGE       study          \n Min.   : 0.000   Min.   : 0.00   Length:615         Length:615        \n 1st Qu.: 5.000   1st Qu.:41.00   Class :character   Class :character  \n Median : 8.000   Median :48.00   Mode  :character   Mode  :character  \n Mean   : 7.224   Mean   :44.03                                        \n 3rd Qu.: 9.500   3rd Qu.:53.00                                        \n Max.   :13.000   Max.   :63.00                                        \n                                                                       \n\n\n\n\n\nsummary() gives you the information you need because R shows factors with a count of the number of observations for each level.\nWe can build on our previous work by including the factor NATIVE.LANGUAGE as a predictor:\n\nmodel &lt;- lm(mean.acc ~ HLVA + SHIPLEY + FACTOR3 + AGE + NATIVE.LANGUAGE, \n            data = all.studies.subjects)\nsummary(model)\n\n\nCall:\nlm(formula = mean.acc ~ HLVA + SHIPLEY + FACTOR3 + AGE + NATIVE.LANGUAGE, \n    data = all.studies.subjects)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.55939 -0.08115  0.02056  0.10633  0.41598 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           0.1873086  0.0472991   3.960 8.47e-05 ***\nHLVA                  0.0242787  0.0031769   7.642 9.44e-14 ***\nSHIPLEY               0.0073947  0.0011144   6.635 7.70e-11 ***\nFACTOR3               0.0053455  0.0008947   5.975 4.12e-09 ***\nAGE                  -0.0026434  0.0004905  -5.390 1.05e-07 ***\nNATIVE.LANGUAGEOther -0.0900035  0.0141356  -6.367 4.04e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1612 on 555 degrees of freedom\n  (54 observations deleted due to missingness)\nMultiple R-squared:  0.4221,    Adjusted R-squared:  0.4169 \nF-statistic: 81.09 on 5 and 555 DF,  p-value: &lt; 2.2e-16\n\n\nNow, take a look at the results to answer the following questions.\n\nQ.14. Can you report the estimated effect of NATIVE.LANGUAGE (the coding of participant language status: English versus Other)?\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nA.14. The effect of language status (NATIVE.LANGUAGE) on mean accuracy of understanding is significant (estimate = -0.09, t = -6.37, p &lt; .001) indicating that not being a native speaker of English (‘Other’) is associated with lower accuracy.\n\n\n\n\n\nQ.15. Can you report the model and the model fit statistics?\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nA.15. We fitted a linear model with mean comprehension accuracy as the outcome and health literacy (HLVA), reading strategy (FACTOR3), vocabulary (SHIPLEY) and AGE (years), as well as language status as predictors. The model is significant overall, with F(5, 555) = 81.09, p &lt; .001, and explains 42% of variance (adjusted R2 = 0.42).\n\n\n\n\n\nQ.16. What changes, when you compare the models with versus without NATIVE.LANGUAGE?\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nA.16. If you compare the summaries, for the last two models, you can see that the proportion of variance explained, R-sq, increases to 42% (0.4221), suggesting that knowing about participant language background helps to account for their response accuracy in tests of comprehension of health advice.\n\n\n\n\nWhat are we learning here?\nWhat are factors?\n\nA factor is a categorical variable.\nCategorical variables have a “fixed and known set of possible values”, for example, maybe you are working with the factor EDUCATION: in the data collection process, answers to the question “What is your education level?” can have only one of three possible values (further, higher, secondary).\nFor any factor, different values of the variable are called levels so, for EDUCATION the different levels are: further, higher, secondary\n\nR treats factors differently from other kinds of variables:\n\nR will not give you summary statistics (e.g., a mean) for values of a factor. It can’t because: what is the mean of education?\nR will count the number of times each level in a factor appears in a sample.\nIf you are plotting a bar chart or a boxplot to show outcomes for different groups or conditions then the group or condition coding variable (e.g., aes(x = group)) has to be a factor.\n\nR handles factors, by default, by picking one level (e.g., English) as the reference level (or baseline) and comparing outcomes to that baseline, for each other factor level (here, Other).\n\nThis is why, in the summary of estimates for this model, the effect of NATIVE.LANGUAGE is estimated as the average difference in mean.acc outcome for English compared to Other participants.\nThis is why the effect is listed as: NATIVE.LANGUAGEOther in the summary table for the model\n\n\n\n\n\n\n\nFurther information you can explore\n\n\n\nNote that I quote, here, from the powerful {forcats} library of tools for working with factors:\nhttps://forcats.tidyverse.org/\nYou can read more about factor coding schemes here:\nhttps://talklab.psy.gla.ac.uk/tvw/catpred/\nand here:\nhttps://phillipalday.com/stats/coding.html\n\n\n\n\n\n\n\n\nExtra information for students interested in ANOVAs\n\n\n\n\n\nThere are different ways to code factors for analysis:\n\nIf you are doing an analysis where your data come from, say, a factorial design (e.g. a 2 x 2 study design) then you will want to use a different coding scheme: sum or effect coding.\n\nIt is easy to do this, in two steps, proceeding as follows.\n\nWe first change the coding scheme.\n\n\nGet the {memisc} library:\n\n\nlibrary(memisc)\n\n\nMake sure that the variable is being treated as a factor:\n\n\nall.studies.subjects$NATIVE.LANGUAGE &lt;- as.factor(all.studies.subjects$NATIVE.LANGUAGE)\n\n\nCheck the coding:\n\n\ncontrasts(all.studies.subjects$NATIVE.LANGUAGE)\n\n        Other\nEnglish     0\nOther       1\n\n\n\nChange the coding\n\n\ncontrasts(all.studies.subjects$NATIVE.LANGUAGE) &lt;- contr.sum(2, base = 1)\n\nThe arguments in contr.sum(2, base = 1) tell R which level we want to be the reference level.\n\nCheck the coding to show you got what you want:\n\n\ncontrasts(all.studies.subjects$NATIVE.LANGUAGE)\n\n         2\nEnglish -1\nOther    1\n\n\n\nWe then fit a linear model of the outcome, given our predictors:\n\n\nmodel &lt;- lm(mean.acc ~ HLVA + SHIPLEY + FACTOR3 + AGE + NATIVE.LANGUAGE, \n            data = all.studies.subjects)\nsummary(model)\n\n\nCall:\nlm(formula = mean.acc ~ HLVA + SHIPLEY + FACTOR3 + AGE + NATIVE.LANGUAGE, \n    data = all.studies.subjects)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.55939 -0.08115  0.02056  0.10633  0.41598 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       0.1423068  0.0457437   3.111  0.00196 ** \nHLVA              0.0242787  0.0031769   7.642 9.44e-14 ***\nSHIPLEY           0.0073947  0.0011144   6.635 7.70e-11 ***\nFACTOR3           0.0053455  0.0008947   5.975 4.12e-09 ***\nAGE              -0.0026434  0.0004905  -5.390 1.05e-07 ***\nNATIVE.LANGUAGE2 -0.0450018  0.0070678  -6.367 4.04e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1612 on 555 degrees of freedom\n  (54 observations deleted due to missingness)\nMultiple R-squared:  0.4221,    Adjusted R-squared:  0.4169 \nF-statistic: 81.09 on 5 and 555 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\nOptional.Q.1. What changes, when you compare the models with versus without sum coding of NATIVE.LANGUAGE?\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nOptional.A.1. If you compare the summaries, for the last two models, you can see that the estimate for the coefficient of NATIVE.LANGUAGE changes in two ways:\n\n\n\nthe name changes, from NATIVE.LANGUAGEOther to NATIVE.LANGUAGE2, reflecting the change in coding;\n\n\n\n\nthe slope estimate changes, from -0.0900035 to -0.0450018.\n\n\n\nNote: the change in the estimate happens because we go from estimating the average difference in level between 0 (English) versus 1 (Other), a change of one unit to estimating the average difference in level between -1 (English) versus 1 (Other), a change of 2 units.\n\nBeing able to change the coding of nominal or categorical variables is very useful. And enables you to do ANOVA style analyses given factorial study designs e.g.\n\nmodel &lt;- aov(lm(mean.acc ~ HLVA + SHIPLEY + FACTOR3 + AGE + NATIVE.LANGUAGE, \n       data = all.studies.subjects))\nsummary(model)\n\n                 Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nHLVA              1  5.753   5.753  221.47  &lt; 2e-16 ***\nSHIPLEY           1  2.272   2.272   87.44  &lt; 2e-16 ***\nFACTOR3           1  0.751   0.751   28.92 1.11e-07 ***\nAGE               1  0.703   0.703   27.06 2.78e-07 ***\nNATIVE.LANGUAGE   1  1.053   1.053   40.54 4.04e-10 ***\nResiduals       555 14.418   0.026                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n54 observations deleted due to missingness\n\n\nNotice: we use aov() to get an ANOVA summary for the model.\nAs before, you can fit a linear model including both numeric variables and categorical variables as predictors: and then plot the predicted effect of the factor.\n\nWe first fit the model, including NATIVE.LANGUAGE then use the ggpredict() function to get the predictions.\n\n\ndat &lt;- ggpredict(model, \"NATIVE.LANGUAGE\")\nplot(dat)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.4.2.7 How-to Part 7: New: examine associations comparing data from different samples\nThe lecture for developing the linear model includes a discussion of the ways in which the observed associations between variables or the estimated effects of predictor variables on some outcome may differ between different studies, different samples of data.\nTo draw the plots, I used the {ggplot2} library function facet_wrap():\n\nall.studies.subjects %&gt;%\n  ggplot(aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point(size = 2, alpha = .5, colour = \"darkgrey\") +\n  geom_smooth(size = 1.5, colour = \"red\", method = \"lm\", se = FALSE) +\n  xlim(0, 40) +\n  ylim(0, 1.1)+\n  theme_bw() +\n  theme(\n    axis.text = element_text(size = rel(1.15)),\n    axis.title = element_text(size = rel(1.5))\n  ) +\n  xlab(\"Vocabulary (Shipley)\") + ylab(\"Mean accuracy\") +\n  facet_wrap(~ study)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 54 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 54 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nWhat is new here is this bit: facet_wrap(~ study)\nThis is how it works:\n\nfacet_wrap() – the function asks R to take the data-set and split it into subsets.\nfacet_wrap(~ study) – tells the function to split the data-set according to the different levels of a named factor: study.\n\nSo: you need to identify a factor to do this work.\n\nHow-to Task 11 – Optional – Fit a linear model and plot the model predictions, for different study samples, all together, to enable comparisons of effects between studies\nChange the factor in facet_wrap() to show how the vocabulary effect may vary between English monolinguals versus non-native speakers of English.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nall.studies.subjects %&gt;%\n  ggplot(aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point(size = 2, alpha = .5, colour = \"darkgrey\") +\n  geom_smooth(size = 1.5, colour = \"red\", method = \"lm\", se = FALSE) +\n  xlim(0, 40) +\n  ylim(0, 1.1)+\n  theme_bw() +\n  theme(\n    axis.text = element_text(size = rel(1.15)),\n    axis.title = element_text(size = rel(1.5))\n  ) +\n  xlab(\"Vocabulary (Shipley)\") + ylab(\"Mean accuracy\") +\n  facet_wrap(~ NATIVE.LANGUAGE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 54 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 54 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\nWhat are we learning here?\nWe can work with factors to split data-sets up by group or condition, or combinations of groups or conditions. We can then compare patterns, like outcomes, or the ways that outcomes are affected by predictors, in different groups or in different combinations.\n\n\n\n\n\n\nImportant\n\n\n\nBut the critical lesson is that, when we visualize trends or differences between groups or, as here, between different studies, then we often see that the effects of variables varies between samples.\n\nThis variation between samples in the effects we observe is a key insight into what working with people involves: variation.\n\n\n\n\n\n\n\n\n\nFurther information you can explore\n\n\n\nYou can read more about faceting here:\nhttps://ggplot2.tidyverse.org/reference/facet_wrap.html\n\n\n\n\n\n10.4.3 The practical exercises\nNow you will progress through a series of tasks, and challenges, to test what you have learnt.\n\n\n\n\n\n\nWarning\n\n\n\nWe will continue to work with the data file\n\n2022-12-08_all-studies-subject-scores.csv\n\n\n\nWe again split the steps into into parts, tasks and questions.\nWe are going to work through the following workflow steps: each step is labelled as a practical part.\n\nSet-up\nLoad the data\nRevision: using a linear model to answer research questions – one predictor.\nNew: using a linear model to answer research questions – multiple predictors.\nNew: plot predictions from linear models.\nNew: estimate the effects of factors as well as numeric variables.\nNew: examine associations comparing data from different samples.\n\n\n\n\n\n\n\nTip\n\n\n\n\nThe how-to guide showed you what functions you needed and how you should write the function code.\nUse the code structure you have seen and change it to use the data required for these practical exercises: you will need to change the data-set name, the variable names, to get the code to work for the following exercises.\nIdentify what code elements must change, and what code elements have to stay the same.\n\n\n\nNote that, for Part 5, we focus on using the ggpredict() tools you saw in the how-to guide but, for the practical exercises, use these tools to better understand what linear models (generally) do when we use them.\nIn the following, we will guide you through the tasks and questions step by step.\n\n\n\n\n\n\nImportant\n\n\n\nAn answers version of the workbook will be provided after the practical class.\n\n\n\n\n10.4.3.1 Practical Part 1: Set-up\nTo begin, we set up our environment in R.\n\nPractical Task 1 – Run code to empty the R environment\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nrm(list=ls())\n\n\n\n\n\n\nPractical Task 2 – Run code to load relevant libraries\nNotice that in Week 10 we need to work with the libraries ggeffects, patchwork and tidyverse. Use the library() function to make these libraries available to you.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nlibrary(\"ggeffects\")\nlibrary(\"patchwork\")\nlibrary(\"psych\")\nlibrary(\"tidyverse\")\n\n\n\n\n\n\n\n10.4.3.2 Practical Part 2: Load the data\n\nPractical Task 3 – Read in the data file we will be using\nThe data file is called:\n\n2022-12-08_all-studies-subject-scores.csv\n\nUse the read_csv() function to read the data files into R:\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nall.studies.subjects &lt;- read_csv(\"2022-12-08_all-studies-subject-scores.csv\")\n\nRows: 615 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): ResponseId, GENDER, EDUCATION, ETHNICITY, NATIVE.LANGUAGE, study\ndbl (6): mean.acc, mean.self, AGE, SHIPLEY, HLVA, FACTOR3\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\nPractical Task 4 – Get summary statistics for only the numeric variables: AGE and HLVA\nUse the {psych} library describe() function to get a table summary of descriptive statistics for these variables.\n\n\n\n\n\n\nCode\n\n\n\n\n\nHere, we can do this in two steps:\n\nwe select the variables we care about\nwe get just the descriptive statistics we want for those variables\n\n\nall.studies.subjects %&gt;%\n  dplyr::select(AGE, HLVA) %&gt;%\n  describe(skew = FALSE)\n\n     vars   n  mean    sd median min max range   se\nAGE     1 615 28.34 13.56     23  18 100    82 0.55\nHLVA    2 615  7.22  3.09      8   0  13    13 0.12\n\n\n\n\n\n\nPract.Q.1. What is the mean health literacy (HLVA) score in this sample?\n\n\nPract.A.1. 7.22\n\n\nPract.Q.2. What are the minimum and maximum ages in this sample?\n\n\nPract.A.2. 18-100\n\n\nPract.Q.3. Do you see any reason to be concerned about the data in this sample?\n\n\nHint: It is always a good idea to use your visualization skills to examine the distribution of variable values in a dataset:\n\n\nall.studies.subjects %&gt;%\n  ggplot(aes(x = AGE)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\n\nPract.A.3. It looks like the AGE of 100 is maybe an error, or an extreme outlier.\n\n\n\n\n10.4.3.3 Practical Part 3: Revision – using a linear model to answer research questions – one predictor.\nOne of our research questions is:\n\nCan people accurately evaluate whether they correctly understand written health information?\n\nWe can address this question by examining whether someone’s rated evaluation of their own understanding matches their performance on a test of that understanding, and by investigating what variables predict variation in mean self-rated accuracy.\n\n\n\n\n\n\nTip\n\n\n\nNote that ratings of accuracy are ordinal (categorical ordered) data but that, here, we may choose to examine the average of participants’ ratings of their own understanding of health information for the purposes of this example.\n\nWe learn about ordinal models for ordinal data in PSYC412.\n\n\n\nFor these data, participants were asked to respond to questions about health information to get mean.acc scores and were asked to rate their own understanding of the same information.\n\nIf you can evaluate your own understanding then ratings of understanding should be associated with performance on tests of understanding.\n\n\nPractical Task 5 – Estimate the relation between outcome mean self-rated accuracy (mean.self) and tested accuracy of understanding (mean.acc)\nWe can use lm() to estimate whether the ratings of accuracy actually predict the outcome tested accuracy levels.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nmodel &lt;- lm(mean.acc ~ mean.self, data = all.studies.subjects)\nsummary(model)\n\n\nCall:\nlm(formula = mean.acc ~ mean.self, data = all.studies.subjects)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.58815 -0.09437  0.03841  0.13530  0.34080 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.351300   0.030683   11.45   &lt;2e-16 ***\nmean.self   0.058613   0.004544   12.90   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1855 on 559 degrees of freedom\n  (54 observations deleted due to missingness)\nMultiple R-squared:  0.2294,    Adjusted R-squared:  0.228 \nF-statistic: 166.4 on 1 and 559 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nIf you first fit the appropriate model, then read the model summary, you can answer the following questions.\n\nPract.Q.4. What is the estimate for the coefficient of the effect of the predictor mean.self on the outcome mean.acc in this model?\n\n\nPract.A.4. 0.058613\n\n\nPract.Q.5. Is the effect significant?\n\n\nPract.A.5. It is significant, p &lt; .05\n\n\nPract.Q.6. What are the values for t and p for the significance test for the coefficient?\n\n\nPract.A.6. t = 12.90, p &lt;2e-16\n\n\nPract.Q.7. What do you conclude is the answer to the research question, given the linear model results?\n\n\nPract.A.7. The model slope estimate suggests that higher levels of rated understanding can predict higher levels of tested understanding so, yes: it does appear that people can evaluate their own understanding.\n\n\nPract.Q.8. What is the F-statistic for the regression? Report F, DF and the p-value.\n\n\nPract.A.8. F-statistic: 166.4 on 1 and 559 DF, p-value: &lt; 2.2e-16\n\n\nPract.Q.9. Is the regression significant?\n\n\nPract.A.9. Yes: the regression is significant.\n\n\nPract.Q.10. What is the Adjusted R-squared?\n\n\nPract.A.10. Adjusted R-squared: 0.228\n\n\nPract.Q.11. Explain in words what this R-squared value indicates?\n\n\nPract.A.11. The R-squared suggests that 23% of outcome variance can be explained by the model\n\n\n\n\n10.4.3.4 Practical Part 4: New – using a linear model to answer research questions – multiple predictors\nOne of our research questions is:\n\nCan people accurately evaluate whether they correctly understand written health information?\n\nWe have already looked at this question by asking whether ratings of understanding predict performance on tests of understanding. But there is a problem with that analysis – it leaves open the question:\n\nWhat actually predicts ratings of understanding?\n\nWe can look at that follow-up question through the analysis we do next.\n\nPractical Task 6 – Examine the relation between outcome mean self-rated accuracy (mean.self) and multiple predictors including: health literacy (HLVA); vocabulary (SHIPLEY); AGE; reading strategy (FACTOR3); as well as mean.acc\nWe use lm(), as before, but now specify each variable listed here by variable name\n\noutcome: mean self-rated accuracy (mean.self)\npredictors: health literacy (HLVA); vocabulary (SHIPLEY); AGE; reading strategy (FACTOR3); as well as mean.acc.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nmodel &lt;- lm(mean.self ~ HLVA + SHIPLEY + FACTOR3 + AGE + mean.acc,\n            data = all.studies.subjects)\nsummary(model)\n\n\nCall:\nlm(formula = mean.self ~ HLVA + SHIPLEY + FACTOR3 + AGE + mean.acc, \n    data = all.studies.subjects)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.4479 -0.8585  0.1459  0.9327  4.6791 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.756121   0.400363   1.889   0.0595 .  \nHLVA        0.066543   0.029368   2.266   0.0238 *  \nSHIPLEY     0.012512   0.010188   1.228   0.2199    \nFACTOR3     0.062333   0.008079   7.715 5.64e-14 ***\nAGE         0.002707   0.004417   0.613   0.5403    \nmean.acc    2.502024   0.361094   6.929 1.18e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.42 on 555 degrees of freedom\n  (54 observations deleted due to missingness)\nMultiple R-squared:  0.3279,    Adjusted R-squared:  0.3219 \nF-statistic: 54.17 on 5 and 555 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nIf you look at the model summary you can answer the following questions\n\nPract.Q.12. What predictors are significant in this model?\n\n\nPract.A.12. Health literacy (‘HLVA’), reading strategy (‘FACTOR3’) and performance on tests of accuracy of understanding (‘mean.acc’) all appear to significantly predict variation in mean ratings of understanding (‘mean.self’).\n\n\nPract.Q.13. What is the estimate for the coefficient of the effect of the predictor, mean.acc, in this model?\n\n\nPract.A.13. 2.502024\n\n\nPract.Q.14. Is the effect significant?\n\n\nPract.A.14. It is significant because p &lt; .05\n\n\nPract.Q.15. What are the values for t and p for the significance test for the coefficient?\n\n\nPract.A.15. t = 6.929, p = 1.18e-11\n\n\nPract.Q.16. What do you conclude is the answer to the follow-up question, what actually predicts ratings of understanding?\n\n\nPract.A.16. Ratings of understanding appear to be predicted by performance on tests of accuracy of understanding, together with variation in health literacy and reading strategy.\n\n\n\n\n10.4.3.5 Practical Part 5: New – plot predictions from linear models\nIn this part, the practical work involves plotting the predictions from two linear models but the benefit from this practical work is to better your understanding of what linear models are doing.\nSo, our research questions is:\n\nCan people accurately evaluate whether they correctly understand written health information?\n\nWe can look at this question in two ways:\n\nwe can focus on whether mean.self predicts mean.acc?\nor, in reverse, whether mean.acc predicts mean.self?\n\nPeople could (1.) be good at understanding accurately what they read (measured as mean.acc), so that that accurate understanding results in accurate evaluation of their own understanding (measured as mean.self). Or people could (2.) be good at judging their own understanding (mean.self) so that they can then be accurate in their understanding of what they read. This is not just a tricksy way of talking: it could be that people who are good at evaluating if they understand what they read are able to work more effectively at that understanding, or the opposite could be the case.\nThe direction of cause-and-effect is just unclear, here.\n\nPractical Task 7 – We cannot actually resolve the question of causality but we can learn a lot if we look at two models incorporating the same variables\n\nIf we focus on whether (predictor) mean.self predicts (outcome) mean.acc then the model should be: mean.acc ~ mean.self\nIf we focus on whether (predictor) mean.acc predicts (outcome) mean.self then the model should be: mean.self ~ mean.acc\n\nThe two models involve the same variables but arrange the variables differently, in the code, depending on which variable is specified as the outcome versus which variable is specified as the predictor.\n\n\n\n\n\n\nTip\n\n\n\nNote that the comparison between these models teaches us something about what it is that linear models predict.\n\nYou want to focus on the estimate of the coefficient of the predictor variable, for each model, presented in the model summary results.\n\n\n\n\nPract.Q.17. Can you write the code for the two models? Can you give each model a distinctive name?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nmodel.1 &lt;- lm(mean.acc ~ mean.self, data = all.studies.subjects)\nsummary(model.1)\n\n\nCall:\nlm(formula = mean.acc ~ mean.self, data = all.studies.subjects)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.58815 -0.09437  0.03841  0.13530  0.34080 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.351300   0.030683   11.45   &lt;2e-16 ***\nmean.self   0.058613   0.004544   12.90   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1855 on 559 degrees of freedom\n  (54 observations deleted due to missingness)\nMultiple R-squared:  0.2294,    Adjusted R-squared:  0.228 \nF-statistic: 166.4 on 1 and 559 DF,  p-value: &lt; 2.2e-16\n\nmodel.2 &lt;- lm(mean.self ~ mean.acc,\n              data = all.studies.subjects)\nsummary(model.2)\n\n\nCall:\nlm(formula = mean.self ~ mean.acc, data = all.studies.subjects)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.2003 -0.7917  0.2040  0.9781  3.9737 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.6565     0.2317   15.78   &lt;2e-16 ***\nmean.acc      3.9135     0.3034   12.90   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.515 on 559 degrees of freedom\n  (54 observations deleted due to missingness)\nMultiple R-squared:  0.2294,    Adjusted R-squared:  0.228 \nF-statistic: 166.4 on 1 and 559 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nTake a look at the summary for each model.\n\nPract.Q.18. Why do you think it appears that the slope coefficient estimate is different if you compare:\n\n\n(1.) the model, mean.acc ~ mean.self\n\n\nversus\n\n\n(2.) the model, mean.self ~ mean.acc?\n\n\nHint: You may benefit by reflecting on the lectures and practical materials in the chapter for week 8, especially where they concern predictions.\n\n\nPract.A.18. Linear models are prediction models. We use them to predict variation in outcomes given some set of predictor variables. Because of this, predictions must be scaled in the same way as the outcome variable.\n\n\nSo:\n\n\n(1.) the model mean.acc ~ mean.self identifies mean.acc as the outcome so if we are predicting change in mean.acc (scaled 0-1) then we are looking at coefficients that will lie somewhere on the same scale (also 0-1).\n\n\nHere: the model suggests that unit change in mean.self predicts increase of 0.058613 in mean.acc.\n\n\nVersus:\n\n\n(2.) the model, mean.self ~ mean.acc identifies mean.self as the outcome so if we are predicting change in mean.self (scaled 1-9) then we are looking at coefficients that will lie somewhere on the same scale (also 1-9).\n\n\nHere: the model suggests that unit change in mean.acc predicts increase of 3.9135 in mean.self.\n\n\nPract.Q.19. Can you plot the predictions from each model?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nPract.A.19. Here is the code to plot the predictions from both models:\n\nFirst fit the models – give the model objects distinct names:\n\nmodel.1 &lt;- lm(mean.acc ~ mean.self, data = all.studies.subjects)\nsummary(model.1)\n\n\nCall:\nlm(formula = mean.acc ~ mean.self, data = all.studies.subjects)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.58815 -0.09437  0.03841  0.13530  0.34080 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.351300   0.030683   11.45   &lt;2e-16 ***\nmean.self   0.058613   0.004544   12.90   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1855 on 559 degrees of freedom\n  (54 observations deleted due to missingness)\nMultiple R-squared:  0.2294,    Adjusted R-squared:  0.228 \nF-statistic: 166.4 on 1 and 559 DF,  p-value: &lt; 2.2e-16\n\nmodel.2 &lt;- lm(mean.self ~ mean.acc,\n            data = all.studies.subjects)\nsummary(model.2)\n\n\nCall:\nlm(formula = mean.self ~ mean.acc, data = all.studies.subjects)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.2003 -0.7917  0.2040  0.9781  3.9737 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.6565     0.2317   15.78   &lt;2e-16 ***\nmean.acc      3.9135     0.3034   12.90   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.515 on 559 degrees of freedom\n  (54 observations deleted due to missingness)\nMultiple R-squared:  0.2294,    Adjusted R-squared:  0.228 \nF-statistic: 166.4 on 1 and 559 DF,  p-value: &lt; 2.2e-16\n\n\nThen get the predictions:\n\ndat.1 &lt;- ggpredict(model.1, \"mean.self\")\ndat.2 &lt;- ggpredict(model.2, \"mean.acc\")\n\nThen make the plots, and arrange them for easy comparison, side-by-side:\n\nplot.1 &lt;- plot(dat.1)\nplot.2 &lt;- plot(dat.2)\n\nplot.1 + plot.2\n\n\n\n\n\n\n\n\n\n\n\n\nPract.Q.20. Look at the two plots side-by-side: what do you see?\n\nHint: Look at changes in height of the prediction line, given changes in x-axis position of the line.\n\nPract.A.20. A side-by-side comparison shows that for (1.) mean.acc ~ mean.self, increases in mean.self from about 2-8 are associated with a change in mean.acc from about .4 to about .85, while for (2.) mean.self ~ mean.acc, increases in mean.self from about .2-1.0 are associated with a change in mean.acc from about 4 to about 8.\n\nWhat are we learning here?\nLinear models are prediction models. We use them to predict variation in outcomes given some set of predictor variables.\n\n\n\n\n\n\nImportant\n\n\n\nPredictions must be scaled in the same way as the outcome variable.\n\n\n\n\n\n10.4.3.6 Practical Part 6: New – estimate the effects of factors as well as numeric variables\n\nPractical Task 8 – Fit a linear model to examine what variables predict outcome mean self-rated accuracy of mean.self\nInclude in the model both numeric variables and categorical variables as predictors:\n\nhealth literacy (HLVA); vocabulary (SHIPLEY); AGE; reading strategy (FACTOR3); as well as mean.acc and NATIVE.LANGUAGE.\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nmodel &lt;- lm(mean.self ~ HLVA + SHIPLEY + FACTOR3 + AGE + mean.acc +\n                        NATIVE.LANGUAGE,\n            data = all.studies.subjects)\n\n\n\n\n\nPract.Q.21. Can you report the estimated effect of NATIVE.LANGUAGE (the coding of participant language status: English versus other)?\n\nHint: you will need to get a summary of the model you have coded.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nsummary(model)\n\n\nCall:\nlm(formula = mean.self ~ HLVA + SHIPLEY + FACTOR3 + AGE + mean.acc + \n    NATIVE.LANGUAGE, data = all.studies.subjects)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.3580 -0.8804  0.1250  0.9093  4.7323 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           0.992731   0.421865   2.353   0.0190 *  \nHLVA                  0.063174   0.029377   2.150   0.0319 *  \nSHIPLEY               0.011553   0.010184   1.134   0.2571    \nFACTOR3               0.063973   0.008119   7.880 1.75e-14 ***\nAGE                   0.002039   0.004425   0.461   0.6452    \nmean.acc              2.331519   0.373357   6.245 8.46e-10 ***\nNATIVE.LANGUAGEOther -0.225432   0.128793  -1.750   0.0806 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.418 on 554 degrees of freedom\n  (54 observations deleted due to missingness)\nMultiple R-squared:  0.3316,    Adjusted R-squared:  0.3244 \nF-statistic: 45.82 on 6 and 554 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\nPract.A.21. The effect of language status (NATIVE.LANGUAGE) on mean accuracy of understanding is not significant (estimate = -0.225432, t = -1.750, p = .081) indicating that not being a native speaker of English (Other) is not associated with lower self-rated accuracy.\n\n\nPract.Q.22. Can you report the overall model and model fit statistics?\n\n\nPract.A.22. We fitted a linear model with mean self-rated accuracy as the outcome and with the predictors: health literacy (HLVA), reading strategy (FACTOR3), vocabulary (SHIPLEY) and AGE (years), as well as mean accuracy (mean.acc) and language status. The model is significant overall, with F(6, 554) = 45.82, p &lt; .001, and explains 32% of variance (adjusted R2 = 0.32).\n\n\nPract.Q.23. Can you plot the predicted effect of NATIVE.LANGUAGE given your model?\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nWe first fit the model, including NATIVE.LANGUAGE:\n\nmodel &lt;- lm(mean.self ~ HLVA + SHIPLEY + FACTOR3 + AGE + mean.acc +\n              NATIVE.LANGUAGE,\n            data = all.studies.subjects)\n\nThen use the ggpredict() function to get the predictions\n\ndat &lt;- ggpredict(model, \"NATIVE.LANGUAGE\")\n\nSome of the focal terms are of type `character`. This may lead to\n  unexpected results. It is recommended to convert these variables to\n  factors before fitting the model.\n  The following variables are of type character: `NATIVE.LANGUAGE`\n\nplot(dat)\n\n\n\n\n\n\n\n\n\n\n\n\nPract.Q.24. The plot should give you dot-and-whisker representations of the estimated mean.self for English versus Other participants in the dataset. What is the difference in the estimated mean.self between these groups?\n\nHint: The effect or prediction plot will show you dot-and-whisker representations of predicted outcome mean.self. In these plots, the dots represent the estimated mean.self while the lines (whiskers) represent confidence intervals.\n\nPract.A.24. The difference in the estimated ‘mean.self’ between these groups is about .2.\n\n\nPract.Q.25. Compare the difference in the estimated mean.self between these groups, given the plot, with the coefficient estimate from the model summary: what do you see?\n\n\nPract.A.25. The coefficient estimate = -0.225432. This matches the difference shown in the plot.\n\n\n\n\n10.4.3.7 Practical Part 7: New – examine associations comparing data from different samples\n\nPractical Task 8 – Use facet_wrap() to show how the association between mean.self and mean.acc can vary between the different studies in the data-set\nChange the factor in facet_wrap() to show how the association between mean.self and mean.acc can vary between the different studies\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nall.studies.subjects %&gt;%\n  ggplot(aes(x = mean.self, y = mean.acc)) +\n  geom_point(size = 2, alpha = .5, colour = \"darkgrey\") +\n  geom_smooth(size = 1.5, colour = \"red\", method = \"lm\", se = FALSE) +\n  xlim(0, 10) +\n  ylim(0, 1.1)+\n  theme_bw() +\n  theme(\n    axis.text = element_text(size = rel(1.15)),\n    axis.title = element_text(size = rel(1.5))\n  ) +\n  xlab(\"Mean self-rated accuracy\") + ylab(\"Mean accuracy\") +\n  facet_wrap(~ study)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 54 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 54 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\nYou may need to edit the x-axis labeling to make it readable\n\nCan you work out how to do that, given ggplot() help information?\n\nHint: check out the continuous scale information in:\nhttps://ggplot2.tidyverse.org/reference/scale_continuous.html\n\n\n\n10.4.3.8 Practical Part Optional: New – save or export plots so that you can insert them in reports\nLearning how to export plots from R will help you later with reports.\nThe task is to save or export a plot that you produce, so that you can insert it in a report or presentation.\nThere are different ways to do this, we can look at one simple example.\n\nWe can save the last plot we produce using the {tidyverse} function ggsave():\n\n\nggsave(\"facet-plots.png\")\n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 54 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 54 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nNotice, here, that:\n\nggsave(\"facet-plots...\") – we need to give the plot a name;\nggsave(\"...png\") – and we need to tell R what format we require.\n\nThe plot is saved as a file with the name you specify, in the working directory (folder) you are using.\nR will save the plot in the format you specify: here, I choose .png because .png image files can be imported into Microsoft Word documents easily.\n\nNotice that ggsave() will use pretty good defaults but that you can over-ride the defaults, by adding arguments to specify the plot width, height and resolution (in dpi).\n\n\n\n\n\n\n\nFurther information you can explore\n\n\n\nCheck out reference information here:\nhttps://ggplot2.tidyverse.org/reference/ggsave.html\nand\nhttp://www.cookbook-r.com/Graphs/Output_to_a_file/\n\n\n\n\nPractical Task Optional: Now try it for yourself: make a plot, and save it, using what you have learnt so far\nYou will need to:\n\nFit a model\nCreate a set of predictions we can use for plotting\nMake a plot\nSave it\n\nThen, you can import the saved plot into Word.\n\n\n\n\n\n\nCode\n\n\n\n\n\nFit a model:\n\nmodel &lt;- lm(mean.acc ~ HLVA + SHIPLEY + FACTOR3 + AGE,\n            data = all.studies.subjects)\n\nCreate a set of predictions we can use for plotting\n\ndat &lt;- ggpredict(model, \"FACTOR3\")\n\nMake a plot\n\np.model &lt;- plot(dat)\np.model +\n  geom_point(data = all.studies.subjects,\n             aes(x = FACTOR3, y = mean.acc), size = 1.5, alpha = .75, colour = \"darkgrey\") +\n  geom_line(size = 1.5) +\n  theme_bw() +\n  theme(\n    axis.text = element_text(size = rel(1.15)),\n    axis.title = element_text(size = rel(1.25)),\n    plot.title = element_text(size = rel(1.4))\n  ) +\n  xlab(\"Reading strategy (FACTOR3)\") + ylab(\"Mean accuracy\") +\n  ggtitle(\"Effect of reading strategy on \\n mean comprehension accuracy\")\n\nWarning: Removed 54 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nSave it\n\nggsave(\"reading-strategy-prediction.png\", width = 10, height = 10, units = \"cm\")\n\nWarning: Removed 54 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nSome advice:\n\nIt is often helpful to present plots that are almost square i.e. height about = width.\nThis helps to present the best fit line in scatterplots in a way that is easier to perceive.\nWe may experiment with width-height (aspect ratio) until we get something that “looks” right.\nNotice also that different presentation venues will require different levels if image resolution e.g. journals may require .tiff file plots at dpi = 180 or greater.\n\n\n\n\nThe answers\nAfter the practical class, we will reveal the answers that are currently hidden.\nThe answers version of the webpage will present my answers for questions, and some extra information where that is helpful.\n\n\nLook ahead: growing in independence\n\nGeneral advice\nAn old saying goes:\n\nAll models are wrong but some are useful\n\n(attributed to George Box).\n\n\n\n\n\n\nTip\n\n\n\n\nSometimes, it can be useful to adopt a simpler approach as a way to approximate get closer to better methods\nBox also advises:\n\n\nSince all models are wrong the scientist must be alert to what is importantly wrong. It is inappropriate to be concerned about mice when there are tigers abroad.\n\n\nHere, we focus on validity, measurement, generalizability and critical thinking\n\n\n\n\n\n\nSummary\n\nLinear models\n\n\nLinear models are a very general, flexible, and powerful analysis method\nWe can use assuming that prediction outcomes (residuals) are normally distributed\nWith potentially multiple predictor variables\n\n\nThinking about linear models\n\n\nClosing the loop: when we plan an analysis we should try to use contextual information – theory and measurement understanding – to specify our model\nClosing the loop: when we critically evaluate our or others’ findings, we should consider validity, measurement, and generalizability\n\n\nReporting linear models\n\n\nWhen we report an analysis, we should report:\n\n\nExplain what I did, specifying the method (linear model), the outcome variable (accuracy) and the predictor variables (health literacy, reading strategy, reading skill and vocabulary)\nReport the model fit statistics overall (\\(F, R^2\\))\nReport the significant effects (\\(\\beta, t, p\\)) and describe the nature of the effects (does the outcome increase or decrease?)",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 10. Developing the linear model"
    ]
  },
  {
    "objectID": "PSYC411/part2/visualization-intro.html",
    "href": "PSYC411/part2/visualization-intro.html",
    "title": "Week 9. Data visualization practices",
    "section": "",
    "text": "Welcome to your overview of the work we will do together in Week 9.\nThis week, we will focus on perspectives and practices in data visualization.\nIn a wide range of professions, you will benefit if you can exercise skills in the capacity to produce and to interpret data visualizations.\nThe power to produce informative eye-catching visualizations in R is industry-leading, everywhere from news media to medical science. This means that learning how to produce visualizations using R will give you an advantage in many settings.\nOur materials are designed to help you to think about what you are doing — to understand the aims of the practical steps — as well as to learn about producing professional effective data visualizations. This is because, as you progress in your careers, you may come to manage as well as to direct the development of visualizations, so we need to see what best practice looks like.\nWe will continue to work with data collected for the Clearly understood project because we think that working with these data in this research context will help you to make sense of the data, and to see why we ask you to practise the skills we are teaching. (You can read more about the project in the chapter for week 7.)",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 9. Data visualization practices"
    ]
  },
  {
    "objectID": "PSYC411/part2/visualization-intro.html#sec-vis-intro-overview",
    "href": "PSYC411/part2/visualization-intro.html#sec-vis-intro-overview",
    "title": "Week 9. Data visualization practices",
    "section": "",
    "text": "Welcome to your overview of the work we will do together in Week 9.\nThis week, we will focus on perspectives and practices in data visualization.\nIn a wide range of professions, you will benefit if you can exercise skills in the capacity to produce and to interpret data visualizations.\nThe power to produce informative eye-catching visualizations in R is industry-leading, everywhere from news media to medical science. This means that learning how to produce visualizations using R will give you an advantage in many settings.\nOur materials are designed to help you to think about what you are doing — to understand the aims of the practical steps — as well as to learn about producing professional effective data visualizations. This is because, as you progress in your careers, you may come to manage as well as to direct the development of visualizations, so we need to see what best practice looks like.\nWe will continue to work with data collected for the Clearly understood project because we think that working with these data in this research context will help you to make sense of the data, and to see why we ask you to practise the skills we are teaching. (You can read more about the project in the chapter for week 7.)",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 9. Data visualization practices"
    ]
  },
  {
    "objectID": "PSYC411/part2/visualization-intro.html#sec-vis-intro-goals",
    "href": "PSYC411/part2/visualization-intro.html#sec-vis-intro-goals",
    "title": "Week 9. Data visualization practices",
    "section": "9.2 Our learning goals",
    "text": "9.2 Our learning goals\nThis week, we focus on both developing your critical thinking and strengthening your practical skills: our objectives blend concepts and skills targets.\nOur learning objectives: — what are we learning about?\nWe are working together to help you:\n\nGoals — Formulate questions you can ask yourself to help you to work effectively\nAudience — Understand the psychological factors that affect your impact\nDevelopment — Work reflectively through a development process\nImplement — Produce visualizations in line with best practice\n\nOur assessment targets: — how do you know if you have learned?\nWe are working together so you can:\n\nGoals — Identify a set of targets for a development process in your professional teams\nAudience — Explain what you need to do to make a visualization effective\nDevelopment — Locate yourself within the stages of the development process\nImplement — Produce visualizations that look good and are useful",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 9. Data visualization practices"
    ]
  },
  {
    "objectID": "PSYC411/part2/visualization-intro.html#sec-vis-intro-resources",
    "href": "PSYC411/part2/visualization-intro.html#sec-vis-intro-resources",
    "title": "Week 9. Data visualization practices",
    "section": "9.3 Learning resources",
    "text": "9.3 Learning resources\nYou will see, next, the lectures we share to explain the concepts you will learn about, and the practical visualization skills you will develop. Then you will see information about the practical exercises you can use to build and practise your skills.\nEvery week, you will learn best if you first watch the lectures then do the practical exercises.\nYou will realize that we are helping to build familiarity and fluency in your practical skills through the accumulation of practice, and the step-by-step extension of capabilities.\n\n\n\n\n\n\nLinked resources\n\n\n\n\nIn PSYC411: in Week 3, we started learning about histograms, bar charts, scatterplots, and boxplots; in Week 4, we revisited bar charts and scatterplots; in Week 5, we revised boxplots.\nIn PSYC411: in Week 7, we consolidated and extended your skills with scatterplots, and practised how to read plots.\nIn PSYC411: in Week 8, we consolidated and extended your skills with both histograms and scatterplots; learning to combine analysis with visualization.\nIn PSYC413: in Week 9, in the visualizations perspective class, we explore more broadly why being able to produce and read data visualizations is now an important sought-after skill.\nIn PSYC411: in Week 10, we explore how to use convenience functions to do visualization tasks automatically. We reach the most advanced kind of plots: presenting model estimates with raw data.\n\n\n\n\nThroughout our materials, we have called your attention to the ecosystem of free, high-value, information that is available to people working with R. That is because this information is an important benefit of learning to use R.\n\nThis week we grow your capacity to find, and to use, the information you need.\nThis will help to build your independence for the work you will do in your future professional careers.\n\n\n9.3.1 Lectures\nThe lecture materials for this week are presented in four short parts.\nClick on a link and your browser should open a tab showing the Panopto video for the lecture part.\n\nPart 1 (14 minutes) Data visualization perspectives and practices: The concepts and skills we will learn about in week 9: what we will learn and how we will know if we have learned it, the different kinds of goals we think about when we make visualizations, discovery and communication.\n\n\n\nPart 2 (8 minutes): the evidence base, the psychology of seeing and understanding visual information, and how we know what we should do in data visualization.\n\n\n\nPart 3 (19 minutes): why visualization is important, and why the things we can do to show variation and uncertainty matter, when we communicate evidence to audiences.\n\n\n\nPart 4 (25 minutes): the key skills — {ggplot2} as the grammar of graphics, why we have been working on building and editing plots step-by-step, what you can do in future work.\n\n\n\n\n9.3.2 Lecture slides\n\n\n\n\n\n\nDownload the lecture slides\n\n\n\nThe slides presented in the videos can be downloaded here:\n\nThe slides exactly as presented (7 MB).\n\nYou can download the web page .html file and click on it to open it in any browser (e.g., Chrome, Edge or Safari). The slide images are high quality so the file is quite big and may take a few seconds to download.\n\n\nWe are going to work through some practical exercises, next, to develop your critical thinking and practical skills for working with linear models.\n\n\n9.4 Practical materials: data and R-Studio\nWe will work with two data files which you can download by clicking on their names (below):\n\nstudy-one-general-participants.csv.\nstudy-two-general-participants.csv.\n\nOnce you have downloaded the files, you will need to upload them to the R-Studio server to access and use the R files.\n\n\n\n\n\n\nImportant\n\n\n\nHere is a link to the sign-in page for R-Studio Server\n\n\n\n\n9.4.1 Practical materials guide\nAs usual, you will find that the practical exercises are simpler to do if you follow these steps in order.\n\nThe data — We will take a quick look at what is inside the data files so you know what everything means.\nThe how-to guide — We will go through the practical analysis and visualization coding steps, showing all the code required for each step.\nThe practical exercises — We will set out the tasks, questions and challenges that you should complete to learn the practical skills we target this week.\n\nThis week — Week 9 — our emphasis is on finding and making use of online information.\n\nThis work will enable you to find solutions for yourself in your professional working lives\nto figure out practical how and why answers to the questions you will face.\n\n\n\n\n\n\n\nWeek 9 parts\n\n\n\n\nSet-up\nLoad the data\nRevision: locate and use {ggplot2} reference information.\nNew: locate and use tutorial or how-to information.\nNew: locate and use StackOverflow information.\n\n\n\nThere are three main sources of information you can access for free online:\n\nThe people who write software like the {tidyverse} or {ggplot2} libraries provide manuals, reference guides and tutorials. This information is often written as free web books.\nOther people write tutorials or guides or teaching materials designed to show learners (like us) how to use R functions or to do certain things using R. They may present these tutorials or guides as web books, blog sites or video tutorials, e.g., on Youtube or TikTok.\nMany people post questions and answers to discussion forums like Stackoverflow.\n\n\n\n\n\n\n\nImportant\n\n\n\nLearning how to find, understand and use this information is important because:\n\nA lot of scholarly and technical information is online and free.\nBeing able to locate and to use this information is often the way that most professionals work out what they want to do, and how they can do it, if they have a problem or a question.\nThis means you get an advantage if you learn how to do that here.\n\n\n\n\n9.4.1.1 The data files\nEach of the data files we will work with has a similar structure.\nHere are what the first few rows in the data file study.two.gen looks like:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparticipant_ID\nmean.acc\nmean.self\nstudy\nAGE\nSHIPLEY\nHLVA\nFACTOR3\nQRITOTAL\nGENDER\nEDUCATION\nETHNICITY\n\n\n\n\nstudytwo.1\n0.4107143\n6.071429\nstudytwo\n26\n27\n6\n50\n9\nFemale\nHigher\nAsian\n\n\nstudytwo.10\n0.6071429\n8.500000\nstudytwo\n38\n24\n9\n58\n15\nFemale\nSecondary\nWhite\n\n\nstudytwo.100\n0.8750000\n8.928571\nstudytwo\n66\n40\n13\n60\n20\nFemale\nHigher\nWhite\n\n\nstudytwo.101\n0.9642857\n8.500000\nstudytwo\n21\n31\n11\n59\n14\nFemale\nHigher\nWhite\n\n\nstudytwo.102\n0.7142857\n7.071429\nstudytwo\n74\n35\n7\n52\n18\nMale\nHigher\nWhite\n\n\nstudytwo.103\n0.7678571\n5.071429\nstudytwo\n18\n40\n11\n54\n15\nFemale\nFurther\nWhite\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe webpage has a slider under the data table window, so you can scroll across the columns: move your cursor over the window to show the slider.\n\n\nWhen you look at the data table, you can see the columns:\n\nparticipant_ID participant code\nmean.acc average accuracy of response to questions testing understanding of health guidance\nmean.self average self-rated accuracy of understanding of health guidance\nstudy variable coding for what study the data were collected in\nAGE age in years\nHLVA health literacy test score\nSHIPLEY vocabulary knowledge test score\nFACTOR3 reading strategy survey score\nGENDER gender code\nEDUCATION education level code\nETHNICITY ethnicity (Office National Statistics categories) code\n\n\n\n9.4.2 The how-to guide\nWe will take things step-by-step.\nMake sure you complete each part, task and question, in order, before you move on to the next one.\n\n\n9.4.2.1 How-to Part 1: Set-up\nTo begin, we set up our environment in R.\n\nHow-to Task 1 – Run code to empty the R environment\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nrm(list=ls())                            \n\n\n\n\n\n\nHow-to Task 2 – Run code to load libraries\nLoad libraries using library().\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nlibrary(\"tidyverse\")\n\n\n\n\n\n\n\n9.4.2.2 How-to Part 2: Load the data\n\nHow-to Task 3 – Read in the data file we will be using\nThe code in the how-to guide was written to work with the data file:\n\nstudy-one-general-participants.csv.\n\nRead in the data file – using read_csv().\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nstudy.one.gen &lt;- read_csv(\"study-one-general-participants.csv\")\n\n\n\n\n\n\nHow-to Task 4 – Inspect the data file\nUse the summary() function to take a look.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nsummary(study.one.gen)\n\n participant_ID        mean.acc        mean.self        study          \n Length:169         Min.   :0.3600   Min.   :3.440   Length:169        \n Class :character   1st Qu.:0.7600   1st Qu.:6.080   Class :character  \n Mode  :character   Median :0.8400   Median :7.080   Mode  :character  \n                    Mean   :0.8163   Mean   :6.906                     \n                    3rd Qu.:0.9000   3rd Qu.:7.920                     \n                    Max.   :0.9900   Max.   :9.000                     \n      AGE           SHIPLEY           HLVA           FACTOR3     \n Min.   :18.00   Min.   :23.00   Min.   : 3.000   Min.   :34.00  \n 1st Qu.:24.00   1st Qu.:33.00   1st Qu.: 7.000   1st Qu.:46.00  \n Median :32.00   Median :35.00   Median : 9.000   Median :51.00  \n Mean   :34.87   Mean   :34.96   Mean   : 8.905   Mean   :50.33  \n 3rd Qu.:42.00   3rd Qu.:38.00   3rd Qu.:10.000   3rd Qu.:55.00  \n Max.   :76.00   Max.   :40.00   Max.   :14.000   Max.   :63.00  \n    QRITOTAL        GENDER           EDUCATION          ETHNICITY        \n Min.   : 6.00   Length:169         Length:169         Length:169        \n 1st Qu.:12.00   Class :character   Class :character   Class :character  \n Median :13.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :13.36                                                           \n 3rd Qu.:15.00                                                           \n Max.   :19.00                                                           \n\n\n\n\n\n\n\n\n9.4.2.3 How-to Part 3: Revision – locate and use {ggplot2} reference information\nWe have called your attention to {ggplot2} reference information before.\nThis time, we take you through the steps involved in exploiting this information efficiently to produce more plots that communicate more effectively.\n\nHow-to Task 5 – Find out how to produce layered boxplots to examine if vocabulary scores are different for people with different education levels\nYou have seen how to produce boxplots before, in previous classes (weeks 3 and 5): here we add a twist to make the plot dramatically more informative.\n\n\n\n\n\n\nTip\n\n\n\nA box plot is a visualization designed to enable you to visualize the distribution of scores on a numeric variable, sometimes so that you can compare the distribution of scores on that variable in different groups or conditions.\n\n\nThis task is both about finding out how to do things (generally), here, using {ggplot2} reference information, and also about finding out how to produce a particularly effective form of data visualization, combining summary estimates with raw data observations.\nWe can break this task into steps:\n\nFind relevant and useful information;\nLocate example code;\n\nRun the example code;\n\nEdit the code to do the task with the study.one.gen data.\n\nWe want to make a boxplot. You have done that before. But this time we want to make a plot that shows (i.) the summary statistics (median and quartile) information of the boxplot plus (ii.) the raw observation information about sample data values that you see in scatterplot points. That is because, in the lectures on data visualization, we learned about the ways in which summaries can sometimes obscure what is going on with the sample data (think of Anscombe’s Quartet).\nHow do we produce this kind of plot?\n\nA professional would take the wish-list of aims (i.) and (ii.) and start looking for information online to figure out how to produce the kind of plot they want.\n\nWe work through the steps in order:\n1. Find relevant and useful information\nYou find the information you need by doing keyword searches online:\n\nbut what are the keywords?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nDo a search using the words: ggplot reference.\nThat will get you results including this one:\nhttps://ggplot2.tidyverse.org/reference/\n\n\n\nHow do you know if you can or should use the information you find?\n\nYou know the {ggplot2.tidyverse.org} information is ‘official’, relevant, and useful because you can see the hex badge (a hexagon shaped image): “ggplot2”.\n\nWe need more specific information on boxplots in the {ggplot2} library reference information:\n\nHow do we find it?\n\nYou can do a search of the reference index, or you can expand your set of keywords.\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe are looking specifically for information on box plots so do a search of the web page e.g. using CMD-F (on a Mac) or CTRL-F (on a PC) using the words: box or boxplot\nThat will get you a link – click on it:\nhttps://ggplot2.tidyverse.org/reference/geom_boxplot.html\n\n\n\n2. Locate example code\nMost R developers design their information web pages in the same way:\n\nfirst, they show a list of arguments you can edit to make choices about how functions like geom_boxplot() work;\nsecond, they explain what the object – here, a boxplot – represents;\nthird, they may give you information about what information the function can work with;\nfourth, they give you example code \\(\\rightarrow\\) this is the bit you want.\n\n\n\n\n\n\n\nTip\n\n\n\nExample code can be run without reading in any extra data, once you have run:\n\nlibrary(tidyverse)\n\nData referenced in the {tidyverse} help document examples will be available to use, pre-loaded.\n\n\nScroll through the page, looking for an example plot that looks like what we want: a plot which shows (i.) the summary statistics (median and quartile) information of the boxplot plus (ii.) the raw observation information (points) of the sample data.\n\nDo you see it?\n\n\n\n\n\n\n\nTip\n\n\n\nYou can see the example shown under this line of code.\n\np + geom_boxplot(outlier.shape = NA) + geom_jitter(width = 0.2)\n\n\n\n3. Run the example code\nCopy the first bit of example code you see and paste it directly into the R-Studio Console and run it, or paste it into the Script window and run it:\n\nYou have to be careful. Read through the sequence of example code paying attention to each element.\n\n\n\n\n\n\n\nTip\n\n\n\nIf you run this line of code, you will get an error, at first.\n\np + geom_boxplot(outlier.shape = NA) + geom_jitter(width = 0.2)\n\n\n\nThis is because the p + tells you that somewhere previously the sequence of examples builds a part of the plot p to which the next parts geom_boxplot(outlier.shape = NA) + geom_jitter(width = 0.2) are added.\n\nYou need to make sure you run the full sequence of example code to get the plot.\n\n\n\n\n\n\n\nTip\n\n\n\nIf you run this line of code, it will work.\n\np &lt;- ggplot(mpg, aes(class, hwy))\np + geom_boxplot(outlier.shape = NA) + geom_jitter(width = 0.2)\n\n\n\n\n\n\n\n\n\n\nIn this sequence of code:\n\np &lt;- ggplot(mpg, aes(class, hwy)) first constructs a plot object, called p – nothing will appear in the Plots window but the object p will appear in the Environment window in R-Studio.\np + geom_boxplot(outlier.shape = NA) adds the object geom_boxplot() to the object p – this step is where something is produced in the Plots window.\n+ geom_boxplot(outlier.shape = NA) + geom_jitter(width = 0.2) adds one plot on top of another.\n\nThis style of building a plot step-by-step enables you to construct quite complex plots, with multiple layers.\nIn the plot you can see that we get what we want. The plot shows:\n\nUsing geom_boxplot(outlier.shape = NA) – a summary of the data, including the median (the thick line in the middle of each box) and the 25% and 75% quartiles (the lower and upper edges of the box) of the observed values for the variable hwy, for different groups of class.\nUsing geom_jitter(width = 0.2) – the raw observations (of hwy per class) that the boxplots summarize, shown as the black points superimposed on the boxplots.\n\n\n\n\n\n\n\nTip\n\n\n\nThe example code from the {ggplot2} reference information demonstrates why we want to produce this kind of plot:\n\nWe can see that the boxplots, by their differing heights, show us the differences between groups in average outcomes (here, hwy, the y-axis variable).\nWe can also see that the points show us that showing just the boxes hides the fact that different groups (here, class of car) are associated with differences in samples (note the differences in the number or scatter of points).\n\n\n\n4. Edit the code to do the task with the study.one.gen data\nNow we know that we can produce the kind of plot we want, and we have seen the code that would produce that kind of plot.\nThe next step is to take the code structure that we see in the example and convert it so that kind of code does the same work for our own purposes.\nLet’s say that I want to produce a boxplot, showing raw data observations of accuracy of understanding, mean.acc, for each level of education in the study.one.gen data-set.\nHow do we convert the example code to produce the same kind of plot with our data?\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou need to change the data, and the aesthetic mappings – from this:\n\np &lt;- ggplot(mpg, aes(class, hwy))\np + geom_boxplot(outlier.shape = NA) + geom_jitter(width = 0.2)\n\n\nto this:\n\n\np &lt;- ggplot(data = study.one.gen, aes(x = EDUCATION, y = mean.acc))\np + geom_boxplot(outlier.shape = NA) + geom_jitter(width = 0.2)\n\nNotice:\n\nYou can write code quite concisely, like this ggplot(mpg, aes(class, hwy)) or more fully, like this ggplot(data = mpg, aes(x = class, y = hwy)).\nYou change the dataset from the example mpg data to our data study.one.gen.\nWhen you construct a boxplot, x must be mapped = to a categorical variable: a nominal variable or factor like EDUCATION.\nWhen you construct a boxplot, y must be mapped =to a numeric variable: a continuous or interval variable like mean.acc.\ngeom_jitter(width = 0.2) is new: we add that to show the raw data observations.\n\n\n\n\nNow take a look at the plot we have made.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\np &lt;- ggplot(data = study.one.gen, aes(x = EDUCATION, y = mean.acc))\np + geom_boxplot(outlier.shape = NA) + geom_jitter(width = 0.2)\n\n\n\n\n\n\n\n\n\n\n\nYou can make two useful observations:\n\nMean accuracy of understanding (of health information) is different in different education groups, on average – that is what the heights of the middle lines in the boxplots tell us.\nThe distribution of sample observations is really different for different education groups in this sample. This second observation might make us a bit uncertain about the extent to which the average outcome differences (comparing mean accuracy) reflect what we would see in the wider population.\n\nWhat are we learning here?\nThere are two lessons:\n\nIt is very useful to be able to find, read, and convert example code in online help information. Getting practice doing this will really helpful, not only to make it easier for you to do what we ask you to do but also for you to do things you decide you want to do.\n\n\nIn R, the plotting code is designed to work so that we can build complex plots, layer by later.\n\n\nIn general, it is helpful to be able to combine summaries with raw data.\n\n\n\n\n\n\n\nFurther information you can explore\n\n\n\nYou can read more about boxplots here:\nhttps://ggplot2.tidyverse.org/reference/geom_boxplot.html\nYou can read more about jittering here:\nhttps://ggplot2.tidyverse.org/reference/geom_jitter.html\n\n\n\n\n\n9.4.2.4 How-to Part 4: New – locate and use tutorial or how-to information\nThe official information provided by the developers of {ggplot2} or {tidyverse} are very useful but there is a whole other world of information — many hundreds of web-sites — provided by people interested in writing and sharing how-to guides and tutorials to highlight solutions to problems or answers to questions they, or others, have encountered.\n\nHow-to Task 6 – Find out how to modify the colour of the boxplots to examine if vocabulary scores are different for people with different education levels, and to distinguish education by colour\nThis time, we again want to produce some boxplots, but we want to modify the colours of the boxes to communicate information about the groups in our sample data to the reader.\n\nWe are using colour, here, not for decoration but as a third dimension of information, in addition to the height and the horizontal position of the boxes.\n\nHow do we change the colour of plot elements by linking colour differences to data differences?\nWe break this task into the same steps we followed before:\n\nFind relevant and useful information;\nLocate example code;\n\nRun the example code;\n\nEdit the code to do the task with the study.one.gen data.\n\n1. Find relevant and useful information\nAs before, we begin with a keyword search.\n\nWhat keywords do we need?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nDo a search using the words: ggplot boxplot colour\nThat will get you results including this one:\nhttps://r-graph-gallery.com/264-control-ggplot2-boxplot-colors.html\n\n\n\nThe source you are looking for is not officially linked to the {ggplot} project so now you have to decide if it is useful: how do you do that?\nHint: You can decide simply if a source is useful by (1.) trying the example code (does it work?) and evaluating if you understand how it works (do you understand why it works?)\n2. Locate example code\nMost tutorial or how-to writers design their web pages in the same way:\n\nfirst, they identify what the question or problem is they are going to help you with;\nsecond, they explain what they will do;\nthird, they may give you example code \\(\\rightarrow\\) this is the bit you want.\n\n\n\n\n\n\n\nTip\n\n\n\nExample code is often highlighted – can you see it in the webpage?\nhttps://r-graph-gallery.com/264-control-ggplot2-boxplot-colors.html\n\n\n3. Run the example code\nHow do you this?\n\nUsually, tutorial writers tell you first what libraries you need to load, do that, then run their example code.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nCopy a chunk of example code and paste it directly into the Console and run it, or paste it into the Script window and run it:\n\nggplot(mpg, aes(x=class, y=hwy, fill=class)) + \n  geom_boxplot(alpha=0.3) +\n  theme(legend.position=\"none\") +\n  scale_fill_brewer(palette=\"BuPu\")\n\n\n\n\n\n\n\n\nIn this sequence of code:\n\nggplot(mpg, aes(x=class, y=hwy, fill=class)) – tells R to map x to class to distinguish different groups by left-right position, and y to hwy to show outcome hwy scores by height. What is new here is this bit fill=class which tells R to colour the insides of the boxes with different colours for different classes.\ngeom_boxplot(alpha=0.3) + – tells R to draw boxplots, and reduce the opacity of the boxes (increase transparency).\ntheme(legend.position=\"none\") + – tells R to hide the legend.\nscale_fill_brewer(palette=\"BuPu\") – tells R to use the blue-purple Brewer colour palette to draw the colours.\n\n\n\n\nSo far, you have been using the default colour palette – sets of colours – that are built-in in R, or in {ggplot2} but there are many more palettes available.\n4. Edit the code to do the task with the study.one.gen data\nThe next step is to take the code structure that we see in the example and convert it so that kind of code does the same work for our own purposes.\nLet’s say that I want to produce a boxplot, showing the average accuracy of understanding, mean.acc, for each level of education in the study.one.gen data-set.\n\nThis will produce a set of boxplots. Each boxplot summarizes the mean.acc scores for a separate (education level) group. Now, I want to draw each boxplot in a different colour so different education levels are marked by different colours.\n\nHow do we convert the example code to produce the same kind of plot with our data?\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou need to change the data, and the aesthetic mappings – from this:\n\nggplot(mpg, aes(x=class, y=hwy, fill=class)) + \n  geom_boxplot(alpha=0.3) +\n  theme(legend.position=\"none\") +\n  scale_fill_brewer(palette=\"BuPu\")\n\n\nto this:\n\n\nggplot(study.one.gen, aes(x=EDUCATION, y=mean.acc, fill=EDUCATION)) + \n  geom_boxplot(alpha=0.3) +\n  theme(legend.position=\"none\") +\n  scale_fill_brewer(palette=\"BuPu\")\n\n\n\n\n\n\n\n\nNotice:\n\nYou need to make sure that you copy a complete chunk of code:\n\n\nincluding ggplot()\nincluding aes()\nincluding geom_boxplot()\nincluding scale_fill_brewer()\n\n\nYou change the data-set name to study.one.gen and you change the x= and y= aesthetic mappings.\nTo change the colour of the boxplots, you add: fill=EDUCATION as an argument to the aesthetic mappings in aes(...)\nscale_fill_brewer() adds in the colour using the Brewer colour-blind friendly palette.\n\n\n\n\n\n\nHow-to Task 7 – Can you find out more information about colour palettes?\nUsually, information about the availability of options for colour choices is organized in terms of palettes (like a set of colours in painting).\n\nWhat keywords do you need?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nDo a search using the words:\n\nggplot colour palettes\nggplot colour blind friendly palettes\nggplot cookbook colour palettes\n\n\n\n\nResults may include:\nhttps://ggplot2-book.org/scales-colour\n\nor:\n\nhttp://www.cookbook-r.com/Graphs/Colors_(ggplot2)/#:~:text=(size%3D3)-,A%20colorblind%2Dfriendly%20palette,variable%2C%20then%20use%20it%20later\nThese sources are quite technical in places:\n\nCan you find useful example code?\n\nWhat are we learning here?\nYou have seen, previously, that it is quite straightforward to change the colours of things in plots.\nHere, we are learning that:\n\nyou can use colour to communicate key information about data;\nyou can choose colours deliberately to ensure accessibility (by using colour blind palettes) or to suit your tastes.\n\n\n\n\n\n\n\nFurther information you can explore\n\n\n\nThere are several famous colour palettes that have been developed:\n\nViridis:\n\nhttps://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html\n\nBrewer:\n\nhttps://ggplot2.tidyverse.org/reference/scale_brewer.html\n\nMetBrewer provides colour palettes matched to famous art works in the Metropolitan Museum of Art in New York:\n\nhttps://www.blakerobertmills.com/my-work/met-brewer\nAnd some people have contributed deep thinking about how and why we should different colour schemes in different contexts or for diverse audiences:\nhttps://personal.sron.nl/~pault/\n\n\n\n\n\n\n\n\nImportant\n\n\n\nMake intentional choices when you use colour!\n\n\n\n\n\n9.4.2.5 How-to Part 5: New – locate and use StackOverflow information\nEverybody who uses R professionally uses StackOverflow all the time.\n\nIt is a huge, useful, resource of helpful information.\nMost of the time, if you do a search for answers to a question about using R – for data analysis, or plotting – the search will take you to a StackOverflow discussion page, sooner or later.\n\nLet’s learn how to use this resource.\n\nHow-to Task 7 – Find out how to export a nice boxplot so you can include it in a report\nIn writing a report, we often need to insert an image file.\n\nYou have been drawing some plots.\nNow, you need to save the plots so that you can use the plots in a report.\n\n\n\n\n\n\n\nTip\n\n\n\nPeople sometimes paste copies of screenshots of plots into reports. Sometimes, the plots look bad because they are low resolution, or because they are squashed into the wrong aspect ratio.\nThere is no need for this.\n\nYou can export or save the plot to an image file.\nYou can save the file in a high resolution.\nYou can make sure that the image has an aspect ratio that looks good.\n\n\n\nHow do you do this?\nWe break this task into the same steps we followed before:\n\nFind relevant and useful information;\nLocate example code;\n\nRun the example code;\n\nEdit the code to do the task with the study.one.gen data.\n\n1. Find relevant and useful information\nYou find the information you need by doing keyword searches online.\n\nWhat keywords do you need?\n\n\n\n\n\n\n\nHint\n\n\n\nDo a search using the words: how to export ggplot\n\n\nIf you use the right keywords, you should get to resources including this one:\nhttps://ggplot2.tidyverse.org/reference/ggsave.html\nThis is the official {ggplot2} library reference information on how to save plots.\nNotice:\n\nThis time, the example code at the end of the web page may not be helpful to everyone.\nThe usage information at the beginning tells you what you need to know but you will have to look up what arguments like device or dpi mean, and you will need to experiment a bit, trying different arguments, to get it to work for you.\n\nLet’s move on. We want something that is easy to use.\n\nHow do we find easy-to-use helpful information?\n\nOften, you are looking for a how-to guide of some kind.\n\nSo: what keywords do you need?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nDo a search using the words: how to export ggplot\nThat will also get you results including this one:\nhttps://stackoverflow.com/questions/38907514/saving-a-high-resolution-image-in-r\nThis is what we want: the example code, you will find, is usable and it is clearly explained.\nNotice:\n\nOn StackOverflow, people posts questions and then other people post answers to those questions.\nStill others can up-vote or down-vote the questions and the answers.\nThis system means that the most interesting or useful questions are more likely to appear in your search results when you do searches with the right key words.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn working with R: if you have a question someone has usually asked your question before, and someone else has usually posted an answer to that question.\n\nAs a rule: every question you ever have has been answered in a StackOverflow discussion.\n\nYou just need to be able to find that discussion, and to be able to use the answer information.\n\n\nIn the linked-to example, here, you can see that the discussion has been viewed 258k times (a lot).\n\nMany people have up-voted the question because it is so important.\nMany people have up-voted the answers to the question because they are so useful.\n\n2. Locate example code\nOn Stackoverflow, in response to any one question, different people may post alternative answers.\n\nSome students may be concerned that there is no one right answer.\nIn mathematics, there are right or wrong answers but this here is something different: we are working out how to do something with a computer.\nThe fact that there may be alternative different ways of doing the same task is a fair reflection of the reality that with the same set of tools\ndifferent experts can reasonably prefer different approaches or methods.\n\nHere, let’s just pick the simplest example:\n\ntiff(\"test.tiff\", units=\"in\", width=5, height=5, res=300)\n# insert ggplot code\ndev.off()\n\n3. Run the example code\nWhat happens if you run this example code?\n\ntiff(\"test.tiff\", units=\"in\", width=5, height=5, res=300)\n# insert ggplot code\ndev.off()\n\nNothing.\nNotice this bit: insert ggplot code.\nWe have to fix that.\n4. Edit the code to do the task with the study.one.gen data\nWhat the authors of the answer are telling you to do is to write your bit of code to produce a plot where they say: insert ggplot code.\nLet’s do that:\n\ntiff(\"my-plot.tiff\", units=\"in\", width=5, height=5, res=300)\n\nggplot(study.one.gen, aes(x=EDUCATION, y=SHIPLEY, fill=EDUCATION)) + \n  geom_boxplot(alpha=0.3) +\n  theme(legend.position=\"none\") +\n  scale_fill_brewer(palette=\"BuPu\")\n\ndev.off()\n\nquartz_off_screen \n                2 \n\n\nNotice:\n\nThis works but it may appear again as if nothing has happened.\nBut look in R-Studio in the Files window (bottom right screen).\nClick on the `Files’ tab.\nSort the list of files by date by clicking on the Modified column header.\nYou should see the file: my-plot.tiff in the Files space.\nYou can download this file by ticking the box.\n\nNow let’s go back to the way that the {ggplot2} library information recommends.\n\nTake a look at the second answer to the Stackoverflow question:\n\n\nggplot(data=df, aes(x=xvar, y=yvar)) + \ngeom_point()\n\nggsave(path = path, width = width, height = height, device='tiff', dpi=700)\n\nAdapt the example code to do the task:\n\nggplot(study.one.gen, aes(x=EDUCATION, y=SHIPLEY, fill=EDUCATION)) + \n  geom_boxplot(alpha=0.3) +\n  theme(legend.position=\"none\") +\n  scale_fill_brewer(palette=\"BuPu\")\n\n\n\n\n\n\n\nggsave(filename = 'my-plot-2.tiff', device='tiff', dpi=700)\n\nSaving 7 x 5 in image\n\n\nNow you should see:\n\nA plot appear in the Plots window in R-Studio.\nAnd a plot appear in the Files window in R-Studio.\n\nNotice:\n\nI deleted every argument I was not interested in using, e.g. width = ... and height = ...\nI gave the file a new name, so that you can see that this works as well as the dev.off() method\n\nWhich method do you prefer?\nWhat are we learning here?\nWe are learning two things:\n\nStackOverflow has many of the answers you will need, now, and later if you continue to use R.\nYou can save the image files you make for use in your reports.\n\n\n\n\n\n\n\nFurther information you can explore\n\n\n\nHere’s a blog post by Cedric Scherer:\nhttps://www.cedricscherer.com/2019/08/05/a-ggplot2-tutorial-for-beautiful-plotting-in-r/\n\nwhich covers many of the issues we have been discussing.\n\n\n\n\n\n\n9.4.3 The practical exercises\nNow you will progress through a series of tasks, and challenges, to test what you have learnt.\nWhat we want to do, here, is to ensure that you can access and use online information so that you can:\n\ndemonstrate you understand what you are doing when you find out how to do things;\nidentify your options when you assess the online information about coding or data analysis or plotting.\n\nGetting practice learning to do these things will help you generally.\n\n\n\n\n\n\nWarning\n\n\n\nNow we will work with the data file\n\nstudy-two-general-participants.csv\n\n\n\nWe again split the steps into into parts, tasks and questions.\n\nSet-up\nLoad the data\nRevision: locate and use {ggplot2} reference information.\nNew: locate and use tutorial or how-to information.\nNew: locate and use Stackoverflow information.\n\n\n\n\n\n\n\nTip\n\n\n\n\nThe how-to guide showed you ways of doing these tasks.\nNow use the hints and tips to complete the practical exercises that follow.\n\n\n\nWe are going to work through the following tasks, split into separate parts.\n\n\n\n\n\n\nImportant\n\n\n\nAn answers version of the workbook will be provided after the practical class.\n\n\n\n\n9.4.3.1 Practical Part 1: Set-up\nTo begin, we set up our environment in R.\n\nPractical Task 1 – Run code to empty the R environment\n\nrm(list=ls())\n\n\n\nPractical Task 2 – Run code to load relevant libraries\n\nlibrary(\"ggdist\")\nlibrary(\"tidyverse\")\n\nYou may need to load other libraries as you progress.\nNotice, here, that we are using a library we have not used before: {ggdist}.\n\n\n\n9.4.3.2 Practical Part 2: Load the data\n\nPractical Task 3 – Read in the data file we will be using\nThe data file for the practical exercises is:\n\nstudy-two-general-participants.csv\n\nUse the read_csv() function to read the data file into R.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nstudy.two.gen &lt;- read_csv(\"study-two-general-participants.csv\")\n\n\n\n\nWhen you code this, you can choose your own file name, but be sure to give the data object you create a distinct name e.g. study.two.gen.\n\n\nPractical Task 4 – Inspect the data file\nUse the summary() function to take a look.\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nsummary(study.two.gen)\n\n\n\n\n\n\n\n9.4.3.3 Practical Part 3: Revision – locate and use {ggplot2} reference information\n\nPractical Task 5 – Find out how to produce boxplots to examine if vocabulary scores are different for people with different education levels\nSpecifically, find out how to produce a plot comprising:\n\nboxplots showing a summary of vocabulary scores for people with different education levels;\nplus a scatterplot showing individual vocabulary scores for each person in the data-set.\n\nWe did this in the how-to guide: check it out before going on, if you are unsure.\n\nHere, we want to dig in to the information you find to show where you need to look, and what attention you need to invest, to understand the information you see.\n\nWe break this task into the same steps we followed before:\n\nFind relevant and useful information;\nLocate example code;\n\nRun the example code;\n\nEdit the code to do the task with the study.two.gen data.\n\n1. Find relevant and useful information\nHint: If you have never created a box plot before, a good place to start is the {ggplot2} library reference information.\n\nPract.Q.1. What keywords can you use to find the {ggplot2} library reference information?\n\n\nPract.A.1. You can do an effective search using the words: ggplot reference boxplot\n\nThat will get you results including this one:\nhttps://ggplot2.tidyverse.org/reference/geom_boxplot.html\n\nYou know this source is official, relevant, and useful because you can see the hex badge: “ggplot2”.\n\n\nPract.Q.2. What does a boxplot represent? In other words, what summary statistics do the features of a boxplot display?\n\nHint: You need to search the reference information page for the key information about summary statistics.\n\nPract.A.2. The reference page includes this information:\n\n\nThe lower and upper hinges correspond to the first and third quartiles (the 25th and 75th percentiles). This differs slightly from the method used by the boxplot() function, and may be apparent with small samples. See boxplot.stats()for more information on how hinge positions are calculated for boxplot().\n\n\nThe upper whisker extends from the hinge to the largest value no further than 1.5 * IQR from the hinge (where IQR is the inter-quartile range, or distance between the first and third quartiles).\n\n\nThe lower whisker extends from the hinge to the smallest value at most 1.5 * IQR of the hinge.\n\n\nData beyond the end of the whiskers are called “outlying” points and are plotted individually.\n\n2. Locate example code\n\nPract.Q.3. Can you find example code showing how to produce a boxplot that allows you to examine if vocabulary scores are different for people with different education levels, where different levels are shown in different colours?\n\n\nPract.A.3. The web page:\n\nhttps://ggplot2.tidyverse.org/reference/geom_boxplot.html\n\nincludes example code like this, which shows you how to produce a boxplot:\n\n\np &lt;- ggplot(mpg, aes(class, hwy))\np + geom_boxplot()\n\n\nalso which shows you how to add a scatterplot to a boxplot:\n\n\np + geom_boxplot(outlier.shape = NA) + geom_jitter(width = 0.2)\n\n\nand also this, which shows you how to show different levels in different colours:\n\n\np + geom_boxplot(aes(colour = drv))\n\nNotice:\n\nCode chunk example (1.) shows you how to draw the basic plot, using information about variables class, hwy in data-set mpg.\nChunk (2.) shows you how to change the second line of chunk (1.) to get the boxplot \\(+\\) scatterplot we want.\nChunk (3.) presents code to vary colour by a variable but — warning — the variable is not the one you want to use, you want to replace drv with class.\nThis change is required because when you adapt the code you will want to colour the boxplots by the same variable that you use to split and arrange the data on the x-axis (EDUCATION).\nNote that you can put aesthetic mappings aes() inside geoms but, here, it would be neater to put all the aesthetic mappings together in one place, as shown following.\n\n3. Run the example code\n\n\n\n\n\n\nCode\n\n\n\n\n\nThe example code, when you put it together, and adapt it for your needs, will look like this:\n\np &lt;- ggplot(mpg, aes(class, hwy, colour = class))\np + geom_boxplot(outlier.shape = NA) + geom_jitter(width = 0.2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe free online reference information is very helpful but it requires a bit of work to make use of it. This is natural, the authors of the information don’t know you and your situation.\n\nThis is what we are learning to do here.\n\n\n\nYou will need to understand two things before you move on.\n\nPract.Q.4. Can you explain what geom_jitter() does?\n\n\nPract.A.4. You can do a search like you have been doing – with the words: ggplot reference geom_jitter\n\nThat will get you results including this one:\nhttps://ggplot2.tidyverse.org/reference/geom_jitter.html\n\nwhich says:\n\n\nThe jitter geom is a convenient shortcut for geom_point(position = “jitter”). It adds a small amount of random variation to the location of each point, and is a useful way of handling overplotting caused by discreteness in smaller datasets.\n\n\nPract.Q.5. Can you explain what “overplotting” means?\n\n\nPract.A.5. Overplotting refers to where points (or other objects) are printed on top of each other, or printed crowded together, making it difficult to identify different data elements\n\n4. Edit the code to do the task with the study.two.gen data\n\nPract.Q.6. Can you edit and run the example code to produce a complex plot comprising:\n\n\n\nboxplots showing a summary of vocabulary scores for people with different education levels;\n\n\n\n\nplus a scatterplot showing individual vocabulary scores for each person in the data-set;\n\n\n\n\nwith the boxes and the points representing vocabulary score data for different levels shown in different colours.\n\n\n\nPract.A.6. The example code can be edited to:\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\np &lt;- ggplot(data = study.two.gen, aes(x = EDUCATION, y = SHIPLEY, colour = EDUCATION))\np + geom_boxplot(outlier.shape = NA) + geom_jitter(width = 0.2)\n\n\n\n\n\n\n\n\n\n\n\nNow look at the plot you have produced and critically evaluate it.\n\nPract.Q.7. What does the plot tell you about the average (median) vocabulary (SHIPLEY) score recorded for people in the study.two.gen data-set who report different education levels?\n\n\nPract.A.7. The plot indicates that the median vocabulary score is higher for people who report Higher education compared to people who report Further education or people who report Secondary education. We can see this because the middle line in the boxes, representing the median score for people in each education level group, is higher for for people who report Higher education.\n\n\nPract.Q.8. What does the plot tell you about the individual, i.e., per-person vocabulary (SHIPLEY) score recorded for people in the study.two.gen data-set who report different education levels?\n\n\nPract.A.8. The plot indicates a lot of variability in the individual scores. The distributions of scores appears to have similar wide ranges in each group.\n\n\n\n\n9.4.3.4 Practical Part 4: New – locate and use tutorial or how-to information\n\nPractical Task 6 – Find out how to construct a rain cloud plot\nWe can break this task into the usual steps:\n\nFind relevant and useful information;\nLocate example code;\n\nRun the example code;\n\nEdit the code to do the task with the study.two.gen data.\n\nRain cloud plots are now a popular visualization that incorporates elements of the boxplot, the scatterplot and the density plot to give the viewer summary information about the distribution of scores on a variable and also giving information about the variability of outcomes.\nWe work through the steps in order:\n1. Find relevant and useful information\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou can do an effective search using the words: how to ggplot rain cloud plot\n\n\n\nThat will get you results including these:\nhttps://www.cedricscherer.com/2021/06/06/visualizing-distributions-with-raincloud-plots-and-how-to-create-them-with-ggplot2/\n\nsee also:\n\nhttps://z3tt.github.io/Rainclouds/\nand\nhttps://www.r-bloggers.com/2021/07/ggdist-make-a-raincloud-plot-to-visualize-distribution-in-ggplot2/\nand\nhttps://rpubs.com/rana2hin/raincloud\nhttps://wellcomeopenresearch.org/articles/4-63\n\nPract.Q.9. There are reasons for doing the things we ask you to learn to do. Can you explain briefly why rain cloud plots are useful?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nPeople often explain why or how to do things in academic articles. A good source of information for the answer to this question is in the article by Allen et al. in wellcomeopenresearch.\n\n\n\n\nPract.A.9. The explanation, in short is that:\n\n\nconventional visualizations like bar plots can be misleading;\nshowing raw data with statistical summaries helps the viewer to see both statistical summary information and raw outcomes.\n\n2. Locate example code\nMost tutorial or how-to writers design their web pages in the same way:\n\nfirst, they identify what the question or problem is they are going to help you with;\nsecond, they explain what they will do;\nthird, they may give you example code \\(\\rightarrow\\) this is the bit you want.\n\n\n\n\n\n\n\nTip\n\n\n\nYou will find some tutorials easier to follow than others.\n\nPick the one that you can understand most easily.\n\n\n\n\nPract.Q.10. Example code is often highlighted – can you see it in the webpage?\n\n\nPract.A.10. A good example tutorial is the one written by Cedric Scherer:\n\nhttps://www.cedricscherer.com/2021/06/06/visualizing-distributions-with-raincloud-plots-and-how-to-create-them-with-ggplot2/\nBut the example information is a bit simpler in another blog so I focus on this one:\nhttps://z3tt.github.io/Rainclouds/\n3. Run the example code\nHint: A long sequence of code chunks are required to explain rain cloud plots, so scroll through whatever tutorial you are looking at until you find the bit that is presented, e.g., as: Raincloud plots.\nCopy a chunk of example code and paste it directly into the Console and run it, or paste it into the Script window and run it:\n\nggplot(iris, aes(Species, Sepal.Width)) +\n   ggdist::stat_halfeye(adjust = .5, width = .7, .width = 0,\n                        justification = -.2, point_colour = NA) +\n   geom_boxplot(width = .2, outlier.shape = NA) +\n   geom_jitter(width = .05, alpha = .3)\n\n\n\n\n\n\n\n\nHint: Make sure you have copied the complete sequence of code shown in the highlighted window in the tutorial.\nHint: If you get a warning that a function is not available check that you have used library() to get all the libraries the tutorial authors said that you needed.\n4. Edit the code to do the task with the study.two.gen data\n\nPract.Q.11. Can you edit the example code to draw a rain cloud plot including:\n\n\nboxplots showing a summary of vocabulary scores for people with different education levels;\nplus a scatterplot showing individual vocabulary scores for each person in the data-set?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nTo edit the example code to work for you, you will need to change the specification of the data-set and of the x = ... and y = ... aesthetic mappings in aes(...).\n\n\n\n\nPract.A.11. – You need to change the data, and the aesthetic mappings – from something like this:\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(iris, aes(Species, Sepal.Width)) +\n   ggdist::stat_halfeye(adjust = .5, width = .7, .width = 0,\n                        justification = -.2, point_colour = NA) +\n   geom_boxplot(width = .2, outlier.shape = NA) +\n   geom_jitter(width = .05, alpha = .3)\n\n\n\n\n\n\n\n\n\n\n\n\nto this:\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = EDUCATION, y = SHIPLEY)) +\n   ggdist::stat_halfeye(adjust = .5, width = .7, .width = 0,\n                        justification = -.2, point_colour = NA) +\n   geom_boxplot(width = .2, outlier.shape = NA) +\n   geom_jitter(width = .05, alpha = .3)\n\n\n\n\n\n\n\n\n\n\n\nWhat are we learning here?\nSometimes, when you are working with example code from an online source, you need to experiment a bit with the example code to get it to work.\n\nLearning to experiment with codd is a really useful skill to develop:\n\nYou can start by just deleting the arguments inside function calls, e.g., geom_boxplot(width = .2, outlier.shape = NA), one argument at a time, to see what changes.\nYou can then add arguments in, and change their values, to figure out what each argument adds.\n\nYou can’t break things in the world, in R, so you can experiment as much as you like through a process of trial-and-error.\n\n\n\n\n9.4.3.5 Practical Part 5: New – locate and use StackOverflow information\n\nPractical Task 7 – Export one of the nice plots you have been making so you can include it in a report\nIn the how-to guide you can find information on how to do this. Take a look and come back, if you need to.\nAdapt the example code to do the task:\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(iris, aes(Species, Sepal.Width)) +\n   ggdist::stat_halfeye(adjust = .5, width = .7, .width = 0,\n                        justification = -.2, point_colour = NA) +\n   geom_boxplot(width = .2, outlier.shape = NA) +\n   geom_jitter(width = .05, alpha = .3)\n\n\n\n\n\n\n\n ggsave(filename = 'my-plot-2.tiff', device='tiff', dpi=700)\n\nSaving 7 x 5 in image\n\n\n\n\n\nNow you should see:\n\nA plot appear in the ‘Plots’ window in R-Studio\nAnd a plot appear in the ‘Files’ window in R-Studio\n\nNotice:\n\nI gave the file a new name.\nI defined format using device='tiff' and resolution using dpi=700\n\nBut if you do not do these things, R will use (OK) defaults for you.\n\nPract.Q.12. Can you figure out how to insert the plot file you have created into a Word document?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou can search for information on how to do this by using keywords like: word how to insert an image file\n\n\n\n\nPract.A.12. – You can find instructions e.g. here\n\nhttps://support.microsoft.com/en-us/office/insert-pictures-3c51edf4-22e1-460a-b372-9329a8724344:~:text=Insert%20a%20picture%20in%20Word%2C%20PowerPoint%2C%20or%20Excel&text=On%20the%20Insert%20tab%2C%20click,in%20iPhoto%20or%20Photo%20Booth.\n\nPract.Q.13. Can you adapt the colour scheme of the plot to make it look good to you?\n\n\nPract.A.13. Again, we are asking you to do a key words search. If you do, the results may include:\n\nhttps://ggplot2.tidyverse.org/reference/scale_brewer.html\nYou may need to edit the aesthetic mappings to instruct R to map colour (lines) and fill to the categorical variable or factor we are working with:\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = EDUCATION, y = SHIPLEY,\n                             colour = EDUCATION, fill = EDUCATION)) +\n  ggdist::stat_halfeye(adjust = .5, width = .7, .width = 0, justification = -.2) +\n  geom_boxplot(width = .2, outlier.shape = NA) +\n  geom_jitter(width = .05, alpha = .3) +\n  scale_colour_brewer() +\n  scale_fill_brewer()\n\n\n\n\n\n\n\n\n\n\n\n\nPract.Q.14. Can you change the theme?\n\n\nPract.A.14. Again, we are asking you to do a key words search. If you do, the results may include:\n\nhttps://ggplot2.tidyverse.org/reference/ggtheme.html\n\nwhich provides information you can use to edit code as follows:\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\nggplot(data = study.two.gen, aes(x = EDUCATION, y = SHIPLEY,\n                             colour = EDUCATION, fill = EDUCATION)) +\n  ggdist::stat_halfeye(adjust = .5, width = .7, .width = 0, justification = -.2) +\n  geom_boxplot(width = .2, outlier.shape = NA) +\n  geom_jitter(width = .05, alpha = .3) +\n  scale_colour_brewer() +\n  scale_fill_brewer() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou have now completed the Week 9 practical exercises and questions\n\n\n\n\n\n\nImportant\n\n\n\nUnderstanding how you can produce informative, engaging, data visualizations is a key, highly employable, skill.\n\nBut understanding how visualizations are made also enables you to read plots when you see them.\nThis means that, whether you are doing data analysis, or consuming evidence from data analyses, this is an important step in your developmental journey: Well done!\n\nWe will continue to deepen and extend your skills and understanding but everything builds on the key lessons we have been learning here.\n\n\n\n\n\nThe answers\nAfter the practical class, we will reveal the answers that are currently hidden.\nThe answers version of the webpage will present my answers for questions, and some extra information where that is helpful.\n\n\nSummary\nYou start your work with these questions:\n\nWhat are our goals?\nWhat does our audience need or expect?\n\nYou develop your visualization in a reflective process:\n\nBegin with a quick draft to show the distributions or make the comparisons you think about first\nThen reflect, and edit: does this enable me to discover sources of variability in my data?\nThen reflect, and edit: does this enable me to effectively communicate what I want to communicate?\nThen reflect, and edit: does this look good? – do my viewers tell me this works well?\n\n\n\n\n\n\n\nTip\n\n\n\nI can only show you the potential for creative and effective visualization\n\nexperiment and find what looks good and is useful to you\nseek out information – good places to start are:\n\nhttps://ggplot2.tidyverse.org/index.html\nhttps://r-graph-gallery.com",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 9. Data visualization practices"
    ]
  },
  {
    "objectID": "PSYC411/index.html",
    "href": "PSYC411/index.html",
    "title": "Analysing and Interpreting Psychological Data I",
    "section": "",
    "text": "Welcome\nWelcome to this module PSYC411! Very glad to have you here with us!\nRob Davies  and Padraic Monaghan  are the lecturers on this module.\nWe are looking forward to meeting you!\nThis module is designed for students who have never done data analysis or statistics before, as well as those who have background in this.\nWe have set up the module so that there are different exercises depending on your background.\n\nFor those who are new to data analysis and statistics, there is a week-by-week step-by-step guide to all you need to know to become a masters-level expert in interpreting and analysing psychological data.\nFor those of you who have already used our software (R-studio) for analysis before, after revising the foundations of what you need to know you can move on quickly to the extra explorative exercises where you can hone and broaden your skills.\n\n\n\nSteps to get ready\nHere are the steps to go through before the module begins, to help you prepare for the course:\nFirst: Watch this first video which is a chat between Rob Davies and Padraic Monaghan giving you a bit of an insight into what we’d like you to take from this module and what our philosophy is for teaching.\n\nSecond: Watch our welcome video (part 1) to give you an outline of how this module works:\n\nThird: Then watch our welcome video (part 2) on how to access the software for this module. Basically, what you need to do is in a web browser go to: http://psy-rstudio.lancaster.ac.uk and login with your university account name, and your university account password:\n\nNote that you can only access this website from on campus, or via the University’s VPN. For information on connecting to the VPN, see here: ISS help for VPN\n\n\nNext steps\nNext step is to go to the Week 1 materials, once they are released. We aim to release materials for the following week by Thursday of the previous week. You can navigate to the relevant week via the list on the left of the webpage.\n\n\nCourse Contacts\n\n\n\n\nEmail Address\n\n\n\n\nPadraic Monaghan\np.monaghan at lancaster dot ac dot uk\n\n\nRob Davies\nr.davies1 at lancaster dot ac dot uk\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "PSYC411/part1/Week2.html",
    "href": "PSYC411/part1/Week2.html",
    "title": "Week 2. Manipulating data",
    "section": "",
    "text": "Material for week 2 will be released after week 1 practical.\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 2. Manipulating data"
    ]
  },
  {
    "objectID": "PSYC411/part1/Week2.html#overview",
    "href": "PSYC411/part1/Week2.html#overview",
    "title": "Week 2. Manipulating data",
    "section": "",
    "text": "This week, there are three mini lectures, and then a practical workbook to get you going with R-studio. Before the practical on Tuesday, please try to work through the practical workbook in your group.\nBring your questions (and/or answers) to the practical.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 2. Manipulating data"
    ]
  },
  {
    "objectID": "PSYC411/part1/Week2.html#learning-goals",
    "href": "PSYC411/part1/Week2.html#learning-goals",
    "title": "Week 2. Manipulating data",
    "section": "2.2 Learning Goals",
    "text": "2.2 Learning Goals\nBy the end of Week 2, you should be able to:\n\nUnderstand the importance of open data in psychology\nUnderstand different types of data and how best to represent them\nUnderstand bar graphs, scatter plots, and interpret patterns in these graphs\nOpen data sets in R-studio and manipulate those data\nUse R-studio to make simple graphs",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 2. Manipulating data"
    ]
  },
  {
    "objectID": "PSYC411/part1/Week2.html#lectures-and-slides",
    "href": "PSYC411/part1/Week2.html#lectures-and-slides",
    "title": "Week 2. Manipulating data",
    "section": "2.3 Lectures and slides",
    "text": "2.3 Lectures and slides\n\n2.3.1 Lectures\nWatch Lecture week2 part1.\n\nWatch Lecture week2 part2\n\nWatch Lecture week2 part3\n\nTake the quiz on the lecture material (not assessed).\n\n\n2.3.2 Slides\nDownload the lecture slides for:\n\npart 1\npart 2\npart 3",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 2. Manipulating data"
    ]
  },
  {
    "objectID": "PSYC411/part1/Week2.html#practical-materials",
    "href": "PSYC411/part1/Week2.html#practical-materials",
    "title": "Week 2. Manipulating data",
    "section": "2.4 Practical Materials",
    "text": "2.4 Practical Materials\n\n2.4.1 Workbook\nIn your group (or on your own until you’ve formed a group), work through this workbook, note any problems and questions you have, and come prepared to the online practical class to go through the tasks and ask your questions.\nIf you’ve done statistics using R-studio before then Parts 1 and 2 will be again largely revision for you. In this case, Part 3 is where you can focus your work.\nPart 1 of this workbook reproduces what you saw in the week 2 part 3 lecture.\nPart 2 gives you some more exercises in using R studio to revise last week’s material see this article for the psychology behind learning and memory effects of “spacing”.\nPart 3 provides you with more practice at looking at others’ data and manipulating it.\nPart 4 provides some more advanced methods for data manipulation.\nPart 5 looks at different ways that data is stored, and how to adjust that.\nPart 6 provides some extended extra practice in finding and investigating datasets.\nIf you have used R-studio before, then Part 6 will be where you will be developing some new advanced skills.\n\n2.4.1.1 Part One: repeat the steps from lecture 2 part 3\n\nTask Zero: making and opening an R script file\n\nOpen up the R server at https://psy-rstudio.lancaster.ac.uk\nIn R studio, at the File menu, select New File, then R script.\nNow, we can put in our favourite sum to check it’s working.\nAt the top of the R script window type:\n\n\n10.5 + 7                        \n\nWith the cursor on the same line as the sum click on the Run button.\nIn the console, you should see the sum being run, and the answer produced.\n\nSave the R script.\n\nFirst of all, you can make a Directory/Folder to save the R scripts in to.\nYour file structure can be anything you like, but here’s how I organised it: At the bottom right panel of the R-studio window, click on Folder. Add a new Folder, call it, say PSYC411.\nClick on the PSYC411 Folder, and then make another new Folder, called, say “Week2”.\n\nThen, you can save this week’s workbook into the Week2 folder:\n\nClick on the Save icon just above the R script window, browse to the Folder where you want to save the script (e.g., PSYC411/Week2), then name the R script file (e.g., workbook_week2.r) and Save.\nThe .r subscript indicates to R studio that this is an R script file.\n\nClose the R script, by clicking on the little x next to the R script filename just above the R script window.\nNow open it again. In the R studio window File menu, select Open File, and browse to where you saved your R script file and open it.\nTo make it easier to save and open files we can set what is called the “working directory” for R studio. Click “Session” in the menu at the top of the R studio screen, select “Working directory”, then select “Choose directory”.\nBrowse to the Folder where you are going to save your PSYC411 R studio files (for me this is Documents/PSYC411), and click Open.\nOver on the right lower panel, you should now see all the files that are in this Folder - including psyc411_week2.r\n\n\n\nTask One: open and check Practical week1 workbook script file\n\nLast week, we didn’t save a script (as we were just using the console for R studio). But here’s one I prepared earlier… Download the week 1 script file here. This also has the answers (but you can check last week’s answers via the main Week 1 page already too).\n\nThis has now downloaded the file on to your local computer. You now need to make it available in the psy-rstudio system. To do this, we have to “Upload” the file.\nIn the bottom right panel of R studio, click “Upload”.\nFor the Target directory, select the Folder in R-studio that you’d like to upload the file to (e.g., PSYC411/Week1 - maybe first of all you’ll need to make the Folder).\nThen Browse to the file that is on your local computer, and click OK.\nYou should see it in the Files in the bottom right panel on R-studio.\n\nNow you can open psyc411_week1_workbook_answers.r in R-studio (either through File &gt; Open File, at the top left panel, or just by double-clicking it in the Files in the bottom right panel). You can now look through this R script, check your commands and answers to the tasks from last week.\n\n\n\n\n\n\n\nTip\n\n\n\nA really important feature to include in your r script are commments. Comments are preceded by a # which means that anything after the # on a line is not going to be interpreted as a command in R studio, but as a comment.\nGood comment style involves consistent use of # to indicate different sections (e.g., you can use multiples of #’s to partition different sections of the results and make it easy to navigate, i.e., ########), and clear and succinct statements about why the command is there and what it is doing - see examples in the script file.\nComments help us to keep track of what each bit of our r script is doing, and, also importantly, enable others to figure out what on earth it is that we’ve done so they can reproduce our analyses, and access our data more effectively, in the spirit of open science.\n\n\n\n\nTask Two: Opening a data file\n\nMake sure your script file psyc411_week2.r in R studio.\nIt’s generally a good idea to refresh and clear out R studio when you start a new session, so add this line to the beginning of your R studio file:\n\n\nrm(list=ls())                    \n\n\nNext we are going to open a new data file.\n\nDownload the file PSYC411-shipley-scores-anonymous-17_18.csv\n\n\n\n\n\n\nTip\n\n\n\nDO NOT OPEN THIS FILE ON YOUR COMPUTER – IF YOU DO, DELETE IT THEN DOWNLOAD IT AGAIN - THE REASON FOR THIS IS THAT IT CHANGES THE FILE FORMAT AND THEN WON’T OPEN PROPERLY IN R STUDIO.\n\n\nThen, Upload the data file into R-Studio, and put it in PSYC411/Week2 Folder for instance.\nEach row is one person’s data, and each column is a measure taken from the person. Columns are separated by commas, which is what the “csv” refers to” comma-separated values.\n\nIn R studio, we open csv files using a function called read_csv() which comes from a set of functions that are stored in a library called “tidyverse”, we can install these functions by putting library(tidyverse) at the very top of our R script.\n\nType this command in the script file and then run it:\n\nlibrary(tidyverse)\n\n\n\n\n\n\n\nWant to know more about library()?\n\n\n\n\n\nR comes with certain functions pre-installed, such as mean(), but part of the charm of R is that we can install different functions that give us the opportunity to do almost anything! Collections of functions are called packages, and collections of packages are called libraries, but for the purpose of practicality, we typically refer to them interchangably. We install libraries using library() and enter the name of the library in the brackets.\n\n\n\n\nThen type this command in the script file and run it:\n\n\ndat &lt;- read_csv(\"PSYC411-shipley-scores-anonymous-17_18.csv\")\n\nRemember the dat &lt;- notation, which means we put the data into an object called “dat”\n\nIf all went well, we can then look at the data, using the function View: In the script file type:\n\n\nView(dat)\n\nand run it. It should open a spreadsheet where we can see the data.\n\nHave a good look around the data to see if you can make sense of it. For now, we will just focus on two variables from the data: subject_ID which is the participants’ anonymised number, and Gent_1_score, which is the participants’ score on the Gent vocabulary test, the first time they had a go (that’s what the 1 stands for). We can just pick out the columns of data that we want using the select function.\n\nselect is in the library called “tidyverse”, so we don’t need to load this in again because we already have.\n\nsummarydat &lt;- select(dat, subject_ID, Gent_1_score)\n\n\nFinally, let’s just have a quick look at these data. In the script file type:\n\n\nhist(summarydat$Gent_1_score)\n\nand run it.\n\nThis will draw a histogram of the Gent vocabulary scores. summarydat$Gent_1_score means that we look at the Gent_1_score values from the summarydat data – the $ indicates that this is one of the measures from the data. What kind of pattern does the histogram show?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndat &lt;- read_csv(\"data/wk2/PSYC411-shipley-scores-anonymous-17_18.csv\")\n\nRows: 81 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): english_status, Gender, Dyslexia_diagnosis\ndbl (7): subject_ID, Age, Gender_code, Shipley_Voc_Score, Gent_1_score, Gent...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsummarydat &lt;- select(dat, subject_ID, Gent_1_score)\nhist(summarydat$Gent_1_score)\n\n\n\n\n\n\n\n\nLooks close to a normal distribution, slight negative skew possibly.\n\n\n\n\n\n\n2.4.1.2 Part Two: Revision from last week\n\nSave your R script. Now, without looking at your notes, make a new object called “iknow”, and assign this list of numbers to it: 126, 76, 98, 124, 91, 88, 99, 115, 80, 113, 90, 92, 97, 134, 110, 92, 92, 87, 135, 115\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nuse the iknow &lt;- c(number1, number2, …) notation\n\n\n\n\nRemember to type these commands in the R script window. Next, get R-studio to give you the mean and standard deviation of this list of numbers.\nDraw a histogram of the iknow data.\n\n\n\n\n\n\n\nAnswers to 19-21\n\n\n\n\n\n\nlibrary(tidyverse)\niknow &lt;- c(126, 76, 98, 124, 91, 88, 99, 115, 80, 113, 90, 92, 97, 134, 110, 92, 92, 87, 135, 115)\nmean(iknow); sd(iknow)\n\n[1] 102.7\n\n\n[1] 17.59515\n\nhist(iknow)\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.4.1.3 Part Three: More practise at opening files and manipulating data\n\nTask Three: Loading data into R-studio\n\nDownload the data file called ahicesd.csv and Upload them into your directory in R studio: “PSYC411/week2”. Now do the same for participantinfo.csv.\n\nThese data come from this study: Woodworth, R.J., O’Brien-Malone, A., Diamond, M.R. and Schüz, B., 2018. Data from, Web-based Positive Psychology Interventions: A Reexamination of Effectiveness. Journal of Open Psychology Data, 6(1).\nA brief description of the study is as follows: In our study we attempted a partial replication of the study of Seligman, Steen, Park, and Peterson (2005) which had suggested that the web-based delivery of positive psychology exercises could, as the consequence of containing specific, powerful therapeutic ingredients, effect greater increases in happiness and greater reductions in depression than could a placebo control. Participants (n=295) were randomly allocated to one of four intervention groups referred to, in accordance with the terminology in Seligman et al. (2005) as 1: Using Signature Strengths; 2: Three Good Things; 3: Gratitude Visit; 4: Early Memories (placebo control). At the commencement of the study, participants provided basic demongraphic information (age, sex, education, income) in addition to completing a pretest on the Authentic Happiness Inventory (AHI) and the Center for Epidemiologic Studies-Depression (CES-D) scale. Participants were asked to complete intervention-related activities during the week following the pretest. Futher measurements were then made on the AHI and CESD immediately after the intervention period (‘posttest’) and then 1 month after the posttest (day 38), 3 months after the posttest (day 98), and 6 months after the posttest (day 189). Participants were not able to to complete a follow-up questionnaire prior to the time that it was due but might have completed at either at the time that it was due, or later. We recorded the date and time at which follow-up questionnaires were completed.\n\nNext, load the data into R-studio.\n\nThe first file is data from participants’ self-ratings of happiness and depression.\nThe second contains demographic information about the participants. Type this in your R script:\n\ndat &lt;- read_csv(\"ahicesd.csv\")\npinfo &lt;- read_csv(\"participantinfo.csv\")\n\nrun this command (type Ctrl-Enter), then type\n\npinfo &lt;- read_csv(\"participantinfo.csv\")\n\nand run that.\nThis has made two new objects – one called “dat” which contains the experimental data, and one called “pinfo” which contains the demographic information.\n\n\nTask Four: Examining and manipulating data\n\nLet’s have a look at the data now.\n\n\nView(dat)\n\nRun this command, and you should see the data appear above the console window. Have a good long hard look at it.\n\nThe data contains:\n\nid which is the participant number;\noccasion which is whether this is the first (0), second (1), up to sixth (5) time they filled in the questionnaires;\nintervention is which intervention they took part in with respect to attempting to promote their mood;\nahi01-ahi24 are the 24 items on the AHI happiness scale;\ncesd01-cesd20 are the 20 items on the CESD depression scale.\nWay over on the right are the total scores on the AHI and the CESD questionnaires.\n\nNow, view the pinfo data. How can you look at it?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nlibrary(tidyverse)\ndat &lt;- read_csv(\"data/wk2/ahicesd.csv\")\npinfo &lt;- read_csv(\"data/wk2/participantinfo.csv\")\nView(pinfo)\n\n\n\n\n\nLooking at the data replaced the source window, but the source is still there. Just above the View panel you should see a tab named psyc411_week2.r, click on that to get your source panel back. It will have a star/asterisk after the file name if it is unsaved. Remember it’s a good idea to regularly save your source file so you don’t lose work.\nNow, we are going to join together the two files. Type this:\n\n\nall_dat &lt;- inner_join(x = dat, y = pinfo, by = c('id', 'intervention'))\n\nQuestion: what does the c(“id”, “intervention”) bit mean?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThis means we match by two variables – id and intervention. We use the c() notation to indicate that this is a list of things.\n\n\n\nWe’ve now made a new data set called “all_dat”. The “x = dat” bit is the name of the first datafile we want to join, the “y = pinfo” is the name of the second datafile we want to join, the “by = ‘id’, ‘intervention’” bit is the names of variables that the two datasets have in common.\n\nHow would you join two data sets one called “data1” the other called “data2” together if they both have the variable “participantname” in common?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\ndata_full &lt;- inner_join(x = data1, y = data2, by = c(\"participantname\"))\n\n\n\n\n\nNow we just want to keep a few of the variables – we’re not interested in the individual questionnaire items. So, let’s select the variables we want to keep:\n\n\nsummarydata &lt;- select(all_dat, ahiTotal, cesdTotal, sex, age, educ, income, occasion, intervention)\n\nWhere “all_dat” is the name of the object to take data from, and “ahiTotal, cesdTotal, sex, age, educ, income, occasion, intervention” are all the variables we want to keep.\n\nHave a look at the summarydata in the View. How do you do that?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nView(summarydata)\n\n\n\n\n\n\nTask Five: Investigate data\n\nThe next task is to have a closer look at the distributions of the data. Let’s focus on the age of participants. To investigate one column of data from a dataset, you have to refer to it using the “$” symbol. So, to investigate the “age” column from the “summarydata” dataset, you would look at summarydata$age. Draw a histogram (bar graph) of the distribution of age in the participant sample.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nhist(summarydata$age)\n\n\n\n\n\nNow, let’s look at how the AHI and the CESD scores relate. To gain an impression of how two variables relate we can draw a scatter graph.\n\n\nplot(summarydata$ahiTotal, summarydata$cesdTotal)\n\nWhat does the “$” do in this command? What relationship do you find between these two variables?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n$ refers to one column in the data – either $ahiTotal or $cesdTotal columns.\nRelation is negative – higher happiness indicates lower depression.\n\n\n\n\nNow make sure you save psyc411_week2.r that contains all these commands that you ran. Close R studio, and Open R studio and make sure it’s saved all your work.\nThe list of commands is extremely useful for making science open and accessible to other researchers. It’s more and more common for psychology articles to make the R source files available so other researchers can reproduce the data manipulations and analyses used in the paper precisely.\n\n\n\n\n4.2.1.4 Part Four: More (advanced) data manipulation\n\nFor reading in more types of data, you can use commands other than read_csv(). Have a look at https://support.rstudio.com/hc/en-us/articles/218611977-Importing-Data-with-RStudio and have a go at loading in excel files or text files with various formats into R studio, remember last week, we also loaded in spss .sav files using the spss.get() function in the Hmisc library.\n\nAnother way to load in spss .sav files is to use the library called haven and the function read_sav which then works quite similarly to the read_csv file.\nNote that using lots of different resources online is one of the points of using R-studio – everything is open and shared. It isn’t cheating - it’s what everyone who uses R-studio does, it’s what I did when I was checking some of the commands in this module - so do use google (or baidu, or any search engine) to explore other commands, and to get hints if you ever get stuck.\n\nWhat if the datafiles you wanted to combine together had different names for participants – maybe, for instance, in datafile “dataone” it’s called “id” and in “datatwo” it’s called “participant”. You can do this by specifying how variables in different datasets relate to each other.\n\nTake a look at the help information for inner_join by typing ?inner_join then in the help pane on the right, scroll down to find the section titled “Arguments”, and read what it has to say about the argument “by”. See if you can understand how to join these files together. The examples at the bottom of the help section may be useful too.\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe important help information is: “To join by different variables on x and y, use a named vector. For example, by = c(”a” = “b”) will match x$a to y$b.”\n\ndata_full &lt;- inner_join(x = dataone, y = datatwo, by = c(\"id\" = \"participant\"))\n\n\n\n\n\nIn the data set from Woodworth et al., we might want to just look at the pretests. We can use a function called filter to pull out just some of the rows.\n\n\nsummarydata_occasion0 &lt;- filter(summarydata, occasion == 0)\n\nThis pulls out just the first occasion of testing (the pretest).\nHow would you pull out the second occasion of testing?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nfilter(summarydata, occasion == 1)\n\n\n\n\n\nHow would you pull out just those participants who had the first intervention from the summarydata data?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nfilter(summarydata, intervention == 1)\n\n\n\n\n\nBack to the data set with just occasion == 0 selected. Now plot the relationship between the AHI and CESD scores at this pretest. What is the pattern now?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nstill the same\n\nplot(summarydata_occasion0$ahiTotal, summarydata_occasion0$cesdTotal)\n\n\n\n\n\nHow about just the participants who had intervention 1. What is the relationship between their scores on the AHI and CESD in the first test?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\ncan do this in two ways:\n\nsummarydata_occasion0_intervention1 &lt;- filter(summarydata, occasion == 0 & intervention == 1)\nplot(summarydata_occasion0_intervention1$ahiTotal, summarydata_occasion0_intervention1$cesdTotal)\n\nor\n\nsummarydata_occasion0_intervention1 &lt;- filter(summarydata_occasion0, intervention == 1)\nplot(summarydata_occasion0$ahiTotal, summarydata_occasion0$cesdTotal)\n\nThe relationship is still similar.\n\n\n\n\n\n4.1.3.5 Part Five: Different ways in which data are stored: long and wide data format\nWhen we look at other people’s data sets there are two generic ways in which they can be presented. The first is called “wide format”, and this is when several measures are presented on a single row in the data. This is the format of the data in the vocabulary scores: psyc411-shipley-scores-anonymous-17_18.csv, where there are three vocabulary tests each in different columns. The other format is called “long format”, and this is where each observation is on a different line – and if there is more than one observation from the same subject then that subject has multiple lines in the data. There are some functions in the tidyverse library that helps us convert from wide to long and long to wide. This part practises these conversions.\n\nTask Six: Wide to long format conversion\n\nLet’s go back to the psyc411-shipley-scores-anonymous-17_18.csv data. This should still be in the object called “dat”. If it’s not, then load the data again into dat, using the read_csv() function.\nMake sure the library(tidyverse) is loaded, if not, run the command library(tidyverse).\nThe aim here is to convert the data so that each Gent vocabulary score is on a separate line, we use the pivot_longer function for this. First we specify what the new object should be (datlong), then we say where the old data is (dat), then we make a new variable to keep the names of the tests (names_to = “test”), then we make a new variable to keep the scores from the tests (values_to = “vocab”), then we specify the list of old variables to combine into the new scores variable (c(“Gent_1_score”, “Gent_2_score”) ) – remember lists are written as c(). So, run this command:\n\n\ndatlong &lt;- pivot_longer(dat, names_to = \"test\", values_to = \"vocab\", cols = c(\"Gent_1_score\", \"Gent_2_score\")) \n\n\nHave a look at the new object datlong that results: View(datlong). This function pivot_longer has taken as input the data in dat, it has created a new variable called “test” which reports whether it is the Gent_1_score or the Gent_2_score that is the measurement, and a new variable called “vocab” where the actual scores are listed. Then, the following list of variables let’s the function pivot_longer know which variables from the object dat we are converting (or lengthening). It also includes all the other variables, but unconverted. How many rows of data are there now corresponding to data from subject_ID number 1?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n2\n\n\n\n\nLet’s tidy things up so we only have subject_ID, and the Gent vocabulary scores by using select:\n\n\ndatlongsummary &lt;- select(datlong, subject_ID, test, vocab)\n\n\n\nTask Seven: Long to wide format conversion\n\nNow, we will have a go at converting from long to wide format. Let’s start with the datlongsummary object. We will convert this so that Gent_1_score and Gent_2_score are listed alongside each other – one row per person. The command for this is the reverse of pivot_longer, called pivot_wider. Run this command:\n\n\ndatwide &lt;- pivot_wider(datlongsummary, names_from = \"test\", values_from = \"vocab\")\n\nThis command takes the data from datlongsummary and puts the different measures reported in the variable test into different columns again, filling in the values from the variable vocab.\n\n\n\n4.1.3.6 Part Six: Extra practice finding and interpreting data sets\nThis part involves you exploring online psychology articles and finding an article which has made its data set available. More and more journals require data to be available when articles are published, an influential journal that has supported open science practices is Psychological Science. Go to the webpage of Psychological Science: https://journals.sagepub.com/home/pss. Browse the journal (you can view it on campus, off campus might be more difficult). Articles which have a dataset available have a “badge” indicating open data.\nIf data is made available, it is usually present as a “Supplemental Material” or via a link in the article (often data is stored on sites such as Open Science Framework (osf.io) or Figshare). For example, for this article:\nat the end of the article there is a note on how to access the data:\nYour task is to: download an article, download its data, and be able to describe what is in the data – i.e., what the rows and the columns in the data refer to. You can do this individually or as a group. At the next practical, you will talk one of the demonstrators through your description of the data.\nYou are free to explore the journal in its entirety to find an article that interests you, but some data sets can be really complicated to interpret. Some that you might begin with are as follows:\nvon Hippel, W., Ronay, R., Baker, E., Kjelsaas, K., & Murphy, S. C. (2016). Quick thinkers are smooth talkers: Mental speed facilitates charisma. Psychological Science, 27(1), 119-122. https://journals.sagepub.com/doi/full/10.1177/0956797615616255\ndata is available on osf.io (open science foundation) have a look in the Archive of OSF Storage for this article, under “Clean Data”, “Study 1 data long format.csv”. See if you can figure that one out.\n\n\n\n\n\n\nThere are two things to be aware of with data.\n\n\n\n\nFirst, real life data, just like real life, is usually not complete – it contains missing values. The usual way to indicate missing values is “NA” in R. But if the data set contains missing values indicated in a different way, then you have to specify this when you input the data. So, if missing values are indicated by “999”, you have to do it like this:\n\nread_csv(\"file.csv\", na.strings=\"999\")\n\nwhere file.csv is the file you are inputting. If missing values are indicated by “na”, you have to do this:\n\nread_csv(\"file.csv\", na.strings=\"na\")\n\nif missing values are “.”, then like this:\n\nread_csv(\"file.csv\", na.strings=\".\")\n\nThe second thing you might need to know is how to input files other than comma separated files. For excel files, you need to do this: library(readxl), then you can use the command read_excel(\"file.xls\"), this should work for xls and xlsx files. For inputting data that is in text, but not separated by commas, have a look at the help for the read_csv() function: ?read_csv. Hint – you’ll be looking to change the sep option for the read_csv command.\n\n\n\n\n\n\n\n\n\nWhat happens if I can’t upload a file into the r server?\n\n\n\nSometimes, very annoyingly, a file is too large to upload into the r server.\nThis isn’t a problem if you have r studio installed on your own computer, but using the server it is rather annoying.\nIf the data is stored online somewhere, then if you can find the web address for the data file then you can upload it directly in the following way:\nHere is an example on osf:\nThis is from the study Wald, K. A., Abraham, M., Pike, B., & Galinsky, A. D. (2024). Gender Differences in Climbing up the Ladder: Why Experience Closes the Ambition Gender Gap. Psychological Science, in press. and the data is here on osf\nNavigate to the actual data file in osf, in this case we go to OSF storage, then Study 1, then data, then the file gender_study1_cleaned_data.csv.\nClick on the file, then on the next screen click on the three dots that are next to the file name, then right-click on Download and copy the link. This is the web address for the data file gender_study1_cleaned_data.csv\nThen, paste this into the read_csv command, and load it directly into an object in r studio, i.e.:\ndat &lt;- read_csv('https://osf.io/download/zstxk/?view_only=e7257f621e7c4b50b4032efd667d0db1')\nImportantly, you need to use single quotation marks ’, either side of the web address.\n\n\n\nSee if you can find and load the data from any (or all) of these studies. Then have a go at exploring the data - e.g., can you first of all recreate the numbers in the paper (e.g., age and number of participants, mean and sd of the main dependent variable?)\n\n\n\n\n2.4.2 Data\nHere are links to all the data referred to in this practical:\n\nPSYC411-shipley-scores-anonymous-17_18.csv\nahicesd.csv\nparticipantinfo.csv\n\n\n\n2.4.3 Answers\nThe answers to the workbook now appear below each question in the workbook, above, so you can check your answers.\nIt’s really important for your learning that you have a go first of all at the workbook before looking at the answers.\n\n\n2.5 Extras\n\nOptionally, watch Brian Nosek’s keynote at the 2018 British Psychological Society conference on the replicability crisis (note hosted on youtube).",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 2. Manipulating data"
    ]
  },
  {
    "objectID": "PSYC411/part1/Week4.html",
    "href": "PSYC411/part1/Week4.html",
    "title": "Week 4. Testing nominal data",
    "section": "",
    "text": "Material for week 4 will be released after week 3 practical.\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 4. Testing nominal data"
    ]
  },
  {
    "objectID": "PSYC411/part1/Week4.html#overview",
    "href": "PSYC411/part1/Week4.html#overview",
    "title": "Week 4. Testing nominal data",
    "section": "",
    "text": "This week, there are three mini lectures, and the practical workbook working with R-studio.\nBefore the practical on Tuesday, please try to work through the practical workbook in your group.\nBring your questions (and/or answers) to the practical.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 4. Testing nominal data"
    ]
  },
  {
    "objectID": "PSYC411/part1/Week4.html#learning-goals",
    "href": "PSYC411/part1/Week4.html#learning-goals",
    "title": "Week 4. Testing nominal data",
    "section": "4.2 Learning Goals",
    "text": "4.2 Learning Goals\n\nUnderstand the value of conducting statistical tests and interpreting p-values\nUnderstand null effects and null hypotheses\nUnderstand the difference between parametric and non-parametric data\nUnderstand when to apply the Chi-squared test\nUnderstand the relation between Cramer’s V test and the Chi-squared test\nBe able to apply the Chi-squared test to data and interpret the result\nBe able to apply Cramer’s V test to data",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 4. Testing nominal data"
    ]
  },
  {
    "objectID": "PSYC411/part1/Week4.html#lectures-and-slides",
    "href": "PSYC411/part1/Week4.html#lectures-and-slides",
    "title": "Week 4. Testing nominal data",
    "section": "4.3 Lectures and slides",
    "text": "4.3 Lectures and slides\n\n4.3.1 Lectures\nWatch Lecture week 4 part 1:\n\nWatch Lecture week 4 part 2:\n\nWatch Lecture week 4 part 3:\n\nTake the quiz (not assessed) on the lecture materials.\n\n\n4.3.2 Slides\nDownload the lecture slides for:\n\npart 1\npart 2\npart 3",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 4. Testing nominal data"
    ]
  },
  {
    "objectID": "PSYC411/part1/Week4.html#practical-materials",
    "href": "PSYC411/part1/Week4.html#practical-materials",
    "title": "Week 4. Testing nominal data",
    "section": "4.4 Practical Materials",
    "text": "4.4 Practical Materials\n\n4.4.1 Workbook\nThe materials in this workbook share some material with Glasgow University Psychology Department Teaching in R website\nIn your group, work through this workbook, note any problems and questions you have, and come prepared to the online practical class to go through the tasks and ask your questions.\n\n4.4.1.1 Part 1: Revision for last week\n\nTask 1: Your data from the paper in Psychological Science\n\nYour take-home task was to produce some graphs of the data set downloaded from a paper in Psychological Science. Show your graphs and R script to the rest of your group.\n\n\n\n\n4.4.1.2 Part 2: Load in the Vocabulary Scores Data and Produce Graphs\n\nTask 2: Load in the Data\n\nRemember to clear out R first and load the tidyverse library:\n\n\nrm(list=ls())\nlibrary(tidyverse)\n\nThe data set on the Shipley and Gent vocabulary scores is now updated with the data from your group, so it now contains eight years of PSYC411 students’ data. I’ve omitted Age as this might impact anonymity of the data. Download the data from here: PSYC411-shipley-scores-anonymous-17_24.csv and read the data into an object in R studio called vdat (for vocabulary data).\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nvdat &lt;- read_csv(\"data/wk4/PSYC411-shipley-scores-anonymous-17_24.csv\")\n\nRows: 270 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): english_status, Gender\ndbl (6): subject_ID, Age, Shipley_Voc_Score, Gent_1_score, Gent_2_score, aca...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\nAs a reminder, when we want to look at a particular variable (a column) in an object in R studio, we refer to it using the $ notation. So, for the object vdat and the variable academic_year you would refer to it as vdat$academic_year. For this data set, we need to change academic year to be a nominal (factor) variable. Why are we setting academic year to be nominal and not interval/ratio?:\n\n\nvdat$academic_year &lt;- as.factor(vdat$academic_year)\n\nview(vdat) #view the data\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIt has to be nominal unless we have a hypothesis about the order of years of groups being important to the data. But that is very unlikely to have an effect (though you could look up the “Flynn Effect” in case you wanted to generate one…). But for now we want each year to be treated independently, rather than in order, in our analysis.\n\n\n\n\nMake sure the tidyverse library is loaded. Select all the variables apart from Age and save as a new object called “summaryvdat”. We will omit these variables because they are not complete for the dataset.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n# there are two ways to do this, either list the variables you are omitting with a `-c(variables,variables,...)` command:\nsummaryvdat &lt;- select(vdat, -c(Age))\n\n# or list the variables we do want to include:\nsummaryvdat &lt;- select(vdat, subject_ID, english_status, Gender, Shipley_Voc_Score, Gent_1_score, Gent_2_score, academic_year)\n\n\n\n\n\nArrange the data according to Gent_2_score, from highest to lowest. Save this as a new object called “summaryvdat_sort”\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nsummaryvdat_sort &lt;- arrange(summaryvdat, desc(Gent_2_score))\n\n\n\n\n\n\nTask 3: Draw Graphs of the Vocabulary Data\n\nDraw graphs of the following relations:\n\n\nEnglish status and academic year\nGender and academic year\nVocabulary score and academic year\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nggplot(summaryvdat, aes(x = academic_year, fill = english_status)) + \n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\nggplot(summaryvdat, aes(x = academic_year, fill = Gender)) + \n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\nggplot(summaryvdat, aes(x = academic_year, y = Gent_1_score)) + \n  geom_boxplot()\n\nWarning: Removed 5 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\nSave your script file.\n\n\n\n\n4.4.1.3 Part 3: Grouping data in R studio\n\nTask 4: Loading and joining data in R studio\n\nNow, let’s clear out R-studio before we get started again using rm(list=ls()).\nGo to the data files from week 2 and load them into Rstudio again (“ahicesd.csv”, and “participantinfo.csv”). If you need to download these again, you can get them here: ahicesd.csv, participantinfo.csv.\n\n\nRemember these data come from this study: Woodworth, R.J., O’Brien-Malone, A., Diamond, M.R. and Schuz, B. (2018). Data from, “Web-based Positive Psychology Interventions: A Reexamination of Effectiveness”. Journal of Open Psychology Data, 6(1).\nRemind yourself of the aim of the study and the variables that are in the data set (see end of this script file for repeat description on the study).\n\n\nNext, load and join the ahicesd.csv and participantinfo.csv data in R studio. Call the joined data set “all_dat” (see week 2 workbook for reminders about this)\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\ndat &lt;- read_csv(\"data/wk2/ahicesd.csv\")\n\nRows: 992 Columns: 50\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (50): id, occasion, elapsed.days, intervention, ahi01, ahi02, ahi03, ahi...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npinfo &lt;- read_csv(\"data/wk2/participantinfo.csv\")\n\nRows: 295 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (6): id, intervention, sex, age, educ, income\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nall_dat &lt;- inner_join(x = dat, y = pinfo, by = c(\"id\", \"intervention\"))\n\n\n\n\n\n\nTask 5: Selecting and manipulating data\n\nWe’re not interested in the individual questionnaire items. So, let’s select all the variables we want to keep (omitting the individual questionnaire items), and save this to an object called summary_all_dat (again see week 2 workbook for reminder)\nNext, we will add another variable to the data. We use the function mutate() for this. Let’s scale the ahiTotal and cesdTotal values and add them to the summary_all_dat set.\n\n\nWhat are the minimum and maximum values of the new variable ahiTotalscale?\nWhat do these scale values mean? (reminder: they are Z scores).\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nhint: use the arrange() function, or the min() and max() functions.\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nsummary_all_dat &lt;- select(all_dat, c(\"id\", \"occasion\", \"intervention\", \"ahiTotal\",\"cesdTotal\",\"sex\", \"age\", \"educ\", \"income\"))\nsummary_all_dat_scale &lt;- mutate(summary_all_dat, ahiTotalscale = scale(ahiTotal),\n                                cesdTotalscale = scale(cesdTotal))\nsummarise(summary_all_dat_scale, min(ahiTotalscale), max(ahiTotalscale))\n\n# A tibble: 1 × 2\n  `min(ahiTotalscale)` `max(ahiTotalscale)`\n                 &lt;dbl&gt;                &lt;dbl&gt;\n1                -2.87                 2.90\n\n\nThe values are z-scores so the numbers indicate how many standard deviations the values are from the mean\n\n\n\n\nThe next way we will work with the data is to organise the observations into different groups. We will use the function summarise(). So, instead of mean(summary_all_dat_scale$ahiTotal) you can use this, which turns out to be a much more powerful way of looking at the data:\n\n\nsummarise(summary_all_dat_scale, mean(ahiTotal))\n\n\nThey should give the same results - check that they do. This function summarise() is more powerful because you can look at several values at the same time, e.g.:\n\n\nsummarise(summary_all_dat_scale, mean(ahiTotal), sd(ahiTotal), mean(cesdTotal), sd(cesdTotal))\n\n\nWhat is the result of this command?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nsummarise(summary_all_dat_scale, mean(ahiTotal), sd(ahiTotal), \n          mean(cesdTotal), sd(cesdTotal))\n\n# A tibble: 1 × 4\n  `mean(ahiTotal)` `sd(ahiTotal)` `mean(cesdTotal)` `sd(cesdTotal)`\n             &lt;dbl&gt;          &lt;dbl&gt;             &lt;dbl&gt;           &lt;dbl&gt;\n1             72.8           14.2              13.1            11.7\n\n\n\n\n\n\nBut now let’s think about what kind of patterns we’d like to investigate in the data. There are four interventions conducted in this study. Let’s look at each of these interventions and their effect of ahiTotal and cesdTotal.\n\nWe can look at subgroups of data either by using the filter() function, or by using the function group_by(). The advantage of group_by() is that we can look at several groups at the same time, rather than dividing up the data file into pieces. Let’s organise by the different interventions.\n\nsummary_all_dat_scale_intervention &lt;- group_by(summary_all_dat_scale, intervention)\n\n\nThis command takes the data summary_all_dat_scale, and then groups it according to the four interventions in the data. We can’t yet see any difference in summary_all_dat_scale_intervention but it’s in there, lurking, just waiting.\n\nNow, we can look at the means for each intervention using the summarise function again. Run the summarise function on summarydata_scale_intervention. What happens?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nsummarise(summary_all_dat_scale, mean(ahiTotal), sd(ahiTotal), \n          mean(cesdTotal), sd(cesdTotal))\n\n# A tibble: 1 × 4\n  `mean(ahiTotal)` `sd(ahiTotal)` `mean(cesdTotal)` `sd(cesdTotal)`\n             &lt;dbl&gt;          &lt;dbl&gt;             &lt;dbl&gt;           &lt;dbl&gt;\n1             72.8           14.2              13.1            11.7\n\n\nIt separates out the means and SDs by the group_by condition - i.e, by intervention.\n\n\n\n\nYou can also group by several factors at the same time. We can group by intervention and get means and standard deviations, but that is not going to give us a huge amount of insight into how the interventions affect the happiness measure because we are combining the mean of ahiTotal across all occasions of testing, including testing before the intervention has been applied.\n\nSo, let’s group by intervention and occasion of testing:\n\nsummary_all_dat_intocc &lt;- group_by(summary_all_dat_scale, intervention, occasion)\n\n\nNow produce the means and standard deviations of the happiness score (ahiTotal) for each intervention at each testing occasion.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nsummary_all_dat_intocc &lt;- group_by(summary_all_dat_scale, intervention, occasion)\nsummarise(summary_all_dat_intocc, mean(ahiTotal), sd(ahiTotal))\n\n`summarise()` has grouped output by 'intervention'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 24 × 4\n# Groups:   intervention [4]\n   intervention occasion `mean(ahiTotal)` `sd(ahiTotal)`\n          &lt;dbl&gt;    &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n 1            1        0             68.4           14.0\n 2            1        1             69.5           13.4\n 3            1        2             70.3           15.7\n 4            1        3             75.0           12.8\n 5            1        4             76.5           16.2\n 6            1        5             75.5           14.5\n 7            2        0             68.8           13.0\n 8            2        1             71.6           12.5\n 9            2        2             73.0           14.0\n10            2        3             72.5           14.2\n# ℹ 14 more rows\n\n\n\n\n\n\nThis doesn’t print all the lines out, so you can make a new object (e.g., called sum_output) and view that, or you can filter out some of the lines so we only look at the first and second occasion of testing.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nsum_output &lt;- summarise(summary_all_dat_intocc, mean(ahiTotal), sd(ahiTotal))\nView(sum_output)\n\n\n\n\n\n\n\n4.4.1.4 Part 4: Graphing groups\n\nTask 6: Graph some groups\n\nDraw a scatter plot of ahiTotal and cesdTotal values for the whole data set.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse the ggplot() function with geom_point()\nAnd make it a bit more beautiful using the labs() addition.\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nggplot(summary_all_dat, aes(x = ahiTotal, y = cesdTotal)) + \n  geom_point() + \n  labs(title = \"Happiness and Depression Scores\", \n       x = \"Happiness Score\", \n       y = \"Depression Score\")\n\n\n\n\n\n\n\n\n\n\n\n\nNow redraw the plot, but colour the points according to whether they are first, second, third, etc occasion of testing. Add in col = \"occasion\" into the aes() part of the geom_point function, so that this part looks like this: aes(x = ahiTotal, y = cesdTotal, col = occasion)\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nggplot(summary_all_dat, aes(x = ahiTotal, y = cesdTotal, col = occasion)) + \n  geom_point() + \n  labs(title = \"Happiness and Depression Scores\", \n       x = \"Happiness Score\", \n       y = \"Depression Score\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.4.1.5 Part 5: Working out whether nominal data is random or structured: Repeating the analyses from Lecture week4 part3\n\nTask 7: Chi-squared and Cramer’s V\n\nLet’s now have a look at running Chi-squared and Cramer’s V tests in R. Download the titanic data.\n\nRead the titanic.csv into an object called “titanic”.\nView the data. It should correspond to the data in the overhead slides.\n\nMake a bar graph to count the numbers of survived and died by class.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\ntitanic &lt;- read_csv(\"data/wk4/titanic.csv\")\n\nNew names:\nRows: 1309 Columns: 3\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(2): class, survival dbl (1): ...1\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\nggplot(titanic, aes(x = class, fill = survival)) + \n  geom_bar( position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\n\n\nNow let’s see if there is a significant relation between class and survival using Chi-squared:\n\n\nchisq.test(x = titanic$class, y = titanic$survival)\n\n\nThe results give the chi-squared value, the number of degrees of freedom, and the p-value. P = 2.2e-16 means p = .0000000000000022. That’s highly significant. That means the observations are divided across the categories in a way that is very unlikely to be due to chance (for this number (P = 2.2e-16), it means there’s a 2 in a quadrillion chance that titanic survival was not related to class).\n\nIn a report, you would write:\nChi-squared(2, N= 1309) = 127.86, p &lt; .001.\n\nTo understand where the significant effect comes from, we need to look at where in our table of counts there is a big discrepancy between the expected frequency and the actual frequency. We can do this by analysing the “standardised residuals” of the chi-squared test.\n\nRepeat the chi-squared test on the titanic data set, and save the result of the test into a new object called “titanic_chisq_result”.\nThen, look at the standardised residuals that are saved in the test results - the standardised residuals are saved in a variable called stdres.\n\n\n\n\n\n\nHint\n\n\n\n\n\nHere is how you do it\n\ntitanic_chisq_result &lt;- chisq.test(x = titanic$class, y = titanic$survival)\ntitanic_chisq_result$stdres\n\n             titanic$survival\ntitanic$class       died   survived\n       first  -10.110480  10.110480\n       second  -1.837589   1.837589\n       third   10.254442 -10.254442\n\n\n\n\n\nNegative values indicate that actual counts are lower than expected, positive values indicate that actual counts are higher than expected. The standardised residuals indicate that there are fewer first class and more third class than expected that died, second class died at a level close to that expected from the overall numbers of deaths.\n\nNow, let’s compute Cramer’s V. First, we need to make sure we have the library lsr loaded in.\n\n\nlibrary(lsr)\n\n\nThen run the test:\n\n\ncramersV(x = titanic$class, y = titanic$survival)\n\n\nYour next task is to run some Chi-squared and Cramer’s V tests on some of the other nominal data. Open the data “PSYC411-shipley-scores-anonymous-17_24.csv” again. Investigate the association between gender and year (are there different distributions of males and females in each of our masters’ year cohorts) using Chi-squared and Cramer’s V. Is it significant?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nlibrary(lsr)\nvdat &lt;- read_csv(\"data/wk4/PSYC411-shipley-scores-anonymous-17_24.csv\"); \n\nRows: 270 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): english_status, Gender\ndbl (6): subject_ID, Age, Shipley_Voc_Score, Gent_1_score, Gent_2_score, aca...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nvdat$academic_year &lt;- as.factor(vdat$academic_year)\n\nchisq.test( x = vdat$Gender, vdat$academic_year)\n\nWarning in chisq.test(x = vdat$Gender, vdat$academic_year): Chi-squared\napproximation may be incorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  vdat$Gender and vdat$academic_year\nX-squared = 11.79, df = 14, p-value = 0.6232\n\ncramersV( x = vdat$Gender, vdat$academic_year)\n\nWarning in stats::chisq.test(...): Chi-squared approximation may be incorrect\n\n\n[1] 0.1488686\n\n\nit isn’t significant, X-squared = 11.79, df = 14, p-value = .623, but there is &lt; 5 in the “Other” gender category which gives us the error message “Chi-squared approximation may be incorrect”. if we focus just on Male/Female data, if, for instance, we are interested for the purposes of our investigation only in these larger categories:\n\nvdatmf &lt;- filter(vdat, Gender == \"Male\" | Gender == \"Female\")\nchisq.test( x = vdatmf$Gender, vdatmf$academic_year)\n\n\n    Pearson's Chi-squared test\n\ndata:  vdatmf$Gender and vdatmf$academic_year\nX-squared = 5.9864, df = 7, p-value = 0.5413\n\ncramersV( x = vdatmf$Gender, vdatmf$academic_year)\n\n[1] 0.1505847\n\n\nIt still isn’t significant, X-squared = 5.99, df = 7, p-value = .541\n\n\n\n\nWhat about the association between english_status and Gender?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nchisq.test( x = vdat$Gender, vdat$english_status)\n\nWarning in chisq.test(x = vdat$Gender, vdat$english_status): Chi-squared\napproximation may be incorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  vdat$Gender and vdat$english_status\nX-squared = 2.086, df = 2, p-value = 0.3524\n\ncramersV( x = vdat$Gender, vdat$english_status)\n\nWarning in stats::chisq.test(...): Chi-squared approximation may be incorrect\n\n\n[1] 0.08940014\n\n\nnot significant, X-squared = 2.09, df = 2, p-value = .352.\nAnd just for Male/Female:\n\nchisq.test( x = vdatmf$Gender, vdatmf$english_status)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  vdatmf$Gender and vdatmf$english_status\nX-squared = 0.62108, df = 1, p-value = 0.4306\n\ncramersV( x = vdatmf$Gender, vdatmf$english_status)\n\n[1] 0.04896934\n\n\nStill not significant, X-squared = 0.62, df = 1, p-value = .431\n\n\n\n\nWhat about the association between english_status and academic year?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nchisq.test( x = vdat$english_status, vdat$academic_year)\n\n\n    Pearson's Chi-squared test\n\ndata:  vdat$english_status and vdat$academic_year\nX-squared = 4.3238, df = 7, p-value = 0.7418\n\ncramersV( x = vdat$english_status, vdat$academic_year)\n\n[1] 0.1284637\n\n\nNot significant, X-squared = 4.32, df = 7, p-value = .742\n\n\n\n\n\n\n4.4.1.6 Part 6: More practice using Chi-squared and Cramer’s V test\n\nTask 8: More Chi-squared and Cramer’s V tests\n\nLook at the “ahicesd.csv” and “participantinfo.csv” data sets from week 2 again. Which nominal measures could you look at an association between? Report the Chi-squared test and Cramer’s V results for these associations. Are these associations significant? How do you interpret the significant associations?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFor example, you could have a go at occasion and intervention. This would tell you whether the participants are unevenly distributed across the occasions and interventions:\n\na &lt;- group_by(all_dat, occasion, intervention)\nchisq.test( x = all_dat$occasion, all_dat$intervention)\n\n\n    Pearson's Chi-squared test\n\ndata:  all_dat$occasion and all_dat$intervention\nX-squared = 9.7806, df = 15, p-value = 0.8333\n\ncramersV( x = all_dat$occasion, all_dat$intervention)\n\n[1] 0.05732779\n\n\nchi2(15, N = 992) = 9.78, p = .833\nCramer’s V = .06\nNot significant, X-squared = 2.73, df = 6, p-value = .842\n\n\n\n\n\n\n4.4.1.7 Part 7: Extra practise downloading and analysing data\n\nHere is another dataset for you to investigate:\n\nPapoutsi, C., Zimianiti, E., Bosker, H. R., & Frost, R. L. (2024). Statistical learning at a virtual cocktail party. Psychonomic Bulletin & Review, 31(2), 849-861. https://doi.org/10.3758/s13423-023-02384-1\nThe data are available on OSF, but also a cleaned version of the dataset is available here. If this doesn’t work when you try to upload into psy-rstudio.lancaster.ac.uk, then you can always get the data from the osf site using this command:\n\ndat &lt;- read_csv('https://osf.io/download/ky4u6/')\n\nThere is also something in the osf site called a R-markdown file - Data_analysis_script.rmd\nThis is a special kind of R-script, a “R-markdown” file, which also stores the results alongside the commands.\nYou should be able to scroll through it and see some of the R studio commands that might be familiar.\nFor more information on R-markdown, you can see here: https://r4ds.had.co.nz/r-markdown.html\n\nOur challenge to you is to make a Figure that looks a bit like their Figure 2. e.g., construct a boxplot of some of these data (though the Figure they use is called a pirate plot). If you’re keen to learn, there is more information on pirate plots here: pirateplots\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThis is the dataset you need\n\npapitsou &lt;- read_csv(\"data/wk4/papatsou_cleaneddata.csv\")\n\nRows: 10236 Columns: 24\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (13): participant, test, test_speaker, word1, word2, test_type, testpair...\ndbl (11): trialNo, tablerow, soundrow, correct_answer, rt, status, trialaccu...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\nHave a further browse of Psychological Science for data sets that you can download and begin to explore. Practise applying the data manipulation and graphing functions to these data sets. Or here is another one you might find interesting:\n\nWoodworth, R. J., O’Brien‐Malone, A., Diamond, M. R., & Schüz, B. (2017). Web‐based positive psychology interventions: A reexamination of effectiveness. Journal of Clinical Psychology, 73(3), 218-232. Data available here\nDescription of their study:\nIn our study we attempted a partial replication of the study of Seligman, Steen, Park, and Peterson (2005) which had suggested that the web-based delivery of positive psychology exercises could, as the consequence of containing specific, powerful therapeutic ingredients, effect greater increases in happiness and greater reductions in depression than could a placebo control. Participants (n=295) were randomly allocated to one of four intervention groups referred to, in accordance with the terminology in Seligman et al. (2005) as 1: Using Signature Strengths; 2: Three Good Things; 3: Gratitude Visit; 4: Early Memories (placebo control). At the commencement of the study, participants provided basic demographic information (age, sex, education, income) in addition to completing a pretest on the Authentic Happiness Inventory (AHI) and the Center for Epidemiologic Studies-Depression (CES-D) scale. Participants were asked to complete intervention-related activities during the week following the pretest. Futher measurements were then made on the AHI and CESD immediately after the intervention period (‘posttest’) and then 1 month after the posttest (day 38), 3 months after the posttest (day 98), and 6 months after the posttest (day 189). Participants were not able to to complete a follow-up questionnaire prior to the time that it was due but might have completed at either at the time that it was due, or later. We recorded the date and time at which follow-up questionnaires were completed.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 4. Testing nominal data"
    ]
  },
  {
    "objectID": "PSYC411/part1/Week4.html#data",
    "href": "PSYC411/part1/Week4.html#data",
    "title": "Week 4. Testing nominal data",
    "section": "4.4.2 Data",
    "text": "4.4.2 Data\nData referred to in this workbook:\n\nPSYC411-shipley-scores-anonymous-17_24.csv\nahicesd.csv\nparticipantinfo.csv\ntitanic data\npapatsou study data",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 4. Testing nominal data"
    ]
  },
  {
    "objectID": "PSYC411/part1/Week4.html#answers",
    "href": "PSYC411/part1/Week4.html#answers",
    "title": "Week 4. Testing nominal data",
    "section": "4.4.3 Answers",
    "text": "4.4.3 Answers\nAnswers now appear in this page, above.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 4. Testing nominal data"
    ]
  },
  {
    "objectID": "PSYC411/part1/Week4.html#extras",
    "href": "PSYC411/part1/Week4.html#extras",
    "title": "Week 4. Testing nominal data",
    "section": "4.5 Extras",
    "text": "4.5 Extras\nSee the guides to reporting numbers and statistical tests in American Psychological Association format (the format that we use in Psychology for all reports).\nWatch the clip of the Titanic film if you like (not assessed!). Hosted on youtube.\nWatch the clip of the Titanic film with a cat if you like (also not assessed!). Hosted on youtube.\n\nRead what is a p-value, on wikipedia. On this topic, wikipedia is good at explaining this, and explaining misinterpretations of p-values as well.",
    "crumbs": [
      "Home",
      "PSYC411",
      "Week 4. Testing nominal data"
    ]
  }
]